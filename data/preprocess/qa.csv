question,answer
"What does this do, and why should one include the `if` statement?

```
if __name__ == ""__main__"":
    print(""Hello, World!"")

```

---

If you are trying to close a question where someone should be using this idiom and isn't, consider closing as a duplicate of [Why is Python running my module when I import it, and how do I stop it?](https://stackoverflow.com/questions/6523791) instead. For questions where someone simply hasn't called any functions, or incorrectly expects a function named `main` to be used as an entry point automatically, use [Why doesn't the main() function run when I start a Python script? Where does the script start running?](https://stackoverflow.com/questions/17257631).","Short Answer
============

It's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:

* If you import the guardless script in another script (e.g. `import my_script_without_a_name_eq_main_guard`), then the latter script will trigger the former to run *at import time* and *using the second script's command line arguments*. This is almost always a mistake.
* If you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.

Long Answer
===========

To better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.

Whenever the Python interpreter reads a source file, it does two things:

* it sets a few special variables like `__name__`, and then
* it executes all of the code found in the file.

Let's see how this works and how it relates to your question about the `__name__` checks we always see in Python scripts.

Code Sample
-----------

Let's use a slightly different code sample to explore how imports and scripts work. Suppose the following is in a file called `foo.py`.

```
# Suppose this is foo.py.

print(""before import"")
import math

print(""before function_a"")
def function_a():
    print(""Function A"")

print(""before function_b"")
def function_b():
    print(""Function B {}"".format(math.sqrt(100)))

print(""before __name__ guard"")
if __name__ == '__main__':
    function_a()
    function_b()
print(""after __name__ guard"")

```

Special Variables
-----------------

When the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the `__name__` variable.

**When Your Module Is the Main Program**

If you are running your module (the source file) as the main program, e.g.

```
python foo.py

```

the interpreter will assign the hard-coded string `""__main__""` to the `__name__` variable, i.e.

```
# It's as if the interpreter inserts this at the top
# of your module when run as the main program.
__name__ = ""__main__"" 

```

**When Your Module Is Imported By Another**

On the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:

```
# Suppose this is in some other main program.
import foo

```

The interpreter will search for your `foo.py` file (along with searching for a few other variants), and prior to executing that module, it will assign the name `""foo""` from the import statement to the `__name__` variable, i.e.

```
# It's as if the interpreter inserts this at the top
# of your module when it's imported from another module.
__name__ = ""foo""

```

Executing the Module's Code
---------------------------

After the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.

**Always**

1. It prints the string `""before import""` (without quotes).
2. It loads the `math` module and assigns it to a variable called `math`. This is equivalent to replacing `import math` with the following (note that `__import__` is a low-level function in Python that takes a string and triggers the actual import):

```
# Find and load a module given its string name, ""math"",
# then assign it to a local variable called math.
math = __import__(""math"")

```

3. It prints the string `""before function_a""`.
4. It executes the `def` block, creating a function object, then assigning that function object to a variable called `function_a`.
5. It prints the string `""before function_b""`.
6. It executes the second `def` block, creating another function object, then assigning it to a variable called `function_b`.
7. It prints the string `""before __name__ guard""`.

**Only When Your Module Is the Main Program**

8. If your module is the main program, then it will see that `__name__` was indeed set to `""__main__""` and it calls the two functions, printing the strings `""Function A""` and `""Function B 10.0""`.

**Only When Your Module Is Imported by Another**

8. (**instead**) If your module is not the main program but was imported by another one, then `__name__` will be `""foo""`, not `""__main__""`, and it'll skip the body of the `if` statement.

**Always**

9. It will print the string `""after __name__ guard""` in both situations.

***Summary***

In summary, here's what'd be printed in the two cases:

```
# What gets printed if foo is the main program
before import
before function_a
before function_b
before __name__ guard
Function A
Function B 10.0
after __name__ guard

```

```
# What gets printed if foo is imported as a regular module
before import
before function_a
before function_b
before __name__ guard
after __name__ guard

```

Why Does It Work This Way?
--------------------------

You might naturally wonder why anybody would want this. Well, sometimes you want to write a `.py` file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself. Examples:

* Your module is a library, but you want to have a script mode where it runs some unit tests or a demo.
* Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing `.py` files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.
* Your module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.

Beyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. ""Running"" the script is a side effect of importing the script's module.

Food for Thought
----------------

* Question: Can I have multiple `__name__` checking blocks? Answer: it's strange to do so, but the language won't stop you.
* Suppose the following is in `foo2.py`. What happens if you say `python foo2.py` on the command-line? Why?

```
# Suppose this is foo2.py.
import os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters

def function_a():
    print(""a1"")
    from foo2 import function_b
    print(""a2"")
    function_b()
    print(""a3"")

def function_b():
    print(""b"")

print(""t1"")
if __name__ == ""__main__"":
    print(""m1"")
    function_a()
    print(""m2"")
print(""t2"")
      

```

* Now, figure out what will happen in `foo3.py` (having removed the `__name__` check):

```
# Suppose this is foo3.py.
import os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters

def function_a():
    print(""a1"")
    from foo3 import function_b
    print(""a2"")
    function_b()
    print(""a3"")

def function_b():
    print(""b"")

print(""t1"")
print(""m1"")
function_a()
print(""m2"")
print(""t2"")

```

* What will this do when used as a script? When imported as a module?

```
# Suppose this is in foo4.py
__name__ = ""__main__""

def bar():
    print(""bar"")
    
print(""before __name__ guard"")
if __name__ == ""__main__"":
    bar()
print(""after __name__ guard"")

```"
What is the difference between a method [decorated](https://peps.python.org/pep-0318/) with [`@staticmethod`](http://docs.python.org/library/functions.html#staticmethod) and one decorated with [`@classmethod`](http://docs.python.org/library/functions.html#classmethod)?,"Maybe a bit of example code will help: Notice the difference in the call signatures of `foo`, `class_foo` and `static_foo`:

```
class A(object):
    def foo(self, x):
        print(f""executing foo({self}, {x})"")

    @classmethod
    def class_foo(cls, x):
        print(f""executing class_foo({cls}, {x})"")

    @staticmethod
    def static_foo(x):
        print(f""executing static_foo({x})"")

a = A()

```

Below is the usual way an object instance calls a method. The object instance, `a`, is implicitly passed as the first argument.

```
a.foo(1)
# executing foo(<__main__.A object at 0xb7dbef0c>, 1)

```

---

**With classmethods**, the class of the object instance is implicitly passed as the first argument instead of `self`.

```
a.class_foo(1)
# executing class_foo(<class '__main__.A'>, 1)

```

You can also call `class_foo` using the class. In fact, if you define something to be
a classmethod, it is probably because you intend to call it from the class rather than from a class instance. `A.foo(1)` would have raised a TypeError, but `A.class_foo(1)` works just fine:

```
A.class_foo(1)
# executing class_foo(<class '__main__.A'>, 1)

```

One use people have found for class methods is to create [inheritable alternative constructors](https://stackoverflow.com/a/1950927/190597).

---

**With staticmethods**, neither `self` (the object instance) nor `cls` (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:

```
a.static_foo(1)
# executing static_foo(1)

A.static_foo('hi')
# executing static_foo(hi)

```

Staticmethods are used to group functions which have some logical connection with a class to the class.

---

`foo` is just a function, but when you call `a.foo` you don't just get the function,
you get a ""partially applied"" version of the function with the object instance `a` bound as the first argument to the function. `foo` expects 2 arguments, while `a.foo` only expects 1 argument.

`a` is bound to `foo`. That is what is meant by the term ""bound"" below:

```
print(a.foo)
# <bound method A.foo of <__main__.A object at 0xb7d52f0c>>

```

With `a.class_foo`, `a` is not bound to `class_foo`, rather the class `A` is bound to `class_foo`.

```
print(a.class_foo)
# <bound method type.class_foo of <class '__main__.A'>>

```

Here, with a staticmethod, even though it is a method, `a.static_foo` just returns
a good 'ole function with no arguments bound. `static_foo` expects 1 argument, and
`a.static_foo` expects 1 argument too.

```
print(a.static_foo)
# <function static_foo at 0xb7d479cc>

```

And of course the same thing happens when you call `static_foo` with the class `A` instead.

```
print(A.static_foo)
# <function static_foo at 0xb7d479cc>

```"
"What is the best way to extend a dictionary with another one while avoiding the use of a `for` loop? For instance:

```
>>> a = { ""a"" : 1, ""b"" : 2 }
>>> b = { ""c"" : 3, ""d"" : 4 }
>>> a
{'a': 1, 'b': 2}
>>> b
{'c': 3, 'd': 4}

```

Result:

```
{ ""a"" : 1, ""b"" : 2, ""c"" : 3, ""d"" : 4 }

```

Something like:

```
a.extend(b)  # This does not work

```","```
a.update(b)

```

[Latest Python Standard Library Documentation](http://docs.python.org/library/stdtypes.html#dict.update)"
"Here is my code:

```

import imaplib
from email.parser import HeaderParser

conn = imaplib.IMAP4_SSL('imap.gmail.com')
conn.login('example@gmail.com', 'password')
conn.select()
conn.search(None, 'ALL')
data = conn.fetch('1', '(BODY[HEADER])')
header_data = data[1][0][1].decode('utf-8')

```

At this point I get the error message:

> AttributeError: 'str' object has no attribute 'decode'

Python 3 doesn't have str.decode() anymore, so how can I fix this?","You are trying to decode an object that is *already decoded*. You have a `str`, there is no need to decode from UTF-8 anymore.

Simply drop the `.decode('utf-8')` part:

```
header_data = data[1][0][1]

```"
"I have a dynamic DataFrame which works fine, but when there are no data to be added into the DataFrame I get an error. And therefore I need a solution to create an empty DataFrame with only the column names.

For now I have something like this:

```
df = pd.DataFrame(columns=COLUMN_NAMES) # Note that there is no row data inserted.

```

PS: It is important that the column names would still appear in a DataFrame.

But when I use it like this I get something like that as a result:

```
Index([], dtype='object')
Empty DataFrame

```

The ""Empty DataFrame"" part is good! But instead of the Index thing I need to still display the columns.

An important thing that I found out: I am converting this DataFrame to a PDF using Jinja2, so therefore I'm calling out a method to first output it to HTML like that:

```
df.to_html()

```

This is where the columns get lost I think.

In general, I followed this example: <http://pbpython.com/pdf-reports.html>. The css is also from the link. That's what I do to send the dataframe to the PDF:

```
env = Environment(loader=FileSystemLoader('.'))
template = env.get_template(""pdf_report_template.html"")
template_vars = {""my_dataframe"": df.to_html()}

html_out = template.render(template_vars)
HTML(string=html_out).write_pdf(""my_pdf.pdf"", stylesheets=[""pdf_report_style.css""])

```","You can create an empty DataFrame with either column names or an Index:

```
In [4]: import pandas as pd
In [5]: df = pd.DataFrame(columns=['A','B','C','D','E','F','G'])
In [6]: df
Out[6]:
Empty DataFrame
Columns: [A, B, C, D, E, F, G]
Index: []

```

Or

```
In [7]: df = pd.DataFrame(index=range(1,10))
In [8]: df
Out[8]:
Empty DataFrame
Columns: []
Index: [1, 2, 3, 4, 5, 6, 7, 8, 9]

```

Edit:
Even after your amendment with the .to\_html, I can't reproduce. This:

```
df = pd.DataFrame(columns=['A','B','C','D','E','F','G'])
df.to_html('test.html')

```

Produces:

```
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
      <th>E</th>
      <th>F</th>
      <th>G</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>

```"
"I am using PyCharm to work on a project. The project is opened and configured with an interpreter, and can run successfully. The remote interpreter paths are mapped properly. This seems to be the correct configuration, but PyCharm is highlighting my valid code with ""unresolved reference"" errors, even for built-in Python functions. Why don't these seem to be detected, even though the code runs? Is there any way to get PyCharm to recognize these correctly?

---

*This specific instance of the problem is with a remote interpreter, but the problem appears on local interpreters as well.*",File | Invalidate Caches... and restarting PyCharm helps.
"I'm having a problem with deleting empty directories. Here is my code:

```
for dirpath, dirnames, filenames in os.walk(dir_to_search):
    # other codes

    try:
        os.rmdir(dirpath)
    except OSError as ex:
        print(ex)

```

The argument `dir_to_search` is where I'm passing the directory where the work needs to be done. That directory looks like this:

```
test/20/...
test/22/...
test/25/...
test/26/...

```

Note that all the above folders are empty. When I run this script the folders `20`,`25` alone gets deleted! But the folders `25` and `26` aren't deleted, even though they are empty folders.

### Edit:

The exception that I'm getting are:

```
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/29'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/29/tmp'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/28'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/28/tmp'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/26'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/25'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/27'
[Errno 39] Directory not empty: '/home/python-user/shell-scripts/s3logs/test/2012/10/27/tmp'

```

Where am I making a mistake?","Try [`shutil.rmtree`](https://docs.python.org/library/shutil.html#shutil.rmtree) to delete files and directories:

```
import shutil
shutil.rmtree('/path/to/your/dir/')

```"
"I would like to get the time spent on the cell execution in addition to the original output from cell.

To this end, I tried `%%timeit -r1 -n1` but it doesn't expose the variable defined within cell.

`%%time` works for cell which only contains 1 statement.

```
In[1]: %%time
       1
CPU times: user 4 µs, sys: 0 ns, total: 4 µs
Wall time: 5.96 µs
Out[1]: 1

In[2]: %%time
       # Notice there is no out result in this case.
       x = 1
       x
CPU times: user 3 µs, sys: 0 ns, total: 3 µs
Wall time: 5.96 µs

```

What's the best way to do it?

### Update

I have been using [Execute Time in Nbextension](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/execute_time) for quite some time now. It is great.

### Update 2021-03

As of now, [this](https://stackoverflow.com/a/66663966/213525) is the correct answer. Essentially, `%%time` and `%%timeit` both now work as one would expect.","The only way I found to overcome this problem is by executing the last statement with print.

[Do not forget that](http://ipython.org/ipython-doc/dev/interactive/tutorial.html#magic-functions) cell magic starts with `%%` and line magic starts with `%`.

```
%%time
clf = tree.DecisionTreeRegressor().fit(X_train, y_train)
res = clf.predict(X_test)
print(res)

```

Notice that any changes performed inside the cell are taken into consideration in the next cells:[![example](https://i.sstatic.net/Ub4a3.png)]"
"From my understanding, Python has a separate namespace for functions, so if I want to use a global variable in a function, I should probably use `global`.

However, I was able to access a global variable even without `global`:

```
>>> sub = ['0', '0', '0', '0']
>>> def getJoin():
...     return '.'.join(sub)
...
>>> getJoin()
'0.0.0.0'

```

Why does this work?

---

See also [UnboundLocalError on local variable when reassigned after first use](https://stackoverflow.com/questions/370357) for the error that occurs when attempting to *assign to* the global variable without `global`. See [Using global variables in a function](https://stackoverflow.com/questions/423379/) for the general question of how to use globals.","The keyword `global` is only useful to change or create global variables in a local context, although creating global variables is seldom considered a good solution.

```
def bob():
    me = ""locally defined""    # Defined only in local context
    print(me)

bob()
print(me)     # Asking for a global variable

```

The above will give you:

```
locally defined
Traceback (most recent call last):
  File ""file.py"", line 9, in <module>
    print(me)
NameError: name 'me' is not defined

```

While if you use the `global` statement, the variable will become available ""outside"" the scope of the function, effectively becoming a global variable.

```
def bob():
    global me
    me = ""locally defined""   # Defined locally but declared as global
    print(me)

bob()
print(me)     # Asking for a global variable

```

So the above code will give you:

```
locally defined
locally defined

```

In addition, due to the nature of python, you could also use `global` to declare functions, classes or other objects in a local context. Although I would advise against it since it causes nightmares if something goes wrong or needs debugging."
"Using Python 3's function annotations, it is possible to specify the type of items contained within a homogeneous list (or other collection) for the purpose of type hinting in PyCharm and other IDEs?

A pseudo-python code example for a list of int:

```
def my_func(l:list<int>):
    pass

```

I know it's possible using Docstring...

```
def my_func(l):
    """"""
    :type l: list[int]
    """"""
    pass

```

... but I prefer the annotation style if it's possible.","As of May 2015, [PEP 484 (Type Hints)](https://www.python.org/dev/peps/pep-0484/) has been formally accepted. The draft implementation is also available at [github under ambv/typehinting](https://github.com/ambv/typehinting).

In September 2015, Python 3.5 was released with support for Type Hints and includes a [new *typing* module](https://docs.python.org/3/library/typing.html). This allows for the specification of types contained within collections. As of November 2015, JetBrains PyCharm 5.0 fully supports Python 3.5 to include Type Hints as illustrated below.

[![PyCharm 5.0 Code Completion using Type Hints](https://i.sstatic.net/KHn4f.jpg)](https://i.sstatic.net/KHn4f.jpg)

```
from typing import List

def do_something(l: List[str]):
    for s in l:
        s  # str

```

**Original Answer**

As of Aug 2014, I have confirmed that it is not possible to use Python 3 type annotations to specify types within collections (ex: a list of strings).

The use of formatted docstrings such as reStructuredText or Sphinx are viable alternatives and supported by various IDEs.

It also appears that Guido is mulling over the idea of extending type annotations in the spirit of mypy: <http://mail.python.org/pipermail/python-ideas/2014-August/028618.html>"
"Let's assume we have such a trivial daemon written in python:

```
def mainloop():
    while True:
        # 1. do
        # 2. some
        # 3. important
        # 4. job
        # 5. sleep

mainloop()

```

and we daemonize it using `start-stop-daemon` which by default sends `SIGTERM` (`TERM`) signal on `--stop`.

Let's suppose the current step performed is `#2`. And at this very moment we're sending `TERM` signal.

What happens is that the execution terminates immediately.

I've found that I can handle the signal event using `signal.signal(signal.SIGTERM, handler)` but the thing is that it still interrupts the current execution and passes the control to `handler`.

So, my question is - is it possible to not interrupt the current execution but handle the `TERM` signal in a separated thread (?) so that I was able to set `shutdown_flag = True` so that `mainloop()` had a chance to stop gracefully?","A class based clean to use solution:

```
import signal
import time

class GracefulKiller:
  kill_now = False
  def __init__(self):
    signal.signal(signal.SIGINT, self.exit_gracefully)
    signal.signal(signal.SIGTERM, self.exit_gracefully)

  def exit_gracefully(self, signum, frame):
    self.kill_now = True

if __name__ == '__main__':
  killer = GracefulKiller()
  while not killer.kill_now:
    time.sleep(1)
    print(""doing something in a loop ..."")
   
  print(""End of the program. I was killed gracefully :)"")

```"
"One of the most talked-about features in Python 3.5 is **type hints**.

An example of **type hints** is mentioned in [this article](http://lwn.net/Articles/650904/) and [this one](http://lwn.net/Articles/640359/) while also mentioning to use type hints responsibly. Can someone explain more about them and when they should be used and when not?","I would suggest reading [PEP 483](https://www.python.org/dev/peps/pep-0483/) and [PEP 484](https://www.python.org/dev/peps/pep-0484/) and watching [this presentation](https://www.youtube.com/watch?v=2wDvzy6Hgxg) by [Guido](https://en.wikipedia.org/wiki/Guido_van_Rossum) on type hinting.

**In a nutshell**: *Type hinting is literally what the words mean. You hint the type of the object(s) you're using*.

Due to the *dynamic* nature of Python, *inferring or checking the type* of an object being used is especially hard. This fact makes it hard for developers to understand what exactly is going on in code they haven't written and, most importantly, for type checking tools found in many IDEs ([PyCharm](https://en.wikipedia.org/wiki/PyCharm) and [PyDev](https://en.wikipedia.org/wiki/PyDev) come to mind) that are limited due to the fact that they don't have any indicator of what type the objects are. As a result they resort to trying to infer the type with (as mentioned in the presentation) around 50% success rate.

---

To take two important slides from the type hinting presentation:

### ***Why type hints?***

1. **Helps type checkers:** By hinting at what type you want the object to be the type checker can easily detect if, for instance, you're passing an object with a type that isn't expected.
2. **Helps with documentation:** A third person viewing your code will know what is expected where, ergo, how to use it without getting them `TypeErrors`.
3. **Helps IDEs develop more accurate and robust tools:** Development Environments will be better suited at suggesting appropriate methods when know what type your object is. You have probably experienced this with some IDE at some point, hitting the `.` and having methods/attributes pop up which aren't defined for an object.

### ***Why use static type checkers?***

* **Find bugs sooner**: This is self-evident, I believe.
* **The larger your project the more you need it**: Again, makes sense. Static languages offer a robustness and control that
  dynamic languages lack. The bigger and more complex your application becomes the more control and predictability (from
  a behavioral aspect) you require.
* **Large teams are already running static analysis**: I'm guessing this verifies the first two points.

**As a closing note for this small introduction**: This is an **optional** feature and, from what I understand, it has been introduced in order to reap some of the benefits of static typing.

You generally **do not** need to worry about it and **definitely** don't need to use it (especially in cases where you use Python as an auxiliary scripting language). It should be helpful when developing large projects as *it offers much needed robustness, control and additional debugging capabilities*.

---

**Type hinting with mypy**:
---------------------------

In order to make this answer more complete, I think a little demonstration would be suitable. I'll be using [`mypy`](http://mypy-lang.org/), the library which inspired Type Hints as they are presented in the PEP. This is mainly written for anybody bumping into this question and wondering where to begin.

Before I do that let me reiterate the following: [PEP 484](https://www.python.org/dev/peps/pep-0484/) doesn't enforce anything; it is simply setting a direction for function
annotations and proposing guidelines for **how** type checking can/should be performed. You can annotate your functions and
hint as many things as you want; your scripts will still run regardless of the presence of annotations because Python itself doesn't use them.

Anyways, as noted in the PEP, hinting types should generally take three forms:

* Function annotations ([PEP 3107](https://www.python.org/dev/peps/pep-3107/)).
* Stub files for built-in/user modules.
* Special  `# type: type` comments that complement the first two forms. (See: **[What are variable annotations?](https://stackoverflow.com/questions/39971929/what-are-variable-annotations-in-python-3-6/39973133#39973133)** for a Python 3.6 update for `# type: type` comments)

Additionally, you'll want to use type hints in conjunction with the new [`typing`](https://docs.python.org/3/library/typing.html) module introduced in `Py3.5`. In it, many (additional) [ABCs](https://en.wikipedia.org/wiki/Class_(computer_programming)#Abstract_and_Concrete) (abstract base classes) are defined along with helper functions and decorators for use in static checking. Most ABCs in `collections.abc` are included, but in a *generic* form in order to allow subscription (by defining a `__getitem__()` method).

For anyone interested in a more in-depth explanation of these, the [`mypy documentation`](http://mypy.readthedocs.org/en/latest/) is written very nicely and has a lot of code samples demonstrating/describing the functionality of their checker; it is definitely worth a read.

### Function annotations and special comments:

First, it's interesting to observe some of the behavior we can get when using special comments. Special `# type: type` comments
can be added during variable assignments to indicate the type of an object if one cannot be directly inferred. Simple assignments are
generally easily inferred but others, like lists (with regard to their contents), cannot.

**Note:** If we want to use any derivative of *containers* and need to specify the contents for that container we **must** use the ***generic*** types from the `typing` module. **These support indexing.**

```
# Generic List, supports indexing.
from typing import List

# In this case, the type is easily inferred as type: int.
i = 0

# Even though the type can be inferred as of type list
# there is no way to know the contents of this list.
# By using type: List[str] we indicate we want to use a list of strings.
a = []  # type: List[str]

# Appending an int to our list
# is statically not correct.
a.append(i)

# Appending a string is fine.
a.append(""i"")

print(a)  # [0, 'i']

```

If we add these commands to a file and execute them with our interpreter, everything works just fine and `print(a)` just prints
the contents of list `a`. The `# type` comments have been discarded, *treated as plain comments which have no additional semantic meaning*.

By running this with `mypy`, on the other hand, we get the following response:

```
(Python3)jimmi@jim: mypy typeHintsCode.py
typesInline.py:14: error: Argument 1 to ""append"" of ""list"" has incompatible type ""int""; expected ""str""

```

Indicating that a list of `str` objects cannot contain an `int`, which, statically speaking, is sound. This can be fixed by either abiding to the type of `a` and only appending `str` objects or by changing the type of the contents of `a` to indicate that any value is acceptable (Intuitively performed with `List[Any]` after `Any` has been imported from `typing`).

Function annotations are added in the form `param_name : type` after each parameter in your function signature and a return type is specified using the `-> type` notation before the ending function colon; all annotations are stored in the `__annotations__` attribute for that function in a handy dictionary form. Using a trivial example (which doesn't require extra types from the `typing` module):

```
def annotated(x: int, y: str) -> bool:
    return x < y

```

The `annotated.__annotations__` attribute now has the following values:

```
{'y': <class 'str'>, 'return': <class 'bool'>, 'x': <class 'int'>}

```

If we're a complete newbie, or we are familiar with PythonÂ 2.7 concepts and are consequently unaware of the `TypeError` lurking in the comparison of `annotated`, we can perform another static check, catch the error and save us some trouble:

```
(Python3)jimmi@jim: mypy typeHintsCode.py
typeFunction.py: note: In function ""annotated"":
typeFunction.py:2: error: Unsupported operand types for > (""str"" and ""int"")

```

Among other things, calling the function with invalid arguments will also get caught:

```
annotated(20, 20)

# mypy complains:
typeHintsCode.py:4: error: Argument 2 to ""annotated"" has incompatible type ""int""; expected ""str""

```

These can be extended to basically any use case and the errors caught extend further than basic calls and operations. The types you
can check for are really flexible and I have merely given a small sneak peak of its potential. A look in the `typing` module, the
PEPs or the `mypy` documentation will give you a more comprehensive idea of the capabilities offered.

### Stub files:

Stub files can be used in two different non mutually exclusive cases:

* You need to type check a module for which you do not want to directly alter the function signatures
* You want to write modules and have type-checking but additionally want to separate annotations from content.

What stub files (with an extension of `.pyi`) are is an annotated interface of the module you are making/want to use. They contain
the signatures of the functions you want to type-check with the body of the functions discarded. To get a feel of this, given a set
of three random functions in a module named `randfunc.py`:

```
def message(s):
    print(s)

def alterContents(myIterable):
    return [i for i in myIterable if i % 2 == 0]

def combine(messageFunc, itFunc):
    messageFunc(""Printing the Iterable"")
    a = alterContents(range(1, 20))
    return set(a)

```

We can create a stub file `randfunc.pyi`, in which we can place some restrictions if we wish to do so. The downside is that
somebody viewing the source without the stub won't really get that annotation assistance when trying to understand what is supposed
to be passed where.

Anyway, the structure of a stub file is pretty simplistic: Add all function definitions with empty bodies (`pass` filled) and
supply the annotations based on your requirements. Here, let's assume we only want to work with `int` types for our Containers.

```
# Stub for randfucn.py
from typing import Iterable, List, Set, Callable

def message(s: str) -> None: pass

def alterContents(myIterable: Iterable[int])-> List[int]: pass

def combine(
    messageFunc: Callable[[str], Any],
    itFunc: Callable[[Iterable[int]], List[int]]
)-> Set[int]: pass

```

The `combine` function gives an indication of why you might want to use annotations in a different file, they some times clutter up
the code and reduce readability (big no-no for Python). You could of course use type aliases but that sometime confuses more than it
helps (so use them wisely).

---

This should get you familiarized with the basic concepts of type hints in Python. Even though the type checker used has been
`mypy` you should gradually start to see more of them pop-up, some internally in IDEs ([**PyCharm**](http://blog.jetbrains.com/pycharm/2015/11/python-3-5-type-hinting-in-pycharm-5/),) and others as standard Python modules.

I'll try and add additional checkers/related packages in the following list when and if I find them (or if suggested).

***Checkers I know of***:

* [**Mypy**](http://mypy-lang.org/): as described here.
* [**PyType**](https://github.com/google/pytype): By Google, uses different notation from what I gather, probably worth a look.

***Related Packages/Projects***:

* [**typeshed:**](https://github.com/python/typeshed/) Official Python repository housing an assortment of stub files for the standard library.

The `typeshed` project is actually one of the best places you can look to see how type hinting might be used in a project of your own. Let's take as an example [the `__init__` dunders of the `Counter` class](https://github.com/python/typeshed/blob/master/stdlib/3/collections.pyi#L78) in the corresponding `.pyi` file:

```
class Counter(Dict[_T, int], Generic[_T]):
        @overload
        def __init__(self) -> None: ...
        @overload
        def __init__(self, Mapping: Mapping[_T, int]) -> None: ...
        @overload
        def __init__(self, iterable: Iterable[_T]) -> None: ...

```

[Where `_T = TypeVar('_T')` is used to define generic classes](http://mypy.readthedocs.org/en/latest/generics.html#defining-generic-classes). For the `Counter` class we can see that it can either take no arguments in its initializer, get a single `Mapping` from any type to an `int` *or* take an `Iterable` of any type.

---

**Notice**: One thing I forgot to mention was that the `typing` module has been introduced on a *provisional basis*. From **[PEP 411](https://www.python.org/dev/peps/pep-0411/)**:

> A provisional package may have its API modified prior to ""graduating"" into a ""stable"" state. On one hand, this state provides the package with the benefits of being formally part of the Python distribution. On the other hand, the core development team explicitly states that no promises are made with regards to the the stability of the package's API, which may change for the next release. While it is considered an unlikely outcome, such packages may even be removed from the standard library without a deprecation period if the concerns regarding their API or maintenance prove well-founded.

So take things here with a pinch of salt; I'm doubtful it will be removed or altered in significant ways, but one can never know.

---

\*\* Another topic altogether, but valid in the scope of type-hints: [`PEP 526`: Syntax for Variable Annotations](https://docs.python.org/3.6/whatsnew/3.6.html#pep-526-syntax-for-variable-annotations) is an effort to replace `# type` comments by introducing new syntax which allows users to annotate the type of variables in simple `varname: type` statements.

See *[What are variable annotations?](https://stackoverflow.com/questions/39971929/what-are-variable-annotations-in-python-3-6/39973133#39973133)*, as previously mentioned, for a small introduction to these."
"I've been hearing the buzz about virtualenv lately, and I'm interested. But all I've heard is a smattering of praise, and don't have a clear understanding of what it is or how to use it.

I'm looking for (ideally) a follow-along tutorial that can take me from Windows or Linux with no Python on it, and explain every step of (in no particular order):

* what I should do to be able to start using `virtualenv`
* specific reasons why using `virtualenv` is a good idea
* situations where I can/can't use `virtualenv`
* situations where I should/shouldn't use `virtualenv`

And step through (comprehensively) a couple sample situations of the should+can variety.

So what are some good tutorials to cover this stuff? Or if you have the time and interest, perhaps you can answer a few of those questions here. Either in your answer, or as a link to tutorials that answer it, these are the things I'd like to know.","This is very good: <http://simononsoftware.com/virtualenv-tutorial-part-2/>

And this is a slightly more practical one: <https://web.archive.org/web/20160404222648/https://iamzed.com/2009/05/07/a-primer-on-virtualenv/>"
"Suppose the following:

```
>>> s = set([1, 2, 3])

```

How do I get a value (any value) out of `s` without doing `s.pop()`? I want to leave the item in the set until I am sure I can remove it - something I can only be sure of after an asynchronous call to another host.

Quick and dirty:

```
>>> elem = s.pop()
>>> s.add(elem)

```

But do you know of a better way? Ideally in constant time.","Two options that don't require copying the whole set:

```
for e in s:
    break
# e is now an element from s

```

Or...

```
e = next(iter(s))

```

But in general, sets don't support indexing or slicing."
"Given a 1D array of indices:

```
a = array([1, 0, 3])

```

I want to one-hot encode this as a 2D array:

```
b = array([[0,1,0,0], [1,0,0,0], [0,0,0,1]])

```","Create a zeroed array `b` with enough columns, i.e. `a.max() + 1`.  
Then, for each row `i`, set the `a[i]`th column to `1`.

```
>>> a = np.array([1, 0, 3])
>>> b = np.zeros((a.size, a.max() + 1))
>>> b[np.arange(a.size), a] = 1

>>> b
array([[ 0.,  1.,  0.,  0.],
       [ 1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.]])

```"
"I have a Pandas DataFrame with a 'date' column. Now I need to filter out all rows in the DataFrame that have dates outside of the next two months. Essentially, I only need to retain the rows that are within the next two months.

What is the best way to achieve this?","If **date column is the index**, then use .loc for label based indexing or .iloc for positional indexing.

For example:

```
df.loc['2014-01-01':'2014-02-01']

```

See details here <http://pandas.pydata.org/pandas-docs/stable/dsintro.html#indexing-selection>

If the column is **not the index** you have two choices:

1. Make it the index (either temporarily or permanently if it's time-series data)
2. `df[(df['date'] > '2013-01-01') & (df['date'] < '2013-02-01')]`

See [here](http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing) for the general explanation

*Note: .ix is deprecated.*"
How can I implement a general tree in Python? Is there a built-in data structure for this?,"I recommend [anytree](https://pypi.python.org/pypi/anytree) (I am the author).

Example:

```
from anytree import Node, RenderTree

udo = Node(""Udo"")
marc = Node(""Marc"", parent=udo)
lian = Node(""Lian"", parent=marc)
dan = Node(""Dan"", parent=udo)
jet = Node(""Jet"", parent=dan)
jan = Node(""Jan"", parent=dan)
joe = Node(""Joe"", parent=dan)

print(udo)
Node('/Udo')
print(joe)
Node('/Udo/Dan/Joe')

for pre, fill, node in RenderTree(udo):
    print(""%s%s"" % (pre, node.name))
Udo
├── Marc
│   └── Lian
└── Dan
    ├── Jet
    ├── Jan
    └── Joe

print(dan.children)
(Node('/Udo/Dan/Jet'), Node('/Udo/Dan/Jan'), Node('/Udo/Dan/Joe'))

```

[anytree](http://anytree.readthedocs.io/en/latest/) has also a powerful API with:

* simple tree creation
* simple tree modification
* pre-order tree iteration
* post-order tree iteration
* resolve relative and absolute node paths
* walking from one node to an other.
* tree rendering (see example above)
* node attach/detach hookups"
"I want to send a value for `""User-agent""` while requesting a webpage using Python Requests. I am not sure is if it is okay to send this as a part of the header, as in the code below:

```
debug = {'verbose': sys.stderr}
user_agent = {'User-agent': 'Mozilla/5.0'}
response  = requests.get(url, headers = user_agent, config=debug)

```

The debug information isn't showing the headers being sent during the request.

Is it acceptable to send this information in the header? If not, how can I send it?","The `user-agent` should be specified as a field in the header.

Here is a [list of HTTP header fields](https://en.wikipedia.org/wiki/List_of_HTTP_header_fields), and you'd probably be interested in [request-specific fields](https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Request_fields), which includes `User-Agent`.

### If you're using requests v2.13 and newer

The simplest way to do what you want is to create a dictionary and specify your headers directly, like so:

```
import requests

url = 'SOME URL'

headers = {
    'User-Agent': 'My User Agent 1.0',
    'From': 'youremail@domain.example'  # This is another valid field
}

response = requests.get(url, headers=headers)

```

### If you're using requests v2.12.x and older

Older versions of `requests` clobbered default headers, so you'd want to do the following to preserve default headers and then add your own to them.

```
import requests

url = 'SOME URL'

# Get a copy of the default headers that requests would use
headers = requests.utils.default_headers()

# Update the headers with your custom ones
# You don't have to worry about case-sensitivity with
# the dictionary keys, because default_headers uses a custom
# CaseInsensitiveDict implementation within requests' source code.
headers.update(
    {
        'User-Agent': 'My User Agent 1.0',
    }
)

response = requests.get(url, headers=headers)

```"
"I have the following dictionary in python:

```
d = {'1': 'one', '3': 'three', '2': 'two', '5': 'five', '4': 'four'}

```

I need a way to find if a value such as ""one"" or ""two"" exists in this dictionary.

For example, if I wanted to know if the index ""1"" existed I would simply have to type:

```
""1"" in d

```

And then python would tell me if that is true or false, however I need to do that same exact thing except to find if a value exists.","```
>>> d = {'1': 'one', '3': 'three', '2': 'two', '5': 'five', '4': 'four'}
>>> 'one' in d.values()
True

```

Out of curiosity, some comparative timing:

```
>>> T(lambda : 'one' in d.itervalues()).repeat()
[0.28107285499572754, 0.29107213020324707, 0.27941107749938965]
>>> T(lambda : 'one' in d.values()).repeat()
[0.38303399085998535, 0.37257885932922363, 0.37096405029296875]
>>> T(lambda : 'one' in d.viewvalues()).repeat()
[0.32004380226135254, 0.31716084480285645, 0.3171098232269287]
>>> T(lambda : 'four' in d.itervalues()).repeat()
[0.41178202629089355, 0.3959040641784668, 0.3970959186553955]
>>> T(lambda : 'four' in d.values()).repeat()
[0.4631338119506836, 0.43541407585144043, 0.4359898567199707]
>>> T(lambda : 'four' in d.viewvalues()).repeat()
[0.43414998054504395, 0.4213531017303467, 0.41684913635253906]

```

The reason is that each of the above returns a different type of object, which may or may not be well suited for lookup operations:

```
>>> type(d.viewvalues())
<type 'dict_values'>
>>> type(d.values())
<type 'list'>
>>> type(d.itervalues())
<type 'dictionary-valueiterator'>

```"
"I have a directory `apkmirror-scraper-compose` with the following structure:

```
.
├── docker-compose.yml
├── privoxy
│   ├── config
│   └── Dockerfile
├── scraper
│   ├── Dockerfile
│   ├── newnym.py
│   └── requirements.txt
└── tor
    └── Dockerfile

```

I'm trying to run the following `docker-compose.yml`:

```
version: '3'

services:
  privoxy:
    build: ./privoxy
    ports:
      - ""8118:8118""
    links:
      - tor

  tor:
    build:
      context: ./tor
      args:
        password: """"
    ports:
      - ""9050:9050""
      - ""9051:9051""

  scraper:
    build: ./scraper
    links:
      - tor
      - privoxy

```

where the `Dockerfile` for `tor` is

```
FROM alpine:latest
EXPOSE 9050 9051
ARG password
RUN apk --update add tor
RUN echo ""ControlPort 9051"" >> /etc/tor/torrc
RUN echo ""HashedControlPassword $(tor --quiet --hash-password $password)"" >> /etc/tor/torrc
CMD [""tor""]

```

that for `privoxy` is

```
FROM alpine:latest
EXPOSE 8118
RUN apk --update add privoxy
COPY config /etc/privoxy/config
CMD [""privoxy"", ""--no-daemon""]

```

where `config` consists of the two lines

```
listen-address 0.0.0.0:8118
forward-socks5 / tor:9050 .

```

and the `Dockerfile` for `scraper` is

```
FROM python:2.7-alpine
ADD . /scraper
WORKDIR /scraper
RUN pip install -r requirements.txt
CMD [""python"", ""newnym.py""]

```

where `requirements.txt` contains the single line `requests`. Finally, the program `newnym.py` is designed to simply test whether changing the IP address using Tor is working:

```
from time import sleep, time

import requests as req
import telnetlib


def get_ip():
    IPECHO_ENDPOINT = 'http://ipecho.net/plain'
    HTTP_PROXY = 'http://privoxy:8118'
    return req.get(IPECHO_ENDPOINT, proxies={'http': HTTP_PROXY}).text


def request_ip_change():
    tn = telnetlib.Telnet('tor', 9051)
    tn.read_until(""Escape character is '^]'."", 2)
    tn.write('AUTHENTICATE """"\r\n')
    tn.read_until(""250 OK"", 2)
    tn.write(""signal NEWNYM\r\n"")
    tn.read_until(""250 OK"", 2)
    tn.write(""quit\r\n"")
    tn.close()


if __name__ == '__main__':
    dts = []
    try:
        while True:
            ip = get_ip()
            t0 = time()
            request_ip_change()
            while True:
                new_ip = get_ip()
                if new_ip == ip:
                    sleep(1)
                else:
                    break
            dt = time() - t0
            dts.append(dt)
            print(""{} -> {} in ~{}s"".format(ip, new_ip, int(dt)))
    except KeyboardInterrupt:
        print(""Stopping..."")
        print(""Average: {}"".format(sum(dts) / len(dts)))

```

The `docker-compose build` builds successfully, but if I try `docker-compose up`, I get the following error message:

```
Creating network ""apkmirrorscrapercompose_default"" with the default driver
ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network

```

I tried searching for help on this error message, but couldn't find any. What is causing this error?","I've seen it suggested docker may be at its maximum of created networks. The command `docker network prune` can be used to remove all networks not used by at least one container.

My issue ended up being, as [Robert](https://stackoverflow.com/users/1094246/robert) commented about: an issue with openvpn `service openvpn stop` 'solved' the problem."
"I just switched to PyCharm and I am very happy about all the warnings and hints it provides me to improve my code. Except for this one which I don't understand:

> This inspection detects shadowing names defined in outer scopes.

I know it is bad practice to access variable from the outer scope, but what is the problem with shadowing the outer scope?

Here is one example, where PyCharm gives me the warning message:

```
data = [4, 5, 6]

def print_data(data): # <-- Warning: ""Shadows 'data' from outer scope
    print data

print_data(data)

```","There isn't any big deal in your above snippet, but imagine a function with a few more arguments and quite a few more lines of code. Then you decide to rename your `data` argument as `yadda`, but miss one of the places it is used in the function's body... Now `data` refers to the global, and you start having weird behaviour - where you would have a much more obvious `NameError` if you didn't have a global name `data`.

Also remember that in Python everything is an object (including modules, classes and functions), so there's no distinct namespaces for functions, modules or classes. Another scenario is that you import function `foo` at the top of your module, and use it somewhere in your function body. Then you add a new argument to your function and named it - bad luck - `foo`.

Finally, built-in functions and types also live in the same namespace and can be shadowed the same way.

None of this is much of a problem if you have short functions, good naming and a decent unit test coverage, but well, sometimes you have to maintain less than perfect code and being warned about such possible issues might help."
"I am trying to sort some values by attribute, like so:

```
a = sorted(a, lambda x: x.modified, reverse=True)

```

I get this error message:

```
<lambda>() takes exactly 1 argument (2 given)

```

Why? How do I fix it?

---

This question was originally written for Python 2.x. In 3.x, the error message will be different: `TypeError: sorted expected 1 argument, got 2`.","Use

```
a = sorted(a, key=lambda x: x.modified, reverse=True)
#             ^^^^

```

On Python 2.x, the `sorted` function takes its arguments in this order:

```
sorted(iterable, cmp=None, key=None, reverse=False)

```

so without the `key=`, the function you pass in will be considered a `cmp` function which takes 2 arguments."
I get this pep8 warning whenever I use lambda expressions. Are lambda expressions not recommended? If not why?,"The recommendation in [PEP-8](http://www.python.org/dev/peps/pep-0008/#programming-recommendations) you are running into is:

> Always use a def statement instead of an assignment statement that
> binds a lambda expression directly to a name.
>
> Yes:
>
> ```
> def f(x): return 2*x 
>
> ```
>
> No:
>
> ```
> f = lambda x: 2*x 
>
> ```
>
> The first form means that the name of the resulting
> function object is specifically 'f' instead of the generic '<lambda>'.
> This is more useful for tracebacks and string representations in
> general. The use of the assignment statement eliminates the sole
> benefit a lambda expression can offer over an explicit def statement
> (i.e. that it can be embedded inside a larger expression)

Assigning lambdas to names basically just duplicates the functionality of `def` - and in general, it's best to do something a single way to avoid confusion and increase clarity.

The legitimate use case for lambda is where you want to use a function without assigning it, e.g:

```
sorted(players, key=lambda player: player.rank)

```

In general, the main argument against doing this is that `def` statements will result in more lines of code. My main response to that would be: yes, and that is fine. Unless you are code golfing, minimising the number of lines isn't something you should be doing: go for clear over short."
"```
import os

A = os.path.join(os.path.dirname(__file__), '..')

B = os.path.dirname(os.path.realpath(__file__))

C = os.path.abspath(os.path.dirname(__file__))

```

I usually just hard-wire these with the actual path. But there is a reason for these statements that determine path at runtime, and I would really like to understand the `os.path` module so that I can start using it.","When a module is loaded from a file in Python, [`__file__`](https://docs.python.org/3/reference/datamodel.html#module.__file__) is set to its [absolute](https://docs.python.org/3/whatsnew/3.9.html#other-language-changes) path. You can then use that with other functions to find the directory that the file is located in.

Taking your examples one at a time:

```
A = os.path.join(os.path.dirname(__file__), '..')
# A is the parent directory of the directory where program resides.

B = os.path.dirname(os.path.realpath(__file__))
# B is the canonicalised (?) directory where the program resides.

C = os.path.abspath(os.path.dirname(__file__))
# C is the absolute path of the directory where the program resides.

```

You can see the various values returned from these here:

```
import os
print(__file__)
print(os.path.join(os.path.dirname(__file__), '..'))
print(os.path.dirname(os.path.realpath(__file__)))
print(os.path.abspath(os.path.dirname(__file__)))

```

and make sure you run it from different locations (such as `./text.py`, `~/python/text.py` and so forth) to see what difference that makes."
"Python's `easy_install` makes installing new packages extremely convenient. However, as far as I can tell, it doesn't implement the other common features of a dependency manager - listing and removing installed packages.

What is the best way of finding out what's installed, and what is the preferred way of removing installed packages? Are there any files that need to be updated if I remove packages manually (e.g. by `rm /usr/local/lib/python2.6/dist-packages/my_installed_pkg.egg` or similar)?","[pip](http://pypi.python.org/pypi/pip/), an alternative to setuptools/easy\_install, provides an ""uninstall"" command.

Install pip according to the [installation instructions](http://pip.readthedocs.org/en/stable/installing/):

```
$ wget https://bootstrap.pypa.io/get-pip.py
$ python get-pip.py

```

Then you can use `pip uninstall` to remove packages installed with `easy_install`"
"I got an error with the following exception message:

```
UnicodeEncodeError: 'ascii' codec can't encode character u'\ufeff' in
position 155: ordinal not in range(128)

```

Not sure what `u'\ufeff'` is, it shows up when I'm web scraping. How can I remedy the situation? The `.replace()` string method doesn't work on it.","I ran into this on Python 3 and found this question (and [solution](https://stackoverflow.com/a/17912811/704616)).
When opening a file, Python 3 supports the encoding keyword to automatically handle the encoding.

Without it, the BOM is included in the read result:

```
>>> f = open('file', mode='r')
>>> f.read()
'\ufefftest'

```

Giving the correct encoding, the BOM is omitted in the result:

```
>>> f = open('file', mode='r', encoding='utf-8-sig')
>>> f.read()
'test'

```

Just my 2 cents."
"In Python, the `(?P<group_name>â€¦)` [syntax](http://docs.python.org/library/re.html#regular-expression-syntax) allows one to refer to the matched string through its name:

```
>>> import re
>>> match = re.search('(?P<name>.*) (?P<phone>.*)', 'John 123456')
>>> match.group('name')
'John'

```

What does ""P"" stand for? I could not find any hint in the [official documentation](http://docs.python.org/library/re.html#regular-expression-syntax).

I would love to get ideas about how to help my students remember this syntax. Knowing what ""P"" does stand for (or might stand for) would be useful.","Since we're all guessing, I might as well give mine: I've always thought it stood for Python. That may sound pretty stupid -- what, P for Python?! -- but in my defense, I vaguely remembered [this thread](http://markmail.org/message/oyezhwvefvotacc3) [emphasis mine]:

> Subject: Claiming (?P...) regex syntax extensions
>
> From: Guido van Rossum (gui...@CNRI.Reston.Va.US)
>
> Date: Dec 10, 1997 3:36:19 pm
>
> I have an unusual request for the Perl developers (those that develop
> the Perl language). I hope this (perl5-porters) is the right list. I
> am cc'ing the Python string-sig because it is the origin of most of
> the work I'm discussing here.
>
> You are probably aware of Python. I am Python's creator; I am
> planning to release a next ""major"" version, Python 1.5, by the end of
> this year. I hope that Python and Perl can co-exist in years to come;
> cross-pollination can be good for both languages. (I believe Larry
> had a good look at Python when he added objects to Perl 5; O'Reilly
> publishes books about both languages.)
>
> As you may know, Python 1.5 adds a new regular expression module that
> more closely matches Perl's syntax. We've tried to be as close to the
> Perl syntax as possible within Python's syntax. However, the regex
> syntax has some Python-specific extensions, which all begin with (?P .
> Currently there are two of them:
>
> `(?P<foo>...)` Similar to regular grouping parentheses, but the text  
> matched by the group is accessible after the match has been performed,
> via the symbolic group name ""foo"".
>
> `(?P=foo)` Matches the same string as that matched by the group named
> ""foo"". Equivalent to \1, \2, etc. except that the group is referred  
> to by name, not number.
>
> I hope that this Python-specific extension won't conflict with any
> future Perl extensions to the Perl regex syntax. If you have plans to
> use (?P, please let us know as soon as possible so we can resolve the
> conflict. **Otherwise, it would be nice if the (?P syntax could be
> permanently reserved for Python-specific syntax extensions.** (Is
> there some kind of registry of extensions?)

to which Larry Wall replied:

> [...] There's no registry as of now--yours is the first request from
> outside perl5-porters, so it's a pretty low-bandwidth activity.
> (Sorry it was even lower last week--I was off in New York at Internet
> World.)
>
> Anyway, as far as I'm concerned, you may certainly have 'P' with my
> blessing. (Obviously Perl doesn't need the 'P' at this point. :-) [...]

So I don't know what the original choice of P was motivated by -- pattern? placeholder? penguins? -- but you can understand why I've always associated it with Python. Which considering that (1) I don't like regular expressions and avoid them wherever possible, and (2) this thread happened fifteen years ago, is kind of odd."
"What is the best way to clear out all the `__pycache__` folders and `.pyc`/`.pyo` files from a Python project?

I have seen multiple users suggest the `pyclean` script bundled with Debian, but this does not remove the folders. I want a simple way to clean up the project before pushing the files to my DVS.","You can do it manually with the next command:

```
find . | grep -E ""(/__pycache__$|\.pyc$|\.pyo$)"" | xargs rm -rf

```

This will remove all `.pyc` and `.pyo` files as well as `__pycache__` directories recursively starting from the current directory."
"I am trying to `import Tkinter`. However, I get an error stating that `Tkinter` has not been installed:

```
ImportError: No module named _tkinter, please install the python-tk package

```

I could probably install it using synaptic manager (can I?), however, I would have to install it on every machine I program on. Would it be possible to add the Tkinter library into my workspace and reference it from there?","It is not very easy to install Tkinter locally to use with system-provided Python. You may build it from sources, but this is usually not the best idea with a binary package-based distro you're apparently running.

It's safer to `apt-get install python3-tk` on your machine(s).
(Works on Debian-derived distributions like for Ubuntu; refer to your package manager and package list on other distributions.)"
"I have a string which is like this:

```
this is ""a test""

```

I'm trying to write something in Python to split it up by space while ignoring spaces within quotes. The result I'm looking for is:

```
['this', 'is', 'a test']

```

PS. I know you are going to ask ""what happens if there are quotes within the quotes, well, in my application, that will never happen.","You want `split`, from the built-in [`shlex`](https://docs.python.org/library/shlex.html) module.

```
>>> import shlex
>>> shlex.split('this is ""a test""')
['this', 'is', 'a test']

```

This should do exactly what you want.

If you want to preserve the quotation marks, then you can pass the `posix=False` kwarg.

```
>>> shlex.split('this is ""a test""', posix=False)
['this', 'is', '""a test""']

```"
"Suppose you have three objects you acquire via context manager, for instance A lock, a db connection and an ip socket.
You can acquire them by:

```
with lock:
   with db_con:
       with socket:
            #do stuff

```

But is there a way to do it in one block? something like

```
with lock,db_con,socket:
   #do stuff

```

Furthermore, is it possible, given an array of unknown length of objects that have context managers, is it possible to somehow do:

```
a=[lock1, lock2, lock3, db_con1, socket, db_con2]
with a as res:
    #now all objects in array are acquired

```

If the answer is ""no"", is it because the need for such a feature implies bad design, or maybe I should suggest it in a pep? :-P","In **Python 2.7 and 3.1 and above**, you can write:

```
with A() as X, B() as Y, C() as Z:
    do_something()

```

This is normally the best method to use, but if you have an unknown-length list of context managers you'll need one of the below methods.

---

In **Python 3.3**, you can enter an unknown-length list of context managers by using [`contextlib.ExitStack`](http://docs.python.org/3/library/contextlib.html#contextlib.ExitStack):

```
with ExitStack() as stack:
    for mgr in ctx_managers:
        stack.enter_context(mgr)
    # ...

```

This allows you to create the context managers as you are adding them to the `ExitStack`, which prevents the possible problem with `contextlib.nested` (mentioned below).

[contextlib2](https://pypi.python.org/pypi/contextlib2) provides [a backport of `ExitStack`](https://contextlib2.readthedocs.io/en/stable/#contextlib2.ExitStack) for Python 2.6 and 2.7.

---

In **Python 2.6 and below**, you can use [`contextlib.nested`](https://docs.python.org/2/library/contextlib.html#contextlib.nested):

```
from contextlib import nested

with nested(A(), B(), C()) as (X, Y, Z):
    do_something()

```

is equivalent to:

```
m1, m2, m3 = A(), B(), C()
with m1 as X:
    with m2 as Y:
        with m3 as Z:
            do_something()

```

Note that this isn't exactly the same as normally using nested `with`, because `A()`, `B()`, and `C()` will all be called initially, before entering the context managers. This will not work correctly if one of these functions raises an exception.

`contextlib.nested` is deprecated in newer Python versions in favor of the above methods."
"What is the most efficient way to rotate a list in python?
Right now I have something like this:

```
>>> def rotate(l, n):
...     return l[n:] + l[:n]
... 
>>> l = [1,2,3,4]
>>> rotate(l,1)
[2, 3, 4, 1]
>>> rotate(l,2)
[3, 4, 1, 2]
>>> rotate(l,0)
[1, 2, 3, 4]
>>> rotate(l,-1)
[4, 1, 2, 3]

```

Is there a better way?","A [`collections.deque`](http://docs.python.org/library/collections.html#deque-objects) is optimized for pulling and pushing on both ends. They even have a dedicated `rotate()` method.

```
from collections import deque
items = deque([1, 2])
items.append(3)        # deque == [1, 2, 3]
items.rotate(1)        # The deque is now: [3, 1, 2]
items.rotate(-1)       # Returns deque to original state: [1, 2, 3]
item = items.popleft() # deque == [2, 3]

```"
"What are the differences between shell languages like [Bash](http://www.gnu.org/software/bash/bash.html) (`bash`), [Z shell](http://zsh.sourceforge.net/) (`zsh`), [Fish](http://fishshell.com/screenshots.html) (`fish`) and the scripting languages above that makes them more suitable for the shell?

When using the command line, the shell languages seem to be much easier. It feels for me much smoother to use bash for example than to use the shell profile in [IPython](https://en.wikipedia.org/wiki/IPython), [despite reports to the contrary](http://transneptune.net/2009/06/16/ipython-as-your-default-shell/). I think most will agree with me that a large portion of medium to large scale programming is easier in Python than in Bash. I use Python as the language I am most familiar with. The same goes for [Perl](https://en.wikipedia.org/wiki/Perl) and [Ruby](https://en.wikipedia.org/wiki/Ruby_%28programming_language%29).

I have tried to articulate the reason, but I am unable to, aside from assuming that the treatment of strings differently in both has something to do with it.

The reason of this question is that I am hoping to develop a language usable in both. If you know of such a language, please post it as well.

As [S.Lott](https://stackoverflow.com/users/10661/s-lott) explains, the question needs some clarification. I am asking about the features of the shell *language* versus that of scripting languages. So the comparison is not about the characteristics of various interactive ([REPL](http://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop)) environments such as history and command line substitution. An alternative expression for the question would be:

Can a programming language that is suitable for design of complex systems be at the same time able to express useful one-liners that can access the file system or control jobs? Can a programming language usefully scale up as well as scale down?","There are a couple of differences that I can think of; just thoughtstreaming here, in no particular order:

1. Python & Co. are designed to be good at scripting. Bash & Co. are designed to be *only* good at scripting, with absolutely no compromise. IOW: Python is designed to be good both at scripting and non-scripting, Bash cares only about scripting.
2. Bash & Co. are untyped, Python & Co. are strongly typed, which means that the number `123`, the string `123` and the file `123` are quite different. They are, however, not *statically* typed, which means they need to have different literals for those, in order to keep them apart.  
   Example:

   ```
                   | Ruby             | Bash    
   -----------------------------------------
   number          | 123              | 123
   string          | '123'            | 123
   regexp          | /123/            | 123
   file            | File.open('123') | 123
   file descriptor | IO.open('123')   | 123
   URI             | URI.parse('123') | 123
   command         | `123`            | 123

   ```
3. Python & Co. are designed to scale *up* to 10000, 100000, maybe even 1000000 line programs, Bash & Co. are designed to scale *down* to 10 *character* programs.
4. In Bash & Co., files, directories, file descriptors, processes are all first-class objects, in Python, only Python objects are first-class, if you want to manipulate files, directories etc., you have to wrap them in a Python object first.
5. Shell programming is basically dataflow programming. Nobody realizes that, not even the people who write shells, but it turns out that shells are quite good at that, and general-purpose languages not so much. In the general-purpose programming world, dataflow seems to be mostly viewed as a concurrency model, not so much as a programming paradigm.

I have the feeling that trying to address these points by bolting features or DSLs onto a general-purpose programming language doesn't work. At least, I have yet to see a convincing implementation of it. There is **RuSH** (Ruby shell), which tries to implement a shell in Ruby, there is [**rush**](http://rush.heroku.com/), which is an internal DSL for shell programming in Ruby, there is [**Hotwire**](http://code.google.com/p/hotwire-shell/), which is a Python shell, but IMO none of those come even close to competing with Bash, Zsh, fish and friends.

Actually, IMHO, the best current shell is [**Microsoft PowerShell**](http://msdn.microsoft.com/en-us/library/windows/desktop/dd835506%28v=vs.85%29.aspx), which is very surprising considering that for several *decades* now, Microsoft has continually had the *worst* shells *evar*. I mean, `COMMAND.COM`? Really? (Unfortunately, they still have a crappy terminal. It's still the ""command prompt"" that has been around since, what? Windows 3.0?)

PowerShell was basically created by ignoring everything Microsoft has ever done (`COMMAND.COM`, `CMD.EXE`, VBScript, JScript) and instead starting from the Unix shell, then removing all backwards-compatibility cruft (like backticks for command substitution) and massaging it a bit to make it more Windows-friendly (like using the now unused backtick as an escape character instead of the backslash which is the path component separator character in Windows). After that, is when the magic happens.

They address **problem 1 and 3** from above, by basically making the opposite choice compared to Python. Python cares about large programs first, scripting second. Bash cares only about scripting. PowerShell cares about scripting first, large programs second. A defining moment for me was watching a video of an interview with Jeffrey Snover (PowerShell's lead designer), when the interviewer asked him how big of a program one could write with PowerShell and Snover answered without missing a beat: ""80 characters."" At that moment I realized that this is *finally* a guy at Microsoft who ""gets"" shell programming (probably related to the fact that PowerShell was *neither* developed by Microsoft's programming language group (i.e. lambda-calculus math nerds) nor the OS group (kernel nerds) but rather the server group (i.e. sysadmins who actually *use* shells)), and that I should probably take a serious look at PowerShell.

**Number 2** is solved by having arguments be statically typed. So, you can write just `123` and PowerShell knows whether it is a string or a number or a file, because the cmdlet (which is what shell commands are called in PowerShell) declares the types of its arguments to the shell. This has pretty deep ramifications: unlike Unix, where each command is responsible for parsing its own arguments (the shell basically passes the arguments as an array of strings), argument parsing in PowerShell is done by the *shell*. The cmdlets specify all their options and flags and arguments, as well as their types and names and documentation(!) to the shell, which then can perform argument parsing, tab completion, IntelliSense, inline documentation popups etc. in one centralized place. (This is not revolutionary, and the PowerShell designers acknowledge shells like the DIGITAL Command Language (DCL) and the IBM OS/400 Command Language (CL) as prior art. For anyone who has ever used an AS/400, this should sound familiar. In OS/400, you can write a shell command and if you don't know the syntax of certain arguments, you can simply leave them out and hit `F4`, which will bring a menu (similar to an HTML form) with labelled fields, dropdown, help texts etc. This is only possible because the OS knows about all the possible arguments and their types.) In the Unix shell, this information is often duplicated three times: in the argument parsing code in the command itself, in the `bash-completion` script for tab-completion and in the manpage.

**Number 4** is solved by the fact that PowerShell operates on strongly typed objects, which includes stuff like files, processes, folders and so on.

**Number 5** is particularly interesting, because PowerShell is the only shell I know of, where the people who wrote it were actually *aware* of the fact that shells are essentially dataflow engines and deliberately implemented it as a dataflow engine.

Another nice thing about PowerShell are the naming conventions: all cmdlets are named `Action-Object` and moreover, there are also standardized names for specific actions and specific objects. (Again, this should sound familar to OS/400 users.) For example, everything which is related to receiving some information is called `Get-Foo`. And everything operating on (sub-)objects is called `Bar-ChildItem`. So, the equivalent to `ls` is `Get-ChildItem` (although PowerShell also provides builtin aliases `ls` and `dir` – in fact, whenever it makes sense, they provide both Unix and `CMD.EXE` aliases as well as abbreviations (`gci` in this case)).

But the **killer feature** IMO is the strongly typed object pipelines. While PowerShell is derived from the Unix shell, there is one very important distinction: in Unix, all communication (both via pipes and redirections as well as via command arguments) is done with untyped, unstructured strings. In PowerShell, it's all strongly typed, structured objects. This is so incredibly powerful that I seriously wonder why noone else has thought of it. (Well, they have, but they never became popular.) In my shell scripts, I estimate that up to one third of the commands is only there to act as an adapter between two other commands that don't agree on a common textual format. Many of those adapters go away in PowerShell, because the cmdlets exchange structured objects instead of unstructured text. And if you look *inside* the commands, then they pretty much consist of three stages: parse the textual input into an internal object representation, manipulate the objects, convert them back into text. Again, the first and third stage basically go away, because the data already comes in as objects.

However, the designers have taken great care to preserve the dynamicity and flexibility of shell scripting through what they call an *Adaptive Type System*.

Anyway, I don't want to turn this into a PowerShell commercial. There are plenty of things that are *not* so great about PowerShell, although most of those have to do either with Windows or with the specific implementation, and not so much with the concepts. (E.g. the fact that it is implemented in .NET means that the very first time you start up the shell can take up to several seconds if the .NET framework is not already in the filesystem cache due to some other application that needs it. Considering that you often use the shell for well under a second, that is completely unacceptable.)

The most important point I want to make is that if you want to look at existing work in scripting languages and shells, **you shouldn't stop at Unix and the Ruby/Python/Perl/PHP family**. For example, [Tcl](http://www.tcl.tk/) was already mentioned. [Rexx](http://en.wikipedia.org/wiki/REXX) would be another scripting language. [Emacs Lisp](http://www.gnu.org/software/emacs/emacs.html) would be yet another. And in the shell realm there are some of the already mentioned mainframe/midrange shells such as the OS/400 command line and DCL. Also, Plan9's rc."
"I've been trying to round long float numbers like:

```
32.268907563;
32.268907563;
31.2396694215;
33.6206896552;
...

```

With no success so far. I tried `math.ceil(x)`, `math.floor(x)` (although that would round up or down, which is not what I'm looking for) and `round(x)` which didn't work either (still float numbers).

What could I do?

**Code:**

```
for i in widthRange:
    for j in heightRange:
        r, g, b = rgb_im.getpixel((i, j))
        h, s, v = colorsys.rgb_to_hsv(r/255.0, g/255.0, b/255.0)
        h = h * 360
        int(round(h))
        print(h)

```","**TL;DR:**

```
round(x)

```

will round it and change it to integer.

You are not assigning `round(h)` to any variable. When you call `round(h)`, it returns the integer number but does nothing else; you have to change that line for:

```
h = round(h)

```

to assign the new value to `h`.

---

As @plowman said in the comments, Python's `round()` doesn't work as one would normally expect, and that's because the way the number is stored as a variable is usually not the way you see it on screen. There are [lots of answers](https://stackoverflow.com/questions/56820/round-in-python-doesnt-seem-to-be-rounding-properly#56833) that explain this behavior.

One way to avoid this problem is to use the Decimal as stated by [this answer](https://stackoverflow.com/a/15398691/4345659).

In order for this answer to work properly without using extra libraries it would be convenient to use a custom rounding function. I came up with the following solution, that as far as I tested avoided all the storing issues. It is based on using the string representation, obtained with `repr()` (NOT `str()`!). It looks hacky but it was the only way I found to solve all the cases. It works with both Python2 and Python3.

```
def proper_round(num, dec=0):
    num = str(num)[:str(num).index('.')+dec+2]
    if num[-1]>='5':
        return float(num[:-2-(not dec)]+str(int(num[-2-(not dec)])+1))
    return float(num[:-1])

```

Tests:

```
>>> print(proper_round(1.0005,3))
1.001
>>> print(proper_round(2.0005,3))
2.001
>>> print(proper_round(3.0005,3))
3.001
>>> print(proper_round(4.0005,3))
4.001
>>> print(proper_round(5.0005,3))
5.001
>>> print(proper_round(1.005,2))
1.01
>>> print(proper_round(2.005,2))
2.01
>>> print(proper_round(3.005,2))
3.01
>>> print(proper_round(4.005,2))
4.01
>>> print(proper_round(5.005,2))
5.01
>>> print(proper_round(1.05,1))
1.1
>>> print(proper_round(2.05,1))
2.1
>>> print(proper_round(3.05,1))
3.1
>>> print(proper_round(4.05,1))
4.1
>>> print(proper_round(5.05,1))
5.1
>>> print(proper_round(1.5))
2.0
>>> print(proper_round(2.5))
3.0
>>> print(proper_round(3.5))
4.0
>>> print(proper_round(4.5))
5.0
>>> print(proper_round(5.5))
6.0
>>> 
>>> print(proper_round(1.000499999999,3))
1.0
>>> print(proper_round(2.000499999999,3))
2.0
>>> print(proper_round(3.000499999999,3))
3.0
>>> print(proper_round(4.000499999999,3))
4.0
>>> print(proper_round(5.000499999999,3))
5.0
>>> print(proper_round(1.00499999999,2))
1.0
>>> print(proper_round(2.00499999999,2))
2.0
>>> print(proper_round(3.00499999999,2))
3.0
>>> print(proper_round(4.00499999999,2))
4.0
>>> print(proper_round(5.00499999999,2))
5.0
>>> print(proper_round(1.0499999999,1))
1.0
>>> print(proper_round(2.0499999999,1))
2.0
>>> print(proper_round(3.0499999999,1))
3.0
>>> print(proper_round(4.0499999999,1))
4.0
>>> print(proper_round(5.0499999999,1))
5.0
>>> print(proper_round(1.499999999))
1.0
>>> print(proper_round(2.499999999))
2.0
>>> print(proper_round(3.499999999))
3.0
>>> print(proper_round(4.499999999))
4.0
>>> print(proper_round(5.499999999))
5.0

```

Finally, the corrected answer would be:

```
# Having proper_round defined as previously stated
h = int(proper_round(h))

```

---

**Tests:**

```
>>> proper_round(6.39764125, 2)
6.31 # should be 6.4
>>> proper_round(6.9764125, 1)
6.1  # should be 7

```

The gotcha here is that the `dec`-th decimal can be 9 and if the `dec+1`-th digit >=5 the 9 will become a 0 and a 1 should be carried to the `dec-1`-th digit.

If we take this into consideration, we get:

```
def proper_round(num, dec=0):
    num = str(num)[:str(num).index('.')+dec+2]
    if num[-1]>='5':
      a = num[:-2-(not dec)]       # integer part
      b = int(num[-2-(not dec)])+1 # decimal part
      return float(a)+b**(-dec+1) if a and b == 10 else float(a+str(b))
    return float(num[:-1])

```

In the situation described above `b = 10` and the previous version would just concatenate `a` and `b` which would result in a concatenation of `10` where the trailing 0 would disappear. This version transforms `b` to the right decimal place based on `dec`, as a proper carry."
"Is there a way to programmatically prevent [Google Colab](https://colab.research.google.com/) from disconnecting on a timeout?

[![](https://i.sstatic.net/lkvoo.jpg)](https://i.sstatic.net/lkvoo.jpg)

The following describes the conditions causing a notebook to automatically disconnect:

> *Google Colab notebooks have an idle timeout of 90 minutes and absolute timeout of 12 hours. This means, if user does not interact with his Google Colab notebook for more than 90 minutes, its instance is automatically terminated. Also, maximum lifetime of a Colab instance is 12 hours.*

Naturally, we want to automatically squeeze the maximum out of the instance, [without having to manually interact with it constantly](https://i.sstatic.net/lkvoo.jpg). Here I will assume commonly seen system requirements:

* [UbuntuÂ 18.04](https://en.wikipedia.org/wiki/Ubuntu_version_history#Ubuntu_18.04_LTS_.28Bionic_Beaver.29) LTS (Bionic Beaver), Windows 10, or Mac operating systems
* In case of Linux-based systems, using popular [desktop environments](https://en.wikipedia.org/wiki/Desktop_environment) like [GNOME](https://en.wikipedia.org/wiki/GNOME) 3 or [Unity](https://en.wikipedia.org/wiki/Unity_(user_interface))
* Firefox or Chromium browsers

I should point out here that such behavior **does not violate** [Google Colab's Terms of Use](https://colab.one/terms-of-use), although it is not encouraged according to their [FAQ](https://research.google.com/colaboratory/faq.html) (in short: morally it is not okay to use up all of the GPUs if you don't really need it).

---

My current solution is very dumb:

* First, I turn the screensaver off, so my screen is always on.
* I have an [Arduino](https://en.wikipedia.org/wiki/Arduino) board, so I just turned it into a [rubber ducky USB](https://www.quora.com/What-is-a-USB-Rubber-Ducky) device and make it emulate primitive user interaction while I sleep (just because I have it at hand for other use cases).

Are there better ways?","As of March 2021, none of these methods will work as Google added a [CAPTCHA](https://en.wikipedia.org/wiki/CAPTCHA) button that randomly pops up after some time.

Prior to that, the solution was very easy, and didn't need any JavaScript. Just create a new cell at the bottom having the following line:

```
while True:pass

```

Now keep the cell in the run sequence so that the infinite loop won't stop and thus keep your session alive.

*Old method:*

Set a JavaScript interval to click on the connect button every 60 seconds.

Open developer-settings (in your web-browser) with Ctrl+Shift+I then click on console tab and type this on the console prompt. (for mac press Option+Command+I)

```
function ConnectButton(){
  console.log(""Connect pushed"");
  document.querySelector(""#top-toolbar > colab-connectbutton"").shadowRoot.querySelector(""#connect"").click()
}
setInterval(ConnectButton,60000);

```"
"If I have a function like this:

```
def foo(name, opts={}):
  pass

```

And I want to add type hints to the parameters, how do I do it? The way I assumed gives me a syntax error:

```
def foo(name: str, opts={}: dict) -> str:
  pass

```

The following doesn't throw a syntax error but it doesn't seem like the intuitive way to handle this case:

```
def foo(name: str, opts: dict={}) -> str:
  pass

```

I can't find anything in the [`typing` documentation](https://docs.python.org/3/library/typing.html) or on a Google search.

Edit: I didn't know how default arguments worked in Python, but for the sake of this question, I will keep the examples above. In general it's much better to do the following:

```
def foo(name: str, opts: dict=None) -> str:
  if not opts:
    opts={}
  pass

```","Your second way is correct.

```
def foo(opts: dict = {}):
    pass

print(foo.__annotations__)

```

this outputs

```
{'opts': <class 'dict'>}

```

Although it is not explicitly mentioned in [PEP 484](https://www.python.org/dev/peps/pep-0484/), type hints are a specific use of function annotations, as outlined in PEP 3107. [The syntax section](https://www.python.org/dev/peps/pep-3107/#syntax) clearly demonstrates that keyword arguments can be annotated in this manner.

I strongly advise against using mutable keyword arguments. More information [here](http://docs.python-guide.org/en/latest/writing/gotchas/#mutable-default-arguments)."
"I have a Flask server running through port 5000, and it's fine. I can access it at <http://example.com:5000>

But is it possible to simply access it at <http://example.com>? I'm assuming that means I have to change the port from 5000 to 80. But when I try that on Flask, I get this error message when I run it.

```
Traceback (most recent call last):
  File ""xxxxxx.py"", line 31, in <module>
app.run(host=""0.0.0.0"", port=int(""80""), debug=True)
   File ""/usr/local/lib/python2.6/dist-packages/flask/app.py"", line 772, in run
run_simple(host, port, self, **options)
  File ""/usr/local/lib/python2.6/dist-packages/werkzeug/serving.py"", line 706, in run_simple
    test_socket.bind((hostname, port))
  File ""<string>"", line 1, in bind
socket.error: [Errno 98] Address already in use

```

Running `lsof -i :80` returns

```
COMMAND   PID     USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
apache2   467     root    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2  4413 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14346 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14570 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14571 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14573 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)

```

Do I need to kill these processes first? Is that safe? Or is there another way to keep Flask running on port 5000 but have the main website domain redirect somehow?","1- Stop other applications that are using port 80.
2- run application with port 80 :

```
if __name__ == '__main__':
      app.run(host='0.0.0.0', port=80)

```"
"I'm running Ubuntu 9:10 and a package called M2Crypto is installed (version is 0.19.1). I need to download, build and install the latest version of the M2Crypto package (0.20.2).

The 0.19.1 package has files in a number of locations including (/usr/share/pyshared and /usr/lib/pymodules.python2.6).

How can I completely uninstall version 0.19.1 from my system before installing 0.20.2?","The best way I've found is to run this command from terminal

```
sudo pip install [package_name] --upgrade

```

`sudo` will ask to enter your root password to confirm the action.

---

Note: Some users may have pip3 installed instead. In that case, use

```
sudo pip3 install [package_name] --upgrade

```"
"I need to take an image and save it after some process. The figure looks fine when I display it, but after saving the figure, I got some white space around the saved image. I have tried the `'tight'` option for `savefig` method, did not work either. The code:

```
import matplotlib.image as mpimg
import matplotlib.pyplot as plt

fig = plt.figure(1)
img = mpimg.imread(""image.jpg"")
plt.imshow(img)
ax = fig.add_subplot(1, 1, 1)

extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
plt.savefig('1.png', bbox_inches=extent)

plt.axis('off') 
plt.show()

```

I am trying to draw a basic graph by using NetworkX on a figure and save it. I realized that without a graph it works, but when added a graph I get white space around the saved image;

```
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import networkx as nx

G = nx.Graph()
G.add_node(1)
G.add_node(2)
G.add_node(3)
G.add_edge(1, 3)
G.add_edge(1, 2)
pos = {1:[100, 120], 2:[200, 300], 3:[50, 75]}

fig = plt.figure(1)
img = mpimg.imread(""image.jpg"")
plt.imshow(img)
ax = fig.add_subplot(1, 1, 1)

nx.draw(G, pos=pos)

extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
plt.savefig('1.png', bbox_inches=extent)

plt.axis('off') 
plt.show()

```","You can remove the white space padding by setting `bbox_inches=""tight""` in `savefig`:

```
plt.savefig(""test.png"",bbox_inches='tight')

```

You'll have to put the argument to `bbox_inches` as a string, perhaps this is why it didn't work earlier for you.

---

**Possible duplicates:**

[Matplotlib plots: removing axis, legends and white spaces](https://stackoverflow.com/questions/9295026/matplotlib-plots-removing-axis-legends-and-white-spaces)

[How to set the margins for a matplotlib figure?](https://stackoverflow.com/questions/10898919/how-to-set-the-margins-for-a-matplotlib-figure)

[Reduce left and right margins in matplotlib plot](https://stackoverflow.com/questions/4042192/reduce-left-and-right-margins-in-matplotlib-plot)"
"I have now:

```
list1 = [1, 2, 3]
list2 = [4, 5, 6]

```

I wish to have:

```
[1, 2, 3]
 +  +  +
[4, 5, 6]
|| || ||
[5, 7, 9]

```

Simply an element-wise addition of two lists.

I can surely iterate the two lists, but I don't want do that.

What is **the most Pythonic way** of doing so?","Use [`map`](https://docs.python.org/2/library/functions.html#map) with [`operator.add`](https://docs.python.org/2/library/operator.html#operator.add):

```
>>> from operator import add
>>> list( map(add, list1, list2) )
[5, 7, 9]

```

or [`zip`](https://docs.python.org/2/library/functions.html#zip) with a list comprehension:

```
>>> [sum(x) for x in zip(list1, list2)]
[5, 7, 9]

```

### Timing comparisons:

```
>>> list2 = [4, 5, 6]*10**5
>>> list1 = [1, 2, 3]*10**5
>>> %timeit from operator import add;map(add, list1, list2)
10 loops, best of 3: 44.6 ms per loop
>>> %timeit from itertools import izip; [a + b for a, b in izip(list1, list2)]
10 loops, best of 3: 71 ms per loop
>>> %timeit [a + b for a, b in zip(list1, list2)]
10 loops, best of 3: 112 ms per loop
>>> %timeit from itertools import izip;[sum(x) for x in izip(list1, list2)]
1 loops, best of 3: 139 ms per loop
>>> %timeit [sum(x) for x in zip(list1, list2)]
1 loops, best of 3: 177 ms per loop

```"
How can I create a temporary directory and get its path in Python?,"In Python 3, [`TemporaryDirectory`](https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory) from the [`tempfile`](https://docs.python.org/3/library/tempfile.html#module-tempfile) module can be used.

From the [examples](https://docs.python.org/3/library/tempfile.html#tempfile-examples):

```
import tempfile

with tempfile.TemporaryDirectory() as tmpdirname:
     print('created temporary directory', tmpdirname)

# directory and contents have been removed

```

To manually control when the directory is removed, don't use a context manager, as in the following example:

```
import tempfile

temp_dir = tempfile.TemporaryDirectory()
print(temp_dir.name)
# use temp_dir, and when done:
temp_dir.cleanup()

```

The documentation also says:

> On completion of the context or destruction of the temporary directory object the newly created temporary directory and all its contents are removed from the filesystem.

At the end of the program, for example, Python will clean up the directory if it wasn't removed, e.g. by the context manager or the `cleanup()` method. Python's `unittest` may complain of `ResourceWarning: Implicitly cleaning up <TemporaryDirectory...` if you rely on this, though."
"I know that Python does not support method overloading, but I've run into a problem that I can't seem to solve in a nice Pythonic way.

I am making a game where a character needs to shoot a variety of bullets, but how do I write different functions for creating these bullets? For example suppose I have a function that creates a bullet travelling from point A to B with a given speed. I would write a function like this:

```
def add_bullet(sprite, start, headto, speed):
    # Code ...

```

But I want to write other functions for creating bullets like:

```
def add_bullet(sprite, start, direction, speed):
def add_bullet(sprite, start, headto, spead, acceleration):
def add_bullet(sprite, script): # For bullets that are controlled by a script
def add_bullet(sprite, curve, speed): # for bullets with curved paths
# And so on ...

```

And so on with many variations. Is there a better way to do it without using so many keyword arguments cause its getting kinda ugly fast. Renaming each function is pretty bad too because you get either `add_bullet1`, `add_bullet2`, or `add_bullet_with_really_long_name`.

To address some answers:

1. No I can't create a Bullet class hierarchy because thats too slow. The actual code for managing bullets is in C and my functions are wrappers around C API.
2. I know about the keyword arguments but checking for all sorts of combinations of parameters is getting annoying, but default arguments help allot like `acceleration=0`","What you are asking for is called *multiple dispatch*. See [Julia](http://nbviewer.ipython.org/gist/StefanKarpinski/b8fe9dbb36c1427b9f22) language examples which demonstrates different types of dispatches.

However, before looking at that, we'll first tackle why *overloading* is not really what you want in Python.

Why Not Overloading?
====================

First, one needs to understand the concept of overloading and why it's not applicable to Python.

> When working with languages that can discriminate data types at
> compile-time, selecting among the alternatives can occur at
> compile-time. The act of creating such alternative functions for
> compile-time selection is usually referred to as overloading a
> function. ([Wikipedia](http://en.wikipedia.org/wiki/Multiple_dispatch))

Python is a [dynamically](https://pythonconquerstheuniverse.wordpress.com/2009/10/03/static-vs-dynamic-typing-of-programming-languages/) typed language, so the concept of overloading simply does not apply to it. However, all is not lost, since we can create such *alternative functions* at run-time:

> In programming languages that defer data type identification until
> run-time the selection among alternative
> functions must occur at run-time, based on the dynamically determined
> types of function arguments. Functions whose alternative
> implementations are selected in this manner are referred to most
> generally as *multimethods*. ([Wikipedia](http://en.wikipedia.org/wiki/Multiple_dispatch))

So we should be able to do *multimethods* in Pythonâ€”or, as it is alternatively called: *multiple dispatch*.

Multiple dispatch
=================

The multimethods are also called *multiple dispatch*:

> Multiple dispatch or multimethods is the feature of some
> object-oriented programming languages in which a function or method
> can be dynamically dispatched based on the run time (dynamic) type of
> more than one of its arguments. ([Wikipedia](http://en.wikipedia.org/wiki/Multiple_dispatch))

Python does not support this out of the box1, but, as it happens, there is an excellent Python package called [multipledispatch](https://pypi.python.org/pypi/multipledispatch/) that does exactly that.

Solution
========

Here is how we might use [multipledispatch](https://pypi.python.org/pypi/multipledispatch/)2 package to implement your methods:

```
>>> from multipledispatch import dispatch
>>> from collections import namedtuple
>>> from types import *  # we can test for lambda type, e.g.:
>>> type(lambda a: 1) == LambdaType
True

>>> Sprite = namedtuple('Sprite', ['name'])
>>> Point = namedtuple('Point', ['x', 'y'])
>>> Curve = namedtuple('Curve', ['x', 'y', 'z'])
>>> Vector = namedtuple('Vector', ['x','y','z'])

>>> @dispatch(Sprite, Point, Vector, int)
... def add_bullet(sprite, start, direction, speed):
...     print(""Called Version 1"")
...
>>> @dispatch(Sprite, Point, Point, int, float)
... def add_bullet(sprite, start, headto, speed, acceleration):
...     print(""Called version 2"")
...
>>> @dispatch(Sprite, LambdaType)
... def add_bullet(sprite, script):
...     print(""Called version 3"")
...
>>> @dispatch(Sprite, Curve, int)
... def add_bullet(sprite, curve, speed):
...     print(""Called version 4"")
...

>>> sprite = Sprite('Turtle')
>>> start = Point(1,2)
>>> direction = Vector(1,1,1)
>>> speed = 100 #km/h
>>> acceleration = 5.0 #m/s**2
>>> script = lambda sprite: sprite.x * 2
>>> curve = Curve(3, 1, 4)
>>> headto = Point(100, 100) # somewhere far away

>>> add_bullet(sprite, start, direction, speed)
Called Version 1

>>> add_bullet(sprite, start, headto, speed, acceleration)
Called version 2

>>> add_bullet(sprite, script)
Called version 3

>>> add_bullet(sprite, curve, speed)
Called version 4

```

1. Python 3 currently supports [single dispatch](https://docs.python.org/3/library/functools.html#functools.singledispatch)
2. Take care not to use [multipledispatch](https://pypi.python.org/pypi/multipledispatch/) in a multi-threaded environment or you will get weird behavior."
"What is the recommended way of handling settings for local development and the production server? Some of them (like constants, etc) can be changed/accessed in both, but some of them (like paths to static files) need to remain different, and hence should not be overwritten every time the new code is deployed.

Currently, I am adding all constants to `settings.py`. But every time I change some constant locally, I have to copy it to the production server and edit the file for production specific changes... :(

*Edit: looks like there is no standard answer to this question, I've accepted the most popular method.*","[Two Scoops of Django: Best Practices for Django 1.5](http://twoscoopspress.org/products/two-scoops-of-django-1-5) suggests using version control for your settings files and storing the files in a separate directory:

```
project/
    app1/
    app2/
    project/
        __init__.py
        settings/
            __init__.py
            base.py
            local.py
            production.py
    manage.py

```

The `base.py` file contains common settings (such as MEDIA\_ROOT or ADMIN), while `local.py` and `production.py` have site-specific settings:

In the base file `settings/base.py`:

```
INSTALLED_APPS = (
    # common apps...
)

```

In the local development settings file `settings/local.py`:

```
from project.settings.base import *

DEBUG = True
INSTALLED_APPS += (
    'debug_toolbar', # and other apps for local development
)

```

In the file production settings file `settings/production.py`:

```
from project.settings.base import *

DEBUG = False
INSTALLED_APPS += (
    # other apps for production site
)

```

Then when you run django, you add the `--settings` option:

```
# Running django for local development
$ ./manage.py runserver 0:8000 --settings=project.settings.local

# Running django shell on the production site
$ ./manage.py shell --settings=project.settings.production

```

The authors of the book have also put up [a sample project layout template](https://github.com/twoscoops/django-twoscoops-project) on Github."
"docker started throwing this error:

> standard\_init\_linux.go:178: exec user process caused ""exec format error""

whenever I run a specific docker container with CMD or ENTRYPOINT, with no regard to any changes to the file other then removing CMD or ENTRYPOINT. here is the docker file I have been working with which worked perfectly until about an hour ago:

```
FROM buildpack-deps:jessie

ENV PATH /usr/local/bin:$PATH

ENV LANG C.UTF-8

RUN apt-get update && apt-get install -y --no-install-recommends \
        tcl \
        tk \
    && rm -rf /var/lib/apt/lists/*

ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D
ENV PYTHON_VERSION 3.6.0

ENV PYTHON_PIP_VERSION 9.0.1

RUN set -ex \
    && buildDeps=' \
        tcl-dev \
        tk-dev \
    ' \
    && apt-get update && apt-get install -y $buildDeps --no-install-recommends && rm -rf /var/lib/apt/lists/* \
    \
    && wget -O python.tar.xz ""https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz"" \
    && wget -O python.tar.xz.asc ""https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc"" \
    && export GNUPGHOME=""$(mktemp -d)"" \
    && gpg --keyserver ha.pool.sks-keyservers.net --recv-keys ""$GPG_KEY"" \
    && gpg --batch --verify python.tar.xz.asc python.tar.xz \
    && rm -r ""$GNUPGHOME"" python.tar.xz.asc \
    && mkdir -p /usr/src/python \
    && tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \
    && rm python.tar.xz \
    \
    && cd /usr/src/python \
    && ./configure \
        --enable-loadable-sqlite-extensions \
        --enable-shared \
    && make -j$(nproc) \
    && make install \
    && ldconfig \
    \
    && if [ ! -e /usr/local/bin/pip3 ]; then : \
        && wget -O /tmp/get-pip.py 'https://bootstrap.pypa.io/get-pip.py' \
        && python3 /tmp/get-pip.py ""pip==$PYTHON_PIP_VERSION"" \
        && rm /tmp/get-pip.py \
    ; fi \
    && pip3 install --no-cache-dir --upgrade --force-reinstall ""pip==$PYTHON_PIP_VERSION"" \
    && [ ""$(pip list |tac|tac| awk -F '[ ()]+' '$1 == ""pip"" { print $2; exit }')"" = ""$PYTHON_PIP_VERSION"" ] \
    \
    && find /usr/local -depth \
        \( \
            \( -type d -a -name test -o -name tests \) \
            -o \
            \( -type f -a -name '*.pyc' -o -name '*.pyo' \) \
        \) -exec rm -rf '{}' + \
    && apt-get purge -y --auto-remove $buildDeps \
    && rm -rf /usr/src/python ~/.cache

RUN cd /usr/local/bin \
    && { [ -e easy_install ] || ln -s easy_install-* easy_install; } \
    && ln -s idle3 idle \
    && ln -s pydoc3 pydoc \
    && ln -s python3 python \
    && ln -s python3-config python-config

RUN pip install uwsgi

RUN mkdir /config

RUN mkdir /logs

ENV HOME /var/www

WORKDIR /config

ADD conf/requirements.txt /config

RUN pip install -r /config/requirements.txt

ADD conf/wsgi.py /config

ADD conf/wsgi.ini /config

ADD conf/__init__.py /config

ADD start.sh /bin/start.sh

RUN chmod +x /bin/start.sh

EXPOSE 8000

ENTRYPOINT [""start.sh"", ""uwsgi"", ""--ini"", ""wsgi.ini""]

```","I forgot to put

```
#!/bin/bash

```

at the top of the sh file, problem solved."
"I want to scrape all the data of a page implemented by a infinite scroll. The following python code works.

```
for i in range(100):
    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
    time.sleep(5)

```

This means every time I scroll down to the bottom, I need to wait 5 seconds, which is generally enough for the page to finish loading the newly generated contents. But, this may not be time efficient. The page may finish loading the new contents within 5 seconds. How can I detect whether the page finished loading the new contents every time I scroll down? If I can detect this, I can scroll down again to see more contents once I know the page finished loading. This is more time efficient.","The `webdriver` will wait for a page to load by default via `.get()` method.

As you may be looking for some specific element as @user227215 said, you should use `WebDriverWait` to wait for an element located in your page:

```
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException

browser = webdriver.Firefox()
browser.get(""url"")
delay = 3 # seconds
try:
    myElem = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.ID, 'IdOfMyElement')))
    print ""Page is ready!""
except TimeoutException:
    print ""Loading took too much time!""

```

I have used it for checking alerts. You can use any other type methods to find the locator.

**EDIT 1:**

I should mention that the `webdriver` will wait for a page to load by default. It does not wait for loading inside frames or for ajax requests. It means when you use `.get('url')`, your browser will wait until the page is completely loaded and then go to the next command in the code. But when you are posting an ajax request, `webdriver` does not wait and it's your responsibility to wait an appropriate amount of time for the page or a part of page to load; so there is a module named `expected_conditions`."
"How do I print the summary of a model in PyTorch like what `model.summary()` does in Keras:

```
Model Summary:
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 1, 15, 27)     0                                            
____________________________________________________________________________________________________
convolution2d_1 (Convolution2D)  (None, 8, 15, 27)     872         input_1[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 8, 7, 27)      0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 1512)          0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 1)             1513        flatten_1[0][0]                  
====================================================================================================
Total params: 2,385
Trainable params: 2,385
Non-trainable params: 0

```","Yes, you can get exact Keras representation, using the [pytorch-summary](https://github.com/sksq96/pytorch-summary) package.

Example for VGG16:

```
from torchvision import models
from torchsummary import summary

vgg = models.vgg16()
summary(vgg, (3, 224, 224))

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 224, 224]           1,792
              ReLU-2         [-1, 64, 224, 224]               0
            Conv2d-3         [-1, 64, 224, 224]          36,928
              ReLU-4         [-1, 64, 224, 224]               0
         MaxPool2d-5         [-1, 64, 112, 112]               0
            Conv2d-6        [-1, 128, 112, 112]          73,856
              ReLU-7        [-1, 128, 112, 112]               0
            Conv2d-8        [-1, 128, 112, 112]         147,584
              ReLU-9        [-1, 128, 112, 112]               0
        MaxPool2d-10          [-1, 128, 56, 56]               0
           Conv2d-11          [-1, 256, 56, 56]         295,168
             ReLU-12          [-1, 256, 56, 56]               0
           Conv2d-13          [-1, 256, 56, 56]         590,080
             ReLU-14          [-1, 256, 56, 56]               0
           Conv2d-15          [-1, 256, 56, 56]         590,080
             ReLU-16          [-1, 256, 56, 56]               0
        MaxPool2d-17          [-1, 256, 28, 28]               0
           Conv2d-18          [-1, 512, 28, 28]       1,180,160
             ReLU-19          [-1, 512, 28, 28]               0
           Conv2d-20          [-1, 512, 28, 28]       2,359,808
             ReLU-21          [-1, 512, 28, 28]               0
           Conv2d-22          [-1, 512, 28, 28]       2,359,808
             ReLU-23          [-1, 512, 28, 28]               0
        MaxPool2d-24          [-1, 512, 14, 14]               0
           Conv2d-25          [-1, 512, 14, 14]       2,359,808
             ReLU-26          [-1, 512, 14, 14]               0
           Conv2d-27          [-1, 512, 14, 14]       2,359,808
             ReLU-28          [-1, 512, 14, 14]               0
           Conv2d-29          [-1, 512, 14, 14]       2,359,808
             ReLU-30          [-1, 512, 14, 14]               0
        MaxPool2d-31            [-1, 512, 7, 7]               0
           Linear-32                 [-1, 4096]     102,764,544
             ReLU-33                 [-1, 4096]               0
          Dropout-34                 [-1, 4096]               0
           Linear-35                 [-1, 4096]      16,781,312
             ReLU-36                 [-1, 4096]               0
          Dropout-37                 [-1, 4096]               0
           Linear-38                 [-1, 1000]       4,097,000
================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 218.59
Params size (MB): 527.79
Estimated Total Size (MB): 746.96
----------------------------------------------------------------

```"
"I have a list with 15 numbers. How can I produce all 32,768 combinations of those numbers (i.e., any number of elements, in the original order)?

I thought of looping through the decimal integers 1â€“32768 and using the binary representation of each numbers as a filter to pick out the appropriate list elements. Is there a better way to do it?

---

For combinations **of a specific length**, see [Get all (n-choose-k) combinations of length n](https://stackoverflow.com/questions/27974126). Please use that question to close duplicates instead where appropriate.

When closing questions about combinatorics as duplicates, it is very important to make sure of what OP **actually wants, not** the words that were used to describe the problem. It is extremely common for people who want, for example, a Cartesian product (see [How to get the cartesian product of multiple lists](https://stackoverflow.com/questions/533905)) to ask about ""combinations"".","[This answer](https://stackoverflow.com/questions/464864/python-code-to-pick-out-all-possible-combinations-from-a-list/464882#464882) missed one aspect: the OP asked for ALL combinations... not just combinations of length ""r"".

So you'd either have to loop through all lengths ""L"":

```
import itertools

stuff = [1, 2, 3]
for L in range(len(stuff) + 1):
    for subset in itertools.combinations(stuff, L):
        print(subset)

```

Or -- if you want to get snazzy (or bend the brain of whoever reads your code after you) -- you can generate the chain of ""combinations()"" generators, and iterate through that:

```
from itertools import chain, combinations
def all_subsets(ss):
    return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))

for subset in all_subsets(stuff):
    print(subset)

```"
"I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?","Easiest way I know of is to use ""child\_process"" package which comes packaged with node.

Then you can do something like:

```
const spawn = require(""child_process"").spawn;
const pythonProcess = spawn('python',[""path/to/script.py"", arg1, arg2, ...]);

```

Then all you have to do is make sure that you `import sys` in your python script, and then you can access `arg1` using `sys.argv[1]`, `arg2` using `sys.argv[2]`, and so on.

To send data back to node just do the following in the python script:

```
print(dataToSendBack)
sys.stdout.flush()

```

And then node can listen for data using:

```
pythonProcess.stdout.on('data', (data) => {
 // Do something with the data returned from python script
});

```

Since this allows multiple arguments to be passed to a script using spawn, you can restructure a python script so that one of the arguments decides which function to call, and the other argument gets passed to that function, etc.

Hope this was clear. Let me know if something needs clarification."
"I am sketching the architecture for a set of programs that share various interrelated objects stored in a database. I want one of the programs to act as a service which provides a higher level interface for operations on these objects, and the other programs to access the objects through that service.

I am currently aiming for Python and the Django framework as the technologies to implement that service with. I'm pretty sure I figure how to daemonize the Python program in Linux. However, it is an optional spec item that the system should support Windows. I have little experience with Windows programming and no experience at all with Windows services.

**Is it possible to run a Python programs as a Windows service (i. e. run it automatically without user login)?** I won't necessarily have to implement this part, but I need a rough idea how it would be done in order to decide whether to design along these lines.

*Edit: Thanks for all the answers so far, they are quite comprehensive. I would like to know one more thing: **How is Windows aware of my service? Can I manage it with the native Windows utilities?** **What is the equivalent of putting a start/stop script in /etc/init.d?***","Yes you can. I do it using the pythoncom libraries that come included with [ActivePython](http://www.activestate.com/Products/activepython/index.mhtml) or can be installed with [pywin32](https://sourceforge.net/projects/pywin32/) (Python for Windows extensions).

This is a basic skeleton for a simple service:

```
import win32serviceutil
import win32service
import win32event
import servicemanager
import socket


class AppServerSvc (win32serviceutil.ServiceFramework):
    _svc_name_ = ""TestService""
    _svc_display_name_ = ""Test Service""

    def __init__(self,args):
        win32serviceutil.ServiceFramework.__init__(self,args)
        self.hWaitStop = win32event.CreateEvent(None,0,0,None)
        socket.setdefaulttimeout(60)

    def SvcStop(self):
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        win32event.SetEvent(self.hWaitStop)

    def SvcDoRun(self):
        servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE,
                              servicemanager.PYS_SERVICE_STARTED,
                              (self._svc_name_,''))
        self.main()

    def main(self):
        pass

if __name__ == '__main__':
    win32serviceutil.HandleCommandLine(AppServerSvc)

```

Your code would go in the `main()` methodâ€”usually with some kind of infinite loop that might be interrupted by checking a flag, which you set in the `SvcStop` method"
"What named colors are available in matplotlib for use in plots? I can find a list on the matplotlib documentation that claims that these are the only names:

```
b: blue
g: green
r: red
c: cyan
m: magenta
y: yellow
k: black
w: white

```

However, I've found that these colors can also be used, at least in this context:

```
scatter(X,Y, color='red')
scatter(X,Y, color='orange')
scatter(X,Y, color='darkgreen')

```

but these are not on the above list. Does anyone know an exhaustive list of the named colors that are available?","I constantly forget the names of the colors I want to use and keep coming back to this question =)

The previous answers are great, but I find it a bit difficult to get an overview of the available colors from the posted image. I prefer the colors to be grouped with similar colors, so I slightly tweaked the [matplotlib answer](http://matplotlib.org/examples/color/named_colors.html) that was mentioned in a comment above to get a color list sorted in columns. The order is not identical to how I would sort by eye, but I think it gives a good overview.

*I updated the image and code to reflect that 'rebeccapurple' has been added and the three sage colors have been moved under the 'xkcd:' prefix since I posted this answer originally.*

[![enter image description here](https://i.sstatic.net/lFZum.png)](https://i.sstatic.net/lFZum.png)

I really didn't change much from the matplotlib example, but here is the code for completeness.

```
import matplotlib.pyplot as plt
from matplotlib import colors as mcolors


colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)

# Sort colors by hue, saturation, value and name.
by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)
                for name, color in colors.items())
sorted_names = [name for hsv, name in by_hsv]

n = len(sorted_names)
ncols = 4
nrows = n // ncols

fig, ax = plt.subplots(figsize=(12, 10))

# Get height and width
X, Y = fig.get_dpi() * fig.get_size_inches()
h = Y / (nrows + 1)
w = X / ncols

for i, name in enumerate(sorted_names):
    row = i % nrows
    col = i // nrows
    y = Y - (row * h) - h

    xi_line = w * (col + 0.05)
    xf_line = w * (col + 0.25)
    xi_text = w * (col + 0.3)

    ax.text(xi_text, y, name, fontsize=(h * 0.8),
            horizontalalignment='left',
            verticalalignment='center')

    ax.hlines(y + h * 0.1, xi_line, xf_line,
              color=colors[name], linewidth=(h * 0.8))

ax.set_xlim(0, X)
ax.set_ylim(0, Y)
ax.set_axis_off()

fig.subplots_adjust(left=0, right=1,
                    top=1, bottom=0,
                    hspace=0, wspace=0)
plt.show()

```

---

Additional named colors
-----------------------

*Updated 2017-10-25. I merged my previous updates into this section.*

### xkcd

If you would like to use additional named colors when plotting with matplotlib, you can use the [xkcd crowdsourced color names](http://xkcd.com/color/rgb/), via the 'xkcd:' prefix:

```
plt.plot([1,2], lw=4, c='xkcd:baby poop green')

```

Now you have access to a plethora of named colors!

[![enter image description here](https://i.sstatic.net/nCk6u.jpg)](https://i.sstatic.net/nCk6u.jpg)

### Tableau

The default Tableau colors are available in matplotlib via the 'tab:' prefix:

```
plt.plot([1,2], lw=4, c='tab:green')

```

There are ten distinct colors:

[![enter image description here](https://i.sstatic.net/K6Q8n.png)](https://i.sstatic.net/K6Q8n.png)

### HTML

You can also plot colors by their [HTML hex code](https://www.computerhope.com/tips/tip143.htm):

```
plt.plot([1,2], lw=4, c='#8f9805')

```

This is more similar to specifying and RGB tuple rather than a named color (apart from the fact that the hex code is passed as a string), and I will not include an image of the 16 million colors you can choose from...

---

For more details, please refer to [the matplotlib colors documentation](https://matplotlib.org/users/colors.html) and the source file specifying the available colors, [`_color_data.py`](https://github.com/matplotlib/matplotlib/blob/master/lib/matplotlib/_color_data.py).

---"
"I have written the following Python code:

```
#!/usr/bin/python
# -*- coding: utf-8 -*-

import os, glob

path = '/home/my/path'
for infile in glob.glob( os.path.join(path, '*.png') ):
    print infile

```

Now I get this:

```
/home/my/path/output0352.png
/home/my/path/output0005.png
/home/my/path/output0137.png
/home/my/path/output0202.png
/home/my/path/output0023.png
/home/my/path/output0048.png
/home/my/path/output0069.png
/home/my/path/output0246.png
/home/my/path/output0071.png
/home/my/path/output0402.png
/home/my/path/output0230.png
/home/my/path/output0182.png
/home/my/path/output0121.png
/home/my/path/output0104.png
/home/my/path/output0219.png
/home/my/path/output0226.png
/home/my/path/output0215.png
/home/my/path/output0266.png
/home/my/path/output0347.png
/home/my/path/output0295.png
/home/my/path/output0131.png
/home/my/path/output0208.png
/home/my/path/output0194.png

```

In which way is it ordered?

To clarify: I am not interested in ordering - I know `sorted`. I want to know in which order it comes by default.

It might help you to get my ls -l output:

```
-rw-r--r-- 1 moose moose 627669 2011-07-17 17:26 output0005.png
-rw-r--r-- 1 moose moose 596417 2011-07-17 17:26 output0023.png
-rw-r--r-- 1 moose moose 543639 2011-07-17 17:26 output0048.png
-rw-r--r-- 1 moose moose 535384 2011-07-17 17:27 output0069.png
-rw-r--r-- 1 moose moose 543216 2011-07-17 17:27 output0071.png
-rw-r--r-- 1 moose moose 561776 2011-07-17 17:27 output0104.png
-rw-r--r-- 1 moose moose 501865 2011-07-17 17:27 output0121.png
-rw-r--r-- 1 moose moose 547144 2011-07-17 17:27 output0131.png
-rw-r--r-- 1 moose moose 530596 2011-07-17 17:27 output0137.png
-rw-r--r-- 1 moose moose 532567 2011-07-17 17:27 output0182.png
-rw-r--r-- 1 moose moose 553562 2011-07-17 17:27 output0194.png
-rw-r--r-- 1 moose moose 574065 2011-07-17 17:27 output0202.png
-rw-r--r-- 1 moose moose 552197 2011-07-17 17:27 output0208.png
-rw-r--r-- 1 moose moose 559809 2011-07-17 17:27 output0215.png
-rw-r--r-- 1 moose moose 549046 2011-07-17 17:27 output0219.png
-rw-r--r-- 1 moose moose 566661 2011-07-17 17:27 output0226.png
-rw-r--r-- 1 moose moose 561678 2011-07-17 17:27 output0246.png
-rw-r--r-- 1 moose moose 525550 2011-07-17 17:27 output0266.png
-rw-r--r-- 1 moose moose 565715 2011-07-17 17:27 output0295.png
-rw-r--r-- 1 moose moose 568381 2011-07-17 17:28 output0347.png
-rw-r--r-- 1 moose moose 532768 2011-07-17 17:28 output0352.png
-rw-r--r-- 1 moose moose 535818 2011-07-17 17:28 output0402.png

```

It is not ordered by filename or size.

Other links: [`glob`](http://docs.python.org/library/glob.html), [`ls`](http://en.wikipedia.org/wiki/Ls)","Order is arbitrary, but you can sort them yourself

If you want sorted by name:

```
sorted(glob.glob('*.png'))

```

sorted by modification time:

```
import os
sorted(glob.glob('*.png'), key=os.path.getmtime)

```

sorted by size:

```
import os
sorted(glob.glob('*.png'), key=os.path.getsize)

```

etc."
"I need to count unique `ID` values in every `domain`.

I have data:

```
 ID, domain
123, vk.com
123, vk.com
123, twitter.com
456, vk.com'
456, facebook.com
456, vk.com
456, google.com
789, twitter.com
789, vk.com

```

I try `df.groupby(['domain', 'ID']).count()`

But I want to get

```
domain       count
vk.com        3
twitter.com   2
facebook.com  1
google.com    1

```","You need [`nunique`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.SeriesGroupBy.nunique.html):

```
df = df.groupby('domain')['ID'].nunique()

print (df)
domain
'facebook.com'    1
'google.com'      1
'twitter.com'     2
'vk.com'          3
Name: ID, dtype: int64

```

If you need to [`strip`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.strip.html) `'` characters:

```
df = df.ID.groupby([df.domain.str.strip(""'"")]).nunique()
print (df)
domain
facebook.com    1
google.com      1
twitter.com     2
vk.com          3
Name: ID, dtype: int64

```

Or as [Jon Clements](https://stackoverflow.com/questions/38309729/count-unique-values-with-pandas#comment64036411_38309823) commented:

```
df.groupby(df.domain.str.strip(""'""))['ID'].nunique()

```

You can retain the column name like this:

```
df = df.groupby(by='domain', as_index=False).agg({'ID': pd.Series.nunique})
print(df)
    domain  ID
0       fb   1
1      ggl   1
2  twitter   2
3       vk   3

```

The difference is that `nunique()` returns a Series and `agg()` returns a DataFrame."
How can I install the [MySQLdb](http://mysql-python.sourceforge.net/MySQLdb.html) module for Python using pip?,"It's easy to do, but hard to remember the correct spelling:

```
pip install mysqlclient

```

If you need 1.2.x versions (legacy Python only), use `pip install MySQL-python`

Note: Some dependencies might have to be in place when running the above command. Some hints on how to install these on various platforms:

Ubuntu 14, Ubuntu 16, Debian 8.6 (jessie)
-----------------------------------------

```
sudo apt-get install python-pip python-dev libmysqlclient-dev

```

Fedora 24:
----------

```
sudo dnf install python python-devel mysql-devel redhat-rpm-config gcc

```

Mac OS
------

```
brew install mysql-connector-c

```

if that fails, try

```
brew install mysql

```"
"I'm bit confused about how the global variables work. I have a large project, with around 50 files, and I need to define global variables for all those files.

What I did was define them in my projects `main.py` file, as following:

```
# ../myproject/main.py

# Define global myList
global myList
myList = []

# Imports
import subfile

# Do something
subfile.stuff()
print(myList[0])

```

I'm trying to use `myList` in `subfile.py`, as following

```
# ../myproject/subfile.py

# Save ""hey"" into myList
def stuff():
    globals()[""myList""].append(""hey"")

```

**An other way I tried, but didn't work either**

```
# ../myproject/main.py

# Import globfile    
import globfile

# Save myList into globfile
globfile.myList = []

# Import subfile
import subfile

# Do something
subfile.stuff()
print(globfile.myList[0])

```

And inside `subfile.py` I had this:

```
# ../myproject/subfile.py

# Import globfile
import globfile

# Save ""hey"" into myList
def stuff():
    globfile.myList.append(""hey"")

```

But again, it didn't work. How should I implement this? I understand that it cannot work like that, when the two files don't really know each other (well subfile doesn't know main), but I can't think of how to do it, without using io writing or pickle, which I don't want to do.","The problem is you defined `myList` from `main.py`, but `subfile.py` needs to use it. Here is a clean way to solve this problem: move all globals to a file, I call this file `settings.py`. This file is responsible for defining globals and initializing them:

```
# settings.py

def init():
    global myList
    myList = []

```

Next, your `subfile` can import globals:

```
# subfile.py

import settings

def stuff():
    settings.myList.append('hey')

```

Note that `subfile` does not call `init()`â€” that task belongs to `main.py`:

```
# main.py

import settings
import subfile

settings.init()          # Call only once
subfile.stuff()         # Do stuff with global var
print settings.myList[0] # Check the result

```

This way, you achieve your objective while avoid initializing global variables more than once."
"I found the platform module but it says it returns 'Windows' and it's returning 'Microsoft' on my machine. I notice in another thread here on stackoverflow it returns 'Vista' sometimes.

So, the question is, how do implemement?

```
if is_windows():
  ...

```

In a forward compatible way? If I have to check for things like 'Vista' then it will break when the next version of windows comes out.

---

Note: The answers claiming this is a duplicate question do not actually answer the question `is_windows`. They answer the question ""what platform"". Since many flavors of windows exist none of them comprehensively describe how to get an answer of `isWindows`.","Python [os](http://docs.python.org/library/os.html) module

Specifically for Python 3.6/3.7:

> `os.name`: The name of the operating
> system dependent module imported. The
> following names have currently been
> registered: 'posix', 'nt', 'java'.

In your case, you want to check for 'nt' as `os.name` output:

```
import os

if os.name == 'nt':
     ...

```

There is also a note on `os.name`:

> See also [`sys.platform`](https://docs.python.org/3.5/library/sys.html#sys.platform) has a finer granularity. [`os.uname()`](https://docs.python.org/3.5/library/os.html#os.uname) gives
> system-dependent version information.
>
> The [platform](https://docs.python.org/3.5/library/platform.html#module-platform) module provides
> detailed checks for the systemâ€™s identity."
"I'm installing several Python packages in Ubuntu 12.04 using the following `requirements.txt` file:

```
numpy>=1.8.2,<2.0.0
matplotlib>=1.3.1,<2.0.0
scipy>=0.14.0,<1.0.0
astroML>=0.2,<1.0
scikit-learn>=0.14.1,<1.0.0
rpy2>=2.4.3,<3.0.0

```

and these two commands:

```
$ pip install --download=/tmp -r requirements.txt
$ pip install --user --no-index --find-links=/tmp -r requirements.txt

```

(the first one downloads the packages and the second one installs them).

The process is frequently stopped with the error:

```
  Could not find a version that satisfies the requirement <package> (from matplotlib<2.0.0,>=1.3.1->-r requirements.txt (line 2)) (from versions: )
No matching distribution found for <package> (from matplotlib<2.0.0,>=1.3.1->-r requirements.txt (line 2))

```

which I fix manually with:

```
pip install --user <package>

```

and then run the second `pip install` command again.

But that only works for *that* particular package. When I run the second `pip install` command again, the process is stopped now complaining about *another* required package and I need to repeat the process again, ie: install the new required package manually (with the command above) and then run the second `pip install` command.

So far I've had to manually install `six`, `pytz`, `nose`, and now it's complaining about needing `mock`.

Is there a way to tell `pip` to automatically install *all* needed dependencies so I don't have to do it manually one by one?

**Add**: This only happens in Ubuntu 12.04 BTW. In Ubuntu 14.04 the `pip install` commands applied on the `requirements.txt` file work without issues.","*Although it doesn't really answers this specific question. Others got the same error message with this mistake.*

For those who like me initial forgot the `-r`: Use `pip install -r requirements.txt` the `-r` is essential for the command.

The original answer:

<https://stackoverflow.com/a/42876654/10093070>"
"I come from pandas background and am used to reading data from CSV files into a dataframe and then simply changing the column names to something useful using the simple command:

```
df.columns = new_column_name_list

```

However, the same doesn't work in PySpark dataframes created using sqlContext.
The only solution I could figure out to do this easily is the following:

```
df = sqlContext.read.format(""com.databricks.spark.csv"").options(header='false', inferschema='true', delimiter='\t').load(""data.txt"")
oldSchema = df.schema
for i,k in enumerate(oldSchema.fields):
  k.name = new_column_name_list[i]
df = sqlContext.read.format(""com.databricks.spark.csv"").options(header='false', delimiter='\t').load(""data.txt"", schema=oldSchema)

```

This is basically defining the variable twice and inferring the schema first then renaming the column names and then loading the dataframe again with the updated schema.

Is there a better and more efficient way to do this like we do in pandas?

My Spark version is 1.5.0","There are many ways to do that:

* Option 1. Using [selectExpr](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=selectexpr#pyspark.sql.DataFrame.selectExpr).

  ```
   data = sqlContext.createDataFrame([(""Alberto"", 2), (""Dakota"", 2)], 
                                     [""Name"", ""askdaosdka""])
   data.show()
   data.printSchema()

   # Output
   #+-------+----------+
   #|   Name|askdaosdka|
   #+-------+----------+
   #|Alberto|         2|
   #| Dakota|         2|
   #+-------+----------+

   #root
   # |-- Name: string (nullable = true)
   # |-- askdaosdka: long (nullable = true)

   df = data.selectExpr(""Name as name"", ""askdaosdka as age"")
   df.show()
   df.printSchema()

   # Output
   #+-------+---+
   #|   name|age|
   #+-------+---+
   #|Alberto|  2|
   #| Dakota|  2|
   #+-------+---+

   #root
   # |-- name: string (nullable = true)
   # |-- age: long (nullable = true)

  ```
* Option 2. Using [withColumnRenamed](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=selectexpr#pyspark.sql.DataFrame.withColumnRenamed), notice that this method allows you to ""overwrite"" the same column. For Python3, replace `xrange` with `range`.

  ```
   from functools import reduce

   oldColumns = data.schema.names
   newColumns = [""name"", ""age""]

   df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), data)
   df.printSchema()
   df.show()

  ```
* Option 3. using
  [alias](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html), in Scala you can also use [as](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column).

  ```
   from pyspark.sql.functions import col

   data = data.select(col(""Name"").alias(""name""), col(""askdaosdka"").alias(""age""))
   data.show()

   # Output
   #+-------+---+
   #|   name|age|
   #+-------+---+
   #|Alberto|  2|
   #| Dakota|  2|
   #+-------+---+

  ```
* Option 4. Using [sqlContext.sql](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql), which lets you use SQL queries on `DataFrames` registered as tables.

  ```
   sqlContext.registerDataFrameAsTable(data, ""myTable"")
   df2 = sqlContext.sql(""SELECT Name AS name, askdaosdka as age from myTable"")

   df2.show()

   # Output
   #+-------+---+
   #|   name|age|
   #+-------+---+
   #|Alberto|  2|
   #| Dakota|  2|
   #+-------+---+

  ```"
"I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...

```
  File ""C:\Importer\src\dfman\importer.py"", line 26, in import_chr
    data = pd.read_csv(filepath, names=fields)
  File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 400, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 205, in _read
    return parser.read()
  File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 608, in read
    ret = self._engine.read(nrows)
  File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 1028, in read
    data = self._reader.read(nrows)
  File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\parser.c:6745)
  File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:6964)
  File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\parser.c:7780)
  File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\parser.c:8793)
  File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\parser.c:9484)
  File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:10642)
  File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\parser.c:10853)
  File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\parser.c:15657)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 6: invalid    continuation byte

```

The source/creation of these files all come from the same place. What's the best way to correct this to proceed with the import?","`read_csv` takes an `encoding` option to deal with files in different formats. I mostly use `read_csv('file', encoding = ""ISO-8859-1"")`, or alternatively `encoding = ""utf-8""` for reading, and generally `utf-8` for `to_csv`.

You can also use one of several `alias` options like `'latin'` or `'cp1252'` (Windows) instead of `'ISO-8859-1'` (see [python docs](https://docs.python.org/3/library/codecs.html#standard-encodings), also for numerous other encodings you may encounter).

See [relevant Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html),
[python docs examples on csv files](http://docs.python.org/3/library/csv.html#examples), and plenty of related questions here on SO. A good background resource is [What every developer should know about unicode and character sets](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/).

To detect the encoding (assuming the file contains non-ascii characters), you can use `enca` (see [man page](https://linux.die.net/man/1/enconv)) or `file -i` (linux) or `file -I` (osx) (see [man page](https://linux.die.net/man/1/file))."
"I am trying to plot some data from a camera in real time using OpenCV. However, the real-time plotting (using matplotlib) doesn't seem to be working.

I've isolated the problem into this simple example:

```
fig = plt.figure()
plt.axis([0, 1000, 0, 1])

i = 0
x = list()
y = list()

while i < 1000:
    temp_y = np.random.random()
    x.append(i)
    y.append(temp_y)
    plt.scatter(i, temp_y)
    i += 1
    plt.show()

```

I would expect this example to plot 1000 points individually. What actually happens is that the window pops up with the first point showing (ok with that), then waits for the loop to finish before it populates the rest of the graph.

Any thoughts why I am not seeing points populated one at a time?","Here's the working version of the code in question (requires at least version Matplotlib 1.1.0 from 2011-11-14):

```
import numpy as np
import matplotlib.pyplot as plt

plt.axis([0, 10, 0, 1])

for i in range(10):
    y = np.random.random()
    plt.scatter(i, y)
    plt.pause(0.05)

plt.show()

```

Note the call to `plt.pause(0.05)`, which both draws the new data and runs the GUI's event loop (allowing for mouse interaction)."
"What does `for row_number, row in enumerate(cursor):` do in Python?

What does `enumerate` mean in this context?","The [`enumerate()` function](http://docs.python.org/2/library/functions.html#enumerate) adds a counter to an iterable.

So for each element in `cursor`, a tuple is produced with `(counter, element)`; the `for` loop binds that to `row_number` and `row`, respectively.

Demo:

```
>>> elements = ('foo', 'bar', 'baz')
>>> for elem in elements:
...     print elem
... 
foo
bar
baz
>>> for count, elem in enumerate(elements):
...     print count, elem
... 
0 foo
1 bar
2 baz

```

By default, `enumerate()` starts counting at `0` but if you give it a second integer argument, it'll start from that number instead:

```
>>> for count, elem in enumerate(elements, 42):
...     print count, elem
... 
42 foo
43 bar
44 baz

```

If you were to re-implement `enumerate()` in Python, here are two ways of achieving that; one using [`itertools.count()`](https://docs.python.org/3/library/itertools.html#itertools.count) to do the counting, the other manually counting in a [generator function](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do):

```
from itertools import count

def enumerate(it, start=0):
    # return an iterator that adds a counter to each element of it
    return zip(count(start), it)

```

and

```
def enumerate(it, start=0):
    count = start
    for elem in it:
        yield (count, elem)
        count += 1

```

The [actual implementation in C](https://github.com/python/cpython/blob/master/Objects/enumobject.c) is closer to the latter, with optimisations to reuse a single tuple object for the common `for i, ...` unpacking case and using a standard C integer value for the counter until the counter becomes too large to avoid using a Python integer object (which is unbounded)."
"I got a message saying `script xyz.py returned exit code 0`. What does this mean?

What do the exit codes in Python mean? How many are there? Which ones are important?","You're looking for calls to `sys.exit()` (`exit()` calls `sys.exit()`) in the script. The argument to that method is returned to the environment as the exit code.

It's fairly likely that the script is never calling the *exit* method, and that 0 is the default exit code."
"What does `view()` do to the tensor `x`? What do negative values mean?

```
x = x.view(-1, 16 * 5 * 5)

```","`view()` reshapes the tensor without copying memory, similar to numpy's [`reshape()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html).

Given a tensor `a` with 16 elements:

```
import torch
a = torch.range(1, 16)

```

To reshape this tensor to make it a `4 x 4` tensor, use:

```
a = a.view(4, 4)

```

Now `a` will be a `4 x 4` tensor. *Note that after the reshape the total number of elements need to remain the same. Reshaping the tensor `a` to a `3 x 5` tensor would not be appropriate.*

### What is the meaning of parameter -1?

If there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (*Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1*). This is a way of telling the library: ""give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen"".

This can be seen in [this model definition code](https://stackoverflow.com/revisions/42479902/9). After the line `x = self.pool(F.relu(self.conv2(x)))` in the forward function, you will have a 16 depth feature map. You have to flatten this to give it to the fully connected layer. So you tell PyTorch to reshape the tensor you obtained to have specific number of columns and tell it to decide the number of rows by itself."
"If I do the following:

```
import subprocess
from cStringIO import StringIO
subprocess.Popen(['grep','f'],stdout=subprocess.PIPE,stdin=StringIO('one\ntwo\nthree\nfour\nfive\nsix\n')).communicate()[0]

```

I get:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in ?
  File ""/build/toolchain/mac32/python-2.4.3/lib/python2.4/subprocess.py"", line 533, in __init__
    (p2cread, p2cwrite,
  File ""/build/toolchain/mac32/python-2.4.3/lib/python2.4/subprocess.py"", line 830, in _get_handles
    p2cread = stdin.fileno()
AttributeError: 'cStringIO.StringI' object has no attribute 'fileno'

```

Apparently a cStringIO.StringIO object doesn't quack close enough to a file duck to suit subprocess.Popen. How do I work around this?","[`Popen.communicate()`](https://docs.python.org/3/library/subprocess.html?highlight=subprocess#subprocess.Popen.communicate) documentation:

> Note that if you want to send data to
> the processâ€™s stdin, you need to
> create the Popen object with
> stdin=PIPE. Similarly, to get anything
> other than None in the result tuple,
> you need to give stdout=PIPE and/or
> stderr=PIPE too.
>
> **Replacing os.popen\***

```
    pipe = os.popen(cmd, 'w', bufsize)
    # ==>
    pipe = Popen(cmd, shell=True, bufsize=bufsize, stdin=PIPE).stdin

```

> **Warning** Use communicate() rather than
> stdin.write(), stdout.read() or
> stderr.read() to avoid deadlocks due
> to any of the other OS pipe buffers
> filling up and blocking the child
> process.

So your example could be written as follows:

```
from subprocess import Popen, PIPE, STDOUT

p = Popen(['grep', 'f'], stdout=PIPE, stdin=PIPE, stderr=STDOUT)    
grep_stdout = p.communicate(input=b'one\ntwo\nthree\nfour\nfive\nsix\n')[0]
print(grep_stdout.decode())
# -> four
# -> five
# ->

```

---

On Python 3.5+ (3.6+ for `encoding`), you could use [`subprocess.run`](https://docs.python.org/3/library/subprocess.html#subprocess.run), to pass input as a string to an external command and get its exit status, and its output as a string back in one call:

```
#!/usr/bin/env python3
from subprocess import run, PIPE

p = run(['grep', 'f'], stdout=PIPE,
        input='one\ntwo\nthree\nfour\nfive\nsix\n', encoding='ascii')
print(p.returncode)
# -> 0
print(p.stdout)
# -> four
# -> five
# -> 

```"
"I've never used [SOAP](http://en.wikipedia.org/wiki/SOAP) before and I'm sort of new to Python. I'm doing this to get myself acquainted with both technologies. I've installed [SOAPlib](http://trac.optio.webfactional.com/wiki/soaplib) and I've tried to read their [Client](http://trac.optio.webfactional.com/wiki/Client) documentation, but I don't understand it too well. Is there anything else I can look into which is more suited for being a SOAP Client library for Python?

Edit: Just in case it helps, I'm using Python 2.6.","***Update (2016):***

If you only need SOAP client, there is well maintained library called [zeep](https://pypi.org/project/zeep/). It supports both Python 2 and 3 :)

---

***Update:***

Additionally to what is mentioned above, I will refer to [Python WebServices](https://wiki.python.org/moin/WebServices#SOAP) page which is always up-to-date with all actively maintained and recommended modules to **SOAP** and all other webservice types.

---

Unfortunately, at the moment, I don't think there is a ""best"" Python SOAP library. Each of the mainstream ones available has its own pros and cons.

Older libraries:

* [SOAPy](http://soapy.sourceforge.net): Was the ""best,"" but no longer maintained. Does not work on Python 2.5+
* [ZSI](http://pywebsvcs.sourceforge.net): Very painful to use, and development is slow. Has a module called ""SOAPpy"", which is different than SOAPy (above).

""Newer"" libraries:

* [SUDS](https://fedorahosted.org/suds): Very Pythonic, and easy to create WSDL-consuming SOAP clients. Creating SOAP servers is a little bit more difficult. (This package does not work with Python3. For Python3 see SUDS-py3)
* [SUDS-py3](https://pypi.org/project/suds-py3/): The Python3 version of SUDS
* [spyne](https://github.com/arskom/spyne): Creating servers is easy, creating clients a little bit more challenging. Documentation is somewhat lacking.
* [ladon](http://pypi.python.org/pypi/ladon): Creating servers is much like in soaplib (using a decorator). Ladon exposes more interfaces than SOAP at the same time without extra user code needed.
* [pysimplesoap](http://code.google.com/p/pysimplesoap/): very lightweight but useful for both client and server - includes a web2py server integration that ships with web2py.
* [SOAPpy](https://github.com/kiorky/SOAPpy): Distinct from the abandoned SOAPpy that's hosted at the ZSI link above, this version was actually maintained until 2011, now it seems to be abandoned too.
* [soaplib](http://www.python.org/pypi/soaplib): Easy to use python library for writing and calling soap web services. Webservices written with soaplib are simple, lightweight, work well with other SOAP implementations, and can be deployed as WSGI applications.
* [osa](https://bitbucket.org/sboz/osa): A fast/slim easy to use SOAP python client library.

Of the above, I've only used SUDS personally, and I liked it a lot."
"Assume I have a program that uses `argparse` to process command line arguments/options. The following will print the 'help' message:

```
./myprogram -h

```

or:

```
./myprogram --help

```

But, if I run the script without any arguments whatsoever, it doesn't do anything. What I want it to do is to display the usage message when it is called with no arguments. How is that done?","This answer comes from Steven Bethard [on Google groups](http://groups.google.com/group/argparse-users/browse_thread/thread/2dacd5fed110bd0c?pli=1). I'm reposting it here to make it easier for people without a Google account to access.

You can override the default behavior of the `error` method:

```
import argparse
import sys

class MyParser(argparse.ArgumentParser):
    def error(self, message):
        sys.stderr.write('error: %s\n' % message)
        self.print_help()
        sys.exit(2)

parser = MyParser()
parser.add_argument('foo', nargs='+')
args = parser.parse_args()

```

---

Note that the above solution will print the help message whenever the `error`
method is triggered. For example, `test.py --blah` will print the help message
too if `--blah` isn't a valid option.

If you want to print the help message only if no arguments are supplied on the
command line, then perhaps this is still the easiest way:

```
import argparse
import sys

parser=argparse.ArgumentParser()
parser.add_argument('foo', nargs='+')
if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(1)
args=parser.parse_args()

```

---

Note that `parser.print_help()` prints to stdout by default. As [init\_js suggests](https://stackoverflow.com/questions/4042452/display-help-message-with-python-argparse-when-script-is-called-without-any-argu/4042861?noredirect=1#comment84996345_4042861), use `parser.print_help(sys.stderr)` to print to stderr."
"In a comment on [this question](https://stackoverflow.com/questions/2079786/caching-sitemaps-in-django), I saw a statement that recommended using

```
result is not None

```

vs

```
result != None

```

What is the difference? And why might one be recommended over the other?","`==` is an **equality test**. It checks whether the right hand side and the left hand side are equal objects (according to their `__eq__` or `__cmp__` methods.)

`is` is an **identity test**. It checks whether the right hand side and the left hand side are the very same object. No methodcalls are done, objects can't influence the `is` operation.

You use `is` (and `is not`) for singletons, like `None`, where you don't care about objects that might want to pretend to be `None` or where you want to protect against objects breaking when being compared against `None`."
"I have a python `datetime` instance that was created using `datetime.utcnow()` and persisted in database.

For display, I would like to convert the `datetime` instance retrieved from the database to local `datetime` using the default local timezone (i.e., as if the `datetime` was created using `datetime.now()`).

How can I convert the UTC `datetime` to a local `datetime` using only python standard library (e.g., no `pytz` dependency)?

It seems one solution would be to use `datetime.astimezone(tz)`, but how would you get the default local timezone?","In Python 3.3+:

```
from datetime import datetime, timezone

def utc_to_local(utc_dt):
    return utc_dt.replace(tzinfo=timezone.utc).astimezone(tz=None)

```

In Python 2/3:

```
import calendar
from datetime import datetime, timedelta

def utc_to_local(utc_dt):
    # get integer timestamp to avoid precision lost
    timestamp = calendar.timegm(utc_dt.timetuple())
    local_dt = datetime.fromtimestamp(timestamp)
    assert utc_dt.resolution >= timedelta(microseconds=1)
    return local_dt.replace(microsecond=utc_dt.microsecond)

```

Using `pytz` (both Python 2/3):

```
import pytz

local_tz = pytz.timezone('Europe/Moscow') # use your local timezone name here
# NOTE: pytz.reference.LocalTimezone() would produce wrong result here

## You could use `tzlocal` module to get local timezone on Unix and Win32
# from tzlocal import get_localzone # $ pip install tzlocal

# # get local timezone    
# local_tz = get_localzone()

def utc_to_local(utc_dt):
    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)
    return local_tz.normalize(local_dt) # .normalize might be unnecessary

```

### Example

```
def aslocaltimestr(utc_dt):
    return utc_to_local(utc_dt).strftime('%Y-%m-%d %H:%M:%S.%f %Z%z')

print(aslocaltimestr(datetime(2010,  6, 6, 17, 29, 7, 730000)))
print(aslocaltimestr(datetime(2010, 12, 6, 17, 29, 7, 730000)))
print(aslocaltimestr(datetime.utcnow()))

```

### Output

Python 3.3

```
2010-06-06 21:29:07.730000 MSD+0400
2010-12-06 20:29:07.730000 MSK+0300
2012-11-08 14:19:50.093745 MSK+0400

```

Python 2

```
2010-06-06 21:29:07.730000 
2010-12-06 20:29:07.730000 
2012-11-08 14:19:50.093911 

```

pytz

```
2010-06-06 21:29:07.730000 MSD+0400
2010-12-06 20:29:07.730000 MSK+0300
2012-11-08 14:19:50.146917 MSK+0400

```

Note: it takes into account DST and the recent change of utc offset for MSK timezone.

I don't know whether non-pytz solutions work on Windows."
"I need to know if a variable in Python is a string or a dict. Is there anything wrong with the following code?

```
if type(x) == type(str()):
    do_something_with_a_string(x)
elif type(x) == type(dict()):
    do_somethting_with_a_dict(x)
else:
    raise ValueError

```

**Update**: I accepted avisser's answer (though I will change my mind if someone explains why `isinstance` is preferred over `type(x) is`).

But thanks to nakedfanatic for reminding me that it's often cleaner to use a dict (as a case statement) than an if/elif/else series.

Let me elaborate on my use case. If a variable is a string, I need to put it in a list. If it's a dict, I need a list of the unique values. Here's what I came up with:

```
def value_list(x):
    cases = {str: lambda t: [t],
             dict: lambda t: list(set(t.values()))}
    try:
        return cases[type(x)](x)
    except KeyError:
        return None

```

If `isinstance` is preferred, how would you write this `value_list()` function?","What happens if somebody passes a unicode string to your function? Or a class derived from dict? Or a class implementing a dict-like interface? Following code covers first two cases. If you are using Python 2.6 you might want to use [`collections.Mapping`](https://docs.python.org/2/library/collections.html#collections.Mapping) instead of `dict` as per the [ABC PEP](http://www.python.org/dev/peps/pep-3119/).

```
def value_list(x):
    if isinstance(x, dict):
        return list(set(x.values()))
    elif isinstance(x, basestring):
        return [x]
    else:
        return None

```"
"According to the documentation, they're pretty much interchangeable. Is there a stylistic reason to use one over the other?","I like to use double quotes around strings that are used for interpolation or that are natural language messages, and single quotes for small symbol-like strings, but will break the rules if the strings contain quotes, or if I forget. I use triple double quotes for docstrings and raw string literals for regular expressions even if they aren't needed.

For example:

```
LIGHT_MESSAGES = {
    'English': ""There are %(number_of_lights)s lights."",
    'Pirate':  ""Arr! Thar be %(number_of_lights)s lights.""
}

def lights_message(language, number_of_lights):
    """"""Return a language-appropriate string reporting the light count.""""""
    return LIGHT_MESSAGES[language] % locals()

def is_pirate(message):
    """"""Return True if the given message sounds piratical.""""""
    return re.search(r""(?i)(arr|avast|yohoho)!"", message) is not None

```"
"Python allows easy creation of an integer from a string of a given base via

```
int(str, base). 

```

I want to perform the inverse: **creation of a string from an integer**,
i.e. I want some function `int2base(num, base)`, such that:

```
int(int2base(x, b), b) == x

```

The function name/argument order is unimportant.

For any number `x` and base `b` that `int()` will accept.

This is an easy function to write: in fact it's easier than describing it in this question. However, I feel like I must be missing something.

I know about the functions `bin`, `oct`, `hex`, but I cannot use them for a few reasons:

* Those functions are not available on older versions of Python, with which I need compatibility with (2.2)
* I want a general solution that can be called the same way for different bases
* I want to allow bases other than 2, 8, 16

### Related

* [Python elegant inverse function of int(string, base)](https://stackoverflow.com/questions/2063425/python-elegant-inverse-function-of-intstring-base)
* [Integer to base-x system using recursion in python](https://stackoverflow.com/questions/2088201/integer-to-base-x-system-using-recursion-in-python)
* [Base 62 conversion in Python](https://stackoverflow.com/questions/1119722/base-62-conversion-in-python)
* [How to convert an integer to the shortest url-safe string in Python?](https://stackoverflow.com/questions/561486/how-to-convert-an-integer-to-the-shortest-url-safe-string-in-python)","Surprisingly, people were giving only solutions that convert to small bases (smaller than the length of the English alphabet). There was no attempt to give a solution which converts to any arbitrary base from 2 to infinity.

So here is a super simple solution:

```
def numberToBase(n, b):
    if n == 0:
        return [0]
    digits = []
    while n:
        digits.append(int(n % b))
        n //= b
    return digits[::-1]

```

so if you need to convert some super huge number to the base `577`,

`numberToBase(67854 ** 15 - 102, 577)`, will give you a correct solution:
`[4, 473, 131, 96, 431, 285, 524, 486, 28, 23, 16, 82, 292, 538, 149, 25, 41, 483, 100, 517, 131, 28, 0, 435, 197, 264, 455]`,

Which you can later convert to any base you want

1. at some point of time you will notice that sometimes there is no built-in library function to do things that you want, so you need to write your own. If you disagree, post you own solution with a built-in function which can convert a base 10 number to base 577.
2. this is due to lack of understanding what a number in some base means.
3. I encourage you to think for a little bit why base in your method works only for n <= 36. Once you are done, it will be obvious why my function returns a list and has the signature it has."
"The book that I am reading on Python repeatedly shows code like `eval(input('blah'))`.

How exactly does this modify the result from calling `input`?

---

**See also:** [Why is using 'eval' a bad practice?](https://stackoverflow.com/q/1832940) to understand the **critical security risks** created by using `eval` or `exec` on untrusted input (i.e.: anything that is even partially under the user's control, rather than the program's control).

**See also:** [How can I sandbox Python in pure Python?](https://stackoverflow.com/q/3068139). The short version is that doing this properly will always be harder than choosing a proper tool instead of `eval` or `exec`.

See [Using python's eval() vs. ast.literal\_eval()](https://stackoverflow.com/questions/15197673) for a potentially safer technique.

See [How do I use raw\_input in Python 3?](https://stackoverflow.com/questions/954834) for background context on why a book might have contained code like this, or why OP might originally have expected an `input` result not to require further processing.","The eval function lets a Python program run Python code within itself.

eval example (interactive shell):

```
>>> x = 1
>>> eval('x + 1')
2
>>> eval('x')
1

```"
"I'm using python3.3 and I'm having a cryptic error when trying to pickle a simple dictionary.

Here is the code:

```
import os
import pickle
from pickle import *
os.chdir('c:/Python26/progfiles/')

def storvars(vdict):      
    f = open('varstor.txt','w')
    pickle.dump(vdict,f,)
    f.close()
    return

mydict = {'name':'john','gender':'male','age':'45'}
storvars(mydict)

```

and I get:

```
Traceback (most recent call last):
  File ""C:/Python26/test18.py"", line 31, in <module>
    storvars(mydict)
  File ""C:/Python26/test18.py"", line 14, in storvars
    pickle.dump(vdict,f,)
TypeError: must be str, not bytes

```","The output file needs to be opened in binary mode:

```
f = open('varstor.txt','w')

```

needs to be:

```
f = open('varstor.txt','wb')

```"
"I have two numpy arrays of different shapes, but with the same length (leading dimension). I want to shuffle each of them, such that corresponding elements continue to correspond -- i.e. shuffle them in unison with respect to their leading indices.

This code works, and illustrates my goals:

```
def shuffle_in_unison(a, b):
    assert len(a) == len(b)
    shuffled_a = numpy.empty(a.shape, dtype=a.dtype)
    shuffled_b = numpy.empty(b.shape, dtype=b.dtype)
    permutation = numpy.random.permutation(len(a))
    for old_index, new_index in enumerate(permutation):
        shuffled_a[new_index] = a[old_index]
        shuffled_b[new_index] = b[old_index]
    return shuffled_a, shuffled_b

```

For example:

```
>>> a = numpy.asarray([[1, 1], [2, 2], [3, 3]])
>>> b = numpy.asarray([1, 2, 3])
>>> shuffle_in_unison(a, b)
(array([[2, 2],
       [1, 1],
       [3, 3]]), array([2, 1, 3]))

```

However, this feels clunky, inefficient, and slow, and it requires making a copy of the arrays -- I'd rather shuffle them in-place, since they'll be quite large.

Is there a better way to go about this? Faster execution and lower memory usage are my primary goals, but elegant code would be nice, too.

One other thought I had was this:

```
def shuffle_in_unison_scary(a, b):
    rng_state = numpy.random.get_state()
    numpy.random.shuffle(a)
    numpy.random.set_state(rng_state)
    numpy.random.shuffle(b)

```

This works...but it's a little scary, as I see little guarantee it'll continue to work -- it doesn't look like the sort of thing that's guaranteed to survive across numpy version, for example.","Your can use NumPy's [array indexing](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.indexing.html):

```
def unison_shuffled_copies(a, b):
    assert len(a) == len(b)
    p = numpy.random.permutation(len(a))
    return a[p], b[p]

```

This will result in creation of separate unison-shuffled arrays."
"How are ""keyword arguments"" different from regular arguments? Can't all arguments be passed as `name=value` instead of using positional syntax?","There are two related concepts, both called ""*keyword arguments*"".

On the calling side, which is what other commenters have mentioned, you have the ability to specify some function arguments by name. You have to mention them after all of the arguments without names (*positional arguments*), and there must be *default values* for any parameters which were not mentioned at all.

The other concept is on the function definition side: you can define a function that takes parameters by name -- and you don't even have to specify what those names are. These are pure *keyword arguments*, and can't be passed positionally. The syntax is

```
def my_function(arg1, arg2, **kwargs)

```

Any *keyword arguments* you pass into this function will be placed into a dictionary named `kwargs`. You can examine the keys of this dictionary at run-time, like this:

```
def my_function(**kwargs):
    print str(kwargs)

my_function(a=12, b=""abc"")

{'a': 12, 'b': 'abc'}

```"
"How can I get the output of a process run using `subprocess.call()`?

Passing a `StringIO.StringIO` object to `stdout` gives this error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/subprocess.py"", line 444, in call
    return Popen(*popenargs, **kwargs).wait()
  File ""/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/subprocess.py"", line 588, in __init__
    errread, errwrite) = self._get_handles(stdin, stdout, stderr)
  File ""/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/subprocess.py"", line 945, in _get_handles
    c2pwrite = stdout.fileno()
AttributeError: StringIO instance has no attribute 'fileno'

```","If you have Python version 2.7 or later, you can use [`subprocess.check_output`](http://docs.python.org/library/subprocess.html#subprocess.check_output) which basically does exactly what you want (it returns [standard output](https://en.wikipedia.org/wiki/Standard_streams#Standard_output_.28stdout.29) as a string).

A simple example (Linux version; see the note):

```
import subprocess

print subprocess.check_output([""ping"", ""-c"", ""1"", ""8.8.8.8""])

```

Note that the [ping](https://en.wikipedia.org/wiki/Ping_%28networking_utility%29) command is using the Linux notation (`-c` for count). If you try this on Windows, remember to change it to `-n` for the same result.

As commented below, you can find a more detailed explanation in [this other answer](https://stackoverflow.com/a/8235171/881224)."
"How to convert a string in the format `""%d/%m/%Y""` to timestamp?

```
""01/12/2011"" -> 1322697600

```","```
>>> import time
>>> import datetime
>>> s = ""01/12/2011""
>>> time.mktime(datetime.datetime.strptime(s, ""%d/%m/%Y"").timetuple())
1322697600.0

```"
"I am plotting the same type of information, but for different countries, with multiple subplots with Matplotlib. That is, I have nine plots on a 3x3 grid, all with the same for lines (of course, different values per line).

However, I have not figured out how to put a single legend (since all nine subplots have the same lines) on the figure just once.

How do I do that?","There is also a nice function [`get_legend_handles_labels()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.get_legend_handles_labels.html) you can call on the last axis (if you iterate over them) that would collect everything you need from `label=` arguments:

```
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='upper center')

```

If the `pyplot` interface is being used instead of the `Axes` interface, use:

```
handles, labels = plt.gca().get_legend_handles_labels()

```

To remove legends from subplots, see [Remove the legend on a matplotlib figure](https://stackoverflow.com/q/5735208/7758804).

To merge `twinx` legends, see [Secondary axis with twinx(): how to add to legend](https://stackoverflow.com/q/5484922/7758804)."
"I want to print the whole dataframe, but I don't want to print the index

Besides, one column is datetime type, I just want to print time, not date.

The dataframe looks like:

```
   User ID           Enter Time   Activity Number
0      123  2014-07-08 00:09:00              1411
1      123  2014-07-08 00:18:00               893
2      123  2014-07-08 00:49:00              1041

```

I want it print as

```
User ID   Enter Time   Activity Number
123         00:09:00              1411
123         00:18:00               893
123         00:49:00              1041

```","```
print(df.to_string(index=False))

```"
How to convert a tensor into a numpy array when using Tensorflow with Python bindings?,"**TensorFlow 2.x**
==================

[Eager Execution](https://www.tensorflow.org/guide/eager) is enabled by default, so just call [**`.numpy()`**](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1042-L1067) on the Tensor object.

```
import tensorflow as tf

a = tf.constant([[1, 2], [3, 4]])                 
b = tf.add(a, 1)

a.numpy()
# array([[1, 2],
#        [3, 4]], dtype=int32)

b.numpy()
# array([[2, 3],
#        [4, 5]], dtype=int32)

tf.multiply(a, b).numpy()
# array([[ 2,  6],
#        [12, 20]], dtype=int32)

```

See [NumPy Compatibility](https://www.tensorflow.org/tutorials/customization/basics) for more. It is worth noting (from the docs),

> Numpy array may share a memory with the Tensor object. **Any changes to one may be reflected in the other.**

Bold emphasis mine. A copy may or may not be returned, and this is an implementation detail based on whether the data is in CPU or GPU (in the latter case, a copy has to be made from GPU to host memory).

**But why am I getting the `AttributeError: 'Tensor' object has no attribute 'numpy'`?**.  
A lot of folks have commented about this issue, there are a couple of possible reasons:

* TF 2.0 is not correctly installed (in which case, try re-installing), or
* TF 2.0 is installed, but eager execution is disabled for some reason. In such cases, call [`tf.compat.v1.enable_eager_execution()`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_eager_execution) to enable it, or see below.

---

If Eager Execution is disabled, you can build a graph and then run it through `tf.compat.v1.Session`:

```
a = tf.constant([[1, 2], [3, 4]])                 
b = tf.add(a, 1)
out = tf.multiply(a, b)

out.eval(session=tf.compat.v1.Session())    
# array([[ 2,  6],
#        [12, 20]], dtype=int32)
```

See also [TF 2.0 Symbols Map](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0) for a mapping of the old API to the new one."
"How do I convert a Pandas dataframe into a NumPy array?

```
import numpy as np
import pandas as pd

df = pd.DataFrame(
    {
        'A': [np.nan, np.nan, np.nan, 0.1, 0.1, 0.1, 0.1],
        'B': [0.2, np.nan, 0.2, 0.2, 0.2, np.nan, np.nan],
        'C': [np.nan, 0.5, 0.5, np.nan, 0.5, 0.5, np.nan],
    },
    index=[1, 2, 3, 4, 5, 6, 7],
).rename_axis('ID')

```

That gives this DataFrame:

```
      A    B    C
ID                                 
1   NaN  0.2  NaN
2   NaN  NaN  0.5
3   NaN  0.2  0.5
4   0.1  0.2  NaN
5   0.1  0.2  0.5
6   0.1  NaN  0.5
7   0.1  NaN  NaN

```

I would like to convert this to a NumPy array, like so:

```
array([[ nan,  0.2,  nan],
       [ nan,  nan,  0.5],
       [ nan,  0.2,  0.5],
       [ 0.1,  0.2,  nan],
       [ 0.1,  0.2,  0.5],
       [ 0.1,  nan,  0.5],
       [ 0.1,  nan,  nan]])

```

---

Also, is it possible to preserve the dtypes, like this?

```
array([[ 1, nan,  0.2,  nan],
       [ 2, nan,  nan,  0.5],
       [ 3, nan,  0.2,  0.5],
       [ 4, 0.1,  0.2,  nan],
       [ 5, 0.1,  0.2,  0.5],
       [ 6, 0.1,  nan,  0.5],
       [ 7, 0.1,  nan,  nan]],
     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])

```","Use `df.to_numpy()`
===================

It's better than `df.values`, here's why.\*

It's time to deprecate your usage of `values` and `as_matrix()`.

pandas v0.24.0 introduced two new methods for obtaining NumPy arrays from pandas objects:

1. **`to_numpy()`**, which is defined on `Index`, `Series`, and `DataFrame` objects, and
2. **`array`**, which is defined on `Index` and `Series` objects only.

If you visit the v0.24 docs for [`.values`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html#pandas.DataFrame.values), you will see a big red warning that says:

> ### Warning: We recommend using `DataFrame.to_numpy()` instead.

See [this section of the v0.24.0 release notes](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html#accessing-the-values-in-a-series-or-index), and [this answer](https://stackoverflow.com/a/54324513/4909087) for more information.

\* - `to_numpy()` is my recommended method for any production code that needs to run reliably for many versions into the future. However if you're just making a scratchpad in jupyter or the terminal, using `.values` to save a few milliseconds of typing is a permissable exception. You can always add the fit n finish later.

---



---

**Towards Better Consistency: [`to_numpy()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html)**
-----------------------------------------------------------------------------------------------------------------------------------------

In the spirit of better consistency throughout the API, a new method `to_numpy` has been introduced to extract the underlying NumPy array from DataFrames.

```
# Setup
df = pd.DataFrame(data={'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}, 
                  index=['a', 'b', 'c'])

# Convert the entire DataFrame
df.to_numpy()
# array([[1, 4, 7],
#        [2, 5, 8],
#        [3, 6, 9]])

# Convert specific columns
df[['A', 'C']].to_numpy()
# array([[1, 7],
#        [2, 8],
#        [3, 9]])

```

As mentioned above, this method is also defined on `Index` and `Series` objects (see [here](https://stackoverflow.com/a/54324513/4909087)).

```
df.index.to_numpy()
# array(['a', 'b', 'c'], dtype=object)

df['A'].to_numpy()
#  array([1, 2, 3])

```

By default, a view is returned, so any modifications made will affect the original.

```
v = df.to_numpy()
v[0, 0] = -1
 
df
   A  B  C
a -1  4  7
b  2  5  8
c  3  6  9

```

If you need a copy instead, use `to_numpy(copy=True)`.

---

### pandas >= 1.0 update for ExtensionTypes

If you're using pandas 1.x, chances are you'll be dealing with extension types a lot more. You'll have to be a little more careful that these extension types are correctly converted.

```
a = pd.array([1, 2, None], dtype=""Int64"")                                  
a                                                                          

<IntegerArray>
[1, 2, <NA>]
Length: 3, dtype: Int64 

# Wrong
a.to_numpy()                                                               
# array([1, 2, <NA>], dtype=object)  # yuck, objects

# Correct
a.to_numpy(dtype='float', na_value=np.nan)                                 
# array([ 1.,  2., nan])

# Also correct
a.to_numpy(dtype='int', na_value=-1)
# array([ 1,  2, -1])

```

This is [called out in the docs](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v1.0.0.html#arrays-integerarray-now-uses-pandas-na).

---

### If you need the `dtypes` in the result...

As shown in another answer, [`DataFrame.to_records`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_records.html#pandas-dataframe-to-records) is a good way to do this.

```
df.to_records()
# rec.array([('a', 1, 4, 7), ('b', 2, 5, 8), ('c', 3, 6, 9)],
#           dtype=[('index', 'O'), ('A', '<i8'), ('B', '<i8'), ('C', '<i8')])

```

This cannot be done with `to_numpy`, unfortunately. However, as an alternative, you can use `np.rec.fromrecords`:

```
v = df.reset_index()
np.rec.fromrecords(v, names=v.columns.tolist())
# rec.array([('a', 1, 4, 7), ('b', 2, 5, 8), ('c', 3, 6, 9)],
#           dtype=[('index', '<U1'), ('A', '<i8'), ('B', '<i8'), ('C', '<i8')])

```

Performance wise, it's nearly the same (actually, using `rec.fromrecords` is a bit faster).

```
df2 = pd.concat([df] * 10000)

%timeit df2.to_records()
%%timeit
v = df2.reset_index()
np.rec.fromrecords(v, names=v.columns.tolist())

12.9 ms ± 511 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
9.56 ms ± 291 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

```

---



---

**Rationale for Adding a New Method**
-------------------------------------

`to_numpy()` (in addition to `array`) was added as a result of discussions under two GitHub issues [GH19954](https://github.com/pandas-dev/pandas/issues/19954) and [GH23623](https://github.com/pandas-dev/pandas/issues/23623).

Specifically, [the docs](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html#accessing-the-values-in-a-series-or-index) mention the rationale:

> [...] with `.values` it was unclear whether the returned value would be the
> actual array, some transformation of it, or one of pandas custom
> arrays (like `Categorical`). For example, with `PeriodIndex`, `.values`
> generates a new `ndarray` of period objects each time. [...]

`to_numpy` aims to improve the consistency of the API, which is a major step in the right direction. `.values` will not be deprecated in the current version, but I expect this may happen at some point in the future, so I would urge users to migrate towards the newer API, as soon as you can.

---



---

**Critique of Other Solutions**
-------------------------------

`DataFrame.values` has inconsistent behaviour, as already noted.

`DataFrame.get_values()` was [quietly removed in v1.0](https://github.com/pandas-dev/pandas/pull/29989) and was previously deprecated in v0.25. Before that, it was simply a wrapper around `DataFrame.values`, so everything said above applies.

`DataFrame.as_matrix()` was removed in v1.0 and was previously deprecated in v0.23. Do **NOT** use!"
"I need to choose some elements from the given list, knowing their index. Let say I would like to create a new list, which contains element with index 1, 2, 5, from given list [-2, 1, 5, 3, 8, 5, 6]. What I did is:

```
a = [-2,1,5,3,8,5,6]
b = [1,2,5]
c = [ a[i] for i in b]

```

Is there any better way to do it? something like c = a[b] ?","You can use [`operator.itemgetter`](https://docs.python.org/3/library/operator.html#operator.itemgetter):

```
from operator import itemgetter 
a = [-2, 1, 5, 3, 8, 5, 6]
b = [1, 2, 5]
print(itemgetter(*b)(a))
# Result:
(1, 5, 5)

```

Or you can use [numpy](http://www.numpy.org/):

```
import numpy as np
a = np.array([-2, 1, 5, 3, 8, 5, 6])
b = [1, 2, 5]
print(list(a[b]))
# Result:
[1, 5, 5]

```

---

But really, your current solution is fine. It's probably the neatest out of all of them."
"I want to import a function from another file in the same directory.

Usually, one of the following works:

```
from .mymodule import myfunction

```

```
from mymodule import myfunction

```

...but the other one gives me one of these errors:

```
ImportError: attempted relative import with no known parent package

```

```
ModuleNotFoundError: No module named 'mymodule'

```

```
SystemError: Parent module '' not loaded, cannot perform relative import

```

Why is this?","> unfortunately, this module needs to be inside the package, and it also
> needs to be runnable as a script, sometimes. Any idea how I could
> achieve that?

It's quite common to have a layout like this...

```
main.py
mypackage/
    __init__.py
    mymodule.py
    myothermodule.py

```

...with a `mymodule.py` like this...

```
#!/usr/bin/env python3

# Exported function
def as_int(a):
    return int(a)

# Test function for module  
def _test():
    assert as_int('1') == 1

if __name__ == '__main__':
    _test()

```

...a `myothermodule.py` like this...

```
#!/usr/bin/env python3

from .mymodule import as_int

# Exported function
def add(a, b):
    return as_int(a) + as_int(b)

# Test function for module  
def _test():
    assert add('1', '1') == 2

if __name__ == '__main__':
    _test()

```

...and a `main.py` like this...

```
#!/usr/bin/env python3

from mypackage.myothermodule import add

def main():
    print(add('1', '1'))

if __name__ == '__main__':
    main()

```

...which works fine when you run `main.py` or `mypackage/mymodule.py`, but fails with `mypackage/myothermodule.py`, due to the relative import...

```
from .mymodule import as_int

```

The way you're supposed to run it is by using the -m option and giving the path in the Python module system (rather than in the filesystem)...

```
python3 -m mypackage.myothermodule

```

...but it's somewhat verbose, and doesn't mix well with a shebang line like `#!/usr/bin/env python3`.

An alternative is to avoid using relative imports, and just use...

```
from mypackage.mymodule import as_int

```

Either way, you'll need to run from the parent of `mypackage`, or add that directory to `PYTHONPATH` (either one will ensure that `mypackage` is in the sys.path [module search path](https://docs.python.org/3/library/sys_path_init.html)). Or, if you want it to work ""out of the box"", you can frob the `PYTHONPATH` in code first with this...

```
import sys
import os

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.dirname(SCRIPT_DIR))

from mypackage.mymodule import as_int

```

It's kind of a pain, but there's a clue as to why in [an email](http://mail.python.org/pipermail/python-3000/2007-April/006793.html) written by a certain Guido van Rossum...

> I'm -1 on this and on any other proposed twiddlings of the `__main__`
> machinery. The only use case seems to be running scripts that happen
> to be living inside a module's directory, which I've always seen as an
> antipattern. To make me change my mind you'd have to convince me that
> it isn't.

Whether running scripts inside a package is an antipattern or not is subjective, but personally I find it really useful in a package I have which contains some custom wxPython widgets, so I can run the script for any of the source files to display a `wx.Frame` containing only that widget for testing purposes."
How do I create a virtual environment for a specified version of Python?,"**NOTE:** For **Python 3.3+**, see The Aelfinn's [answer](https://stackoverflow.com/a/39713544/1450294) below.

---

Use the `--python` (or short `-p`) option when creating a virtualenv instance to specify the Python executable you want to use, e.g.:

```
virtualenv --python=""/usr/bin/python2.6"" ""/path/to/new/virtualenv/""

```"
"I have a command line program in Python that takes a while to finish. I want to know the exact time it takes to finish running.

I've looked at the `timeit` module, but it seems it's only for small snippets of code. I want to time the whole program.","The simplest way in Python:

```
import time
start_time = time.time()
main()
print(""--- %s seconds ---"" % (time.time() - start_time))

```

This assumes that your program takes at least a tenth of second to run.

Prints:

```
--- 0.764891862869 seconds ---

```"
"How do I generate a string of size N, made of numbers and uppercase English letters such as:

* 6U1S75
* 4Z4UKK
* U911K4","**Answer in one line:**

```
''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(N))

```

or even shorter starting with Python 3.6 using [`random.choices()`](https://docs.python.org/3/library/random.html#random.choices):

```
''.join(random.choices(string.ascii_uppercase + string.digits, k=N))

```

**A cryptographically more secure version:** [see this post](https://stackoverflow.com/a/23728630/2213647)

```
''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(N))

```

**In details, with a clean function for further reuse:**

```
>>> import string
>>> import random
>>> def id_generator(size=6, chars=string.ascii_uppercase + string.digits):
...    return ''.join(random.choice(chars) for _ in range(size))
...
>>> id_generator()
'G5G74W'
>>> id_generator(3, ""6793YUIO"")
'Y3U'

```

**How does it work ?**

We import `string`, a module that contains sequences of common ASCII characters, and `random`, a module that deals with random generation.

`string.ascii_uppercase + string.digits` just concatenates the list of characters representing uppercase ASCII chars and digits:

```
>>> string.ascii_uppercase
'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
>>> string.digits
'0123456789'
>>> string.ascii_uppercase + string.digits
'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'

```

Then we use a list comprehension to create a list of 'n' elements:

```
>>> range(4) # range create a list of 'n' numbers
[0, 1, 2, 3]
>>> ['elem' for _ in range(4)] # we use range to create 4 times 'elem'
['elem', 'elem', 'elem', 'elem']

```

In the example above, we use `[` to create the list, but we don't in the `id_generator` function so Python doesn't create the list in memory, but generates the elements on the fly, one by one (more about this [here](https://stackoverflow.com/questions/231767/the-python-yield-keyword-explained/231855#231855)).

Instead of asking to create 'n' times the string `elem`, we will ask Python to create 'n' times a random character, picked from a sequence of characters:

```
>>> random.choice(""abcde"")
'a'
>>> random.choice(""abcde"")
'd'
>>> random.choice(""abcde"")
'b'

```

Therefore `random.choice(chars) for _ in range(size)` really is creating a sequence of `size` characters. Characters that are randomly picked from `chars`:

```
>>> [random.choice('abcde') for _ in range(3)]
['a', 'b', 'b']
>>> [random.choice('abcde') for _ in range(3)]
['e', 'b', 'e']
>>> [random.choice('abcde') for _ in range(3)]
['d', 'a', 'c']

```

Then we just join them with an empty string so the sequence becomes a string:

```
>>> ''.join(['a', 'b', 'b'])
'abb'
>>> [random.choice('abcde') for _ in range(3)]
['d', 'c', 'b']
>>> ''.join(random.choice('abcde') for _ in range(3))
'dac'

```"
How do I read from standard input ([`stdin`](https://en.wikipedia.org/wiki/Standard_streams#Standard_input_(stdin)))?,"Use the [`fileinput`](http://docs.python.org/library/fileinput.html) module:

```
import fileinput

for line in fileinput.input():
    pass

```

[`fileinput`](http://docs.python.org/library/fileinput.html) will loop through all the lines in the input specified as file names given in command-line arguments, or the standard input if no arguments are provided.

Note: `line` will contain a trailing newline; to remove it use [`line.rstrip()`](https://docs.python.org/3/library/stdtypes.html#str.rstrip)."
"There are several ways to write to stderr:

```
print >> sys.stderr, ""spam""  # Python 2 only.

sys.stderr.write(""spam\n"")

os.write(2, b""spam\n"")

from __future__ import print_function
print(""spam"", file=sys.stderr)

```

What are the differences between these methods? Which method should be preferred?","I found this to be the only one short, flexible, portable and readable:

```
import sys

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

```

The optional function `eprint` saves some repetition. It can be used in the same way as the standard `print` function:

```
>>> print(""Test"")
Test
>>> eprint(""Test"")
Test
>>> eprint(""foo"", ""bar"", ""baz"", sep=""---"")
foo---bar---baz

```"
"This displays the figure in a GUI:

```
import matplotlib.pyplot as plt
plt.plot([1, 2, 3], [1, 4, 9])
plt.show()

```

But how do I instead save the figure to a file (e.g. foo.png)?","When using [`matplotlib.pyplot.savefig`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html), the file format can be specified by the extension:

```
from matplotlib import pyplot as plt

plt.savefig('foo.png')
plt.savefig('foo.pdf')

```

That gives a rasterized or vectorized output respectively.
In addition, there is sometimes undesirable whitespace around the image, which can be removed with:

```
plt.savefig('foo.png', bbox_inches='tight')

```

Note that if showing the plot, `plt.show()` should follow `plt.savefig()`; otherwise, the file image will be blank."
"How do I select columns `a` and `b` from `df`, and save them into a new dataframe `df1`?

```
index  a   b   c
1      2   3   4
2      3   4   5

```

Unsuccessful attempt:

```
df1 = df['a':'b']
df1 = df.ix[:, 'a':'b']

```","The column names (which are strings) cannot be sliced in the manner you tried.

Here you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the [`__getitem__` syntax](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#basics) (the []'s).

```
df1 = df[['a', 'b']]

```

Alternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:

```
df1 = df.iloc[:, 0:2] # Remember that Python does not slice inclusive of the ending index.

```

Additionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices).

Sometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object. This will happen with the second way of indexing, so you can modify it with the `.copy()` method to get a regular copy. When this happens, changing what you think is the sliced object can sometimes alter the original object. Always good to be on the look out for this.

```
df1 = df.iloc[0, 0:2].copy() # To avoid the case where changing df1 also changes df

```

To use `iloc`, you need to know the column positions (or indices). As the column positions may change, instead of hard-coding indices, you can use `iloc` along with `get_loc` function of `columns` method of dataframe object to obtain column indices.

```
{df.columns.get_loc(c): c for idx, c in enumerate(df.columns)}

```

Now you can use this dictionary to access columns through names and using `iloc`."
"I want to combine these:

```
keys = ['name', 'age', 'food']
values = ['Monty', 42, 'spam']

```

into a single dictionary:

```
{'name': 'Monty', 'age': 42, 'food': 'spam'}

```

How can I do this?","Like this:

```
keys = ['a', 'b', 'c']
values = [1, 2, 3]
dictionary = dict(zip(keys, values))
print(dictionary) # {'a': 1, 'b': 2, 'c': 3}

```

Voila :-) The pairwise [`dict`](https://docs.python.org/3/library/functions.html#func-dict) constructor and [`zip`](https://docs.python.org/3/library/functions.html#zip) function are awesomely useful."
"I'm doing it like:

```
def set_property(property,value):  
def get_property(property):  

```

or

```
object.property = value  
value = object.property

```

What's the pythonic way to use getters and setters?","Try this: [Python Property](http://docs.python.org/library/functions.html?highlight=property#property)

The sample code is:

```
class C(object):
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """"""I'm the 'x' property.""""""
        print(""getter of x called"")
        return self._x

    @x.setter
    def x(self, value):
        print(""setter of x called"")
        self._x = value

    @x.deleter
    def x(self):
        print(""deleter of x called"")
        del self._x


c = C()
c.x = 'foo'  # setter called
foo = c.x    # getter called
del c.x      # deleter called

```"
I see `__all__` in `__init__.py` files. What does it do?,"Linked to, but not explicitly mentioned here, is exactly when `__all__` is used. It is a list of strings defining what symbols in a module will be exported when `from <module> import *` is used on the module.

For example, the following code in a `foo.py` explicitly exports the symbols `bar` and `baz`:

```
__all__ = ['bar', 'baz']

waz = 5
bar = 10
def baz(): return 'baz'

```

These symbols can then be imported like so:

```
from foo import *

print(bar)
print(baz)

# The following will trigger an exception, as ""waz"" is not exported by the module
print(waz)

```

If the `__all__` above is commented out, this code will then execute to completion, as the default behaviour of `import *` is to import all symbols that do not begin with an underscore, from the given namespace.

Reference: <https://docs.python.org/tutorial/modules.html#importing-from-a-package>

**NOTE:** `__all__` affects the `from <module> import *` behavior only. Members that are not mentioned in `__all__` are still accessible from outside the module and can be imported with `from <module> import <member>`."
"I have multiple classes which would become singletons (my use case is for a logger, but this is not important). I do not wish to clutter several classes with added gumph when I can simply inherit or decorate.

Best methods:

---

Method 1: A decorator
---------------------

```
def singleton(class_):
    instances = {}
    def getinstance(*args, **kwargs):
        if class_ not in instances:
            instances[class_] = class_(*args, **kwargs)
        return instances[class_]
    return getinstance

@singleton
class MyClass(BaseClass):
    pass

```

Pros

* Decorators are additive in a way that is often more intuitive than multiple inheritance.

Cons

* While objects created using `MyClass()` would be true singleton objects, `MyClass` itself is a function, not a class, so you cannot call class methods from it. Also for

  ```
  x = MyClass();
  y = MyClass();
  t = type(n)();

  ```

then `x == y` but `x != t && y != t`

---

Method 2: A base class
----------------------

```
class Singleton(object):
    _instance = None
    def __new__(class_, *args, **kwargs):
        if not isinstance(class_._instance, class_):
            class_._instance = object.__new__(class_, *args, **kwargs)
        return class_._instance

class MyClass(Singleton, BaseClass):
    pass

```

Pros

* It's a true class

Cons

* Multiple inheritance - eugh! `__new__` could be overwritten during inheritance from a second base class? One has to think more than is necessary.

---

Method 3: A [metaclass](https://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python)
-------------------------------------------------------------------------------------------------

```
class Singleton(type):
    _instances = {}
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]

#Python2
class MyClass(BaseClass):
    __metaclass__ = Singleton

#Python3
class MyClass(BaseClass, metaclass=Singleton):
    pass

```

Pros

* It's a true class
* Auto-magically covers inheritance
* Uses `__metaclass__` for its proper purpose (and made me aware of it)

Cons

* Are there any?

---

Method 4: decorator returning a class with the same name
--------------------------------------------------------

```
def singleton(class_):
    class class_w(class_):
        _instance = None
        def __new__(class_, *args, **kwargs):
            if class_w._instance is None:
                class_w._instance = super(class_w,
                                    class_).__new__(class_,
                                                    *args,
                                                    **kwargs)
                class_w._instance._sealed = False
            return class_w._instance
        def __init__(self, *args, **kwargs):
            if self._sealed:
                return
            super(class_w, self).__init__(*args, **kwargs)
            self._sealed = True
    class_w.__name__ = class_.__name__
    return class_w

@singleton
class MyClass(BaseClass):
    pass

```

Pros

* It's a true class
* Auto-magically covers inheritance

Cons

* Is there not an overhead for creating each new class? Here we are creating two classes for each class we wish to make a singleton. While this is fine in my case, I worry that this might not scale. Of course there is a matter of debate as to whether it aught to be too easy to scale this pattern...
* What is the point of the `_sealed` attribute
* Can't call methods of the same name on base classes using `super()` because they will recurse. This means you can't customize `__new__` and can't subclass a class that needs you to call up to `__init__`.

---

Method 5: a module
------------------

a module file `singleton.py`

Pros

* Simple is better than complex

Cons

* Not lazily instantiated

This question is not for the discussion of whether or not the [singleton design pattern](http://en.wikipedia.org/wiki/Singleton_pattern) is desirable, is an anti-pattern, or for any religious wars, but to discuss how this pattern is best implemented in Python in such a way that is most Pythonic. In this instance I define 'most Pythonic' to mean that it follows the 'principle of least astonishment'.","Use a Metaclass
---------------

I would recommend **Method #2**, but you're better off using a **metaclass** than a base class. Here is a sample implementation:

```
class Singleton(type):
    _instances = {}
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]

class Logger(object):
    __metaclass__ = Singleton

```

Or in PythonÂ 3:

```
class Logger(metaclass=Singleton):
    pass

```

If you want to run `__init__` every time the class is called, add

```
        else:
            cls._instances[cls].__init__(*args, **kwargs)

```

to the `if` statement in `Singleton.__call__`.

A few words about metaclasses. A metaclass is the **class of a class**; that is, a class is an **instance of its metaclass**. You find the metaclass of an object in Python with `type(obj)`. Normal new-style classes are of type `type`. `Logger` in the code above will be of type `class 'your_module.Singleton'`, just as the (only) instance of `Logger` will be of type `class 'your_module.Logger'`. When you call logger with `Logger()`, Python first asks the metaclass of `Logger`, `Singleton`, what to do, allowing instance creation to be pre-empted. This process is the same as Python asking a class what to do by calling `__getattr__` when you reference one of its attributes by doing `myclass.attribute`.

A metaclass essentially decides **what the definition of a class means** and how to implement that definition. See for example <http://code.activestate.com/recipes/498149/>, which essentially recreates C-style `struct`s in Python using metaclasses. The thread [What are some (concrete) use-cases for metaclasses?](https://stackoverflow.com/questions/392160/what-are-your-concrete-use-cases-for-metaclasses-in-python) also provides some examples, they generally seem to be related to declarative programming, especially as used in [ORMs](https://en.wikipedia.org/wiki/Object-relational_mapping).

In this situation, if you use your **Method #2**, and a subclass defines a `__new__` method, it will be **executed every time** you call `SubClassOfSingleton()` -- because it is responsible for calling the method that returns the stored instance. With a metaclass, it will **only be called once**, when the only instance is created. You want to **customize what it means to call the class**, which is decided by its type.

In general, it **makes sense** to use a metaclass to implement a singleton. A singleton is special because its instance is **created only once**, and a metaclass is the way you customize the **creation of a class**, allowing it to behave differently than a normal class. Using a metaclass gives you **more control** in case you need to customize the singleton class definitions in other ways.

Your singletons **won't need multiple inheritance** (because the metaclass is not a base class), but for **subclasses of the created class** that use multiple inheritance, you need to make sure the singleton class is the **first / leftmost** one with a metaclass that redefines `__call__` This is very unlikely to be an issue. The instance dict is **not in the instance's namespace** so it won't accidentally overwrite it.

You will also hear that the singleton pattern violates the ""Single Responsibility Principle"" -- each class should do **only one thing**. That way you don't have to worry about messing up one thing the code does if you need to change another, because they are separate and encapsulated. The metaclass implementation **passes this test**. The metaclass is responsible for **enforcing the pattern** and the created class and subclasses need not be **aware that they are singletons**. **Method #1** fails this test, as you noted with ""MyClass itself is a a function, not a class, so you cannot call class methods from it.""

Python 2 and 3 Compatible Version
=================================

Writing something that works in both Python2 and 3 requires using a slightly more complicated scheme. Since metaclasses are usually subclasses of type `type`, it's possible to use one to dynamically create an intermediary base class at run time with it as its metaclass and then use *that* as the base class of the public `Singleton` base class. It's harder to explain than to do, as illustrated next:

```
# Works in Python 2 & 3
class _Singleton(type):
    """""" A metaclass that creates a Singleton base class when called. """"""
    _instances = {}
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super(_Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]

class Singleton(_Singleton('SingletonMeta', (object,), {})): pass

class Logger(Singleton):
    pass

```

An ironic aspect of this approach is that it's using subclassing to implement a metaclass. One possible advantage is that, unlike with a pure metaclass, `isinstance(inst, Singleton)` will return `True`.

Corrections
-----------

On another topic, you've probably already noticed this, but the base class implementation in your original post is wrong. `_instances` needs to be **referenced on the class**, you need to use `super()` or you're **recursing**, and `__new__` is actually a static method that you have to **pass the class to**, not a class method, as the actual class **hasn't been created** yet when it is called. All of these things will be true for a metaclass implementation as well.

```
class Singleton(object):
  _instances = {}
  def __new__(class_, *args, **kwargs):
    if class_ not in class_._instances:
        class_._instances[class_] = super(Singleton, class_).__new__(class_, *args, **kwargs)
    return class_._instances[class_]

class MyClass(Singleton):
  pass

c = MyClass()

```

Decorator Returning A Class
---------------------------

I originally was writing a comment, but it was too long, so I'll add this here. **Method #4** is better than the other decorator version, but it's more code than needed for a singleton, and it's not as clear what it does.

The main problems stem from the class being its own base class. First, isn't it weird to have a class be a subclass of a nearly identical class with the same name that exists only in its `__class__` attribute? This also means that you can't define **any methods that call the method of the same name on their base class** with `super()` because they will recurse. This means your class can't customize `__new__`, and can't derive from any classes that need `__init__` called on them.

When to use the singleton pattern
---------------------------------

Your use case is **one of the better examples** of wanting to use a singleton. You say in one of the comments ""To me logging has always seemed a natural candidate for Singletons."" You're **absolutely right**.

When people say singletons are bad, the most common reason is they are **implicit shared state**. While with global variables and top-level module imports are **explicit** shared state, other objects that are passed around are generally instantiated. This is a good point, **with two exceptions**.

The first, and one that gets mentioned in various places, is when the singletons are **constant**. Use of global constants, especially enums, is widely accepted, and considered sane because no matter what, **none of the users can mess them up for any other user**. This is equally true for a constant singleton.

The second exception, which get mentioned less, is the opposite -- when the singleton is **only a data sink**, not a data source (directly or indirectly). This is why loggers feel like a ""natural"" use for singletons. As the various users are **not changing the loggers** in ways other users will care about, there is **not really shared state**. This negates the primary argument against the singleton pattern, and makes them a reasonable choice because of their **ease of use** for the task.

Here is a quote from <http://googletesting.blogspot.com/2008/08/root-cause-of-singletons.html>:

> Now, there is one kind of Singleton which is OK. That is a singleton where all of the reachable objects are immutable. If all objects are immutable than Singleton has no global state, as everything is constant. But it is so easy to turn this kind of singleton into mutable one, it is very slippery slope. Therefore, I am against these Singletons too, not because they are bad, but because it is very easy for them to go bad. (As a side note Java enumeration are just these kind of singletons. As long as you don't put state into your enumeration you are OK, so please don't.)
>
> The other kind of Singletons, which are semi-acceptable are those which don't effect the execution of your code, They have no ""side effects"". Logging is perfect example. It is loaded with Singletons and global state. It is acceptable (as in it will not hurt you) because your application does not behave any different whether or not a given logger is enabled. The information here flows one way: From your application into the logger. Even though loggers are global state since no information flows from loggers into your application, loggers are acceptable. You should still inject your logger if you want your test to assert that something is getting logged, but in general Loggers are not harmful despite being full of state."
"How do I get the filename without the extension from a path in Python?

```
""/path/to/some/file.txt""  â†’  ""file""

```","### Python 3.4+

Use [`pathlib.Path.stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem)

```
>>> from pathlib import Path
>>> Path(""/path/to/file.txt"").stem
'file'
>>> Path(""/path/to/file.tar.gz"").stem
'file.tar'

```

### Python < 3.4

Use [`os.path.splitext`](https://docs.python.org/3/library/os.path.html#os.path.splitext) in combination with [`os.path.basename`](https://docs.python.org/3/library/os.path.html#os.path.basename):

```
>>> os.path.splitext(os.path.basename(""/path/to/file.txt""))[0]
'file'
>>> os.path.splitext(os.path.basename(""/path/to/file.tar.gz""))[0]
'file.tar'

```"
"[Project Euler](http://en.wikipedia.org/wiki/Project_Euler) and other coding contests often have a maximum time to run or people boast of how fast their particular solution runs. With Python, sometimes the approaches are somewhat kludgey - i.e., adding timing code to `__main__`.

What is a good way to profile how long a Python program takes to run?","**Python includes a profiler called [`cProfile`](https://docs.python.org/3/library/profile.html#module-cProfile).** It not only gives the total running time, but also times each function separately, and tells you how many times each function was called, making it easy to determine where you should make optimizations.

You can call it from within your code, or from the interpreter, like this:

```
import cProfile
cProfile.run('foo()')

```

Even more usefully, you can invoke cProfile when running a script:

```
python -m cProfile myscript.py

```

Or when running a module:

```
python -m cProfile -m mymodule

```

To make it even easier, I made a little batch file called 'profile.bat':

```
python -m cProfile %1

```

So all I have to do is run:

```
profile euler048.py

```

And I get this:

```
1007 function calls in 0.061 CPU seconds

Ordered by: standard name
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000    0.061    0.061 <string>:1(<module>)
 1000    0.051    0.000    0.051    0.000 euler048.py:2(<lambda>)
    1    0.005    0.005    0.061    0.061 euler048.py:2(<module>)
    1    0.000    0.000    0.061    0.061 {execfile}
    1    0.002    0.002    0.053    0.053 {map}
    1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler objects}
    1    0.000    0.000    0.000    0.000 {range}
    1    0.003    0.003    0.003    0.003 {sum}

```

For more information, check out this tutorial from PyCon 2013 titled
[***Python Profiling***](https://web.archive.org/web/20170318204046/http://lanyrd.com/2013/pycon/scdywg/)  
[Also via YouTube](https://www.youtube.com/watch?v=QJwVYlDzAXs)."
"How do I convert the following `for`-loop containing an `if`/`else` into a list comprehension?

```
results = []
for x in xs:
    results.append(f(x) if x is not None else '')

```

It should yield `''` if `x` is `None`, and otherwise `f(x)`.
I tried:

```
[f(x) for x in xs if x is not None else '']

```

but it gives a `SyntaxError`. What is the correct syntax?

---

See [Does Python have a ternary conditional operator?](https://stackoverflow.com/questions/394809/) for info on `... if ... else ...`.  
See [List comprehension with condition](https://stackoverflow.com/questions/24442091) for *omitting* values based on a condition: `[... for x in xs if x cond]`.  
See [`elif` in list comprehension conditionals](https://stackoverflow.com/questions/9987483) for `elif`.","You can totally do that. It's just an ordering issue:

```
[f(x) if x is not None else '' for x in xs]

```

In general,

```
[f(x) if condition else g(x) for x in sequence]

```

And, for list comprehensions with `if` conditions only,

```
[f(x) for x in sequence if condition]

```

Note that this actually uses a different language construct, a [conditional expression](https://docs.python.org/3/reference/expressions.html#conditional-expressions), which itself is not part of the [comprehension syntax](https://docs.python.org/3/reference/expressions.html#displays-for-lists-sets-and-dictionaries), while the `if` after the `forâ€¦in` is part of list comprehensions and used to *filter* elements from the source iterable.

---

Conditional expressions can be used in all kinds of situations where you want to choose between two expression values based on some condition. This does the same as the [ternary operator `?:` that exists in other languages](https://docs.python.org/3/faq/programming.html#is-there-an-equivalent-of-c-s-ternary-operator). For example:

```
value = 123
print(value, 'is', 'even' if value % 2 == 0 else 'odd')

```"
"I want to write a function in Python that returns different fixed values based on the value of an input index.

In other languages I would use a `switch` or `case` statement, but Python does not appear to have a `switch` statement. What are the recommended Python solutions in this scenario?","Python 3.10 (2021) introduced the `match`-`case` statement, which provides a first-class implementation of a ""switch"" for Python. For example:

```
def f(x):
    match x:
        case 'a':
            return 1
        case 'b':
            return 2
        case _:
            return 0   # 0 is the default case if x is not found

```

The `match`-`case` statement is considerably more powerful than this simple example.

Documentation:

* [`match` statements](https://docs.python.org/3/tutorial/controlflow.html#match-statements) (under the ""More Control Flow Tools"" page)
* [The `match` statement](https://docs.python.org/3/reference/compound_stmts.html#match) (under ""Compound statements"" page)
* [PEP 634 – Structural Pattern Matching: Specification](https://peps.python.org/pep-0634/)
* [PEP 636 – Structural Pattern Matching: Tutorial](https://peps.python.org/pep-0636/)

---

If you need to support Python ≤ 3.9, use a dictionary instead:

```
def f(x):
    return {
        'a': 1,
        'b': 2,
    }.get(x, 0)  # default case

```"
`float('nan')` represents NaN (not a number). But how do I check for it?,"Use [`math.isnan`](http://docs.python.org/library/math.html#math.isnan):

```
>>> import math
>>> x = float('nan')
>>> math.isnan(x)
True

```"
"I have a series of 20 plots (not subplots) to be made in a single figure. I want the legend to be outside of the box. At the same time, I do not want to change the axes, as the size of the figure gets reduced.

1. I want to keep the legend box outside the plot area (I want the legend to be outside at the right side of the plot area).
2. Is there a way to reduce the font size of the text inside the legend box, so that the size of the legend box will be small?","There are a number of ways to do what you want. To add to [what Christian Alis](https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot/4700762#4700762) and [Navi already said](https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot/4700674#4700674), you can use the `bbox_to_anchor` keyword argument to place the legend partially outside the axes and/or decrease the font size.

Before you consider decreasing the font size (which can make things awfully hard to read), try playing around with placing the legend in different places:

So, let's start with a generic example:

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(10)

fig = plt.figure()
ax = plt.subplot(111)

for i in range(5):
    ax.plot(x, i * x, label='$y = %ix$' % i)

ax.legend()

plt.show()

```

![alt text](https://i.sstatic.net/LQ8xkm.png)

If we do the same thing, but use the `bbox_to_anchor` keyword argument we can shift the legend slightly outside the axes boundaries:

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(10)

fig = plt.figure()
ax = plt.subplot(111)

for i in range(5):
    ax.plot(x, i * x, label='$y = %ix$' % i)

ax.legend(bbox_to_anchor=(1.1, 1.05))

plt.show()

```

![Alt text](https://i.sstatic.net/OtE5Um.png)

Similarly, make the legend more horizontal and/or put it at the top of the figure (I'm also turning on rounded corners and a simple drop shadow):

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(10)

fig = plt.figure()
ax = plt.subplot(111)

for i in range(5):
    line, = ax.plot(x, i * x, label='$y = %ix$'%i)

ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),
          ncol=3, fancybox=True, shadow=True)
plt.show()

```

![alt text](https://i.sstatic.net/zgtBlm.png)

Alternatively, shrink the current plot's width, and put the legend entirely outside the axis of the figure (note: if you use [`tight_layout()`](https://matplotlib.org/3.2.1/tutorials/intermediate/tight_layout_guide.html), then leave out `ax.set_position()`:

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(10)

fig = plt.figure()
ax = plt.subplot(111)

for i in range(5):
    ax.plot(x, i * x, label='$y = %ix$'%i)

# Shrink current axis by 20%
box = ax.get_position()
ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])

# Put a legend to the right of the current axis
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.show()

```

![Alt text](https://i.sstatic.net/v34g8m.png)

And in a similar manner, shrink the plot vertically, and put a horizontal legend at the bottom:

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(10)

fig = plt.figure()
ax = plt.subplot(111)

for i in range(5):
    line, = ax.plot(x, i * x, label='$y = %ix$'%i)

# Shrink current axis's height by 10% on the bottom
box = ax.get_position()
ax.set_position([box.x0, box.y0 + box.height * 0.1,
                 box.width, box.height * 0.9])

# Put a legend below current axis
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),
          fancybox=True, shadow=True, ncol=5)

plt.show()

```

![Alt text](https://i.sstatic.net/cXcYam.png)

Have a look at the [matplotlib legend guide](http://matplotlib.org/users/legend_guide.html#legend-location). You might also take a look at [`plt.figlegend()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figlegend.html)."
"Why does the following class declaration inherit from `object`?

```
class MyClass(object):
    ...

```","> ### Is there any reason for a class declaration to inherit from `object`?

In Python 3, apart from compatibility between Python 2 and 3, *no reason*. In Python 2, *many reasons*.

---

### Python 2.x story:

In Python 2.x (from 2.2 onwards) there's two styles of classes depending on the presence or absence of `object` as a base-class:

1. **""classic"" style** classes: they don't have `object` as a base class:

   ```
   >>> class ClassicSpam:      # no base class
   ...     pass
   >>> ClassicSpam.__bases__
   ()

   ```
2. **""new"" style** classes: they have, directly *or indirectly* (e.g inherit from a [built-in type](https://docs.python.org/3/library/stdtypes.html)), `object` as a base class:

   ```
   >>> class NewSpam(object):           # directly inherit from object
   ...    pass
   >>> NewSpam.__bases__
   (<type 'object'>,)
   >>> class IntSpam(int):              # indirectly inherit from object...
   ...    pass
   >>> IntSpam.__bases__
   (<type 'int'>,) 
   >>> IntSpam.__bases__[0].__bases__   # ... because int inherits from object  
   (<type 'object'>,)

   ```

Without a doubt, when writing a class you'll *always* want to go for new-style classes. The perks of doing so are numerous, to list some of them:

* [Support for descriptors](https://docs.python.org/3/howto/descriptor.html). Specifically, the following constructs are made possible with descriptors:

  1. [`classmethod`](https://docs.python.org/3/library/functions.html#classmethod): A method that receives the class as an implicit argument instead of the instance.
  2. [`staticmethod`](https://docs.python.org/3/library/functions.html#staticmethod): A method that does not receive the implicit argument `self` as a first argument.
  3. properties with [`property`](https://docs.python.org/3/library/functions.html#property): Create functions for managing the getting, setting and deleting of an attribute.
  4. [`__slots__`](https://docs.python.org/3/reference/datamodel.html#slots): Saves memory consumptions of a class and also results in faster attribute access. Of course, it does [impose limitations](https://docs.python.org/3/reference/datamodel.html#notes-on-using-slots).
* The [`__new__`](https://docs.python.org/3/reference/datamodel.html#object.__new__) static method: lets you customize how new class instances are created.
* [Method resolution order (MRO)](https://www.python.org/download/releases/2.3/mro/): in what order the base classes of a class will be searched when trying to resolve which method to call.
* Related to MRO, [`super` calls](https://docs.python.org/3/library/functions.html#super). Also see, [`super()` considered super.](https://rhettinger.wordpress.com/2011/05/26/super-considered-super/)

If you don't inherit from `object`, forget these. A more exhaustive description of the previous bullet points along with other perks of ""new"" style classes can be found [here](https://www.python.org/download/releases/2.2.3/descrintro/).

One of the downsides of new-style classes is that the class itself is more memory demanding. Unless you're creating many class objects, though, I doubt this would be an issue and it's a negative sinking in a sea of positives.

---

### Python 3.x story:

In Python 3, things are simplified. Only new-style classes exist (referred to plainly as classes) so, the only difference in adding `object` is requiring you to type in 8 more characters. This:

```
class ClassicSpam:
    pass

```

is completely equivalent (apart from their name :-) to this:

```
class NewSpam(object):
     pass

```

and to this:

```
class Spam():
    pass

```

All have `object` in their `__bases__`.

```
>>> [object in cls.__bases__ for cls in {Spam, NewSpam, ClassicSpam}]
[True, True, True]

```

---

So, what should you do?
-----------------------

**In Python 2:** *always inherit from `object` explicitly*. Get the perks.

**In Python 3:** inherit from `object` if you are writing code that tries to be Python agnostic, that is, it needs to work both in Python 2 and in Python 3. Otherwise don't, it really makes no difference since Python inserts it for you behind the scenes."
There is no built in `reverse` method for Python's `str` object. How can I reverse a string?,"Using [slicing](https://stackoverflow.com/questions/509211/understanding-slicing):

```
>>> 'hello world'[::-1]
'dlrow olleh'

```

---

Slice notation takes the form `[start:stop:step]`. In this case, we omit the `start` and `stop` positions since we want the whole string. We also use `step = -1`, which means, ""repeatedly step from right to left by 1 character""."
"I'm deploying a Django app to a dev server and am hitting this error when I run `pip install -r requirements.txt`:

```
Traceback (most recent call last):
  File ""/var/www/mydir/virtualenvs/dev/bin/pip"", line 5, in <module>
    from pkg_resources import load_entry_point
ImportError: No module named pkg_resources

```

`pkg_resources` appears to be distributed with `setuptools`. Initially I thought this might not be installed to the Python in the virtualenv, so I installed `setuptools 2.6` (same version as Python) to the Python site-packages in the virtualenv with the following command:

```
sh setuptools-0.6c11-py2.6.egg --install-dir /var/www/mydir/virtualenvs/dev/lib/python2.6/site-packages

```

**EDIT:** This only happens inside the virtualenv. If I open a console outside the virtualenv then `pkg_resources` is present, but I am still getting the same error.

Any ideas as to why `pkg_resources` is not on the path?","**July 2018 Update**

Most people should now use `pip install setuptools` (possibly with `sudo`).

Some may need to (re)install the `python-setuptools` package via their package manager (`apt-get install`, `yum install`, etc.).

This issue can be highly dependent on your OS and dev environment. See the legacy/other answers below if the above isn't working for you.

**Explanation**

This error message is caused by a missing/broken Python `setuptools` package. Per Matt M.'s comment and [setuptools issue #581](https://github.com/pypa/setuptools/issues/581), the bootstrap script referred to below is no longer the recommended installation method.

The bootstrap script instructions will remain below, in case it's still helpful to anyone.

**Legacy Answer**

I encountered the same `ImportError` today while trying to use pip. Somehow the `setuptools` package had been deleted in my Python environment.

To fix the issue, run the setup script for `setuptools`:

```
wget https://bootstrap.pypa.io/ez_setup.py -O - | python

```

(or if you don't have `wget` installed (e.g. OS X), try

```
curl https://bootstrap.pypa.io/ez_setup.py | python

```

possibly with `sudo` prepended.)

If you have any version of [`distribute`](http://pythonhosted.org/setuptools/merge-faq.html), or any `setuptools` below 0.6, you will have to uninstall it first.\*

See [Installation Instructions](https://pypi.python.org/pypi/setuptools/0.9.8#installation-instructions) for further details.

---

\* If you already have a working `distribute`, upgrading it to the ""compatibility wrapper"" that switches you over to `setuptools` is easier. But if things are already broken, don't try that."
"I have the following DataFrame (`df`):

```
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.rand(10, 5))

```

I add more column(s) by assignment:

```
df['mean'] = df.mean(1)

```

How can I move the column `mean` to the front, i.e. set it as first column leaving the order of the other columns untouched?","One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed.

This is what you have now:

```
In [6]: df
Out[6]:
          0         1         2         3         4      mean
0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543
1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208
2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596
3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653
4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371
5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165
6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529
7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149
8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195
9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593

In [7]: cols = df.columns.tolist()

In [8]: cols
Out[8]: [0L, 1L, 2L, 3L, 4L, 'mean']

```

Rearrange `cols` in any way you want. This is how I moved the last element to the first position:

```
In [12]: cols = cols[-1:] + cols[:-1]

In [13]: cols
Out[13]: ['mean', 0L, 1L, 2L, 3L, 4L]

```

Then reorder the dataframe like this:

```
In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]

In [17]: df
Out[17]:
       mean         0         1         2         3         4
0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616
1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551
2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694
3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019
4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485
5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447
6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473
7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914
8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561
9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399

```"
"I spent most of the day yesterday searching for a clear answer for installing `pip` (package manager for Python). I can't find a good solution.

How do I install it?","On Linux or MacOS:

```
python -m ensurepip --upgrade

```

If you want to install `pip` for Python 3, replace `python` with `python3`.

See <https://pip.pypa.io/en/stable/installation/> for more details."
How do I check if a directory exists in Python?,"Use [**`os.path.isdir`**](http://docs.python.org/dev/library/os.path.html#os.path.isdir) for directories only:

```
>>> import os
>>> os.path.isdir('new_folder')
True

```

Use [**`os.path.exists`**](http://docs.python.org/dev/library/os.path.html#os.path.exists) for both files and directories:

```
>>> import os
>>> os.path.exists(os.path.join(os.getcwd(), 'new_folder', 'file.txt'))
False

```

Alternatively, you can use [**`pathlib`**](https://docs.python.org/dev/library/pathlib.html):

```
 >>> from pathlib import Path
 >>> Path('new_folder').is_dir()
 True
 >>> (Path.cwd() / 'new_folder' / 'file.txt').exists()
 False

```"
What is the Python 3 equivalent of `python -m SimpleHTTPServer`?,"From [the docs](https://docs.python.org/2/library/simplehttpserver.html):

> The `SimpleHTTPServer` module has been merged into `http.server` in Python 3.0. The 2to3 tool will automatically adapt imports when converting your sources to 3.0.

So, your command is `python -m http.server`, or depending on your installation, it can be:

```
python3 -m http.server

```"
What is `setup.py` and how can it be configured or used?,"`setup.py` is a Python file, the presence of which is an indication that the module/package you are about to install has likely been packaged and distributed with [Distutils](https://docs.python.org/3.9/library/distutils.html), which is the standard for distributing Python Modules.

This allows you to easily [install Python packages](https://docs.python.org/3/installing/index.html#installing-index). Often it's enough to write:

```
$ pip install . 

```

`pip` will use `setup.py` to install your module. Avoid calling `setup.py` directly."
"How do I force Python's `print` function to flush the buffered output to the screen?

---

**See also:** [Disable output buffering](https://stackoverflow.com/questions/107705) if the goal is to change the buffering behaviour generally. This question is about explicitly flushing output after a specific `print` call, even though output is still being buffered.

**For duplicate closers:** if a beginner is asking a question about trying to make output appear immediately while not using a newline at the end, **please instead use** [Why doesn't print output show up immediately in the terminal when there is no newline at the end?](https://stackoverflow.com/questions/25897335) to close the question. The current question isn't good enough because the person asking will likely **not have a concept** of buffering or flushing; the other question is intended to explain those concepts first, whereas this question is about the technical details.","In Python 3, [`print`](https://docs.python.org/library/functions.html#print) can take an optional `flush` argument:

```
print(""Hello, World!"", flush=True)

```

In Python 2, after calling `print`, do:

```
import sys
sys.stdout.flush()

```

By default, [`print`](https://docs.python.org/2/reference/simple_stmts.html#the-print-statement) prints to [`sys.stdout`](https://docs.python.org/2/library/sys.html#sys.stdout) (see the documentation for more about [file objects](https://docs.python.org/2/library/stdtypes.html#file-objects))."
"Can I use list comprehension syntax to create a dictionary?

For example, by iterating over pairs of keys and values:

```
d = {... for k, v in zip(keys, values)}

```","Use a [dict comprehension](http://www.python.org/dev/peps/pep-0274/) (Python 2.7 and later):

```
{key: value for key, value in zip(keys, values)}

```

---

Alternatively, use the [`dict`](https://docs.python.org/3/library/stdtypes.html#typesmapping) constructor:

```
pairs = [('a', 1), ('b', 2)]
dict(pairs)                          # → {'a': 1, 'b': 2}
dict((k, v + 10) for k, v in pairs)  # → {'a': 11, 'b': 12}

```

Given separate lists of keys and values, use the `dict` constructor with [`zip`](https://docs.python.org/3/library/functions.html#zip):

```
keys = ['a', 'b']
values = [1, 2]
dict(zip(keys, values))              # → {'a': 1, 'b': 2}

```"
"How do I convert an integer to a string?

```
42   ‚ü∂   ""42""

```

---

For the reverse, see [How do I parse a string to a float or int?](https://stackoverflow.com/questions/379906/). Floats can be handled similarly, but handling the decimal points can be tricky because [floating-point values are not precise](https://stackoverflow.com/questions/588004/). See [Converting a float to a string without rounding it](https://stackoverflow.com/questions/1317558) for more specific advice.","```
>>> str(42)
'42'

>>> int('42')
42

```

Links to the documentation:

* [`int()`](https://docs.python.org/3/library/functions.html#int)
* [`str()`](https://docs.python.org/3/library/functions.html#func-str)

`str(x)` converts any object `x` to a string by calling `x.__str__()`, or [`repr(x)`](https://docs.python.org/3/library/functions.html#repr) if `x` doesn't have a `__str__()` method."
"I am having trouble installing psycopg2. I get the following error when I try to `pip install psycopg2`:

```
Error: pg_config executable not found.

Please add the directory containing pg_config to the PATH

or specify the full executable path with the option:



    python setup.py build_ext --pg-config /path/to/pg_config build ...



or with the pg_config option in 'setup.cfg'.

----------------------------------------
Command python setup.py egg_info failed with error code 1 in /tmp/pip-build/psycopg2

```

But the problem is `pg_config` is actually in my `PATH`; it runs without any problem:

```
$ which pg_config
/usr/pgsql-9.1/bin/pg_config

```

I tried adding the pg\_config path to the `setup.cfg` file and building it using the source files I downloaded from their website (<http://initd.org/psycopg/>) and I get the following error message!

```
Error: Unable to find 'pg_config' file in '/usr/pgsql-9.1/bin/'

```

But it is actually THERE!!!

I am baffled by these errors. Can anyone help please?

By the way, I `sudo` all the commands. Also I am on RHEL 5.5.","`pg_config` is in `postgresql-devel` (`libpq-dev` in Debian/Ubuntu, `libpq-devel` on Centos/Fedora/Cygwin/Babun.)"
"So I have difficulty with the concept of `*args` and `**kwargs`.

So far I have learned that:

* `*args` = list of arguments - as positional arguments
* `**kwargs` = dictionary - whose keys become separate keyword arguments and the values become values of these arguments.

I don't understand what programming task this would be helpful for.

Maybe:

I think to enter lists and dictionaries as arguments of a function AND at the same time as a wildcard, so I can pass ANY argument?

Is there a simple example to explain how `*args` and `**kwargs` are used?

Also the tutorial I found used just the ""\*"" and a variable name.

Are `*args` and `**kwargs` just placeholders or do you use exactly `*args` and `**kwargs` in the code?","[The syntax is the `*` and `**`](http://docs.python.org/tutorial/controlflow.html#arbitrary-argument-lists). The names `*args` and `**kwargs` are only by convention but there's no hard requirement to use them.

You would use `*args` when you're not sure how many arguments might be passed to your function, i.e. it allows you pass an arbitrary number of arguments to your function. For example:

```
>>> def print_everything(*args):
        for count, thing in enumerate(args):
...         print( '{0}. {1}'.format(count, thing))
...
>>> print_everything('apple', 'banana', 'cabbage')
0. apple
1. banana
2. cabbage

```

Similarly, `**kwargs` allows you to handle named arguments that you have not defined in advance:

```
>>> def table_things(**kwargs):
...     for name, value in kwargs.items():
...         print( '{0} = {1}'.format(name, value))
...
>>> table_things(apple = 'fruit', cabbage = 'vegetable')
cabbage = vegetable
apple = fruit

```

You can use these along with named arguments too. The explicit arguments get values first and then everything else is passed to `*args` and `**kwargs`. The named arguments come first in the list. For example:

```
def table_things(titlestring, **kwargs)

```

You can also use both in the same function definition but `*args` must occur before `**kwargs`.

You can also use the `*` and `**` syntax when calling a function. For example:

```
>>> def print_three_things(a, b, c):
...     print( 'a = {0}, b = {1}, c = {2}'.format(a,b,c))
...
>>> mylist = ['aardvark', 'baboon', 'cat']
>>> print_three_things(*mylist)
a = aardvark, b = baboon, c = cat

```

As you can see in this case it takes the list (or tuple) of items and unpacks it. By this it matches them to the arguments in the function. Of course, you could have a `*` both in the function definition and in the function call."
"How does Python's *slice notation* work? That is: when I write code like `a[x:y:z]`, `a[:]`, `a[::2]` etc., how can I understand which elements end up in the slice?

---

See [Why are slice and range upper-bound exclusive?](https://stackoverflow.com/questions/11364533) to learn why `xs[0:2] == [xs[0], xs[1]]`, *not* `[..., xs[2]]`.  
See [Make a new list containing every Nth item in the original list](https://stackoverflow.com/questions/1403674/) for `xs[::N]`.  
See [How does assignment work with list slices?](https://stackoverflow.com/questions/10623302) to learn what `xs[0:2] = [""a"", ""b""]` does.","The syntax is:

```
a[start:stop]  # items start through stop-1
a[start:]      # items start through the rest of the array
a[:stop]       # items from the beginning through stop-1
a[:]           # a copy of the whole array

```

There is also the `step` value, which can be used with any of the above:

```
a[start:stop:step] # start through not past stop, by step

```

The key point to remember is that the `:stop` value represents the first value that is *not* in the selected slice. So, the difference between `stop` and `start` is the number of elements selected (if `step` is 1, the default).

The other feature is that `start` or `stop` may be a *negative* number, which means it counts from the end of the array instead of the beginning. So:

```
a[-1]    # last item in the array
a[-2:]   # last two items in the array
a[:-2]   # everything except the last two items

```

Similarly, `step` may be a negative number:

```
a[::-1]    # all items in the array, reversed
a[1::-1]   # the first two items, reversed
a[:-3:-1]  # the last two items, reversed
a[-3::-1]  # everything except the last two items, reversed

```

Python is kind to the programmer if there are fewer items than you ask for. For example, if you ask for `a[:-2]` and `a` only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.

### Relationship with the `slice` object

A [`slice` object](https://docs.python.org/3/library/functions.html#slice) can represent a slicing operation, i.e.:

```
a[start:stop:step]

```

is equivalent to:

```
a[slice(start, stop, step)]

```

Slice objects also behave slightly differently depending on the number of arguments, similar to `range()`, i.e. both `slice(stop)` and `slice(start, stop[, step])` are supported.
To skip specifying a given argument, one might use `None`, so that e.g. `a[start:]` is equivalent to `a[slice(start, None)]` or `a[::-1]` is equivalent to `a[slice(None, None, -1)]`.

While the `:`-based notation is very helpful for simple slicing, the explicit use of `slice()` objects simplifies the programmatic generation of slicing."
"I need to install psycopg2 v2.4.1 specifically. I accidentally did:

```
pip install psycopg2

```

Instead of:

```
pip install psycopg2==2.4.1

```

That installs 2.4.4 instead of the earlier version.

Now even after I pip uninstall psycopg2 and attempt to reinstall with the correct version, it appears that pip is re-using the cache it downloaded the first time.

How can I force pip to clear out its download cache and use the specific version I'm including in the command?","If using pip 6.0 or newer, try adding the `--no-cache-dir` option ([source](https://pip.pypa.io/en/stable/topics/caching/)).

If using pip older than pip 6.0, upgrade it with `pip install -U pip`."
"I want to catch and log exceptions without exiting, e.g.,

```
try:
    do_stuff()
except Exception as err:
    print(Exception, err)
    # I want to print the entire traceback here,
    # not just the exception name and details

```

I want to print the exact same output that is printed when the exception is raised without the try/except intercepting the exception, and I do *not* want it to exit my program.","[`traceback.format_exc()`](http://docs.python.org/3/library/traceback.html#traceback.format_exc) will yield more info if that's what you want.

```
import traceback

def do_stuff():
    raise Exception(""test exception"")

try:
    do_stuff()
except Exception:
    print(traceback.format_exc())

```

This outputs:

```
Traceback (most recent call last):
  File ""main.py"", line 9, in <module>
    do_stuff()
  File ""main.py"", line 5, in do_stuff
    raise Exception(""test exception"")
Exception: test exception

```"
"I created a DataFrame from a list of lists:

```
table = [
    ['a',  '1.2',  '4.2' ],
    ['b',  '70',   '0.03'],
    ['x',  '5',    '0'   ],
]

df = pd.DataFrame(table)

```

How do I convert the columns to specific types? In this case, I want to convert columns 2 and 3 into floats.

Is there a way to specify the types while converting the list to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the dtype for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns, and I don't want to specify exactly which columns are of which type. All I can guarantee is that each column contains values of the same type.","You have four main options for converting types in pandas:

1. [`to_numeric()`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) - provides functionality to safely convert non-numeric types (e.g. strings) to a suitable numeric type. (See also [`to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) and [`to_timedelta()`](https://pandas.pydata.org/docs/reference/api/pandas.to_timedelta.html).)
2. [`astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) - convert (almost) any type to (almost) any other type (even if it's not necessarily sensible to do so). Also allows you to convert to [categorial](https://pandas.pydata.org/docs/user_guide/categorical.html) types (very useful).
3. [`infer_objects()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.infer_objects.html) - a utility method to convert object columns holding Python objects to a pandas type if possible.
4. [`convert_dtypes()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html) - convert DataFrame columns to the ""best possible"" dtype that supports `pd.NA` (pandas' object to indicate a missing value).

Read on for more detailed explanations and usage of each of these methods.

---

1. `to_numeric()`
=================

The best way to convert one or more columns of a DataFrame to numeric values is to use [`pandas.to_numeric()`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html).

This function will try to change non-numeric objects (such as strings) into integers or floating-point numbers as appropriate.

Basic usage
-----------

The input to [`to_numeric()`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) is a Series or a single column of a DataFrame.

```
>>> s = pd.Series([""8"", 6, ""7.5"", 3, ""0.9""]) # mixed string and numeric values
>>> s
0      8
1      6
2    7.5
3      3
4    0.9
dtype: object

>>> pd.to_numeric(s) # convert everything to float values
0    8.0
1    6.0
2    7.5
3    3.0
4    0.9
dtype: float64

```

As you can see, a new Series is returned. Remember to assign this output to a variable or column name to continue using it:

```
# convert Series
my_series = pd.to_numeric(my_series)

# convert column ""a"" of a DataFrame
df[""a""] = pd.to_numeric(df[""a""])

```

You can also use it to convert multiple columns of a DataFrame via the [`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) method:

```
# convert all columns of DataFrame
df = df.apply(pd.to_numeric) # convert all columns of DataFrame

# convert just columns ""a"" and ""b""
df[[""a"", ""b""]] = df[[""a"", ""b""]].apply(pd.to_numeric)

```

As long as your values can all be converted, that's probably all you need.

Error handling
--------------

But what if some values can't be converted to a numeric type?

[`to_numeric()`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) also takes an `errors` keyword argument that allows you to force non-numeric values to be `NaN`, or simply ignore columns containing these values.

Here's an example using a Series of strings `s` which has the object dtype:

```
>>> s = pd.Series(['1', '2', '4.7', 'pandas', '10'])
>>> s
0         1
1         2
2       4.7
3    pandas
4        10
dtype: object

```

The default behaviour is to raise if it can't convert a value. In this case, it can't cope with the string 'pandas':

```
>>> pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')
ValueError: Unable to parse string

```

Rather than fail, we might want 'pandas' to be considered a missing/bad numeric value. We can coerce invalid values to `NaN` as follows using the `errors` keyword argument:

```
>>> pd.to_numeric(s, errors='coerce')
0     1.0
1     2.0
2     4.7
3     NaN
4    10.0
dtype: float64

```

The third option for `errors` is just to ignore the operation if an invalid value is encountered:

```
>>> pd.to_numeric(s, errors='ignore')
# the original Series is returned untouched

```

This last option is particularly useful for converting your entire DataFrame, but don't know which of our columns can be converted reliably to a numeric type. In that case, just write:

```
df.apply(pd.to_numeric, errors='ignore')

```

The function will be applied to each column of the DataFrame. Columns that can be converted to a numeric type will be converted, while columns that cannot (e.g. they contain non-digit strings or dates) will be left alone.

Downcasting
-----------

By default, conversion with [`to_numeric()`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) will give you either an `int64` or `float64` dtype (or whatever integer width is native to your platform).

That's usually what you want, but what if you wanted to save some memory and use a more compact dtype, like `float32`, or `int8`?

[`to_numeric()`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) gives you the option to downcast to either `'integer'`, `'signed'`, `'unsigned'`, `'float'`. Here's an example for a simple series `s` of integer type:

```
>>> s = pd.Series([1, 2, -7])
>>> s
0    1
1    2
2   -7
dtype: int64

```

Downcasting to `'integer'` uses the smallest possible integer that can hold the values:

```
>>> pd.to_numeric(s, downcast='integer')
0    1
1    2
2   -7
dtype: int8

```

Downcasting to `'float'` similarly picks a smaller than normal floating type:

```
>>> pd.to_numeric(s, downcast='float')
0    1.0
1    2.0
2   -7.0
dtype: float32

```

---

2. `astype()`
=============

The [`astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) method enables you to be explicit about the dtype you want your DataFrame or Series to have. It's very versatile in that you can try and go from one type to any other.

Basic usage
-----------

Just pick a type: you can use a NumPy dtype (e.g. `np.int16`), some Python types (e.g. bool), or pandas-specific types (like the categorical dtype).

Call the method on the object you want to convert and [`astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) will try and convert it for you:

```
# convert all DataFrame columns to the int64 dtype
df = df.astype(int)

# convert column ""a"" to int64 dtype and ""b"" to complex type
df = df.astype({""a"": int, ""b"": complex})

# convert Series to float16 type
s = s.astype(np.float16)

# convert Series to Python strings
s = s.astype(str)

# convert Series to categorical type - see docs for more details
s = s.astype('category')

```

Notice I said ""try"" - if [`astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) does not know how to convert a value in the Series or DataFrame, it will raise an error. For example, if you have a `NaN` or `inf` value you'll get an error trying to convert it to an integer.

As of pandas 0.20.0, this error can be suppressed by passing `errors='ignore'`. Your original object will be returned untouched.

Be careful
----------

[`astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) is powerful, but it will sometimes convert values ""incorrectly"". For example:

```
>>> s = pd.Series([1, 2, -7])
>>> s
0    1
1    2
2   -7
dtype: int64

```

These are small integers, so how about converting to an unsigned 8-bit type to save memory?

```
>>> s.astype(np.uint8)
0      1
1      2
2    249
dtype: uint8

```

The conversion worked, but the -7 was wrapped round to become 249 (i.e. 28 - 7)!

Trying to downcast using `pd.to_numeric(s, downcast='unsigned')` instead could help prevent this error.

---

3. `infer_objects()`
====================

Version 0.21.0 of pandas introduced the method [`infer_objects()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.infer_objects.html) for converting columns of a DataFrame that have an object datatype to a more specific type (soft conversions).

For example, here's a DataFrame with two columns of object type. One holds actual integers and the other holds strings representing integers:

```
>>> df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')
>>> df.dtypes
a    object
b    object
dtype: object

```

Using [`infer_objects()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.infer_objects.html), you can change the type of column 'a' to int64:

```
>>> df = df.infer_objects()
>>> df.dtypes
a     int64
b    object
dtype: object

```

Column 'b' has been left alone since its values were strings, not integers. If you wanted to force both columns to an integer type, you could use `df.astype(int)` instead.

---

4. `convert_dtypes()`
=====================

Version 1.0 and above includes a method [`convert_dtypes()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html) to convert Series and DataFrame columns to the best possible dtype that supports the `pd.NA` missing value.

Here ""best possible"" means the type most suited to hold the values. For example, this a pandas integer type, if all of the values are integers (or missing values): an object column of Python integer objects are converted to `Int64`, a column of NumPy `int32` values, will become the pandas dtype `Int32`.

With our `object` DataFrame `df`, we get the following result:

```
>>> df.convert_dtypes().dtypes                                             
a     Int64
b    string
dtype: object

```

Since column 'a' held integer values, it was converted to the `Int64` type (which is capable of holding missing values, unlike `int64`).

Column 'b' contained string objects, so was changed to pandas' `string` dtype.

By default, this method will infer the type from object values in each column. We can change this by passing `infer_objects=False`:

```
>>> df.convert_dtypes(infer_objects=False).dtypes                          
a    object
b    string
dtype: object

```

Now column 'a' remained an object column: pandas knows it can be described as an 'integer' column (internally it ran [`infer_dtype`](https://github.com/pandas-dev/pandas/blob/6b2d0260c818e62052eaf535767f3a8c4b446c69/pandas/_libs/lib.pyx#L1188-L1434)) but didn't infer exactly what dtype of integer it should have so did not convert it. Column 'b' was again converted to 'string' dtype as it was recognised as holding 'string' values."
"How do I sort a dictionary by its keys?

Example input:

```
{2:3, 1:89, 4:5, 3:0}

```

Desired output:

```
{1:89, 2:3, 3:0, 4:5}

```","> **Note:** for Python 3.7+, see [this answer](https://stackoverflow.com/a/47017849)

Standard Python dictionaries are unordered (until Python 3.7). Even if you sorted the (key,value) pairs, you wouldn't be able to store them in a `dict` in a way that would preserve the ordering.

The easiest way is to use [`OrderedDict`](http://docs.python.org/library/collections.html#collections.OrderedDict), which remembers the order in which the elements have been inserted:

```
In [1]: import collections

In [2]: d = {2:3, 1:89, 4:5, 3:0}

In [3]: od = collections.OrderedDict(sorted(d.items()))

In [4]: od
Out[4]: OrderedDict([(1, 89), (2, 3), (3, 0), (4, 5)])

```

Never mind the way `od` is printed out; it'll work as expected:

```
In [11]: od[1]
Out[11]: 89

In [12]: od[3]
Out[12]: 0

In [13]: for k, v in od.iteritems(): print k, v
   ....: 
1 89
2 3
3 0
4 5

```

Python 3
--------

For Python 3 users, one needs to use the `.items()` instead of `.iteritems()`:

```
In [13]: for k, v in od.items(): print(k, v)
   ....: 
1 89
2 3
3 0
4 5

```"
"What are the differences between these two code snippets?

Using [`type`](https://docs.python.org/3/library/functions.html#type):

```
import types

if type(a) is types.DictType:
    do_something()
if type(b) in types.StringTypes:
    do_something_else()

```

Using [`isinstance`](https://docs.python.org/3/library/functions.html#isinstance):

```
if isinstance(a, dict):
    do_something()
if isinstance(b, str) or isinstance(b, unicode):
    do_something_else()

```","To summarize the contents of other (already good!) answers, `isinstance` caters for inheritance (an instance of a derived class *is an* instance of a base class, too), while checking for equality of `type` does not (it demands identity of types and rejects instances of subtypes, AKA subclasses).

Normally, in Python, you want your code to support inheritance, of course (since inheritance is so handy, it would be bad to stop code using yours from using it!), so `isinstance` is less bad than checking identity of `type`s because it seamlessly supports inheritance.

It's not that `isinstance` is *good*, mind you—it's just *less bad* than checking equality of types. The normal, Pythonic, preferred solution is almost invariably ""duck typing"": try using the argument *as if* it was of a certain desired type, do it in a `try`/`except` statement catching all exceptions that could arise if the argument was not in fact of that type (or any other type nicely duck-mimicking it;-), and in the `except` clause, try something else (using the argument ""as if"" it was of some other type).

`basestring` **is**, however, quite a special case—a builtin type that exists **only** to let you use `isinstance` (both `str` and `unicode` subclass `basestring`). Strings are sequences (you could loop over them, index them, slice them, ...), but you generally want to treat them as ""scalar"" types—it's somewhat incovenient (but a reasonably frequent use case) to treat all kinds of strings (and maybe other scalar types, i.e., ones you can't loop on) one way, all containers (lists, sets, dicts, ...) in another way, and `basestring` plus `isinstance` helps you do that—the overall structure of this idiom is something like:

```
if isinstance(x, basestring)
  return treatasscalar(x)
try:
  return treatasiter(iter(x))
except TypeError:
  return treatasscalar(x)

```

You could say that `basestring` is an *Abstract Base Class* (""ABC"")—it offers no concrete functionality to subclasses, but rather exists as a ""marker"", mainly for use with `isinstance`. The concept is obviously a growing one in Python, since [PEP 3119](http://www.python.org/dev/peps/pep-3119/), which introduces a generalization of it, was accepted and has been implemented starting with Python 2.6 and 3.0.

The PEP makes it clear that, while ABCs can often substitute for duck typing, there is generally no big pressure to do that (see [here](http://www.python.org/dev/peps/pep-3119/#abcs-vs-duck-typing)). ABCs as implemented in recent Python versions do however offer extra goodies: `isinstance` (and `issubclass`) can now mean more than just ""[an instance of] a derived class"" (in particular, any class can be ""registered"" with an ABC so that it will show as a subclass, and its instances as instances of the ABC); and ABCs can also offer extra convenience to actual subclasses in a very natural way via Template Method design pattern applications (see [here](http://en.wikipedia.org/wiki/Template_method_pattern) and [here](http://www.catonmat.net/blog/learning-python-design-patterns-through-video-lectures/) [[part II]] for more on the TM DP, in general and specifically in Python, independent of ABCs).

For the underlying mechanics of ABC support as offered in Python 2.6, see [here](http://docs.python.org/library/abc.html); for their 3.1 version, very similar, see [here](http://docs.python.org/3.1/library/abc.html). In both versions, standard library module [collections](http://docs.python.org/3.1/library/collections.html#module-collections) (that's the 3.1 version—for the very similar 2.6 version, see [here](http://docs.python.org/library/collections.html#module-collections)) offers several useful ABCs.

For the purpose of this answer, the key thing to retain about ABCs (beyond an arguably more natural placement for TM DP functionality, compared to the classic Python alternative of mixin classes such as [UserDict.DictMixin](http://docs.python.org/library/userdict.html?highlight=userdict#UserDict.DictMixin)) is that they make `isinstance` (and `issubclass`) much more attractive and pervasive (in Python 2.6 and going forward) than they used to be (in 2.5 and before), and therefore, by contrast, make checking type equality an even worse practice in recent Python versions than it already used to be."
"I set `dict2 = dict1`. When I edit `dict2`, [the original `dict1` also changes](https://stackoverflow.com/questions/2438938). How can I avoid this?

```
>>> dict1 = {""key1"": ""value1"", ""key2"": ""value2""}
>>> dict2 = dict1
>>> dict2[""key2""] = ""WHY?!""
>>> dict1
{'key2': 'WHY?!', 'key1': 'value1'}

```","Python *never* implicitly copies objects. When you set `dict2 = dict1`, you are making them refer to the same exact dict object, so when you mutate it, all references to it keep referring to the object in its current state.

If you want to copy the dict (which is rare), you have to do so explicitly with

```
dict2 = dict(dict1)

```

or

```
dict2 = dict1.copy()

```"
"So what I'm looking for here is something like PHP's [print\_r](https://www.php.net/print_r) function.

This is so I can debug my scripts by seeing what's the state of the object in question.","You want `vars()` mixed with `pprint()`:

```
from pprint import pprint
pprint(vars(your_object))

```"
"[TypeError: 'str' does not support the buffer interface](https://stackoverflow.com/questions/5471158/typeerror-str-does-not-support-the-buffer-interface) suggests two possible methods to convert a string to bytes:

```
b = bytes(mystring, 'utf-8')

b = mystring.encode('utf-8')

```

What are the differences between them? Which one should I opt for and why?

---

See [Convert bytes to a string in Python 3](https://stackoverflow.com/questions/606191) for the other way around.","If you look at the docs for `bytes`, it points you to [`bytearray`](https://docs.python.org/3/library/functions.html#func-bytearray):

> bytearray([source[, encoding[, errors]]])
>
> Return a new array of bytes. The bytearray type is a mutable sequence of integers in the range 0 <= x < 256. It has most of the usual methods of mutable sequences, described in Mutable Sequence Types, as well as most methods that the bytes type has, see Bytes and Byte Array Methods.
>
> The optional source parameter can be used to initialize the array in a few different ways:
>
> **If it is a string, you must also give the encoding (and optionally, errors) parameters; bytearray() then converts the string to bytes using str.encode().**
>
> **If it is an integer, the array will have that size and will be initialized with null bytes.**
>
> **If it is an object conforming to the buffer interface, a read-only buffer of the object will be used to initialize the bytes array.**
>
> **If it is an iterable, it must be an iterable of integers in the range 0 <= x < 256, which are used as the initial contents of the array.**
>
> **Without an argument, an array of size 0 is created.**

So `bytes` can do much more than just encode a string. It's Pythonic that it would allow you to call the constructor with any type of source parameter that makes sense.

For encoding a string, I think that `some_string.encode(encoding)` is more Pythonic than using the constructor, because it is the most self documenting -- ""take this string and encode it with this encoding"" is clearer than `bytes(some_string, encoding)` -- there is no explicit verb when you use the constructor.

I checked the Python source. If you pass a unicode string to `bytes` using CPython, it calls [PyUnicode\_AsEncodedString](http://hg.python.org/cpython/file/5a12416890c0/Objects/unicodeobject.c#l2328), which is the implementation of `encode`; so you're just skipping a level of indirection if you call `encode` yourself.

Also, see Serdalis' comment -- `unicode_string.encode(encoding)` is also more Pythonic because its inverse is `byte_string.decode(encoding)` and symmetry is nice."
I would like a clear example showing tasks being divided across multiple threads.,"Since this question was asked in 2010, there has been real simplification in how to do simple multithreading with Python with **[map](https://docs.python.org/2/library/functions.html#map)** and **[pool](https://docs.python.org/2/library/multiprocessing.html)**.

The code below comes from an article/blog post that you should definitely check out (no affiliation) - **[Parallelism in one line: A Better Model for Day to Day Threading Tasks](https://chriskiehl.com/article/parallelism-in-one-line)**. I'll summarize below - it ends up being just a few lines of code:

```
from multiprocessing.dummy import Pool as ThreadPool
pool = ThreadPool(4)
results = pool.map(my_function, my_array)

```

Which is the multithreaded version of:

```
results = []
for item in my_array:
    results.append(my_function(item))

```

---

**Description**

> Map is a cool little function, and the key to easily injecting parallelism into your Python code. For those unfamiliar, map is something lifted from functional languages like Lisp. It is a function which maps another function over a sequence.
>
> Map handles the iteration over the sequence for us, applies the function, and stores all of the results in a handy list at the end.

![Enter image description here](https://i.sstatic.net/Yq37m.png)

---

**Implementation**

> Parallel versions of the map function are provided by two libraries:multiprocessing, and also its little known, but equally fantastic step child:multiprocessing.dummy.

`multiprocessing.dummy` is exactly the same as multiprocessing module, [but uses threads instead](https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy) (*[an important distinction](https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b) - use multiple processes for CPU-intensive tasks; threads for (and during) I/O*):

> multiprocessing.dummy replicates the API of multiprocessing, but is no more than a wrapper around the threading module.

```
import urllib2
from multiprocessing.dummy import Pool as ThreadPool

urls = [
  'http://www.python.org',
  'http://www.python.org/about/',
  'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html',
  'http://www.python.org/doc/',
  'http://www.python.org/download/',
  'http://www.python.org/getit/',
  'http://www.python.org/community/',
  'https://wiki.python.org/moin/',
]

# Make the Pool of workers
pool = ThreadPool(4)

# Open the URLs in their own threads
# and return the results
results = pool.map(urllib2.urlopen, urls)

# Close the pool and wait for the work to finish
pool.close()
pool.join()

```

And the timing results:

```
Single thread:   14.4 seconds
       4 Pool:   3.1 seconds
       8 Pool:   1.4 seconds
      13 Pool:   1.3 seconds

```

---

**Passing multiple arguments** (works like this [only in Python 3.3 and later](https://stackoverflow.com/a/28975239/2327328)):

To pass multiple arrays:

```
results = pool.starmap(function, zip(list_a, list_b))

```

Or to pass a constant and an array:

```
results = pool.starmap(function, zip(itertools.repeat(constant), list_a))

```

If you are using an earlier version of Python, you can pass multiple arguments via [this workaround](https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments/5443941#5443941)).

(Thanks to [user136036](https://stackoverflow.com/users/2441026/user136036) for the helpful comment.)"
"I'm having problems dealing with unicode characters from text fetched from different web pages (on different sites). I am using BeautifulSoup.

The problem is that the error is not always reproducible; it sometimes works with some pages, and sometimes, it barfs by throwing a `UnicodeEncodeError`. I have tried just about everything I can think of, and yet I have not found anything that works consistently without throwing some kind of Unicode-related error.

One of the sections of code that is causing problems is shown below:

```
agent_telno = agent.find('div', 'agent_contact_number')
agent_telno = '' if agent_telno is None else agent_telno.contents[0]
p.agent_info = str(agent_contact + ' ' + agent_telno).strip()

```

Here is a stack trace produced on SOME strings when the snippet above is run:

```
Traceback (most recent call last):
  File ""foobar.py"", line 792, in <module>
    p.agent_info = str(agent_contact + ' ' + agent_telno).strip()
UnicodeEncodeError: 'ascii' codec can't encode character u'\xa0' in position 20: ordinal not in range(128)

```

I suspect that this is because some pages (or more specifically, pages from some of the sites) may be encoded, whilst others may be unencoded. All the sites are based in the UK and provide data meant for UK consumption - so there are no issues relating to internalization or dealing with text written in anything other than English.

Does anyone have any ideas as to how to solve this so that I can CONSISTENTLY fix this problem?","Read the Python [Unicode HOWTO](https://docs.python.org/2.7/howto/unicode.html). This error is the [very first example](https://docs.python.org/2.7/howto/unicode.html#the-unicode-type).

Do not use `str()` to convert from unicode to encoded text / bytes.

Instead, use [`.encode()`](http://docs.python.org/library/stdtypes.html#str.encode) to encode the string:

```
p.agent_info = u' '.join((agent_contact, agent_telno)).encode('utf-8').strip()

```

or work entirely in unicode."
"How can I do the equivalent of [`mv`](https://ss64.com/bash/mv.html) in Python?

```
mv ""path/to/current/file.foo"" ""path/to/new/destination/for/file.foo""

```","[`os.rename()`](http://docs.python.org/library/os.html#os.rename), [`os.replace()`](https://docs.python.org/library/os.html#os.replace), or [`shutil.move()`](http://docs.python.org/library/shutil.html#shutil.move)

All employ the same syntax:

```
import os
import shutil

os.rename(""path/to/current/file.foo"", ""path/to/new/destination/for/file.foo"")
os.replace(""path/to/current/file.foo"", ""path/to/new/destination/for/file.foo"")
shutil.move(""path/to/current/file.foo"", ""path/to/new/destination/for/file.foo"")

```

* The filename (`""file.foo""`) must be included in both the source and destination arguments. If it differs between the two, the file will be renamed as well as moved.
* The directory within which the new file is being created must already exist.
* On Windows, a file with that name must not exist or an exception will be raised, but `os.replace()` will silently replace a file even in that occurrence.
* `shutil.move` simply calls `os.rename` in most cases. However, if the destination is on a different disk than the source, it will instead copy and then delete the source file."
"I have a Python script which takes as input a list of integers, which I need to work with four integers at a time. Unfortunately, I don't have control of the input, or I'd have it passed in as a list of four-element tuples. Currently, I'm iterating over it this way:

```
for i in range(0, len(ints), 4):
    # dummy op for example code
    foo += ints[i] * ints[i + 1] + ints[i + 2] * ints[i + 3]

```

It looks a lot like ""C-think"", though, which makes me suspect there's a more pythonic way of dealing with this situation. The list is discarded after iterating, so it needn't be preserved. Perhaps something like this would be better?

```
while ints:
    foo += ints[0] * ints[1] + ints[2] * ints[3]
    ints[0:4] = []

```

Still doesn't quite ""feel"" right, though. :-/

**Update:** With the release of Python 3.12, I've changed the accepted answer. For anyone who has not (or cannot) make the jump to Python 3.12 yet, I encourage you to check out the [previous accepted answer](https://stackoverflow.com/questions/434287#answer-434411) or any of the other excellent, backwards-compatible answers below.

Related question: [How do you split a list into evenly sized chunks in Python?](https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python)","```
def chunker(seq, size):
    return (seq[pos:pos + size] for pos in range(0, len(seq), size))

```

Works with any sequence:

```
text = ""I am a very, very helpful text""

for group in chunker(text, 7):
   print(repr(group),)
# 'I am a ' 'very, v' 'ery hel' 'pful te' 'xt'

print('|'.join(chunker(text, 10)))
# I am a ver|y, very he|lpful text

animals = ['cat', 'dog', 'rabbit', 'duck', 'bird', 'cow', 'gnu', 'fish']

for group in chunker(animals, 3):
    print(group)
# ['cat', 'dog', 'rabbit']
# ['duck', 'bird', 'cow']
# ['gnu', 'fish']

```"
"How do I get the current file's directory path?
I tried:

```
>>> os.path.abspath(__file__)
'C:\\python27\\test.py'

```

But I want:

```
'C:\\python27\\'

```","The special variable [`__file__`](https://stackoverflow.com/questions/9271464/what-does-the-file-variable-mean-do) contains the path to the current file. From that we can get the directory using either [`pathlib`](https://docs.python.org/3/library/pathlib.html) or the [`os.path`](https://docs.python.org/3.8/library/os.path.html) module.

Python 3
--------

For the directory of the script being run:

```
import pathlib
pathlib.Path(__file__).parent.resolve()

```

For the current working directory:

```
import pathlib
pathlib.Path().resolve()

```

Python 2 and 3
--------------

For the directory of the script being run:

```
import os
os.path.dirname(os.path.abspath(__file__))

```

If you mean the current working directory:

```
import os
os.path.abspath(os.getcwd())

```

Note that before and after `file` is two underscores, not just one.

Also note that if you are running interactively or have loaded code from something other than a file (eg: a database or online resource), `__file__` may not be set since there is no notion of ""current file"". The above answer assumes the most common scenario of running a python script that is in a file.

References
----------

1. [pathlib](https://docs.python.org/3/library/pathlib.html) in the python documentation.
2. [os.path - Python 2.7](https://docs.python.org/2.7/library/os.path.html), [os.path - Python 3](https://docs.python.org/3/library/os.path.html)
3. [os.getcwd - Python 2.7](https://docs.python.org/2.7/library/os.html#os.getcwd), [os.getcwd - Python 3](https://docs.python.org/3/library/os.html#os.getcwd)
4. [what does the \_\_file\_\_ variable mean/do?](https://stackoverflow.com/questions/9271464/what-does-the-file-variable-mean-do)"
What does `assert` mean? How is it used?,"The `assert` statement exists in almost every programming language. It has two main uses:

1. It helps detect problems early in your program, where the cause is clear, rather than later when some other operation fails. A type error in Python, for example, can go through several layers of code before actually raising an `Exception` if not caught early on.
2. It works as documentation for other developers reading the code, who see the `assert` and can confidently say that its condition holds from now on.

When you do...

```
assert condition

```

... you're telling the program to test that condition, and immediately trigger an error if the condition is false.

In Python, it's roughly equivalent to this:

```
if not condition:
    raise AssertionError()

```

Try it in the Python shell:

```
>>> assert True # nothing happens
>>> assert False
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AssertionError

```

Assertions can include an optional message, and you can disable them when running the interpreter.

To print a message if the assertion fails:

```
assert False, ""Oh no! This assertion failed!""

```

Do **not** use parenthesis to call `assert` like a function. It is a statement. If you do `assert(condition, message)` you'll be running the `assert` with a `(condition, message)` tuple as first parameter.

As for disabling them, when running `python` in optimized mode, where `__debug__` is `False`, assert statements will be ignored. Just pass the `-O` flag:

```
python -O script.py

```

See [here](http://docs.python.org/reference/simple_stmts.html#assert) for the relevant documentation."
How do I refer to the [null object](https://en.wikipedia.org/wiki/Null_pointer) in Python?,"In Python, the 'null' object is the singleton `None`.

To check if something is `None`, use the [`is`](https://docs.python.org/3/reference/expressions.html#is) identity operator:

```
if foo is None:
    ...

```"
How can I check if a list has any duplicates and return a new list without duplicates?,"The common approach to get a unique collection of items is to use a [`set`](http://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset). Sets are *unordered* collections of *distinct* objects. To create a set from any iterable, you can simply pass it to the built-in [`set()`](http://docs.python.org/3/library/functions.html#func-set) function. If you later need a real list again, you can similarly pass the set to the [`list()`](http://docs.python.org/3/library/functions.html#func-list) function.

The following example should cover whatever you are trying to do:

```
>>> t = [1, 2, 3, 1, 2, 3, 5, 6, 7, 8]
>>> list(set(t))
[1, 2, 3, 5, 6, 7, 8]
>>> s = [1, 2, 3]
>>> list(set(t) - set(s))
[8, 5, 6, 7]

```

As you can see from the example result, *the original order is not maintained*. As mentioned above, sets themselves are unordered collections, so the order is lost. When converting a set back to a list, an arbitrary order is created.

### Maintaining order

If order is important to you, then you will have to use a different mechanism. A very common solution for this is to rely on [`OrderedDict`](https://docs.python.org/3/library/collections.html#collections.OrderedDict) to keep the order of keys during insertion:

```
>>> from collections import OrderedDict
>>> list(OrderedDict.fromkeys(t))
[1, 2, 3, 5, 6, 7, 8]

```

[Starting with Python 3.7](https://mail.python.org/pipermail/python-dev/2017-December/151283.html), the built-in dictionary is guaranteed to maintain the insertion order as well, so you can also use that directly if you are on Python 3.7 or later (or CPython 3.6):

```
>>> list(dict.fromkeys(t))
[1, 2, 3, 5, 6, 7, 8]

```

Note that this may have some overhead of creating a dictionary first, and then creating a list from it. If you don’t actually need to preserve the order, you’re often better off using a set, especially because it gives you a lot more operations to work with. Check out [this question](https://stackoverflow.com/q/480214/216074) for more details and alternative ways to preserve the order when removing duplicates.

---

Finally note that both the `set` as well as the `OrderedDict`/`dict` solutions require your items to be *hashable*. This usually means that they have to be immutable. If you have to deal with items that are not hashable (e.g. list objects), then you will have to use a slow approach in which you will basically have to compare every item with every other item in a nested loop."
"I have this JSON in a file:

```
{
    ""maps"": [
        {
            ""id"": ""blabla"",
            ""iscategorical"": ""0""
        },
        {
            ""id"": ""blabla"",
            ""iscategorical"": ""0""
        }
    ],
    ""masks"": [
        ""id"": ""valore""
    ],
    ""om_points"": ""value"",
    ""parameters"": [
        ""id"": ""valore""
    ]
}

```

I wrote this script to print all of the JSON data:

```
import json
from pprint import pprint

with open('data.json') as f:
    data = json.load(f)

pprint(data)

```

This program raises an exception, though:

```
Traceback (most recent call last):
  File ""<pyshell#1>"", line 5, in <module>
    data = json.load(f)
  File ""/usr/lib/python3.5/json/__init__.py"", line 319, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python3.5/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python3.5/json/decoder.py"", line 355, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting ',' delimiter: line 13 column 13 (char 213)

```

How can I parse the JSON and extract its values?","Your data is not valid [JSON](https://www.json.org/) format. You have `[]` when you should have `{}` for the `""masks""` and `""parameters""` elements:

* `[]` are for JSON arrays, which are called `list` in Python
* `{}` are for JSON objects, which are called `dict` in Python

Here's how your JSON file should look:

```
{
    ""maps"": [
        {
            ""id"": ""blabla"",
            ""iscategorical"": ""0""
        },
        {
            ""id"": ""blabla"",
            ""iscategorical"": ""0""
        }
    ],
    ""masks"": {
        ""id"": ""valore""
    },
    ""om_points"": ""value"",
    ""parameters"": {
        ""id"": ""valore""
    }
}

```

Then you can use your code:

```
import json
from pprint import pprint

with open('data.json') as f:
    data = json.load(f)

pprint(data)

```

With data, you can now also find values like so:

```
data[""maps""][0][""id""]
data[""masks""][""id""]
data[""om_points""]

```

Try those out and see if it starts to make sense."
"How do I print the error/exception in the `except:` block?

```
try:
    ...
except:
    print(exception)

```","For Python 2.6 and later and Python 3.x:

```
except Exception as e: print(e)

```

For Python 2.5 and earlier, use:

```
except Exception,e: print str(e)

```"
"How do I check which version of the Python interpreter is running my script?

---

See [Find full path of the Python interpreter (Python executable)?](https://stackoverflow.com/questions/2589711) if you are looking to find exactly *which interpreter* is being used - for example, to debug a Pip installation problem, or to check which virtual environment (if any) is active.","This information is available in the [`sys.version`](http://docs.python.org/library/sys.html#sys.version) string in the [`sys`](http://docs.python.org/library/sys.html) module:

```
>>> import sys

```

Human readable:

```
>>> print(sys.version)  # parentheses necessary in python 3.       
2.5.2 (r252:60911, Jul 31 2008, 17:28:52) 
[GCC 4.2.3 (Ubuntu 4.2.3-2ubuntu7)]

```

For further processing, use [`sys.version_info`](http://docs.python.org/library/sys.html#sys.version_info) or [`sys.hexversion`](http://docs.python.org/library/sys.html#sys.hexversion):

```
>>> sys.version_info
(2, 5, 2, 'final', 0)
# or
>>> sys.hexversion
34014192

```

To ensure a script runs with a minimal version requirement of the Python interpreter add this to your code:

```
assert sys.version_info >= (2, 5)

```

This compares major and minor version information. Add micro (=`0`, `1`, etc) and even releaselevel (=`'alpha'`,`'final'`, etc) to the tuple as you like. Note however, that it is almost always better to ""duck"" check if a certain feature is there, and if not, workaround (or bail out). Sometimes features go away in newer releases, being replaced by others."
"How to make a Python class serializable?

```
class FileItem:
    def __init__(self, fname):
        self.fname = fname

```

Attempt to serialize to JSON:

```
>>> import json
>>> x = FileItem('/foo/bar')
>>> json.dumps(x)
TypeError: Object of type 'FileItem' is not JSON serializable

```","Here is a simple solution for a simple feature:

`.toJSON()` Method
------------------

Instead of a JSON serializable class, implement a serializer method:

```
import json

class Object:
    def toJSON(self):
        return json.dumps(
            self,
            default=lambda o: o.__dict__, 
            sort_keys=True,
            indent=4)

```

So you just call it to serialize:

```
me = Object()
me.name = ""Onur""
me.age = 35
me.dog = Object()
me.dog.name = ""Apollo""

print(me.toJSON())

```

will output:

```
{
    ""age"": 35,
    ""dog"": {
        ""name"": ""Apollo""
    },
    ""name"": ""Onur""
}

```

---

For a fully-featured library, you can use [**orjson**](https://github.com/ijl/orjson)."
"Background
----------

I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:

```
E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE

```

I want to know what exactly it means? Do I need to change something?

How should I suspend the warning if I insist to use `quote_df['TVol'] = quote_df['TVol']/TVOL_SCALE`?

The function that gives warnings
--------------------------------

```
def _decode_stock_quote(list_of_150_stk_str):
    """"""decode the webpage and return dataframe""""""

    from cStringIO import StringIO

    str_of_all = """".join(list_of_150_stk_str)

    quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}
    quote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)
    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
    quote_df['TClose'] = quote_df['TPrice']
    quote_df['RT']     = 100 * (quote_df['TPrice']/quote_df['TPCLOSE'] - 1)
    quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
    quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE
    quote_df['STK_ID'] = quote_df['STK'].str.slice(13,19)
    quote_df['STK_Name'] = quote_df['STK'].str.slice(21,30)#.decode('gb2312')
    quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])
    
    return quote_df

```

More warning messages
---------------------

```
E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
E:\FinReporter\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE
E:\FinReporter\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])

```","The `SettingWithCopyWarning` was created to flag potentially confusing ""chained"" assignments, such as the following, which does not always work as expected, particularly when the first selection returns a *copy*. [see [GH5390](https://github.com/pydata/pandas/pull/5390) and [GH5597](https://github.com/pydata/pandas/issues/5597) for background discussion.]

```
df[df['A'] > 2]['B'] = new_val  # new_val not set in df

```

The warning offers a suggestion to rewrite as follows:

```
df.loc[df['A'] > 2, 'B'] = new_val

```

However, this doesn't fit your usage, which is equivalent to:

```
df = df[df['A'] > 2]
df['B'] = new_val

```

While it's clear that you don't care about writes making it back to the original frame (since you are overwriting the reference to it), unfortunately this pattern cannot be differentiated from the first chained assignment example. Hence the (false positive) warning. The potential for false positives is addressed in the [docs on indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy), if you'd like to read further. You can safely disable this new warning with the following assignment.

```
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

```

---

Other Resources
---------------

* [pandas User Guide: Indexing and selecting data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)
* [Python Data Science Handbook: Data Indexing and Selection](https://jakevdp.github.io/PythonDataScienceHandbook/03.02-data-indexing-and-selection.html)
* [Real Python: SettingWithCopyWarning in Pandas: Views vs Copies](https://realpython.com/pandas-settingwithcopywarning/)
* [Dataquest: SettingwithCopyWarning: How to Fix This Warning in Pandas](https://www.dataquest.io/blog/settingwithcopywarning/)
* [Towards Data Science: Explaining the SettingWithCopyWarning in pandas](https://towardsdatascience.com/explaining-the-settingwithcopywarning-in-pandas-ebc19d799d25)"
"Sometimes I download the Python source code from [GitHub](https://en.wikipedia.org/wiki/GitHub) and don't know how to install all the dependencies. If there isn't any *requirements.txt* file I have to create it by hand.

Given the Python source code directory, is it possible to create *requirements.txt* automatically from the import section?","You can use the following code to generate a `requirements.txt` file:

```
pip install pipreqs
pipreqs /path/to/project

```

The benefits of using `pipreqs` from [its GitHub](https://github.com/bndr/pipreqs).

> ### Why not pip freeze?
>
> * `pip freeze` only saves the packages that are installed with `pip install` in your environment.
> * `pip freeze` saves all packages in the environment including those that you don't use in your current project (if you don't have `virtualenv`).
> * and sometimes you just need to create `requirements.txt` for a new project without installing modules."
"I want to install [pip](http://pypi.python.org/pypi/pip). It should support Python 3, but it requires setuptools, which is available only for Python 2.

How can I install pip with Python 3?","edit: Manual installation and use of `setuptools` is not the standard process anymore.

If you're running Python 2.7.9+ or Python 3.4+
----------------------------------------------

Congrats, you *should* already have `pip` installed. If you do not, read onward.

If you're running a Unix-like System
------------------------------------

You can usually install the package for `pip` through your package manager if your version of Python is older than 2.7.9 or 3.4, or if your system did not include it for whatever reason.

Instructions for some of the more common distros follow.

### Installing on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 2.x

Run the following command from a terminal:

```
sudo apt-get install python-pip 

```

### Installing on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 3.x

Run the following command from a terminal:

```
sudo apt-get install python3-pip

```

**Note:**

On a fresh Debian/Ubuntu install, the package may not be found until you do:

```
sudo apt-get update

```

### Installing `pip` on CentOS 7 for Python 2.x

On CentOS 7, you have to install setup tools first, and then use that to install `pip`, as there is no direct package for it.

```
sudo yum install python-setuptools
sudo easy_install pip

```

### Installing `pip` on CentOS 7 for Python 3.x

Assuming you installed Python 3.4 [from EPEL](https://fedoraproject.org/wiki/EPEL), you can install Python 3's setup tools and use it to install `pip`.

```
# First command requires you to have enabled EPEL for CentOS7
sudo yum install python34-setuptools
sudo easy_install pip

```

### If your Unix/Linux distro doesn't have it in package repos

Install using the manual way detailed below.

The manual way
--------------

If you want to do it the manual way, the now-recommended method is to install using the `get-pip.py` script from [`pip`'s installation instructions](https://pip.pypa.io/en/stable/installing.html).

> Install pip
>
> To install pip, securely download [`get-pip.py`](https://bootstrap.pypa.io/get-pip.py)
>
> Then run the following (which may require administrator access):
>
> ```
> python get-pip.py 
>
> ```
>
> If `setuptools` is not already installed, `get-pip.py` will install setuptools for you."
"I have this DataFrame and want only the records whose EPS column is not NaN:

```
                 STK_ID  EPS  cash
STK_ID RPT_Date                   
601166 20111231  601166  NaN   NaN
600036 20111231  600036  NaN    12
600016 20111231  600016  4.3   NaN
601009 20111231  601009  NaN   NaN
601939 20111231  601939  2.5   NaN
000001 20111231  000001  NaN   NaN

```

...i.e. something like `df.drop(....)` to get this resulting dataframe:

```
                  STK_ID  EPS  cash
STK_ID RPT_Date                   
600016 20111231  600016  4.3   NaN
601939 20111231  601939  2.5   NaN

```

How do I do that?","Don't drop, just take the rows where EPS is not NA:

```
df = df[df['EPS'].notna()]

```"
"In nodejs, I can do `npm install package --save-dev` to save the installed package into the package.

How do I achieve the same thing in Python package manager `pip`? I would like to save the package name and its version into, say, `requirements.pip` just after installing the package using something like `pip install package --save-dev requirements.pip`.","There isn't an equivalent with `pip`.

Best way is to `pip install package && pip freeze > requirements.txt`

You can see all the available options on their [documentation page](https://pip.pypa.io/en/latest/cli/).

If it really bothers you, it wouldn't be too difficult to write a custom bash script (`pips`) that takes a `-s` argument and freezes to your `requirements.txt` file automatically.

**Edit 1**

Since writing this there has been no change in providing an auto `--save-dev` option similar to NPM however Kenneth Reitz (author of `requests` and many more) has released some more info about a [better pip workflow](https://kennethreitz.org/essays/2016/02/25/a-better-pip-workflow) to better handle `pip` updates.

**Edit 2**

Linked from the ""better pip workflow"" article above it is now recommended to use [`pipenv`](https://pipenv.readthedocs.io/en/latest/) to manage requirements and virtual environments. Having used this a lot recently I would like to summarise how simple the transition is:

*Install `pipenv` (on Mac)*

```
brew install pipenv

```

*`pipenv` creates and manages it's own virtual environments so in a project with an existing `requirements.txt`, installing all requirements (I use Python3.7 but you can remove the `--three` if you do not) is as simple as:*

```
pipenv --three install

```

*Activating the virtualenv to run commands is also easy*

```
pipenv shell

```

*Installing requirements will automatically update the `Pipfile` and `Pipfile.lock`*

```
pipenv install <package>

```

*It's also possible to update out-of-date packages*

```
pipenv update

```

I highly recommend [checking it out](https://pipenv.readthedocs.io/en/latest/basics/) especially if coming from a `npm` background as it has a similar feel to `package.json` and `package-lock.json`"
"According to <http://www.faqs.org/docs/diveintopython/fileinfo_private.html>:

> Like most languages, Python has the
> concept of private elements:
>
> * Private
>   functions, which can't be called from
>   **outside their module**

However, if I define two files:

```
#a.py
__num=1

```

and:

```
#b.py
import a
print a.__num

```

when i run `b.py` it prints out `1` without giving any exception. Is diveintopython wrong, or did I misunderstand something? And is there some way to **do** define a module's function as private?","In Python, ""privacy"" depends on ""consenting adults'"" levels of agreement - you can't *force* it. A single leading underscore means you're not **supposed** to access it ""from the outside"" -- **two** leading underscores (w/o trailing underscores) carry the message even more forcefully... but, in the end, it still depends on social convention and consensus: Python's introspection is forceful enough that you can't **handcuff** every other programmer in the world to respect your wishes.

((Btw, though it's a closely held secret, much the same holds for C++: with most compilers, a simple `#define private public` line before `#include`ing your `.h` file is all it takes for wily coders to make hash of your ""privacy""...!-))"
"I have the following dataframes:

```
> df1
  id  begin conditional confidence discoveryTechnique  
0 278    56       false        0.0                  1   
1 421    18       false        0.0                  1 

> df2
   concept 
0  A  
1  B

```

How do I merge on the indices to get:

```
  id  begin conditional confidence discoveryTechnique concept 
0 278    56       false        0.0                  1       A 
1 421    18       false        0.0                  1       B

```

I ask because it is my understanding that `merge()` i.e. `df1.merge(df2)` uses columns to do the matching. In fact, doing this I get:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 4618, in merge
    copy=copy, indicator=indicator)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 58, in merge
    copy=copy, indicator=indicator)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 491, in __init__
    self._validate_specification()
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 812, in _validate_specification
    raise MergeError('No common columns to perform merge on')
pandas.tools.merge.MergeError: No common columns to perform merge on

```

Is it bad practice to merge on index? Is it impossible? If so, how can I shift the index into a new column called ""index""?","Use [`merge`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html), which is an inner join by default:

```
pd.merge(df1, df2, left_index=True, right_index=True)

```

Or [`join`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html), which is a left join by default:

```
df1.join(df2)

```

Or [`concat`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), which is an outer join by default:

```
pd.concat([df1, df2], axis=1)

```

**Samples**:

```
df1 = pd.DataFrame({'a':range(6),
                    'b':[5,3,6,9,2,4]}, index=list('abcdef'))

print (df1)
   a  b
a  0  5
b  1  3
c  2  6
d  3  9
e  4  2
f  5  4

df2 = pd.DataFrame({'c':range(4),
                    'd':[10,20,30, 40]}, index=list('abhi'))

print (df2)
   c   d
a  0  10
b  1  20
h  2  30
i  3  40

```

---

```
# Default inner join
df3 = pd.merge(df1, df2, left_index=True, right_index=True)
print (df3)
   a  b  c   d
a  0  5  0  10
b  1  3  1  20

# Default left join
df4 = df1.join(df2)
print (df4)
   a  b    c     d
a  0  5  0.0  10.0
b  1  3  1.0  20.0
c  2  6  NaN   NaN
d  3  9  NaN   NaN
e  4  2  NaN   NaN
f  5  4  NaN   NaN

# Default outer join
df5 = pd.concat([df1, df2], axis=1)
print (df5)
     a    b    c     d
a  0.0  5.0  0.0  10.0
b  1.0  3.0  1.0  20.0
c  2.0  6.0  NaN   NaN
d  3.0  9.0  NaN   NaN
e  4.0  2.0  NaN   NaN
f  5.0  4.0  NaN   NaN
h  NaN  NaN  2.0  30.0
i  NaN  NaN  3.0  40.0

```"
"I'm trying to avoid using so many comparisons and simply use a list, but not sure how to use it with `str.startswith`:

```
if link.lower().startswith(""js/"") or link.lower().startswith(""catalog/"") or link.lower().startswith(""script/"") or link.lower().startswith(""scripts/"") or link.lower().startswith(""katalog/""):
    # then ""do something""

```

What I would like it to be is:

```
if link.lower().startswith() in [""js"",""catalog"",""script"",""scripts"",""katalog""]:
    # then ""do something""

```

Is there a way to do this?","`str.startswith` allows you to supply a tuple of strings to test for:

```
if link.lower().startswith((""js"", ""catalog"", ""script"", ""katalog"")):

```

From the [docs](http://docs.python.org/3/library/stdtypes.html#str.startswith):

> `str.startswith(prefix[, start[, end]])`
>
> Return `True` if string starts with the `prefix`, otherwise return `False`. **`prefix` can also be a tuple of prefixes to look for.**

Below is a demonstration:

```
>>> ""abcde"".startswith((""xyz"", ""abc""))
True
>>> prefixes = [""xyz"", ""abc""]
>>> ""abcde"".startswith(tuple(prefixes)) # You must use a tuple though
True
>>>

```"
"How do you convert a Python `time.struct_time` object into a `datetime.datetime` object?

I have a library that provides the first one and a second library that wants the second one.","Use [time.mktime()](http://docs.python.org/library/time.html#time.mktime) to convert the time tuple (in localtime) into seconds since the Epoch, then use [datetime.fromtimestamp()](http://docs.python.org/library/datetime.html#datetime.datetime.fromtimestamp) to get the datetime object.

```
from datetime import datetime
from time import mktime

dt = datetime.fromtimestamp(mktime(struct))

```"
What's the best way to download a python package and its dependencies from pypi for offline installation on another machine? Is there any easy way to do this with pip or easy\_install? I'm trying to install the requests library on a FreeBSD box that is not connected to the internet.,"### On the system that has access to internet

The pip `download` command lets you download packages without installing them:

```
pip download -r requirements.txt

```

(In previous versions of pip, this was spelled `pip install --download -r requirements.txt`.)

### On the system that has no access to internet

Copy over the downloaded packages to this system and then you can use

```
pip install --no-index --find-links /path/to/download/dir/ -r requirements.txt

```

to install those downloaded modules, without accessing the network."
"I've installed Python 3.5 and while running

```
pip install mysql-python

```

it gives me the following error

> error: Microsoft Visual C++ 14.0 is required (Unable to find vcvarsall.bat)

I have added the following lines to my Path

```
C:\Program Files\Python 3.5\Scripts\;
C:\Program Files\Python 3.5\;

C:\Windows\System32;
C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC;
C:\Program Files (x86)\Microsoft Visual Studio 11.0\VC

```

I have a 64-bit WindowsÂ 7 setup on my PC.

What could be the solution for mitigating this error and installing the modules correctly via `pip`.","Your path only lists Visual Studio 11 and 12, it wants 14, which is [Visual Studio 2015](http://landinghub.visualstudio.com/visual-cpp-build-tools). If you install that, and remember to tick the box for *Languages* â†’ *C++* then it should work.

On my Python 3.5 install, the error message was a little more useful, and included the URL to get it from:

> error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": <http://landinghub.visualstudio.com/visual-cpp-build-tools>

New working [link](https://visualstudio.microsoft.com/visual-cpp-build-tools).

As [suggested by Fire](https://stackoverflow.com/questions/29846087/error-microsoft-visual-c-14-0-is-required-unable-to-find-vcvarsall-bat#comment90424430_40888720), you may also need to upgrade `setuptools` package for the error to disappear:

```
pip install --upgrade setuptools

```"
"**What are the most common pandas ways to select/filter rows of a [dataframe whose index is a MultiIndex](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#)?**

* Slicing based on a single value/label
* Slicing based on multiple labels from one or more levels
* Filtering on boolean conditions and expressions
* Which methods are applicable in what circumstances

Assumptions for simplicity:

1. input dataframe does not have duplicate index keys
2. input dataframe below only has two levels. (Most solutions shown here generalize to N levels)

---

### Example input:

> ```
> mux = pd.MultiIndex.from_arrays([
>     list('aaaabbbbbccddddd'),
>     list('tuvwtuvwtuvwtuvw')
> ], names=['one', 'two'])
>
> df = pd.DataFrame({'col': np.arange(len(mux))}, mux)
>
>          col
> one two     
> a   t      0
>     u      1
>     v      2
>     w      3
> b   t      4
>     u      5
>     v      6
>     w      7
>     t      8
> c   u      9
>     v     10
> d   w     11
>     t     12
>     u     13
>     v     14
>     w     15
>
> ```

Question 1: Selecting a Single Item
-----------------------------------

How do I select rows having ""a"" in level ""one""?

```
         col
one two     
a   t      0
    u      1
    v      2
    w      3

```

Additionally, how would I be able to drop level ""one"" in the output?

```
     col
two     
t      0
u      1
v      2
w      3

```

**Question 1b**  
How do I slice all rows with value ""t"" on level ""two""?

```
         col
one two     
a   t      0
b   t      4
    t      8
d   t     12

```

Question 2: Selecting Multiple Values in a Level
------------------------------------------------

How can I select rows corresponding to items ""b"" and ""d"" in level ""one""?

```
         col
one two     
b   t      4
    u      5
    v      6
    w      7
    t      8
d   w     11
    t     12
    u     13
    v     14
    w     15

```

**Question 2b**  
How would I get all values corresponding to ""t"" and ""w"" in level ""two""?

```
         col
one two     
a   t      0
    w      3
b   t      4
    w      7
    t      8
d   w     11
    t     12
    w     15

```

Question 3: Slicing a Single Cross Section `(x, y)`
---------------------------------------------------

How do I retrieve a cross section, i.e., a single row having a specific values for the index from `df`? Specifically, how do I retrieve the cross section of `('c', 'u')`, given by

```
         col
one two     
c   u      9

```

Question 4: Slicing Multiple Cross Sections `[(a, b), (c, d), ...]`
-------------------------------------------------------------------

How do I select the two rows corresponding to `('c', 'u')`, and `('a', 'w')`?

```
         col
one two     
c   u      9
a   w      3

```

Question 5: One Item Sliced per Level
-------------------------------------

How can I retrieve all rows corresponding to ""a"" in level ""one"" or ""t"" in level ""two""?

```
         col
one two     
a   t      0
    u      1
    v      2
    w      3
b   t      4
    t      8
d   t     12

```

Question 6: Arbitrary Slicing
-----------------------------

How can I slice specific cross sections? For ""a"" and ""b"", I would like to select all rows with sub-levels ""u"" and ""v"", and for ""d"", I would like to select rows with sub-level ""w"".

```
         col
one two     
a   u      1
    v      2
b   u      5
    v      6
d   w     11
    w     15

```

> Question 7 will use a unique setup consisting of a numeric level:
>
> ```
> np.random.seed(0)
> mux2 = pd.MultiIndex.from_arrays([
>     list('aaaabbbbbccddddd'),
>     np.random.choice(10, size=16)
> ], names=['one', 'two'])
>
> df2 = pd.DataFrame({'col': np.arange(len(mux2))}, mux2)
>
>          col
> one two     
> a   5      0
>     0      1
>     3      2
>     3      3
> b   7      4
>     9      5
>     3      6
>     5      7
>     2      8
> c   4      9
>     7     10
> d   6     11
>     8     12
>     8     13
>     1     14
>     6     15
>
> ```

Question 7: Filtering by numeric inequality on individual levels of the multiindex
----------------------------------------------------------------------------------

How do I get all rows where values in level ""two"" are greater than 5?

```
         col
one two     
b   7      4
    9      5
c   7     10
d   6     11
    8     12
    8     13
    6     15

```

---

Note: This post will *not* go through how to create MultiIndexes, how to perform assignment operations on them, or any performance related discussions (these are separate topics for another time).","### [MultiIndex / Advanced Indexing](https://pandas.pydata.org/pandas-docs/stable/advanced.html#multiindex-advanced-indexing)

> **Note**  
> This post will be structured in the following manner:
>
> 1. The questions put forth in the OP will be addressed, one by one
> 2. For each question, one or more methods applicable to solving this problem and getting the expected result will be demonstrated.
>
> **Note**s (much like this one) will be included for readers interested in learning about additional functionality, implementation details,
> and other info cursory to the topic at hand. These notes have been
> compiled through scouring the docs and uncovering various obscure
> features, and from my own (admittedly limited) experience.
>
> All code samples have created and tested on **pandas v0.23.4, python3.7**. If something is not clear, or factually incorrect, or if you did not
> find a solution applicable to your use case, please feel free to
> suggest an edit, request clarification in the comments, or open a new
> question, ....as applicable.

Here is an introduction to some common idioms (henceforth referred to as the Four Idioms) we will be frequently re-visiting

1. *[`DataFrame.loc`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)* - A general solution for selection by label (+ *[`pd.IndexSlice`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.IndexSlice.html)* for more complex applications involving slices)
2. *[`DataFrame.xs`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html)* - Extract a particular cross section from a Series/DataFrame.
3. *[`DataFrame.query`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html)* - Specify slicing and/or filtering operations dynamically (i.e., as an expression that is evaluated dynamically. Is more applicable to some scenarios than others. Also see [this section of the docs](https://pandas.pydata.org/pandas-docs/stable/indexing.html#multiindex-query-syntax) for querying on MultiIndexes.
4. Boolean indexing with a mask generated using *[`MultiIndex.get_level_values`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.MultiIndex.get_level_values.html)* (often in conjunction with *[`Index.isin`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.isin.html)*, especially when filtering with multiple values). This is also quite useful in some circumstances.

It will be beneficial to look at the various slicing and filtering problems in terms of the Four Idioms to gain a better understanding what can be applied to a given situation. It is very important to understand that not all of the idioms will work equally well (if at all) in every circumstance. If an idiom has not been listed as a potential solution to a problem below, that means that idiom cannot be applied to that problem effectively.

---

> ### Question 1
>
> How do I select rows having ""a"" in level ""one""?
>
> ```
>          col
> one two     
> a   t      0
>     u      1
>     v      2
>     w      3
>
> ```

You can use `loc`, as a general purpose solution applicable to most situations:

```
df.loc[['a']]

```

At this point, if you get

```
TypeError: Expected tuple, got str

```

That means you're using an older version of pandas. Consider upgrading! Otherwise, use `df.loc[('a', slice(None)), :]`.

Alternatively, you can use `xs` here, since we are extracting a single cross section. Note the `levels` and `axis` arguments (reasonable defaults can be assumed here).

```
df.xs('a', level=0, axis=0, drop_level=False)
# df.xs('a', drop_level=False)

```

Here, the `drop_level=False` argument is needed to prevent `xs` from dropping level ""one"" in the result (the level we sliced on).

Yet another option here is using `query`:

```
df.query(""one == 'a'"")

```

If the index did not have a name, you would need to change your query string to be `""ilevel_0 == 'a'""`.

Finally, using `get_level_values`:

```
df[df.index.get_level_values('one') == 'a']
# If your levels are unnamed, or if you need to select by position (not label),
# df[df.index.get_level_values(0) == 'a']

```

> Additionally, how would I be able to drop level ""one"" in the output?
>
> ```
>      col
> two     
> t      0
> u      1
> v      2
> w      3
>
> ```

This can be *easily* done using either

```
df.loc['a'] # Notice the single string argument instead the list.

```

Or,

```
df.xs('a', level=0, axis=0, drop_level=True)
# df.xs('a')

```

Notice that we can omit the `drop_level` argument (it is assumed to be `True` by default).

> **Note**  
> You may notice that a filtered DataFrame may still have all the levels, even if they do not show when printing the DataFrame out. For example,
>
> ```
> v = df.loc[['a']]
> print(v)
>          col
> one two     
> a   t      0
>     u      1
>     v      2
>     w      3
>
> print(v.index)
> MultiIndex(levels=[['a', 'b', 'c', 'd'], ['t', 'u', 'v', 'w']],
>            labels=[[0, 0, 0, 0], [0, 1, 2, 3]],
>            names=['one', 'two'])
>
> ```
>
> You can get rid of these levels using [*`MultiIndex.remove_unused_levels`*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.MultiIndex.remove_unused_levels.html):
>
> ```
> v.index = v.index.remove_unused_levels()
>
> ```

> ```
> print(v.index)
> MultiIndex(levels=[['a'], ['t', 'u', 'v', 'w']],
>            labels=[[0, 0, 0, 0], [0, 1, 2, 3]],
>            names=['one', 'two'])
>
> ```

---

> ### Question 1b
>
> How do I slice all rows with value ""t"" on level ""two""?
>
> ```
>          col
> one two     
> a   t      0
> b   t      4
>     t      8
> d   t     12
>
> ```

Intuitively, you would want something involving [*`slice()`*](https://docs.python.org/3/library/functions.html#slice):

```
df.loc[(slice(None), 't'), :]

```

It Just Works!™ But it is clunky. We can facilitate a more natural slicing syntax using the `pd.IndexSlice` API here.

```
idx = pd.IndexSlice
df.loc[idx[:, 't'], :]

```

This is much, much cleaner.

> **Note**  
> Why is the trailing slice `:` across the columns required? This is because, `loc` can be used to select and slice along both axes (`axis=0` or
> `axis=1`). Without explicitly making it clear which axis the slicing
> is to be done on, the operation becomes ambiguous. See the big red box in the [documentation on slicing](http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers).
>
> If you want to remove any shade of ambiguity, `loc` accepts an `axis`
> parameter:
>
> ```
> df.loc(axis=0)[pd.IndexSlice[:, 't']]
>
> ```
>
> Without the `axis` parameter (i.e., just by doing `df.loc[pd.IndexSlice[:, 't']]`), slicing is assumed to be on the columns,
> and a `KeyError` will be raised in this circumstance.
>
> This is documented in [slicers](https://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers). For the purpose of this post, however, we will explicitly specify all axes.

With `xs`, it is

```
df.xs('t', axis=0, level=1, drop_level=False)

```

With `query`, it is

```
df.query(""two == 't'"")
# Or, if the first level has no name, 
# df.query(""ilevel_1 == 't'"") 

```

And finally, with `get_level_values`, you may do

```
df[df.index.get_level_values('two') == 't']
# Or, to perform selection by position/integer,
# df[df.index.get_level_values(1) == 't']

```

All to the same effect.

---

> ### Question 2
>
> How can I select rows corresponding to items ""b"" and ""d"" in level ""one""?
>
> ```
>          col
> one two     
> b   t      4
>     u      5
>     v      6
>     w      7
>     t      8
> d   w     11
>     t     12
>     u     13
>     v     14
>     w     15
>
> ```

Using loc, this is done in a similar fashion by specifying a list.

```
df.loc[['b', 'd']]

```

To solve the above problem of selecting ""b"" and ""d"", you can also use `query`:

```
items = ['b', 'd']
df.query(""one in @items"")
# df.query(""one == @items"", parser='pandas')
# df.query(""one in ['b', 'd']"")
# df.query(""one == ['b', 'd']"", parser='pandas')

```

> **Note**  
> Yes, the default parser is `'pandas'`, but it is important to highlight this syntax isn't conventionally python. The
> Pandas parser generates a slightly different parse tree from the
> expression. This is done to make some operations more intuitive to
> specify. For more information, please read my post on
> [Dynamic Expression Evaluation in pandas using pd.eval()](https://stackoverflow.com/questions/53779986/dynamic-expression-evaluation-in-pandas-using-pd-eval).

And, with `get_level_values` + `Index.isin`:

```
df[df.index.get_level_values(""one"").isin(['b', 'd'])]

```

---

> ### Question 2b
>
> How would I get all values corresponding to ""t"" and ""w"" in level ""two""?
>
> ```
>          col
> one two     
> a   t      0
>     w      3
> b   t      4
>     w      7
>     t      8
> d   w     11
>     t     12
>     w     15
>
> ```

With `loc`, this is possible *only* in conjuction with `pd.IndexSlice`.

```
df.loc[pd.IndexSlice[:, ['t', 'w']], :] 

```

The first colon `:` in `pd.IndexSlice[:, ['t', 'w']]` means to slice across the first level. As the depth of the level being queried increases, you will need to specify more slices, one per level being sliced across. You will not need to specify more levels *beyond* the one being sliced, however.

With `query`, this is

```
items = ['t', 'w']
df.query(""two in @items"")
# df.query(""two == @items"", parser='pandas') 
# df.query(""two in ['t', 'w']"")
# df.query(""two == ['t', 'w']"", parser='pandas')

```

With `get_level_values` and `Index.isin` (similar to above):

```
df[df.index.get_level_values('two').isin(['t', 'w'])]

```

---

> ### Question 3
>
> How do I retrieve a cross section, i.e., a single row having a specific values
> for the index from `df`? Specifically, how do I retrieve the cross
> section of `('c', 'u')`, given by
>
> ```
>          col
> one two     
> c   u      9
>
> ```

Use `loc` by specifying a tuple of keys:

```
df.loc[('c', 'u'), :]

```

Or,

```
df.loc[pd.IndexSlice[('c', 'u')]]

```

> **Note**  
> At this point, you may run into a [*`PerformanceWarning`*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.errors.PerformanceWarning.html#pandas-errors-performancewarning) that looks like this:
>
> ```
> PerformanceWarning: indexing past lexsort depth may impact performance.
>
> ```
>
> This just means that your index is not sorted. pandas depends on the index being sorted (in this case, lexicographically, since we are dealing with string values) for optimal search and retrieval. A quick fix would be to sort your
> DataFrame in advance using [*`DataFrame.sort_index`*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index). This is especially desirable from a performance standpoint if you plan on doing
> multiple such queries in tandem:
>
> ```
> df_sort = df.sort_index()
> df_sort.loc[('c', 'u')]
>
> ```
>
> You can also use [*`MultiIndex.is_lexsorted()`*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.MultiIndex.is_lexsorted.html) to check whether the index
> is sorted or not. This function returns `True` or `False` accordingly.
> You can call this function to determine whether an additional sorting
> step is required or not.

With `xs`, this is again simply passing a single tuple as the first argument, with all other arguments set to their appropriate defaults:

```
df.xs(('c', 'u'))

```

With `query`, things become a bit clunky:

```
df.query(""one == 'c' and two == 'u'"")

```

You can see now that this is going to be relatively difficult to generalize. But is still OK for this particular problem.

With accesses spanning multiple levels, `get_level_values` can still be used, but is not recommended:

```
m1 = (df.index.get_level_values('one') == 'c')
m2 = (df.index.get_level_values('two') == 'u')
df[m1 & m2]

```

---

> ### Question 4
>
> How do I select the two rows corresponding to `('c', 'u')`, and `('a', 'w')`?
>
> ```
>          col
> one two     
> c   u      9
> a   w      3
>
> ```

With `loc`, this is still as simple as:

```
df.loc[[('c', 'u'), ('a', 'w')]]
# df.loc[pd.IndexSlice[[('c', 'u'), ('a', 'w')]]]

```

With `query`, you will need to dynamically generate a query string by iterating over your cross sections and levels:

```
cses = [('c', 'u'), ('a', 'w')]
levels = ['one', 'two']
# This is a useful check to make in advance.
assert all(len(levels) == len(cs) for cs in cses) 

query = '(' + ') or ('.join([
    ' and '.join([f""({l} == {repr(c)})"" for l, c in zip(levels, cs)]) 
    for cs in cses
]) + ')'

print(query)
# ((one == 'c') and (two == 'u')) or ((one == 'a') and (two == 'w'))

df.query(query)

```

100% DO NOT RECOMMEND! But it is possible.

**What if I have multiple levels?**  
One option in this scenario would be to use [`droplevel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.droplevel.html) to drop the levels you're not checking, then use [`isin`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html) to test membership, and then boolean index on the final result.

```
df[df.index.droplevel(unused_level).isin([('c', 'u'), ('a', 'w')])]

```

---

> ### Question 5
>
> How can I retrieve all rows corresponding to ""a"" in level ""one"" or
> ""t"" in level ""two""?
>
> ```
>          col
> one two     
> a   t      0
>     u      1
>     v      2
>     w      3
> b   t      4
>     t      8
> d   t     12
>
> ```

This is actually very difficult to do with `loc` while ensuring correctness *and* still maintaining code clarity. `df.loc[pd.IndexSlice['a', 't']]` is incorrect, it is interpreted as `df.loc[pd.IndexSlice[('a', 't')]]` (i.e., selecting a cross section). You may think of a solution with `pd.concat` to handle each label separately:

```
pd.concat([
    df.loc[['a'],:], df.loc[pd.IndexSlice[:, 't'],:]
])

         col
one two     
a   t      0
    u      1
    v      2
    w      3
    t      0   # Does this look right to you? No, it isn't!
b   t      4
    t      8
d   t     12

```

But you'll notice one of the rows is duplicated. This is because that row satisfied both slicing conditions, and so appeared twice. You will instead need to do

```
v = pd.concat([
        df.loc[['a'],:], df.loc[pd.IndexSlice[:, 't'],:]
])
v[~v.index.duplicated()]

```

But if your DataFrame inherently contains duplicate indices (that you want), then this will not retain them. **Use with extreme caution**.

With `query`, this is stupidly simple:

```
df.query(""one == 'a' or two == 't'"")

```

With `get_level_values`, this is still simple, but not as elegant:

```
m1 = (df.index.get_level_values('one') == 'a')
m2 = (df.index.get_level_values('two') == 't')
df[m1 | m2] 

```

---

> ### Question 6
>
> How can I slice specific cross sections? For ""a"" and ""b"", I would like to select all rows with sub-levels ""u"" and ""v"", and
> for ""d"", I would like to select rows with sub-level ""w"".
>
> ```
>          col
> one two     
> a   u      1
>     v      2
> b   u      5
>     v      6
> d   w     11
>     w     15
>
> ```

This is a special case that I've added to help understand the applicability of the Four Idioms—this is one case where none of them will work effectively, since the slicing is *very* specific, and does not follow any real pattern.

Usually, slicing problems like this will require explicitly passing a list of keys to `loc`. One way of doing this is with:

```
keys = [('a', 'u'), ('a', 'v'), ('b', 'u'), ('b', 'v'), ('d', 'w')]
df.loc[keys, :]

```

If you want to save some typing, you will recognise that there is a pattern to slicing ""a"", ""b"" and its sublevels, so we can separate the slicing task into two portions and `concat` the result:

```
pd.concat([
     df.loc[(('a', 'b'), ('u', 'v')), :], 
     df.loc[('d', 'w'), :]
   ], axis=0)

```

Slicing specification for ""a"" and ""b"" is slightly cleaner `(('a', 'b'), ('u', 'v'))` because the same sub-levels being indexed are the same for each level.

---

> ### Question 7
>
> How do I get all rows where values in level ""two"" are greater than 5?
>
> ```
>          col
> one two     
> b   7      4
>     9      5
> c   7     10
> d   6     11
>     8     12
>     8     13
>     6     15
>
> ```

This can be done using `query`,

```
df2.query(""two > 5"")

```

And `get_level_values`.

```
df2[df2.index.get_level_values('two') > 5]

```

> **Note**  
> Similar to this example, we can filter based on any arbitrary condition using these constructs. In general, it is useful to remember that `loc` and `xs` are specifically for label-based indexing, while `query` and
> `get_level_values` are helpful for building general conditional masks
> for filtering.

---

> ### Bonus Question
>
> What if I need to slice a `MultiIndex` *column*?

Actually, most solutions here are applicable to columns as well, with minor changes. Consider:

```
np.random.seed(0)
mux3 = pd.MultiIndex.from_product([
        list('ABCD'), list('efgh')
], names=['one','two'])

df3 = pd.DataFrame(np.random.choice(10, (3, len(mux))), columns=mux3)
print(df3)

one  A           B           C           D         
two  e  f  g  h  e  f  g  h  e  f  g  h  e  f  g  h
0    5  0  3  3  7  9  3  5  2  4  7  6  8  8  1  6
1    7  7  8  1  5  9  8  9  4  3  0  3  5  0  2  3
2    8  1  3  3  3  7  0  1  9  9  0  4  7  3  2  7

```

These are the following changes you will need to make to the Four Idioms to have them working with columns.

1. To slice with `loc`, use

   ```
    df3.loc[:, ....] # Notice how we slice across the index with `:`. 

   ```

   or,

   ```
    df3.loc[:, pd.IndexSlice[...]]

   ```
2. To use `xs` as appropriate, just pass an argument `axis=1`.
3. You can access the column level values directly using `df.columns.get_level_values`. You will then need to do something like

   ```
    df.loc[:, {condition}] 

   ```

   Where `{condition}` represents some condition built using `columns.get_level_values`.
4. To use `query`, your only option is to transpose, query on the index, and transpose again:

   ```
    df3.T.query(...).T

   ```

   Not recommended, use one of the other 3 options."
"How do you execute raw SQL in SQLAlchemy?

I have a python web app that runs on flask and interfaces to the database through SQLAlchemy.

I need a way to run the raw SQL. The query involves multiple table joins along with Inline views.

I've tried:

```
connection = db.session.connection()
connection.execute( <sql here> )

```

But I keep getting gateway errors.","SQLAlchemy 2.0:

```
with engine.connect() as connection:
    result = connection.execute(text('SELECT * FROM your_table'))
    # do something with the result..

```

SQLAlchemy 1.x:

```
from sqlalchemy import text

sql = text('select name from penguins')
result = db.engine.execute(sql)
names = [row[0] for row in result]
print names

```

Note that `db.engine.execute()` is ""connectionless"", which is [deprecated in SQLAlchemy 2.0](https://docs.sqlalchemy.org/en/14/core/connections.html#connectionless-execution-implicit-execution)."
"I haven't been able to find an understandable explanation of how to actually use Python's `itertools.groupby()` function. What I'm trying to do is this:

* Take a list - in this case, the children of an objectified `lxml` element
* Divide it into groups based on some criteria
* Then later iterate over each of these groups separately.

I've reviewed [the documentation](https://docs.python.org/3/library/itertools.html#itertools.groupby), but I've had trouble trying to apply them beyond a simple list of numbers.

So, how do I use of `itertools.groupby()`? Is there another technique I should be using? Pointers to good ""prerequisite"" reading would also be appreciated.","**IMPORTANT NOTE:** You *may* have to **sort your data** first.

---

The part I didn't get is that in the example construction

```
groups = []
uniquekeys = []
for k, g in groupby(data, keyfunc):
   groups.append(list(g))    # Store group iterator as a list
   uniquekeys.append(k)

```

`k` is the current grouping key, and `g` is an iterator that you can use to iterate over the group defined by that grouping key. In other words, the `groupby` iterator itself returns iterators.

Here's an example of that, using clearer variable names:

```
from itertools import groupby

things = [(""animal"", ""bear""), (""animal"", ""duck""), (""plant"", ""cactus""), (""vehicle"", ""speed boat""), (""vehicle"", ""school bus"")]

for key, group in groupby(things, lambda x: x[0]):
    for thing in group:
        print(""A %s is a %s."" % (thing[1], key))
    print("""")
    

```

This will give you the output:

> A bear is a animal.  
> A duck is a animal.
>
> A cactus is a plant.
>
> A speed boat is a vehicle.  
> A school bus is a vehicle.

In this example, `things` is a list of tuples where the first item in each tuple is the group the second item belongs to.

The `groupby()` function takes two arguments: (1) the data to group and (2) the function to group it with.

Here, `lambda x: x[0]` tells `groupby()` to use the first item in each tuple as the grouping key.

In the above `for` statement, `groupby` returns three (key, group iterator) pairs - once for each unique key. You can use the returned iterator to iterate over each individual item in that group.

Here's a slightly different example with the same data, using a list comprehension:

```
for key, group in groupby(things, lambda x: x[0]):
    listOfThings = "" and "".join([thing[1] for thing in group])
    print(key + ""s:  "" + listOfThings + ""."")

```

This will give you the output:

> animals: bear and duck.  
> plants: cactus.  
> vehicles: speed boat and school bus."
"I am new to Python's logging package and plan to use it for my project. I would like to customize the time format to my taste. Here is a short code I copied from a tutorial:

```
import logging

# create logger
logger = logging.getLogger(""logging_tryout2"")
logger.setLevel(logging.DEBUG)

# create console handler and set level to debug
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)

# create formatter
formatter = logging.Formatter(""%(asctime)s;%(levelname)s;%(message)s"")

# add formatter to ch
ch.setFormatter(formatter)

# add ch to logger
logger.addHandler(ch)

# ""application"" code
logger.debug(""debug message"")
logger.info(""info message"")
logger.warn(""warn message"")
logger.error(""error message"")
logger.critical(""critical message"")

```

And here is the output:

```
2010-07-10 10:46:28,811;DEBUG;debug message
2010-07-10 10:46:28,812;INFO;info message
2010-07-10 10:46:28,812;WARNING;warn message
2010-07-10 10:46:28,812;ERROR;error message
2010-07-10 10:46:28,813;CRITICAL;critical message

```

I would like to shorten the time format to just: '`2010-07-10 10:46:28`', dropping the mili-second suffix. I looked at the Formatter.formatTime, but I am confused.","From the [official documentation](http://docs.python.org/2/library/logging.html#logging.Formatter) regarding the Formatter class:

> The constructor takes two optional arguments: a message format string and a date format string.

So change

```
# create formatter
formatter = logging.Formatter(""%(asctime)s;%(levelname)s;%(message)s"")

```

to

```
# create formatter
formatter = logging.Formatter(""%(asctime)s;%(levelname)s;%(message)s"",
                              ""%Y-%m-%d %H:%M:%S"")

```"
"I'd like to extract the text from an HTML file using Python. I want essentially the same output I would get if I copied the text from a browser and pasted it into notepad.

I'd like something more robust than using regular expressions that may fail on poorly formed HTML. I've seen many people recommend Beautiful Soup, but I've had a few problems using it. For one, it picked up unwanted text, such as JavaScript source. Also, it did not interpret HTML entities. For example, I would expect &#39; in HTML source to be converted to an apostrophe in text, just as if I'd pasted the browser content into notepad.

**Update** `html2text` looks promising. It handles HTML entities correctly and ignores JavaScript. However, it does not exactly produce plain text; it produces markdown that would then have to be turned into plain text. It comes with no examples or documentation, but the code looks clean.

---

Related questions:

* [Filter out HTML tags and resolve entities in python](https://stackoverflow.com/questions/37486/filter-out-html-tags-and-resolve-entities-in-python)
* [Convert XML/HTML Entities into Unicode String in Python](https://stackoverflow.com/questions/57708/convert-xmlhtml-entities-into-unicode-string-in-python)","The best piece of code I found for extracting text without getting javascript or not wanted things :

```
from urllib.request import urlopen
from bs4 import BeautifulSoup

url = ""http://news.bbc.co.uk/2/hi/health/2284783.stm""
html = urlopen(url).read()
soup = BeautifulSoup(html, features=""html.parser"")

# kill all script and style elements
for script in soup([""script"", ""style""]):
    script.extract()    # rip it out

# get text
text = soup.get_text()

# break into lines and remove leading and trailing space on each
lines = (line.strip() for line in text.splitlines())
# break multi-headlines into a line each
chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
# drop blank lines
text = '\n'.join(chunk for chunk in chunks if chunk)

print(text)

```

You just have to install BeautifulSoup before :

```
pip install beautifulsoup4

```"
"I'm running PyLint from inside Wing IDE on Windows. I have a sub-directory (package) in my project and inside the package I import a module from the top level, ie.

```
__init__.py
myapp.py
one.py
subdir\
    __init__.py
    two.py

```

Inside `two.py` I have `import one` and this works fine at runtime, because the top-level directory (from which `myapp.py` is run) is in the Python path. However, when I run PyLint on two.py it gives me an error:

```
F0401: Unable to import 'one'

```

How do I fix this?","There are two options I'm aware of.

One, change the `PYTHONPATH` environment variable to include the directory above your module.

Alternatively, edit `~/.pylintrc` to include the directory above your module, like this:

```
[MASTER]
init-hook='import sys; sys.path.append(""/path/to/root"")'

```

(Or in other version of pylint, the init-hook requires you to change [General] to [MASTER])

Both of these options ought to work."
"Most of the questions I've found are biased on the fact they're looking for letters in their numbers, whereas I'm looking for numbers in what I'd like to be a numberless string.
I need to enter a string and check to see if it contains any numbers and if it does reject it.

The function `isdigit()` only returns `True` if ALL of the characters are numbers. I just want to see if the user has entered a number so a sentence like `""I own 1 dog""` or something.

Any ideas?","You can use [`any`](https://docs.python.org/3/library/functions.html#any) function, with the [`str.isdigit`](https://docs.python.org/3/library/stdtypes.html#str.isdigit) function, like this

```
def has_numbers(inputString):
    return any(char.isdigit() for char in inputString)

has_numbers(""I own 1 dog"")
# True
has_numbers(""I own no dog"")
# False

```

Alternatively you can use a Regular Expression, like this

```
import re
def has_numbers(inputString):
    return bool(re.search(r'\d', inputString))

has_numbers(""I own 1 dog"")
# True
has_numbers(""I own no dog"")
# False

```"
"How to do `assert almost equal` with `pytest` for floats without resorting to something like:

```
assert x - 0.00001 <= y <= x + 0.00001

```

More specifically it will be useful to know a neat solution for quickly comparing pairs of float, without unpacking them:

```
assert (1.32, 2.4) == i_return_tuple_of_two_floats()

```","I noticed that this question specifically asked about pytest. pytest 3.0 includes an [`approx()` function](https://docs.pytest.org/en/latest/reference/reference.html?highlight=approx#pytest.approx) (well, really class) that is very useful for this purpose.

```
import pytest

assert 2.2 == pytest.approx(2.3)
# fails, default is Â± 2.3e-06
assert 2.2 == pytest.approx(2.3, 0.1)
# passes

# also works the other way, in case you were worried:
assert pytest.approx(2.3, 0.1) == 2.2
# passes

```"
"I have the following line in my header:

```
import config.logging_settings

```

This actually changes my Python logging settings, but Pylint thinks it is an unused import. I do not want to remove `unused-import` warnings in general, so is it possible to just ignore this one specific line?

I wouldn't mind having a `.pylintrc` for this project, so answers changing a configuration file will be accepted.

Otherwise, something like this will also be appreciated:

```
import config.logging_settings # pylint: disable-this-line-in-some-way

```","Message control is documented in [the pylint FAQ](https://docs.pylint.org/faq.html#message-control):

> ### Is it possible to locally disable a particular message?
>
> Yes, this feature has been added in Pylint 0.11. This may be done by adding ""# pylint: disable=some-message,another-one"" at the desired block level or at the end of the desired line of code.

You can use the message code or the symbolic names.

For example,

```
def test():
    # Disable all the no-member violations in this function
    # pylint: disable=no-member
    ...
    # pylint: enable=no-member

```

apply to a specific line only:

```
global VAR  # pylint: disable=global-statement

```

or for less verbosity, disable the ONLY following line (pylint 2.10+):

```
# pylint: disable-next=global-statement
global VAR

```

Pylint's manual also has [further examples](https://pylint.readthedocs.io/en/latest/user_guide/message-control.html).

There is [a wiki](https://pylint-messages.wikidot.com/all-codes) that documents all pylint messages and their codes."
"When you use `pip` to install a package, all the required packages will also be installed with it (dependencies). Does uninstalling that package also remove the dependent packages?","You can install and use the [pip3-autoremove](https://pypi.org/project/pip3-autoremove/) utility to remove a package plus unused dependencies.

```
# install pip3-autoremove
pip install pip3-autoremove
# remove ""somepackage"" plus its dependencies:
pip-autoremove somepackage -y

```"
"The following use of `super()` raises a TypeError: why?

```
>>> from  HTMLParser import HTMLParser
>>> class TextParser(HTMLParser):
...     def __init__(self):
...         super(TextParser, self).__init__()
...         self.all_data = []
...         
>>> TextParser()
(...)
TypeError: must be type, not classobj

```

There is a similar question on StackOverflow: [Python super() raises TypeError](https://stackoverflow.com/questions/489269/python-super-raises-typeerror-why), where the error is explained by the fact that the user class is not a new-style class. However, the class above is a new-style class, as it inherits from `object`:

```
>>> isinstance(HTMLParser(), object)
True

```

What am I missing? How can I use `super()`, here?

Using `HTMLParser.__init__(self)` instead of `super(TextParser, self).__init__()` would work, but I would like to understand the TypeError.

PS: Joachim pointed out that being a new-style-class instance is not equivalent to being an `object`. I read the opposite many times, hence my confusion (example of new-style class instance test based on `object` instance test: <https://stackoverflow.com/revisions/2655651/3>).","Alright, it's the usual ""`super()` cannot be used with an old-style class"".

However, the important point is that **the correct test** for ""is this a new-style **instance** (i.e. object)?"" is

```
>>> class OldStyle: pass
>>> instance = OldStyle()
>>> issubclass(instance.__class__, object)
False

```

and not (as in the question):

```
>>> isinstance(instance, object)
True

```

For **classes**, the correct ""is this a new-style class"" test is:

```
>>> issubclass(OldStyle, object)  # OldStyle is not a new-style class
False
>>> issubclass(int, object)  # int is a new-style class
True

```

The **crucial point** is that with old-style classes, the *class* of an instance and its *type* are distinct. Here, `OldStyle().__class__` is `OldStyle`, which does not inherit from `object`, while `type(OldStyle())` is the `instance` type, which *does* inherit from `object`. Basically, an old-style class just creates objects of type `instance` (whereas a new-style class creates objects whose type is the class itself). This is probably why the instance `OldStyle()` is an `object`: its `type()` inherits from `object` (the fact that its class does *not* inherit from `object` does not count: old-style classes merely construct new objects of type `instance`). Partial reference: <https://stackoverflow.com/a/9699961/42973>.

PS: The difference between a new-style class and an old-style one can also be seen with:

```
>>> type(OldStyle)  # OldStyle creates objects but is not itself a type
classobj
>>> isinstance(OldStyle, type)
False
>>> type(int)  # A new-style class is a type
type

```

(old-style classes are *not* types, so they cannot be the type of their instances)."
"Like in:

```
u'Hello'

```

My guess is that it indicates ""Unicode"", is that correct?

If so, since when has it been available?","You're right, see *[3.1.3. Unicode Strings](http://docs.python.org/2/tutorial/introduction.html#unicode-strings)*.

It's been the syntax since Python 2.0.

Python 3 made them redundant, as the default string type is Unicode. Versions 3.0 through 3.2 removed them, but they were [re-added in 3.3+](https://www.python.org/dev/peps/pep-0414/) for compatibility with Python 2 to aide the 2 to 3 transition."
"I need to select an element from a **drop-down** menu.

For example:

```
<select id=""fruits01"" class=""select"" name=""fruits"">
  <option value=""0"">Choose your fruits:</option>
  <option value=""1"">Banana</option>
  <option value=""2"">Mango</option>
</select>

```

**1)** First I have to click on it. I do this:

```
inputElementFruits = driver.find_element_by_xpath(""//select[id='fruits']"").click()

```

**2)** After that I have to select the good element, lets say `Mango`.

I tried to do it with `inputElementFruits.send_keys(...)` but it did not work.","Selenium provides a convenient [`Select` class](https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.select) to work with `select -> option` constructs:

```
from selenium import webdriver
from selenium.webdriver.support.ui import Select

driver = webdriver.Firefox()
driver.get('url')

select = Select(driver.find_element_by_id('fruits01'))

# select by visible text
select.select_by_visible_text('Banana')

# select by value 
select.select_by_value('1')

```

See also:

* [What is the correct way to select an using Selenium's Python WebDriver?](https://sqa.stackexchange.com/questions/1355/what-is-the-correct-way-to-select-an-option-using-seleniums-python-webdriver)"
"I'm just trying to streamline one of my classes and have introduced some functionality in the same style as the [flyweight design pattern](https://en.wikipedia.org/wiki/Flyweight_pattern).

However, I'm a bit confused as to why `__init__` is always called after `__new__`. I wasn't expecting this. Can anyone tell me why this is happening and how I can implement this functionality otherwise? (Apart from putting the implementation into the `__new__` which feels quite hacky.)

Here's an example:

```
class A(object):
    _dict = dict()

    def __new__(cls):
        if 'key' in A._dict:
            print ""EXISTS""
            return A._dict['key']
        else:
            print ""NEW""
            return super(A, cls).__new__(cls)

    def __init__(self):
        print ""INIT""
        A._dict['key'] = self
        print """"

a1 = A()
a2 = A()
a3 = A()

```

Outputs:

```
NEW
INIT

EXISTS
INIT

EXISTS
INIT

```

Why?","> Use **`__new__`** when you need to control
> the creation of a new instance.

> Use
> **`__init__`** when you need to control initialization of a new instance.
>
> **`__new__`** is the first step of instance creation. It's called first, and is
> responsible for returning a new
> instance of your class.

> In contrast,
> **`__init__`** doesn't return anything; it's only responsible for initializing the
> instance after it's been created.
>
> In general, you shouldn't need to
> override **`__new__`** unless you're
> subclassing an immutable type like
> str, int, unicode or tuple.

From April 2008 post: [When to use `__new__` vs. `__init__`?](http://mail.python.org/pipermail/tutor/2008-April/061426.html) on mail.python.org.

You should consider that what you are trying to do is usually done with a [Factory](http://en.wikipedia.org/wiki/Factory_object) and that's the best way to do it. Using **`__new__`** is not a good clean solution so please consider the usage of a factory. Here's a good example: [ActiveState Fᴀᴄᴛᴏʀʏ ᴘᴀᴛᴛᴇʀɴ Recipe](http://code.activestate.com/recipes/86900/)."
"When should I use a dictionary, list or set?

Are there scenarios that are more suited for each data type?","A `list` keeps order, `dict` and `set` don't: when you care about order, therefore, you must use `list` (if your choice of containers is limited to these three, of course ;-) ).

`dict` associates each key with a value, while `list` and `set` just contain values: very different use cases, obviously.

`set` requires items to be hashable, `list` doesn't: if you have non-hashable items, therefore, you cannot use `set` and must instead use `list`.

`set` forbids duplicates, `list` does not: also a crucial distinction. (A ""multiset"", which maps duplicates into a different count for items present more than once, can be found in `collections.Counter` -- you could build one as a `dict`, if for some weird reason you couldn't import `collections`, or, in pre-2.7 Python as a `collections.defaultdict(int)`, using the items as keys and the associated value as the count).

Checking for membership of a value in a `set` (or `dict`, for keys) is blazingly fast (taking about a constant, short time), while in a list it takes time proportional to the list's length in the average and worst cases. So, if you have hashable items, don't care either way about order or duplicates, and want speedy membership checking, `set` is better than `list`."
Can you give me an example of a `Dockerfile` in which I can install all the packages I need from `poetry.lock` and `pyproject.toml` into my image/container from Docker?,"There are several things to keep in mind when using Poetry together with Docker.

Installation
------------

Official way to install Poetry is via:

```
curl -sSL https://install.python-poetry.org | python3 -

```

This way allows Poetry and its dependencies to be isolated from your dependencies.

You can also use `pip install 'poetry==$POETRY_VERSION'`. But, this will install Poetry and its dependencies into your main `site-packages/`. It might not be ideal.

Also, pin this version in your `pyproject.toml` as well:

```
[build-system]
# Should be the same as `$POETRY_VERSION`:
requires = [""poetry-core>=1.6""]
build-backend = ""poetry.core.masonry.api""

```

It will protect you from version mismatch between your local and Docker environments.

Caching dependencies
--------------------

We want to cache our requirements and only reinstall them when `pyproject.toml` or `poetry.lock` files change. Otherwise builds will be slow. To achieve working cache layer we should put:

```
COPY poetry.lock pyproject.toml /code/

```

after Poetry is installed, but before any other files are added.

Virtualenv
----------

The next thing to keep in mind is `virtualenv` creation. We do not need it in Docker. It is already isolated. So, we use `POETRY_VIRTUALENVS_CREATE=false` or `poetry config virtualenvs.create false` setting to turn it off.

Development vs. Production
--------------------------

If you use the same `Dockerfile` for both development and production as I do, you will need to install different sets of dependencies based on some environment variable:

```
poetry install $(test ""$YOUR_ENV"" == production && echo ""--only=main"")

```

This way `$YOUR_ENV` will control which dependencies set will be installed: all (default) or production only with `--only=main` flag.

You may also want to add some more options for better experience:

1. `--no-interaction` not to ask any interactive questions
2. `--no-ansi` flag to make your output more log friendly

Result
------

You will end up with something similar to:

```
FROM python:3.11.5-slim-bookworm

ARG YOUR_ENV

ENV YOUR_ENV=${YOUR_ENV} \
  PYTHONFAULTHANDLER=1 \
  PYTHONUNBUFFERED=1 \
  PYTHONHASHSEED=random \
  PIP_NO_CACHE_DIR=off \
  PIP_DISABLE_PIP_VERSION_CHECK=on \
  PIP_DEFAULT_TIMEOUT=100 \
  # Poetry's configuration:
  POETRY_NO_INTERACTION=1 \
  POETRY_VIRTUALENVS_CREATE=false \
  POETRY_CACHE_DIR='/var/cache/pypoetry' \
  POETRY_HOME='/usr/local' \
  POETRY_VERSION=1.7.1
  # ^^^
  # Make sure to update it!

# System deps:
RUN curl -sSL https://install.python-poetry.org | python3 -

# Copy only requirements to cache them in docker layer
WORKDIR /code
COPY poetry.lock pyproject.toml /code/

# Project initialization:
RUN poetry install $(test ""$YOUR_ENV"" == production && echo ""--only=main"") --no-interaction --no-ansi

# Creating folders, and files for a project:
COPY . /code

```

You can find a fully working real-life example [here](https://github.com/wemake-services/wemake-django-template/blob/master/%7B%7Bcookiecutter.project_name%7D%7D/docker/django/Dockerfile)."
"I am looking over [this website](http://docs.djangoproject.com/en/1.2/topics/auth/#django.contrib.auth.models.User) but just can't seem to figure out how to do this as it's not working. I need to check if the current site user is logged in (authenticated), and am trying:

```
request.user.is_authenticated

```

despite being sure that the user is logged in, it returns just:

```
>

```

I'm able to do other requests (from the first section in the url above), such as:

```
request.user.is_active

```

which returns a successful response.","Update for Django 2.0 and later
===============================

[`is_authenticated`](https://docs.djangoproject.com/en/stable/ref/contrib/auth/#django.contrib.auth.models.User.is_authenticated) is a read-only **attribute**:

```
if request.user.is_authenticated:
    # do something if the user is authenticated

```

---

#### For Django 1.9 and older

`is_authenticated()` was a function. Called like:

```
if request.user.is_authenticated():
    # do something if the user is authenticated

```

As Peter Rowell pointed out, what may be tripping you up is that in the default Django template language, you don't tack on parenthesis to call functions. So you may have seen something like this in template code:

```
{% if user.is_authenticated %}

```

However, in Python code, it is indeed a method in the `User` class.

*NB: The method was removed in Django 2.0.*"
"To launch programs from my Python-scripts, I'm using the following method:

```
def execute(command):
    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    output = process.communicate()[0]
    exitCode = process.returncode

    if (exitCode == 0):
        return output
    else:
        raise ProcessException(command, exitCode, output)

```

So when i launch a process like `Process.execute(""mvn clean install"")`, my program waits until the process is finished, and only then i get the complete output of my program. This is annoying if i'm running a process that takes a while to finish.

Can I let my program write the process output line by line, by polling the process output before it finishes in a loop or something?

I found [this](https://stackoverflow.com/questions/984941/python-subprocess-popen-from-a-thread) article which might be related.","You can use [iter](https://docs.python.org/3/library/functions.html#iter) to process lines as soon as the command outputs them: `lines = iter(fd.readline, """")`. Here's a full example showing a typical use case (thanks to @jfs for helping out):

```
from __future__ import print_function # Only Python 2.x
import subprocess

def execute(cmd):
    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, universal_newlines=True)
    for stdout_line in iter(popen.stdout.readline, """"):
        yield stdout_line 
    popen.stdout.close()
    return_code = popen.wait()
    if return_code:
        raise subprocess.CalledProcessError(return_code, cmd)

# Example
for path in execute([""locate"", ""a""]):
    print(path, end="""")

```"
"How can I use the [logging](https://docs.python.org/3.7/library/logging.html#module-logging) module in Python to write to a file? Every time I try to use it, it just prints out the message.","An example of using `logging.basicConfig` rather than `logging.fileHandler()`

```
logging.basicConfig(filename=logname,
                    filemode='a',
                    format='%(asctime)s,%(msecs)03d %(name)s %(levelname)s %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.DEBUG)

logging.info(""Running Urban Planning"")

logger = logging.getLogger('urbanGUI')

```

**In order, the five parts do the following:**

1. set the output file (`filename=logname`)
2. set it to append (`filemode='a'`) rather than overwrite (`filemode='w'`)
3. determine the format of the output message (`format=...`)
4. determine the format of the output date and time (`datefmt='%Y-%m-%d %H:%M:%S'`)
5. and determine the minimum message level it will accept (`level=logging.DEBUG`)."
"I am not sure I understand the purpose of the `flask.jsonify` method. I try to make a JSON string from this:

```
data = {""id"": str(album.id), ""title"": album.title}

```

but what I get with `json.dumps` differs from what I get with `flask.jsonify`.

```
json.dumps(data): [{""id"": ""4ea856fd6506ae0db42702dd"", ""title"": ""Business""}]
flask.jsonify(data): {""id"":…, ""title"":…}

```

Obviously I need to get a result that looks more like what `json.dumps` returns. What am I doing wrong?","The `jsonify()` function in flask returns a `flask.Response()` object that already has the appropriate content-type header 'application/json' for use with json responses. Whereas, the `json.dumps()` method will just return an encoded string, which would require manually adding the MIME type header.

See more about the `jsonify()` function [here](https://flask.palletsprojects.com/api/#flask.json.jsonify) for full reference.

Edit:
Also, I've noticed that `jsonify()` handles kwargs or dictionaries, while `json.dumps()` additionally supports lists and others."
"I'd like to create a file with path `x` using python. I've been using `os.system(y)` where `y = 'touch %s' % (x)`. I've looked for a non-directory version of `os.mkdir`, but I haven't been able to find anything. Is there a tool like this to create a file without opening it, or using system or popen/subprocess?","There is no way to create a file without opening it There is `os.mknod(""newfile.txt"")` (*but it requires root privileges on OSX*). The system call to create a file is actually `open()` with the `O_CREAT` flag. So no matter how, you'll always open the file.

So the easiest way to simply create a file without truncating it in case it exists is this:

```
open(x, 'a').close()

```

Actually you could omit the `.close()` since the refcounting GC of CPython will close it immediately after the `open()` statement finished - but it's cleaner to do it explicitely and relying on CPython-specific behaviour is not good either.

In case you want `touch`'s behaviour (i.e. update the mtime in case the file exists):

```
import os
def touch(path):
    with open(path, 'a'):
        os.utime(path, None)

```

You could extend this to also create any directories in the path that do not exist:

```
basedir = os.path.dirname(path)
if not os.path.exists(basedir):
    os.makedirs(basedir)

```"
"How can I treat the last element of the input specially, when iterating with a `for` loop? In particular, if there is code that should only occur ""between"" elements (and not ""after"" the last one), how can I structure the code?

Currently, I write code like so:

```
for i, data in enumerate(data_list):
    code_that_is_done_for_every_element
    if i != len(data_list) - 1:
        code_that_is_done_between_elements

```

How can I simplify or improve this?","Most of the times it is easier (and cheaper) to make the *first* iteration the special case instead of the last one:

```
first = True
for data in data_list:
    if first:
        first = False
    else:
        between_items()

    item()

```

This will work for any iterable, even for those that have no `len()`:

```
file = open('/path/to/file')
for line in file:
    process_line(line)
    
    # No way of telling if this is the last line!

```

Apart from that, I don't think there is a generally superior solution as it depends on what you are trying to do. For example, if you are building a string from a list, it's naturally better to use `str.join()` than using a `for` loop “with special case”.

---

Using the same principle but more compact:

```
for i, line in enumerate(data_list):
    if i > 0:
        between_items()
    item()

```

Looks familiar, doesn't it? :)

---

For @ofko, and others who really need to find out if the current value of an iterable without `len()` is the last one, you will need to look ahead:

```
def lookahead(iterable):
    """"""Pass through all values from the given iterable, augmented by the
    information if there are more values to come after the current one
    (True), or if it is the last value (False).
    """"""
    # Get an iterator and pull the first value.
    it = iter(iterable)
    try:
        last = next(it)
    except StopIteration:
        return
    # Run the iterator to exhaustion (starting from the second value).
    for val in it:
        # Report the *previous* value (more to come).
        yield last, True
        last = val
    # Report the last value.
    yield last, False

```

Then you can use it like this:

```
>>> for i, has_more in lookahead(range(3)):
...     print(i, has_more)
0 True
1 True
2 False

```"
"I would like to use an [IPython notebook](http://ipython.org/notebook.html) as a way to interactively analyze some genome charts I am making with Biopython's [`GenomeDiagram`](http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec329) module. While there is extensive documentation on how to use `matplotlib` to get graphs inline in IPython notebook, GenomeDiagram uses the ReportLab toolkit which I don't think is supported for inline graphing in IPython.

I was thinking, however, that a way around this would be to write out the plot/genome diagram to a file and then open the image inline which would have the same result with something like this:

```
gd_diagram.write(""test.png"", ""PNG"")
display(file=""test.png"")

```

However, I can't figure out how to do this - or know if it's possible. So does anyone know if images can be opened/displayed in IPython?","Courtesy of [this post](http://python.6.n6.nabble.com/IPython-User-ipython-notebook-how-to-display-image-not-from-pylab-td4497427.html), you can do the following:

```
from IPython.display import Image
Image(filename='test.png') 

```"
"I am working on a package in Python. I use virtualenv. I set the path to the root of the module in a .pth path in my virtualenv, so that I can import modules of the package while developing the code and do testing (Question 1: is it a good way to do?). This works fine (here is an example, this is the behavior I want):

```
(VEnvTestRc) zz@zz:~/Desktop/GitFolders/rc$ python
Python 2.7.12 (default, Jul  1 2016, 15:12:24) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from rc import ns
>>> exit()
(VEnvTestRc) zz@zz:~/Desktop/GitFolders/rc$ python tests/test_ns.py 
issued command: echo hello
command output: hello

```

However, if I try to use PyTest, I get some import error messages:

```
(VEnvTestRc) zz@zz:~/Desktop/GitFolders/rc$ pytest
=========================================== test session starts ============================================
platform linux2 -- Python 2.7.12, pytest-3.0.5, py-1.4.31, pluggy-0.4.0
rootdir: /home/zz/Desktop/GitFolders/rc, inifile: 
collected 0 items / 1 errors 

================================================== ERRORS ==================================================
________________________________ ERROR collecting tests/test_ns.py ________________________________
ImportError while importing test module '/home/zz/Desktop/GitFolders/rc/tests/test_ns.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_ns.py:2: in <module>
    from rc import ns
E   ImportError: cannot import name ns
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
========================================= 1 error in 0.09 seconds ==========================================
(VEnvTestRc) zz@zz:~/Desktop/GitFolders/rc$ which pytest
/home/zz/Desktop/VirtualEnvs/VEnvTestRc/bin/pytest

```

I am a bit puzzled, it looks like this indicates an import error, but Python does it fine so why is there a problem specifically with PyTest? Any suggestion to the reason / remedy (Question 2)? I googled and stack-overflowed the 'ImportError: cannot import' error for PyTest, but the hits I got were related to missing python path and remedy to this, which does not seem to be the problem here. Any suggestions?","Found the answer:

DO NOT put a `__init__.py` file in a folder containing TESTS if you plan on using pytest. I had one such file, deleting it solved the problem.

This was actually buried in the comments to the second answer of [PATH issue with pytest 'ImportError: No module named YadaYadaYada'](https://stackoverflow.com/questions/10253826/path-issue-with-pytest-importerror-no-module-named-yadayadayada) so I did not see it, hope it gets more visibility here."
"I have a function in python that can either return a `bool` or a `list`. Is there a way to specify the return types using type hints?

For example, is this the correct way to do it?

```
def foo(id) -> list or bool:
    ...

```","From the documentation - [Union Type](https://docs.python.org/3/library/stdtypes.html#types-union):

> A union object holds the value of the `|` (bitwise or) operation on multiple [type objects](https://docs.python.org/3/library/stdtypes.html#bltin-type-objects). These types are intended primarily for [type annotations](https://docs.python.org/3/glossary.html#term-annotation). The union type expression enables cleaner type hinting syntax compared to [`typing.Union`](https://docs.python.org/3/library/typing.html#typing.Union).

This use of `|` was added in Python 3.10. Hence the proper way to represent more than one return data type is:

```
def foo(client_id: str) -> list | bool:

```

For earlier versions, use [`typing.Union`](https://docs.python.org/3/library/typing.html#typing.Union):

```
from typing import Union


def foo(client_id: str) -> Union[list, bool]:

```

---

But do note that typing is not enforced. Python continues to remain a dynamically-typed language. The annotation syntax has been developed to help during the development of the code prior to being released into production. As [PEP 484](https://www.python.org/dev/peps/pep-0484/) states, ""no type checking happens at runtime.""

```
>>> def foo(a: str) -> list:
...     return ""Works""
... 
>>> foo(1)
'Works'

```

As you can see I am passing an int value and returning a str. However the `__annotations__` will be set to the respective values.

```
>>> foo.__annotations__ 
{'return': <class 'list'>, 'a': <class 'str'>}

```

---

Please go through [PEP 483](https://www.python.org/dev/peps/pep-0483/) for more about Type hints. Also see [What are type hints in Python 3.5?](https://stackoverflow.com/questions/32557920/what-are-type-hints-in-python-3-5)?

Kindly note that this is available only for ***Python 3.5*** and upwards. This is mentioned clearly in [PEP 484](https://www.python.org/dev/peps/pep-0484/)."
"[This was my source I started with.](http://docs.python.org/2/library/functions.html#filter)

My List

```
L = [0, 23, 234, 89, None, 0, 35, 9]

```

When I run this :

```
L = filter(None, L)

```

I get this results

```
[23, 234, 89, 35, 9]

```

But this is not what I need, what I really need is :

```
[0, 23, 234, 89, 0, 35, 9]

```

Because I'm calculating percentile of the data and the 0 make a lot of difference.

How to remove the None value from a list without removing 0 value?","```
>>> L = [0, 23, 234, 89, None, 0, 35, 9]
>>> [x for x in L if x is not None]
[0, 23, 234, 89, 0, 35, 9]

```

Just for fun, here's how you can adapt `filter` to do this without using a `lambda`, (I wouldn't recommend this code - it's just for scientific purposes)

```
>>> from operator import is_not
>>> from functools import partial
>>> L = [0, 23, 234, 89, None, 0, 35, 9]
>>> list(filter(partial(is_not, None), L))
[0, 23, 234, 89, 0, 35, 9]

```"
"I'd like to run in a local environment a Python script which is normally run in a Docker container. The `docker-compose.yml` specifies an [env\_file](https://docs.docker.com/compose/env-file/) which looks (partially) like the following:

```
DB_ADDR=rethinkdb
DB_PORT=28015
DB_NAME=ipercron

```

In order to run this locally, I would like these lines to be converted to

```
os.environ['DB_ADDR'] = 'rethinkdb'
os.environ['DB_PORT'] = '28015'
os.environ['DB_NAME'] = 'ipercron'

```

I could write my parser, but I was wondering if there are any existing modules/tools to read in environment variables from configuration files?","I use [Python Dotenv Library](https://pypi.org/project/python-dotenv). Just install the library `pip install python-dotenv`, create a `.env` file with your environment variables, and import the environment variables in your code like this:

```
import os
from dotenv import load_dotenv

load_dotenv()

MY_ENV_VAR = os.getenv('MY_ENV_VAR')

print(MY_ENV_VAR)

```

From the `.env` file:

```
MY_ENV_VAR=""This is my env var content.""

```

This is the way I do when I need to test code outside my docker system and prepare it to return it into docker again.

---

Added by [esmaeelE](https://stackoverflow.com/users/7508077/esmaeele)

Also you can check is environment variable exists or not.

```
print('MY_ENV_VAR' in os.environ) # True of False
print(os.environ['MY_ENV_VAR']) # Print contents of variable
print(os.environ.get('MY_ENV_VAR')) # Its better when variable not existed

```

And **python-dotenv** package can be installed on Debian with

```
apt install python3-dotenv

```"
"I have a dataframe with ~300K rows and ~40 columns.
I want to find out if any rows contain null values - and put these 'null'-rows into a separate dataframe so that I could explore them easily.

I can create a mask explicitly:

```
mask = False
for col in df.columns: 
    mask = mask | df[col].isnull()
dfnulls = df[mask]

```

Or I can do something like:

```
df.ix[df.index[(df.T == np.nan).sum() > 1]]

```

Is there a more elegant way of doing it (locating rows with nulls in them)?","[Updated to adapt to modern `pandas`, which has `isnull` as a method of `DataFrame`s..]

You can use `isnull` and `any` to build a boolean Series and use that to index into your frame:

```
>>> df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])
>>> df.isnull()
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False
>>> df.isnull().any(axis=1)
0    False
1     True
2     True
3    False
4    False
dtype: bool
>>> df[df.isnull().any(axis=1)]
   0   1   2
1  0 NaN   0
2  0   0 NaN

```

---

[For older `pandas`:]

You could use the function `isnull` instead of the method:

```
In [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])

In [57]: df
Out[57]: 
   0   1   2
0  0   1   2
1  0 NaN   0
2  0   0 NaN
3  0   1   2
4  0   1   2

In [58]: pd.isnull(df)
Out[58]: 
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False

In [59]: pd.isnull(df).any(axis=1)
Out[59]: 
0    False
1     True
2     True
3    False
4    False

```

leading to the rather compact:

```
In [60]: df[pd.isnull(df).any(axis=1)]
Out[60]: 
   0   1   2
1  0 NaN   0
2  0   0 NaN

```"
"I have a C++/Obj-C background and I am just discovering Python (been writing it for about an hour).
I am writing a script to recursively read the contents of text files in a folder structure.

The problem I have is the code I have written will only work for one folder deep. I can see why in the code (see `#hardcoded path`), I just don't know how I can move forward with Python since my experience with it is only brand new.

**Python Code:**

```
import os
import sys

rootdir = sys.argv[1]

for root, subFolders, files in os.walk(rootdir):

    for folder in subFolders:
        outfileName = rootdir + ""/"" + folder + ""/py-outfile.txt"" # hardcoded path
        folderOut = open( outfileName, 'w' )
        print ""outfileName is "" + outfileName

        for file in files:
            filePath = rootdir + '/' + file
            f = open( filePath, 'r' )
            toWrite = f.read()
            print ""Writing '"" + toWrite + ""' to"" + filePath
            folderOut.write( toWrite )
            f.close()

        folderOut.close()

```","Make sure you understand the three return values of `os.walk`:

```
for root, subdirs, files in os.walk(rootdir):

```

has the following meaning:

* `root`: Current path which is ""walked through""
* `subdirs`: Files in `root` of type directory
* `files`: Files in `root` (not in `subdirs`) of type other than directory

And please use `os.path.join` instead of concatenating with a slash! Your problem is `filePath = rootdir + '/' + file` - you must concatenate the currently ""walked"" folder instead of the topmost folder. So that must be `filePath = os.path.join(root, file)`. BTW ""file"" is a builtin, so you don't normally use it as variable name.

Another problem are your loops, which should be like this, for example:

```
import os
import sys

walk_dir = sys.argv[1]

print('walk_dir = ' + walk_dir)

# If your current working directory may change during script execution, it's recommended to
# immediately convert program arguments to an absolute path. Then the variable root below will
# be an absolute path as well. Example:
# walk_dir = os.path.abspath(walk_dir)
print('walk_dir (absolute) = ' + os.path.abspath(walk_dir))

for root, subdirs, files in os.walk(walk_dir):
    print('--\nroot = ' + root)
    list_file_path = os.path.join(root, 'my-directory-list.txt')
    print('list_file_path = ' + list_file_path)

    with open(list_file_path, 'wb') as list_file:
        for subdir in subdirs:
            print('\t- subdirectory ' + subdir)

        for filename in files:
            file_path = os.path.join(root, filename)

            print('\t- file %s (full path: %s)' % (filename, file_path))

            with open(file_path, 'rb') as f:
                f_content = f.read()
                list_file.write(('The file %s contains:\n' % filename).encode('utf-8'))
                list_file.write(f_content)
                list_file.write(b'\n')

```

If you didn't know, the `with` statement for files is a shorthand:

```
with open('filename', 'rb') as f:
    dosomething()

# is effectively the same as

f = open('filename', 'rb')
try:
    dosomething()
finally:
    f.close()

```"
"I am trying to send email (Gmail) using python, but I am getting following error.

```
Traceback (most recent call last):  
File ""emailSend.py"", line 14, in <module>  
server.login(username,password)  
File ""/usr/lib/python2.5/smtplib.py"", line 554, in login  
raise SMTPException(""SMTP AUTH extension not supported by server."")  
smtplib.SMTPException: SMTP AUTH extension not supported by server.

```

The Python script is the following.

```
import smtplib

fromaddr = 'user_me@gmail.com'
toaddrs  = 'user_you@gmail.com'
msg = 'Why,Oh why!'
username = 'user_me@gmail.com'
password = 'pwd'
server = smtplib.SMTP('smtp.gmail.com:587')
server.starttls()
server.login(username,password)
server.sendmail(fromaddr, toaddrs, msg)
server.quit()

```","```
def send_email(user, pwd, recipient, subject, body):
    import smtplib

    FROM = user
    TO = recipient if isinstance(recipient, list) else [recipient]
    SUBJECT = subject
    TEXT = body

    # Prepare actual message
    message = """"""From: %s\nTo: %s\nSubject: %s\n\n%s
    """""" % (FROM, "", "".join(TO), SUBJECT, TEXT)
    try:
        server = smtplib.SMTP(""smtp.gmail.com"", 587)
        server.ehlo()
        server.starttls()
        server.login(user, pwd)
        server.sendmail(FROM, TO, message)
        server.close()
        print 'successfully sent the mail'
    except:
        print ""failed to send mail""

```

if you want to use Port 465 you have to create an `SMTP_SSL` object:

```
# SMTP_SSL Example
server_ssl = smtplib.SMTP_SSL(""smtp.gmail.com"", 465)
server_ssl.ehlo() # optional, called by login()
server_ssl.login(gmail_user, gmail_pwd)  
# ssl server doesn't support or need tls, so don't call server_ssl.starttls() 
server_ssl.sendmail(FROM, TO, message)
#server_ssl.quit()
server_ssl.close()
print 'successfully sent the mail'

```"
"I'm sure I'm forgetting something very simple, but I cannot get certain plots to work with Seaborn.

If I do:

```
import seaborn as sns

```

Then any plots that I create as usual with matplotlib get the Seaborn styling (with the grey grid in the background).

However, if I try to do one of the examples, such as:

```
In [1]: import seaborn as sns

In [2]: sns.set()

In [3]: df = sns.load_dataset('iris')

In [4]: sns.pairplot(df, hue='species', size=2.5)
Out[4]: <seaborn.axisgrid.PairGrid at 0x3e59150>

```

The pairplot function returns a PairGrid object, but the plot doesn't show up.

I'm a little confused because matplotlib seems to be functioning properly, and the Seaborn styles are applied to other matplotlib plots, but the Seaborn functions don't seem to do anything. Does anybody have any idea what might be the problem?","Plots created using seaborn need to be displayed like ordinary matplotlib plots.
This can be done using the

```
plt.show()

```

function from matplotlib.

Originally I posted the solution to use the already imported matplotlib object from seaborn (`sns.plt.show()`) however this is considered to be a bad practice. Therefore, simply directly import the `_matplotlib.pyplot_` module and show your plots with

```
import matplotlib.pyplot as plt
plt.show()

```

If the IPython notebook is used the inline backend can be invoked to remove the necessity of calling show after each plot. The respective magic is

```
%matplotlib inline

```"
I need a working approach of getting all classes that are inherited from a base class in Python.,"New-style classes (i.e. subclassed from `object`, which is the default in Python 3) have a `__subclasses__` method which returns the subclasses:

```
class Foo(object): pass
class Bar(Foo): pass
class Baz(Foo): pass
class Bing(Bar): pass

```

Here are the names of the subclasses:

```
print([cls.__name__ for cls in Foo.__subclasses__()])
# ['Bar', 'Baz']

```

Here are the subclasses themselves:

```
print(Foo.__subclasses__())
# [<class '__main__.Bar'>, <class '__main__.Baz'>]

```

Confirmation that the subclasses do indeed list `Foo` as their base:

```
for cls in Foo.__subclasses__():
    print(cls.__base__)
# <class '__main__.Foo'>
# <class '__main__.Foo'>

```

Note if you want subsubclasses, you'll have to recurse:

```
def all_subclasses(cls):
    return set(cls.__subclasses__()).union(
        [s for c in cls.__subclasses__() for s in all_subclasses(c)])

print(all_subclasses(Foo))
# {<class '__main__.Bar'>, <class '__main__.Baz'>, <class '__main__.Bing'>}

```

Note that if the class definition of a subclass hasn't been executed yet - for example, if the subclass's module hasn't been imported yet - then that subclass doesn't exist yet, and `__subclasses__` won't find it.

---

You mentioned ""given its name"". Since Python classes are first-class objects, you don't need to use a string with the class's name in place of the class or anything like that. You can just use the class directly, and you probably should.

If you do have a string representing the name of a class and you want to find that class's subclasses, then there are two steps: find the class given its name, and then find the subclasses with `__subclasses__` as above.

How to find the class from the name depends on where you're expecting to find it. If you're expecting to find it in the same module as the code that's trying to locate the class, then

```
cls = globals()[name]

```

would do the job, or in the unlikely case that you're expecting to find it in locals,

```
cls = locals()[name]

```

If the class could be in any module, then your name string should contain the fully-qualified name - something like `'pkg.module.Foo'` instead of just `'Foo'`. Use `importlib` to load the class's module, then retrieve the corresponding attribute:

```
import importlib
modname, _, clsname = name.rpartition('.')
mod = importlib.import_module(modname)
cls = getattr(mod, clsname)

```

However you find the class, `cls.__subclasses__()` would then return a list of its subclasses."
"If I've got an array of strings, can I check to see if a string is in the array without doing a `for` loop? Specifically, I'm looking for a way to do it within an `if` statement, so something like this:

```
if [check that item is in array]:

```","Assuming you mean ""list"" where you say ""array"", you can do

```
if item in my_list:
    # whatever

```

This works for any collection, not just for lists. For dictionaries, it checks whether the given key is present in the dictionary."
I have two strings and I would like to check whether the first is a substring of the other. Does Python have such a built-in functionality?,"Try using `in` like this:

```
>>> x = 'hello'
>>> y = 'll'
>>> y in x
True

```"
"In Python, I only want to list all the files in the current directory ONLY. I do not want files listed from any sub directory or parent.

There do seem to be similar solutions out there, but they don't seem to work for me. Here's my code snippet:

```
import os
for subdir, dirs, files in os.walk('./'):
    for file in files:
      do some stuff
      print file

```

Let's suppose I have 2 files, holygrail.py and Tim inside my current directory. I have a folder as well and it contains two files - let's call them Arthur and Lancelot - inside it. When I run the script, this is what I get:

```
holygrail.py
Tim
Arthur
Lancelot

```

I am happy with holygrail.py and Tim. But the two files, Arthur and Lancelot, I do not want listed.","Just use [`os.listdir`](http://docs.python.org/library/os#os.listdir) and [`os.path.isfile`](http://docs.python.org/library/os.path.html#os.path.isfile) instead of [`os.walk`](http://docs.python.org/library/os#os.walk).

**Example:**

```
import os
files = [f for f in os.listdir('.') if os.path.isfile(f)]
for f in files:
    # do something

```

---

But be careful while applying this to other directory, like

```
files = [f for f in os.listdir(somedir) if os.path.isfile(f)]

```

which would not work because `f` is not a full path but relative to the current directory.

Therefore, for filtering on another directory, do `os.path.isfile(os.path.join(somedir, f))`

*(Thanks [Causality](https://stackoverflow.com/users/1311956/causality) for the hint)*"
How do I check if a pandas `DataFrame` is empty? I'd like to print some message in the terminal if the `DataFrame` is empty.,"You can use the attribute `df.empty` to check whether it's empty or not:

```
if df.empty:
    print('DataFrame is empty!')

```

Source: [Pandas Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.empty.html)"
"I have a very similar question to [this question](https://stackoverflow.com/questions/1517129/python-how-do-i-install-scipy-on-64-bit-windows), but I am still one step behind. I have only one version of Python 3 installed on my Windows 7 (*sorry*) 64-bit system.

I installed NumPy following this [link](http://sourceforge.net/projects/numpy/files/NumPy/1.3.0/numpy-1.3.0.win-amd64-py2.6.msi/download?use_mirror=ufpr) - as suggested in the question. The installation went fine but when I execute

```
import numpy

```

I got the following error:

> Import error: No module named numpy","You can simply use

```
pip install numpy

```

Or for python3, use

```
pip3 install numpy

```"
"I have a dataframe with repeat values in column A. I want to drop duplicates, keeping the row with the highest value in column B.

So this:

```
A B
1 10
1 20
2 30
2 40
3 10

```

Should turn into this:

```
A B
1 20
2 40
3 10

```

I'm guessing there's probably an easy way to do this—maybe as easy as sorting the DataFrame before dropping duplicates—but I don't know groupby's internal logic well enough to figure it out. Any suggestions?","This takes the last. Not the maximum though:

```
In [10]: df.drop_duplicates(subset='A', keep=""last"")
Out[10]: 
   A   B
1  1  20
3  2  40
4  3  10

```

You can do also something like:

```
In [12]: df.groupby('A', group_keys=False).apply(lambda x: x.loc[x.B.idxmax()])
Out[12]: 
   A   B
A       
1  1  20
2  2  40
3  3  10

```"
"I want to read a large file (>5GB), line by line, without loading its entire contents into memory. I cannot use `readlines()` since it creates a very large list in memory.","Use a `for` loop on a file object to read it line-by-line. Use `with open(...)` to let a [context manager](https://docs.python.org/3/reference/datamodel.html#context-managers) ensure that the file is closed after reading:

```
with open(""log.txt"") as infile:
    for line in infile:
        print(line)

```"
"Is there any correct type hint to use for a file or file-like object in Python? For example, how would I type-hint the return value of this function?

```
def foo() -> ???:
    return open('bar')

```","Use either the `typing.TextIO` or `typing.BinaryIO` types, for files opened in text mode or binary mode respectively.

From [the docs](https://docs.python.org/3/library/typing.html#typing.IO):

> ### *class* `typing.IO`
>
> Wrapper namespace for I/O stream types.
>
> This defines the generic type `IO[AnyStr]` and aliases `TextIO` and `BinaryIO` for respectively `IO[str]` and `IO[bytes]`. These representing the types of I/O streams such as returned by `open()`."
Is there any performance difference between tuples and lists when it comes to instantiation and retrieval of elements?,"Summary
-------

*Tuples tend to perform better than lists* in almost every category:

1. Tuples can be [constant folded](https://en.wikipedia.org/wiki/Constant_folding).
2. Tuples can be reused instead of copied.
3. Tuples are compact and don't over-allocate.
4. Tuples directly reference their elements.

Tuples can be constant folded
-----------------------------

Tuples of constants can be precomputed by Python's peephole optimizer or AST-optimizer. Lists, on the other hand, get built-up from scratch:

```
    >>> from dis import dis

    >>> dis(compile(""(10, 'abc')"", '', 'eval'))
      1           0 LOAD_CONST               2 ((10, 'abc'))
                  3 RETURN_VALUE   
 
    >>> dis(compile(""[10, 'abc']"", '', 'eval'))
      1           0 LOAD_CONST               0 (10)
                  3 LOAD_CONST               1 ('abc')
                  6 BUILD_LIST               2
                  9 RETURN_VALUE 

```

Tuples do not need to be copied
-------------------------------

Running `tuple(some_tuple)` returns immediately itself. Since tuples are immutable, they do not have to be copied:

```
>>> a = (10, 20, 30)
>>> b = tuple(a)
>>> a is b
True

```

In contrast, `list(some_list)` requires all the data to be copied to a new list:

```
>>> a = [10, 20, 30]
>>> b = list(a)
>>> a is b
False

```

Tuples do not over-allocate
---------------------------

Since a tuple's size is fixed, it can be stored more compactly than lists which need to over-allocate to make `append()` operations efficient.

This gives tuples a nice space advantage:

```
>>> import sys
>>> sys.getsizeof(tuple(iter(range(10))))
128
>>> sys.getsizeof(list(iter(range(10))))
200

```

Here is the comment from *Objects/listobject.c* that explains what lists are doing:

```
/* This over-allocates proportional to the list size, making room
 * for additional growth.  The over-allocation is mild, but is
 * enough to give linear-time amortized behavior over a long
 * sequence of appends() in the presence of a poorly-performing
 * system realloc().
 * The growth pattern is:  0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...
 * Note: new_allocated won't overflow because the largest possible value
 *       is PY_SSIZE_T_MAX * (9 / 8) + 6 which always fits in a size_t.
 */

```

Tuples refer directly to their elements
---------------------------------------

References to objects are incorporated directly in a tuple object. In contrast, lists have an extra layer of indirection to an external array of pointers.

This gives tuples a small speed advantage for indexed lookups and unpacking:

```
$ python3.6 -m timeit -s 'a = (10, 20, 30)' 'a[1]'
10000000 loops, best of 3: 0.0304 usec per loop
$ python3.6 -m timeit -s 'a = [10, 20, 30]' 'a[1]'
10000000 loops, best of 3: 0.0309 usec per loop

$ python3.6 -m timeit -s 'a = (10, 20, 30)' 'x, y, z = a'
10000000 loops, best of 3: 0.0249 usec per loop
$ python3.6 -m timeit -s 'a = [10, 20, 30]' 'x, y, z = a'
10000000 loops, best of 3: 0.0251 usec per loop

```

[Here](https://github.com/python/cpython/blob/54ba556c6c7d8fd5504dc142c2e773890c55a774/Include/cpython/tupleobject.h#L9) is how the tuple `(10, 20)` is stored:

```
    typedef struct {
        Py_ssize_t ob_refcnt;
        struct _typeobject *ob_type;
        Py_ssize_t ob_size;
        PyObject *ob_item[2];     /* store a pointer to 10 and a pointer to 20 */
    } PyTupleObject;

```

[Here](https://github.com/python/cpython/blob/master/Include/listobject.h) is how the list `[10, 20]` is stored:

```
    PyObject arr[2];              /* store a pointer to 10 and a pointer to 20 */

    typedef struct {
        Py_ssize_t ob_refcnt;
        struct _typeobject *ob_type;
        Py_ssize_t ob_size;
        PyObject **ob_item = arr; /* store a pointer to the two-pointer array */
        Py_ssize_t allocated;
    } PyListObject;

```

Note that the tuple object incorporates the two data pointers directly while the list object has an additional layer of indirection to an external array holding the two data pointers."
"I am trying to use networkx with Python. When I run this program it get this error. Is there anything missing?

```
#!/usr/bin/env python

import networkx as nx
import matplotlib
import matplotlib.pyplot
import matplotlib.pyplot as plt

G=nx.Graph()
G.add_node(1)
G.add_nodes_from([2,3,4,5,6,7,8,9,10])
#nx.draw_graphviz(G)
#nx_write_dot(G, 'node.png')
nx.draw(G)
plt.savefig(""/var/www/node.png"")


Traceback (most recent call last):
  File ""graph.py"", line 13, in <module>
    nx.draw(G)
  File ""/usr/lib/pymodules/python2.5/networkx/drawing/nx_pylab.py"", line 124, in draw
    cf=pylab.gcf()
  File ""/usr/lib/pymodules/python2.5/matplotlib/pyplot.py"", line 276, in gcf
    return figure()
  File ""/usr/lib/pymodules/python2.5/matplotlib/pyplot.py"", line 254, in figure
    **kwargs)
  File ""/usr/lib/pymodules/python2.5/matplotlib/backends/backend_tkagg.py"", line 90, in new_figure_manager
    window = Tk.Tk()
  File ""/usr/lib/python2.5/lib-tk/Tkinter.py"", line 1650, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: no display name and no $DISPLAY environment variable

```

---

I get a different error now:

```
#!/usr/bin/env python

import networkx as nx
import matplotlib
import matplotlib.pyplot
import matplotlib.pyplot as plt

matplotlib.use('Agg')

G=nx.Graph()
G.add_node(1)
G.add_nodes_from([2,3,4,5,6,7,8,9,10])
#nx.draw_graphviz(G)
#nx_write_dot(G, 'node.png')
nx.draw(G)
plt.savefig(""/var/www/node.png"")

```

---

```
/usr/lib/pymodules/python2.5/matplotlib/__init__.py:835: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  if warn: warnings.warn(_use_error_msg)
Traceback (most recent call last):
  File ""graph.py"", line 15, in <module>
    nx.draw(G)
  File ""/usr/lib/python2.5/site-packages/networkx-1.2.dev-py2.5.egg/networkx/drawing/nx_pylab.py"", line 124, in draw
    cf=pylab.gcf()
  File ""/usr/lib/pymodules/python2.5/matplotlib/pyplot.py"", line 276, in gcf
    return figure()
  File ""/usr/lib/pymodules/python2.5/matplotlib/pyplot.py"", line 254, in figure
    **kwargs)
  File ""/usr/lib/pymodules/python2.5/matplotlib/backends/backend_tkagg.py"", line 90, in new_figure_manager
    window = Tk.Tk()
  File ""/usr/lib/python2.5/lib-tk/Tkinter.py"", line 1650, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: no display name and no $DISPLAY environment variable

```

---

I get a different error now:

```
#!/usr/bin/env python

import networkx as nx
import matplotlib
import matplotlib.pyplot
import matplotlib.pyplot as plt

matplotlib.use('Agg')

G=nx.Graph()
G.add_node(1)
G.add_nodes_from([2,3,4,5,6,7,8,9,10])
#nx.draw_graphviz(G)
#nx_write_dot(G, 'node.png')
nx.draw(G)
plt.savefig(""/var/www/node.png"")

```

---

```
/usr/lib/pymodules/python2.5/matplotlib/__init__.py:835: UserWarning:  This call to matplotlib.use() has no effect
because the the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  if warn: warnings.warn(_use_error_msg)
Traceback (most recent call last):
  File ""graph.py"", line 15, in <module>
    nx.draw(G)
  File ""/usr/lib/python2.5/site-packages/networkx-1.2.dev-py2.5.egg/networkx/drawing/nx_pylab.py"", line 124, in draw
    cf=pylab.gcf()
  File ""/usr/lib/pymodules/python2.5/matplotlib/pyplot.py"", line 276, in gcf
    return figure()
  File ""/usr/lib/pymodules/python2.5/matplotlib/pyplot.py"", line 254, in figure
    **kwargs)
  File ""/usr/lib/pymodules/python2.5/matplotlib/backends/backend_tkagg.py"", line 90, in new_figure_manager
    window = Tk.Tk()
  File ""/usr/lib/python2.5/lib-tk/Tkinter.py"", line 1650, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: no display name and no $DISPLAY environment variable

```","The main problem is that (on your system) matplotlib chooses an x-using backend by default. I just had the same problem on one of my servers. The solution for me was to add the following code in a place that gets read *before* any other pylab/matplotlib/**pyplot** import:

```
import matplotlib
# Force matplotlib to not use any Xwindows backend.
matplotlib.use('Agg')

```

The alternative is to [set it](https://matplotlib.org/stable/tutorials/introductory/customizing.html#the-matplotlibrc-file) in your `.matplotlibrc`"
"Is there a way of reading one single character from the user input? For instance, they press one key at the terminal and it is returned (sort of like `getch()`). I know there's a function in Windows for it, but I'd like something that is cross-platform.","Here's a link to the ActiveState Recipes site that says how you can read a single character in Windows, Linux and OSX:

[getch()-like unbuffered character reading from stdin on both Windows and Unix](https://code.activestate.com/recipes/134892/)

```
class _Getch:
    """"""Gets a single character from standard input.  Does not echo to the
screen.""""""
    def __init__(self):
        try:
            self.impl = _GetchWindows()
        except ImportError:
            self.impl = _GetchUnix()

    def __call__(self): return self.impl()


class _GetchUnix:
    def __init__(self):
        import tty, sys

    def __call__(self):
        import sys, tty, termios
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        try:
            tty.setraw(sys.stdin.fileno())
            ch = sys.stdin.read(1)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
        return ch


class _GetchWindows:
    def __init__(self):
        import msvcrt

    def __call__(self):
        import msvcrt
        return msvcrt.getch()


getch = _Getch()

```"
"I am trying to determine whether there is an entry in a Pandas column that has a particular value. I tried to do this with `if x in df['id']`. I thought this was working, except when I fed it a value that I knew was not in the column `43 in df['id']` it still returned `True`. When I subset to a data frame only containing entries matching the missing id `df[df['id'] == 43]` there are, obviously, no entries in it. How to I determine if a column in a Pandas data frame contains a particular value and why doesn't my current method work? (FYI, I have the same problem when I use the implementation in this [answer](https://stackoverflow.com/a/19630449/2327821) to a similar question).","`in` of a Series checks whether the value is in the index:

```
In [11]: s = pd.Series(list('abc'))

In [12]: s
Out[12]: 
0    a
1    b
2    c
dtype: object

In [13]: 1 in s
Out[13]: True

In [14]: 'a' in s
Out[14]: False

```

One option is to see if it's in [unique](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) values:

```
In [21]: s.unique()
Out[21]: array(['a', 'b', 'c'], dtype=object)

In [22]: 'a' in s.unique()
Out[22]: True

```

or a python set:

```
In [23]: set(s)
Out[23]: {'a', 'b', 'c'}

In [24]: 'a' in set(s)
Out[24]: True

```

As pointed out by @DSM, it may be more efficient (especially if you're just doing this for one value) to just use in directly on the values:

```
In [31]: s.values
Out[31]: array(['a', 'b', 'c'], dtype=object)

In [32]: 'a' in s.values
Out[32]: True

```"
"Using Flask, how can I read HTTP headers? I want to check the authorization header which is sent by the client.","```
from flask import request
request.headers.get('your-header-name')

```

`request.headers` behaves like a dictionary, so you can also get your header like you would with any dictionary:

```
request.headers['your-header-name']

```"
"I'm building a web application that will is going to manipulate (pad, mix, merge etc) sound files and I've found that [sox](http://sox.sourceforge.net/) does exactly what I want. Sox is a linux command line program and I'm feeling a little uncomfortable with having the python web app starting new sox processes on my server on a per request basis.

Example:

```
import os
os.system('sox input.wav -b 24 output.aiff rate -v -L -b 90 48k')

```

This whole setup seems a little unstable to me.

So my question is, what's the best practice for running command line programs from within a python (or any scripting language) web app?

Message queues would be one thing to implement in order to get around the whole request response cycle. But is there other ways to make these things more elegant?","The [`subprocess`](http://docs.python.org/library/subprocess.html) module is the preferred way of running other programs from Python -- much more flexible and nicer to use than `os.system`.

```
import subprocess
#subprocess.check_output(['ls', '-l'])  # All that is technically needed...
print(subprocess.check_output(['ls', '-l']))

```"
"I'm trying to use pandas to manipulate a .csv file but I get this error:

> pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 12

I have tried to read the pandas docs, but found nothing.

My code is simple:

```
path = 'GOOG Key Ratios.csv'
#print(open(path).read())
data = pd.read_csv(path)

```

How can I resolve this? Should I use the `csv` module or another language?","you could also try;

```
data = pd.read_csv('file1.csv', on_bad_lines='skip')

```

Do note that this will cause the offending lines to be skipped. If you don't expect many bad lines and want to (at least) know their amount and IDs, use `on_bad_lines='warn'`. For advanced handling of bads, you can pass a callable.

**Edit**

For Pandas < 1.3.0 try

```
data = pd.read_csv(""file1.csv"", error_bad_lines=False)

```

as per [pandas API reference](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)."
"I'm making a website where users can log on and download files, using the [Flask micro-framework](http://flask.pocoo.org/) (based on [Werkzeug](http://werkzeug.pocoo.org/)) which uses Python (2.6 in my case).

I need to get the IP address of users when they log on (for logging purposes).
Does anyone know how to do this? Surely there is a way to do it with Python?","See the documentation on [how to access the Request object](http://flask.pocoo.org/docs/quickstart/#accessing-request-data) and then get from this same Request object, the attribute `remote_addr`.

**Code example**

```
from flask import request
from flask import jsonify

@app.route(""/get_my_ip"", methods=[""GET""])
def get_my_ip():
    return jsonify({'ip': request.remote_addr}), 200

```

For more information see the [Werkzeug documentation](http://werkzeug.pocoo.org/docs/wrappers/#werkzeug.wrappers.BaseRequest.remote_addr)."
"I am trying to figure how to do proper error handling with boto3.

I am trying to create an IAM user:

```
def create_user(username, iam_conn):
    try:
        user = iam_conn.create_user(UserName=username)
        return user
    except Exception as e:
        return e

```

When the call to create\_user succeeds, I get a neat object that contains the http status code of the API call and the data of the newly created user.

Example:

```
{'ResponseMetadata': 
      {'HTTPStatusCode': 200, 
       'RequestId': 'omitted'
      },
 u'User': {u'Arn': 'arn:aws:iam::omitted:user/omitted',
           u'CreateDate': datetime.datetime(2015, 10, 11, 17, 13, 5, 882000, tzinfo=tzutc()),
           u'Path': '/',
           u'UserId': 'omitted',
           u'UserName': 'omitted'
          }
}

```

This works great. But when this fails (like if the user already exists), I just get an object of type botocore.exceptions.ClientError with only text to tell me what went wrong.

Example:
ClientError('An error occurred (EntityAlreadyExists) when calling the CreateUser operation: User with name omitted already exists.',)

This (AFAIK) makes error handling very hard because I can't just switch on the resulting http status code (409 for user already exists according to the AWS API docs for IAM). This makes me think that I must be doing something the wrong way. The optimal way would be for boto3 to never throw exceptions, but juts always return an object that reflects how the API call went.

Can anyone enlighten me on this issue or point me in the right direction?","Use the response contained within the exception. Here is an example:

```
import boto3
from botocore.exceptions import ClientError

try:
    iam = boto3.client('iam')
    user = iam.create_user(UserName='fred')
    print(""Created user: %s"" % user)
except ClientError as e:
    if e.response['Error']['Code'] == 'EntityAlreadyExists':
        print(""User already exists"")
    else:
        print(""Unexpected error: %s"" % e)

```

The response dict in the exception will contain the following:

* `['Error']['Code']` e.g. 'EntityAlreadyExists' or 'ValidationError'
* `['ResponseMetadata']['HTTPStatusCode']` e.g. 400
* `['ResponseMetadata']['RequestId']` e.g. 'd2b06652-88d7-11e5-99d0-812348583a35'
* `['Error']['Message']` e.g. ""An error occurred (EntityAlreadyExists) ...""
* `['Error']['Type']` e.g. 'Sender'

For more information see:

* [boto3 error handling](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html)
* [botocore error handling](http://botocore.readthedocs.io/en/latest/client_upgrades.html#error-handling)

**[Updated: 2018-03-07]**

The AWS Python SDK has begun to expose service exceptions on [clients](http://boto3.readthedocs.io/en/latest/guide/clients.html) (though not on [resources](http://boto3.readthedocs.io/en/latest/guide/migration.html#resource-objects)) that you can explicitly catch, so it is now possible to write that code like this:

```
import botocore
import boto3

try:
    iam = boto3.client('iam')
    user = iam.create_user(UserName='fred')
    print(""Created user: %s"" % user)
except iam.exceptions.EntityAlreadyExistsException:
    print(""User already exists"")
except botocore.exceptions.ParamValidationError as e:
    print(""Parameter validation error: %s"" % e)
except botocore.exceptions.ClientError as e:
    print(""Unexpected error: %s"" % e)

```

Unfortunately, there is currently no documentation for these errors/exceptions but you can get a list of the core errors as follows:

```
import botocore
import boto3
[e for e in dir(botocore.exceptions) if e.endswith('Error')]

```

Note that you must import both botocore and boto3. If you only import botocore then you will find that botocore has no attribute named `exceptions`. This is because the exceptions are dynamically populated into botocore by boto3.

You can get a list of service-specific exceptions as follows (replace `iam` with the relevant service as needed):

```
import boto3
iam = boto3.client('iam')
[e for e in dir(iam.exceptions) if e.endswith('Exception')]

```

**[Updated: 2021-09-07]**

In addition to the aforementioned client exception method, there is also a third-party helper package named [aws-error-utils](https://github.com/benkehoe/aws-error-utils)."
"Tried to perform REST GET through python requests with the following code and I got error.

Code snip:

```
import requests
header = {'Authorization': 'Bearer...'}
url = az_base_url + az_subscription_id + '/resourcegroups/Default-Networking/resources?' + az_api_version
r = requests.get(url, headers=header)

```

Error:

```
/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/util/ssl_.py:79: 
          InsecurePlatformWarning: A true SSLContext object is not available. 
          This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. 
          For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning

```

My python version is 2.7.3. I tried to install urllib3 and requests[security] as some other thread suggests, I still got the same error.

Wonder if anyone can provide some tips?","[The docs give a fair indicator of what's required.](https://urllib3.readthedocs.org/en/latest/security.html#pyopenssl), however `requests` allow us to skip a few steps:

You only need to install the `security` [package extras](https://github.com/kennethreitz/requests/blob/5a799dd0f505e6c6c2ff67e227f6a3d25c086342/setup.py#L71) (thanks @admdrew for pointing it out)

```
$ pip install requests[security]

```

or, install them directly:

```
$ pip install pyopenssl ndg-httpsclient pyasn1

```

[Requests will then automatically inject `pyopenssl` into `urllib3`](https://github.com/kennethreitz/requests/blob/a57c87a459d51c5b17d20285109e901b8aa95752/requests/__init__.py#L54)

---

If you're on ubuntu, you may run into trouble installing `pyopenssl`, you'll need these dependencies:

```
$ apt-get install libffi-dev libssl-dev

```"
"Is there any difference at all between these classes besides the name?

```
class WithClass ():
    def __init__(self):
        self.value = ""Bob""
    def my_func(self):
        print(self.value)

class WithoutClass ():
    value = ""Bob""

    def my_func(self):
        print(self.value)

```

Does it make any difference if I use or don't use the `__init__` method for declaring the variable `value`?

My main worry is that I'll be using it one way, when that'll cause me further problems down the road.","Variable set outside `__init__` belong to the class. They're shared by all instances.

Variables created inside `__init__` (and all other method functions) and prefaced with `self.` belong to the object instance."
"It seems that `Pipfile` / `Pipfile.lock` are intended to be replacements for `requirements.txt`, in the context of Python packaging. There isn't much documentation out there on how these actually work, however. I found an evolving description of pipfile on the PyPi section of the Python website [here](https://pypi.python.org/pypi/pipfile/0.0.2) but it's pretty messy and doesn't explain the semantics of the different sections of the file.

Any pointers on how to understand these files?","The concept behind these files is simple and analogue to other already existing tools, if you have some familiarity with Ruby's Bundler or Node's Npm. `Pipenv` is both a package and virtual environment management tool that uses the Pipfile and Pipfile.lock files to achieve these goals.

Pipenv handles the virtual environment for you (no more activate and deactivate required). Below, some basics to get you started, see more at [pipenv website](https://pipenv.readthedocs.io/en/latest/).

Getting Started
===============

Start using pipenv is easy, in your project folder type...

```
$ pipenv install

```

... and if it already has a `requirements.txt` file, it will generate a `Pipfile` file with the requirements and a virtual environment folder, otherwise, it will generate an empty `Pipfile` file. If you disliked or changed your mind about something that you have installed, just type...

```
$ pipenv uninstall <package>

```

... and you're good to go. To activate the virtual environment that pipenv already generated, go with...

```
$ pipenv shell

```

... and your virtual environment will be activated. To leave the environment...

```
$ exit

```

... and you will be back to your original terminal session.

Pipfile
=======

The *Pipfile* file is intended to specify packages requirements for your Python application or library, both to development and execution. You can install a package by simply using...

```
$ pipenv install flask

```

... and it will be added as a dependency for deployment and execution or by using ...

```
$ pipenv install --dev pytest

```

... and it will be used as a dependency for development time. In both cases, if you need to be more specific about the package version, as stated in the [documentation](https://pipenv-fork.readthedocs.io/en/latest/basics.html#specifying-versions-of-a-package) pipenv makes use of the same [version specifiers](https://pip.pypa.io/en/stable/reference/pip_install/#example-requirements-file) used by pip. The file syntax is pretty straight forward, as follows.

```
[[source]] # Here goes your package sources (where you are downloading your packages from).
url = ""https://pypi.python.org/simple""
verify_ssl = true
name = ""pypi""

[packages] # Here goes your package requirements for running the application and its versions (which packages you will use when running the application).
requests = ""*""
flask = ""*""
pandas = ""*""

[dev-packages] # Here goes your package requirements for developing the application and its versions (which packages you will use when developing the application)
pylint = ""*""
wheel = ""*""

[requires] # Here goes your required Python version.
python_version = ""3.6""

```

Pipfile.lock
============

The *Pipfile.lock* is intended to specify, based on the packages present in *Pipfile*, which specific version of those should be used, avoiding the risks of automatically upgrading packages that depend upon each other and breaking your project dependency tree.

You can lock your currently installed packages using...

```
$ pipenv lock

```

... and the tool will lookup your virtual environment folder to generate the lock file for you automatically, based on the currently installed versions. The file syntax is not as obvious as is for *Pipfile* , so for the sake of conciseness, it will not be displayed here."
"I'm trying to set up a server with python from mac terminal.

I navigate to folder location an use:

```
python -m SimpleHTTPServer

```

But this gives me error:

```
socket.error: [Errno 48] Address already in use

```

I had previously open a connection using the same command
for a different website in a different location in my machine.","You already have a process bound to the default port (8000). If you already ran the same module before, it is most likely that process still bound to the port. Try and locate the other process first:

```
$ ps -fA | grep python
  501 81651 12648   0  9:53PM ttys000    0:00.16 python -m SimpleHTTPServer

```

The command arguments are included, so you can spot the one running `SimpleHTTPServer` if more than one `python` process is active. You may want to test if `http://localhost:8000/` still shows a directory listing for local files.

The second number is the process number; stop the server by sending it a signal:

```
kill 81651

```

This sends a standard `SIGTERM` signal; if the process is unresponsive you may have to resort to tougher methods like sending a `SIGKILL` (`kill -s KILL <pid>` or `kill -9 <pid>`) signal instead. See [Wikipedia for more details](https://en.wikipedia.org/wiki/Unix_signal#POSIX_signals).

Alternatively, run the server on a *different* port, by specifying the alternative port on the command line:

```
$ python -m SimpleHTTPServer 8910
Serving HTTP on 0.0.0.0 port 8910 ...

```

then access the server as `http://localhost:8910`; where `8910` can be any number from 1024 and up, provided the port is not already taken."
"I recently upgrade Django from v1.3.1 to v1.4.

In my old `settings.py` I have

```
TEMPLATE_DIRS = (
    os.path.join(os.path.dirname( __file__ ), 'templates').replace('\\', '/'),
    # Put strings here, like ""/home/html/django_templates"" or ""C:/www/django/templates"".
    # Always use forward slashes, even on Windows.
    # Don't forget to use absolute paths, not relative paths.
)

```

This will point to `/Users/hobbes3/Sites/mysite/templates`, but [because Django v1.4 moved the project folder to the same level as the app folders](https://docs.djangoproject.com/en/dev/releases/1.4/#updated-default-project-layout-and-manage-py), my `settings.py` file is now in `/Users/hobbes3/Sites/mysite/mysite/` instead of `/Users/hobbes3/Sites/mysite/`.

How do I use `os.path` to look at a directory one level above from `__file__`. In other words, I want `/Users/hobbes3/Sites/mysite/mysite/settings.py` to find `/Users/hobbes3/Sites/mysite/templates` using relative paths.","```
os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..', 'templates'))

```

You can also use `normpath` to clean up the path, rather than `abspath`. However, in this situation, Django expects an absolute path rather than a relative path.

For cross platform compatability, use `os.pardir` instead of `'..'`."
"Because I am used to the old ways of duck typing in Python, I fail to understand the need for ABC (abstract base classes). The [help](https://docs.python.org/2/library/abc.html) is good on how to use them.

I tried to read the rationale in the [PEP](https://www.python.org/dev/peps/pep-3119/), but it went over my head. If I was looking for a mutable sequence container, I would check for `__setitem__`, or more likely try to use it ([EAFP](https://docs.python.org/2/glossary.html#term-eafp)). I haven't come across a real life use for the [numbers](https://docs.python.org/2/library/numbers.html#module-numbers) module, which does use ABCs, but that is the closest I have to understanding.

Can anyone explain the rationale to me, please?","@Oddthinking's answer is not wrong, but I think it misses the *real*, *practical* reason Python has ABCs in a world of duck-typing.

Abstract methods are neat, but in my opinion they don't really fill any use-cases not already covered by duck typing. Abstract base classes' real power lies in [the way they allow you to customise the behaviour of `isinstance` and `issubclass`](https://docs.python.org/3/library/abc.html#abc.ABCMeta.__subclasshook__). (`__subclasshook__` is basically a friendlier API on top of Python's [`__instancecheck__` and `__subclasscheck__`](https://docs.python.org/3.3/reference/datamodel.html#customizing-instance-and-subclass-checks) hooks.) Adapting built-in constructs to work on custom types is very much part of Python's philosophy.

Python's source code is exemplary. [Here](https://hg.python.org/cpython/file/8e0dc4d3b49c/Lib/_collections_abc.py#l143) is how `collections.Container` is defined in the standard library (at time of writing):

```
class Container(metaclass=ABCMeta):
    __slots__ = ()

    @abstractmethod
    def __contains__(self, x):
        return False

    @classmethod
    def __subclasshook__(cls, C):
        if cls is Container:
            if any(""__contains__"" in B.__dict__ for B in C.__mro__):
                return True
        return NotImplemented

```

This definition of `__subclasshook__` says that any class with a `__contains__` attribute is considered to be a subclass of Container, even if it doesn't subclass it directly. So I can write this:

```
class ContainAllTheThings(object):
    def __contains__(self, item):
        return True

>>> issubclass(ContainAllTheThings, collections.Container)
True
>>> isinstance(ContainAllTheThings(), collections.Container)
True

```

In other words, *if you implement the right interface, you're a subclass!* ABCs provide a formal way to define interfaces in Python, while staying true to the spirit of duck-typing. Besides, this works in a way that honours the [Open-Closed Principle](https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle).

Python's object model looks superficially similar to that of a more ""traditional"" OO system (by which I mean Java\*) - we got yer classes, yer objects, yer methods - but when you scratch the surface you'll find something far richer and more flexible. Likewise, Python's notion of abstract base classes may be recognisable to a Java developer, but in practice they are intended for a very different purpose.

I sometimes find myself writing polymorphic functions that can act on a single item or a collection of items, and I find `isinstance(x, collections.Iterable)` to be much more readable than `hasattr(x, '__iter__')` or an equivalent `try...except` block. (If you didn't know Python, which of those three would make the intention of the code clearest?)

That said, I find that I rarely need to write my own ABC and I typically discover the need for one through refactoring. If I see a polymorphic function making a lot of attribute checks, or lots of functions making the same attribute checks, that smell suggests the existence of an ABC waiting to be extracted.

\*without getting into the debate over whether Java is a ""traditional"" OO system...

---

**Addendum**: Even though an abstract base class can override the behaviour of `isinstance` and `issubclass`, it still doesn't enter the [MRO](https://www.python.org/download/releases/2.3/mro/) of the virtual subclass. This is a potential pitfall for clients: not every object for which `isinstance(x, MyABC) == True` has the methods defined on `MyABC`.

```
class MyABC(metaclass=abc.ABCMeta):
    def abc_method(self):
        pass
    @classmethod
    def __subclasshook__(cls, C):
        return True

class C(object):
    pass

# typical client code
c = C()
if isinstance(c, MyABC):  # will be true
    c.abc_method()  # raises AttributeError

```

Unfortunately this one of those ""just don't do that"" traps (of which Python has relatively few!): avoid defining ABCs with both a `__subclasshook__` and non-abstract methods. Moreover, you should make your definition of `__subclasshook__` consistent with the set of abstract methods your ABC defines."
"I'm currently trying my hands on the new dataclass constructions introduced in Python 3.7. I am currently stuck on trying to do some inheritance of a parent class. It looks like the order of the arguments are botched by my current approach such that the bool parameter in the child class is passed before the other parameters. This is causing a type error.

```
from dataclasses import dataclass

@dataclass
class Parent:
    name: str
    age: int
    ugly: bool = False

    def print_name(self):
        print(self.name)

    def print_age(self):
        print(self.age)

    def print_id(self):
        print(f'The Name is {self.name} and {self.name} is {self.age} year old')

@dataclass
class Child(Parent):
    school: str
    ugly: bool = True


jack = Parent('jack snr', 32, ugly=True)
jack_son = Child('jack jnr', 12, school = 'havard', ugly=True)

jack.print_id()
jack_son.print_id()

```

When I run this code I get this `TypeError`:

```
TypeError: non-default argument 'school' follows default argument

```

How do I fix this?","The way dataclasses combines attributes prevents you from being able to use attributes with defaults in a base class and then use attributes without a default (positional attributes) in a subclass.

That's because the attributes are combined by starting from the bottom of the MRO, and building up an ordered list of the attributes in first-seen order; overrides are kept in their original location. So `Parent` starts out with `['name', 'age', 'ugly']`, where `ugly` has a default, and then `Child` adds `['school']` to the end of that list (with `ugly` already in the list). This means you end up with `['name', 'age', 'ugly', 'school']` and because `school` doesn't have a default, this results in an invalid argument listing for `__init__`.

This is documented in [PEP-557 *Dataclasses*](https://www.python.org/dev/peps/pep-0557/), under [*inheritance*](https://www.python.org/dev/peps/pep-0557/#inheritance):

> When the Data Class is being created by the `@dataclass` decorator, it looks through all of the class's base classes in reverse MRO (that is, starting at `object`) and, for each Data Class that it finds, adds the fields from that base class to an ordered mapping of fields. After all of the base class fields are added, it adds its own fields to the ordered mapping. All of the generated methods will use this combined, calculated ordered mapping of fields. Because the fields are in insertion order, derived classes override base classes.

and under [*Specification*](https://www.python.org/dev/peps/pep-0557/#id7):

> `TypeError` will be raised if a field without a default value follows a field with a default value. This is true either when this occurs in a single class, or as a result of class inheritance.

You do have a few options here to avoid this issue.

The first option is to use separate base classes to force fields with defaults into a later position in the MRO order. At all cost, avoid setting fields directly on classes that are to be used as base classes, such as `Parent`.

The following class hierarchy works:

```
# base classes with fields; fields without defaults separate from fields with.
@dataclass
class _ParentBase:
    name: str
    age: int
    
@dataclass
class _ParentDefaultsBase:
    ugly: bool = False

@dataclass
class _ChildBase(_ParentBase):
    school: str

@dataclass
class _ChildDefaultsBase(_ParentDefaultsBase):
    ugly: bool = True

# public classes, deriving from base-with, base-without field classes
# subclasses of public classes should put the public base class up front.

@dataclass
class Parent(_ParentDefaultsBase, _ParentBase):
    def print_name(self):
        print(self.name)

    def print_age(self):
        print(self.age)

    def print_id(self):
        print(f""The Name is {self.name} and {self.name} is {self.age} year old"")

@dataclass
class Child(_ChildDefaultsBase, Parent, _ChildBase):
    pass

```

By pulling out fields into *separate* base classes with fields without defaults and fields with defaults, and a carefully selected inheritance order, you can produce an MRO that puts all fields without defaults before those with defaults. The reversed MRO (ignoring `object`) for `Child` is:

```
_ParentBase
_ChildBase
_ParentDefaultsBase
Parent
_ChildDefaultsBase

```

Note that while `Parent` doesn't set any new fields, it does inherit the fields from `_ParentDefaultsBase` and should *not* end up 'last' in the field listing order; the above order puts `_ChildDefaultsBase` last so its fields 'win'. The dataclass rules are also satisfied; the classes with fields without defaults (`_ParentBase` and `_ChildBase`) precede the classes with fields with defaults (`_ParentDefaultsBase` and `_ChildDefaultsBase`).

The result is `Parent` and `Child` classes with a sane field older, while `Child` is still a subclass of `Parent`:

```
>>> from inspect import signature
>>> signature(Parent)
<Signature (name: str, age: int, ugly: bool = False) -> None>
>>> signature(Child)
<Signature (name: str, age: int, school: str, ugly: bool = True) -> None>
>>> issubclass(Child, Parent)
True

```

and so you can create instances of both classes:

```
>>> jack = Parent('jack snr', 32, ugly=True)
>>> jack_son = Child('jack jnr', 12, school='havard', ugly=True)
>>> jack
Parent(name='jack snr', age=32, ugly=True)
>>> jack_son
Child(name='jack jnr', age=12, school='havard', ugly=True)

```

Another option is to only use fields with defaults; you can still make in an error to not supply a `school` value, by raising one in `__post_init__`:

```
_no_default = object()

@dataclass
class Child(Parent):
    school: str = _no_default
    ugly: bool = True

    def __post_init__(self):
        if self.school is _no_default:
            raise TypeError(""__init__ missing 1 required argument: 'school'"")

```

but this *does* alter the field order; `school` ends up after `ugly`:

```
<Signature (name: str, age: int, ugly: bool = True, school: str = <object object at 0x1101d1210>) -> None>

```

and a type hint checker *will* complain about `_no_default` not being a string.

You can also use the [`attrs` project](http://www.attrs.org/en/stable/), which was the project that inspired `dataclasses`. It uses a different inheritance merging strategy; it pulls overridden fields in a subclass to the end of the fields list, so `['name', 'age', 'ugly']` in the `Parent` class becomes `['name', 'age', 'school', 'ugly']` in the `Child` class; by overriding the field with a default, `attrs` allows the override without needing to do a MRO dance.

`attrs` supports defining fields without type hints, but lets stick to the [supported type hinting mode](http://www.attrs.org/en/stable/types.html) by setting `auto_attribs=True`:

```
import attr

@attr.s(auto_attribs=True)
class Parent:
    name: str
    age: int
    ugly: bool = False

    def print_name(self):
        print(self.name)

    def print_age(self):
        print(self.age)

    def print_id(self):
        print(f""The Name is {self.name} and {self.name} is {self.age} year old"")

@attr.s(auto_attribs=True)
class Child(Parent):
    school: str
    ugly: bool = True

```"
"Why do the following code samples:

```
np.array([[1, 2], [2, 3, 4]])

np.array([1.2, ""abc""], dtype=float)

```

all give the following error?

```
ValueError: setting an array element with a sequence.

```","#### Possible reason 1: trying to create a jagged array

You may be creating an array from a list that isn't shaped like a multi-dimensional array:

```
numpy.array([[1, 2], [2, 3, 4]])         # wrong!

```

```
numpy.array([[1, 2], [2, [3, 4]]])       # wrong!

```

In these examples, the argument to `numpy.array` contains sequences of different lengths. Those will yield this error message because the input list is not shaped like a ""box"" that can be turned into a multidimensional array.

#### Possible reason 2: providing elements of incompatible types

For example, providing a string as an element in an array of type `float`:

```
numpy.array([1.2, ""abc""], dtype=float)   # wrong!

```

If you really want to have a NumPy array containing both strings and floats, you could use the dtype `object`, which allows the array to hold arbitrary Python objects:

```
numpy.array([1.2, ""abc""], dtype=object)

```"
"Is there a built-in function for getting the size of a file object in bytes? I see some people do something like this:

```
def getSize(fileobject):
    fileobject.seek(0,2) # move the cursor to the end of the file
    size = fileobject.tell()
    return size

file = open('myfile.bin', 'rb')
print getSize(file)

```

But from my experience with Python, it has a lot of helper functions so I'm guessing maybe there is one built-in.","Use [`os.path.getsize(path)`](https://docs.python.org/library/os.path.html#os.path.getsize) which will

> Return the size, in bytes, of *path*. Raise [`OSError`](https://docs.python.org/library/exceptions.html#OSError) if the file does not exist or is inaccessible.

```
import os
os.path.getsize('C:\\Python27\\Lib\\genericpath.py')

```

Or use [`os.stat(path).st_size`](https://docs.python.org/3/library/os.html#os.stat)

```
import os
os.stat('C:\\Python27\\Lib\\genericpath.py').st_size 

```

Or use [`Path(path).stat().st_size`](https://docs.python.org/library/pathlib.html#pathlib.Path.stat) (Python 3.4+)

```
from pathlib import Path
Path('C:\\Python27\\Lib\\genericpath.py').stat().st_size

```"
"Is it possible? When installing [`pip`](http://pip.readthedocs.org/en/latest/installing.html), install the python packages inside my `$HOME` folder. (for example, I want to install `mercurial`, using `pip`, but inside `$HOME` instead of `/usr/local`)

I'm with a mac machine and just thought about this possibility, instead of ""polluting"" my `/usr/local`, I would use my `$HOME` instead.

[PEP370](http://www.python.org/dev/peps/pep-0370/) is exactly about this. Is just creating a `Ëœ/.local` and do a `pip install package` enough to make these packages to be installed only at my $HOME folder?","While you can use a `virtualenv`, you don't need to. The trick is passing the PEP370 `--user` argument to the `setup.py` script. With the latest version of `pip`, one way to do it is:

```
pip install --user mercurial

```

This should result in the `hg` script being installed in `$HOME/.local/bin/hg` and the rest of the hg package in `$HOME/.local/lib/pythonx.y/site-packages/`.

Note, that the above is true for Python 2.6. There has been a [bit of controversy](http://bugs.python.org/issue7175) among the Python core developers about what is the appropriate directory location on Mac OS X for PEP370-style `user` installations. In Python 2.7 and 3.2, the location on Mac OS X was changed from `$HOME/.local` to `$HOME/Library/Python`. This might change in a future release. But, for now, on 2.7 (and 3.2, if `hg` were supported on Python 3), the above locations will be `$HOME/Library/Python/x.y/bin/hg` and `$HOME/Library/Python/x.y/lib/python/site-packages`."
"Is there a way in python to programmatically determine the width of the console? I mean the number of characters that fits in one line without wrapping, not the pixel width of the window.

**Edit**

Looking for a solution that works on Linux","Not sure why it is in the module `shutil`, but it landed there in Python 3.3. See:

[Querying the size of the output terminal](http://docs.python.org/3/library/shutil.html#querying-the-size-of-the-output-terminal)

```
>>> import shutil
>>> shutil.get_terminal_size((80, 20))  # pass fallback
os.terminal_size(columns=87, lines=23)  # returns a named-tuple

```

A low-level implementation is in the os module. Cross-platformâ€”works under Linux, Mac OS, and Windows, probably other Unix-likes. There's a backport as well, though no longer relevant."
"I want to redirect the print to a .txt file using Python. I have a `for` loop, which will `print` the output for each of my .bam file while I want to redirect **all** output to one file. So I tried to put:

```
f = open('output.txt','w')
sys.stdout = f

```

at the beginning of my script. However I get nothing in the .txt file.
My script is:

```
#!/usr/bin/python

import os,sys
import subprocess
import glob
from os import path

f = open('output.txt','w')
sys.stdout = f

path= '/home/xxx/nearline/bamfiles'
bamfiles = glob.glob(path + '/*.bam')

for bamfile in bamfiles:
    filename = bamfile.split('/')[-1]
    print 'Filename:', filename
    samtoolsin = subprocess.Popen([""/share/bin/samtools/samtools"",""view"",bamfile],
                                  stdout=subprocess.PIPE,bufsize=1)
    linelist= samtoolsin.stdout.readlines()
    print 'Readlines finished!'

```

So what's the problem? Any other way besides this `sys.stdout`?

I need my result look like:

```
Filename: ERR001268.bam
Readlines finished!
Mean: 233
SD: 10
Interval is: (213, 252)

```","The most obvious way to do this would be to print to a file object:

```
with open('out.txt', 'w') as f:
    print('Filename:', filename, file=f)  #Â Python 3.x
    # print >> f, 'Filename:', filename   # Python 2.x

```

However, redirecting stdout also works for me. It is probably fine for a one-off script such as this:

```
import sys

orig_stdout = sys.stdout
f = open('out.txt', 'w')
sys.stdout = f

for i in range(2):
    print('i = ', i)

sys.stdout = orig_stdout
f.close()

```

Since Python 3.4 there's a simple context manager available to do this [in the standard library](https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stdout):

```
from contextlib import redirect_stdout

with open('out.txt', 'w') as f:
    with redirect_stdout(f):
        print('data')

```

Redirecting externally from the shell itself is another option, and often preferable:

```
./script.py > out.txt

```

Other questions:

What is the first filename in your script? I don't see it initialized.

My first guess is that glob doesn't find any bamfiles, and therefore the for loop doesn't run. Check that the folder exists, and print out bamfiles in your script.

Also, use [os.path.join and os.path.basename](http://docs.python.org/library/os.path.html) to manipulate paths and filenames."
"Suppose I have two DataFrames like so:

```
left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})

right = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})

```

I want to merge them, so I try something like this:

```
pd.merge(left, right, left_on='key1', right_on='key2')

```

And I'm happy

```
    key1    lval    key2    rval
0   foo     1       foo     4
1   bar     2       bar     5

```

But I'm trying to use the join method, which I've been lead to believe is pretty similar.

```
left.join(right, on=['key1', 'key2'])

```

And I get this:

```
//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)
    406             if self.right_index:
    407                 if not ((len(self.left_on) == self.right.index.nlevels)):
--> 408                     raise AssertionError()
    409                 self.right_on = [None] * n
    410         elif self.right_on is not None:

AssertionError: 

```

What am I missing?","`pandas.merge()` is the underlying function used for all merge/join behavior.

DataFrames provide the `pandas.DataFrame.merge()` and `pandas.DataFrame.join()` methods as a convenient way to access the capabilities of `pandas.merge()`. For example, `df1.merge(right=df2, ...)` is equivalent to `pandas.merge(left=df1, right=df2, ...)`.

These are the main differences between `df.join()` and `df.merge()`:

1. lookup on right table: `df1.join(df2)` always joins via the index of `df2`, but `df1.merge(df2)` can join to one or more columns of `df2` (default) or to the index of `df2` (with `right_index=True`).
2. lookup on left table: by default, `df1.join(df2)` uses the index of `df1` and `df1.merge(df2)` uses column(s) of `df1`. That can be overridden by specifying `df1.join(df2, on=key_or_keys)` or `df1.merge(df2, left_index=True)`.
3. left vs inner join: `df1.join(df2)` does a left join by default (keeps all rows of `df1`), but `df.merge` does an inner join by default (returns only matching rows of `df1` and `df2`).

So, the generic approach is to use `pandas.merge(df1, df2)` or `df1.merge(df2)`. But for a number of common situations (keeping all rows of `df1` and joining to an index in `df2`), you can save some typing by using `df1.join(df2)` instead.

Some notes on these issues from the documentation at <http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging>:

> `merge` is a function in the pandas namespace, and it is also
> available as a DataFrame instance method, with the calling DataFrame
> being implicitly considered the left object in the join.
>
> The related `DataFrame.join` method, uses `merge` internally for the
> index-on-index and index-on-column(s) joins, but joins on indexes by
> default rather than trying to join on common columns (the default
> behavior for `merge`). If you are joining on index, you may wish to
> use `DataFrame.join` to save yourself some typing.

...

> These two function calls are completely equivalent:
>
> ```
> left.join(right, on=key_or_keys)
> pd.merge(left, right, left_on=key_or_keys, right_index=True, how='left', sort=False)
>
> ```"
"In Python, I can do:

```
list = ['a', 'b', 'c']
', '.join(list)    # 'a, b, c'

```

However, if I have a list of objects and try to do the same thing:

```
class Obj:
    def __str__(self):
        return 'name'

list = [Obj(), Obj(), Obj()]
', '.join(list)

```

I get the following error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: sequence item 0: expected string, instance found

```

Is there any easy way? Or do I have to resort to a for-loop?","You could use a list comprehension or a generator expression instead:

```
', '.join([str(x) for x in list])  # list comprehension
', '.join(str(x) for x in list)    # generator expression

```"
I am having trouble with some of pandas functionalities. How do I check what is my installation version?,"Check `pandas.__version__`:

```
In [76]: import pandas as pd

In [77]: pd.__version__
Out[77]: '0.12.0-933-g281dc4e'

```

Pandas also provides a utility function, `pd.show_versions()`, which reports the version of its dependencies as well:

```
In [53]: pd.show_versions(as_json=False)

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-45-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.15.2-113-g5531341
nose: 1.3.1
Cython: 0.21.1
numpy: 1.8.2
scipy: 0.14.0.dev-371b4ff
statsmodels: 0.6.0.dev-a738b4f
IPython: 2.0.0-dev
sphinx: 1.2.2
patsy: 0.3.0
dateutil: 1.5
pytz: 2012c
bottleneck: None
tables: 3.1.1
numexpr: 2.2.2
matplotlib: 1.4.2
openpyxl: None
xlrd: 0.9.3
xlwt: 0.7.5
xlsxwriter: None
lxml: 3.3.3
bs4: 4.3.2
html5lib: 0.999
httplib2: 0.8
apiclient: None
rpy2: 2.5.5
sqlalchemy: 0.9.8
pymysql: None
psycopg2: 2.4.5 (dt dec mx pq3 ext)

```"
"I would like to have a optional argument that will default to a value if only the flag is present with no value specified, but store a user-specified value instead of the default if the user specifies a value. Is there already an action available for this?

An example:

```
python script.py --example
# args.example would equal a default value of 1
python script.py --example 2
# args.example would equal a default value of 2

```

I can create an action, but wanted to see if there was an existing way to do this.","```
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--example', nargs='?', const=1, type=int)
args = parser.parse_args()
print(args)

```

---

```
% test.py 
Namespace(example=None)
% test.py --example
Namespace(example=1)
% test.py --example 2
Namespace(example=2)

```

---

* `nargs='?'` means 0-or-1 arguments
* `const=1` sets the default when there are 0 arguments
* `type=int` converts the argument to int

---

If you want `test.py` to set `example` to 1 even if no `--example` is specified, then include `default=1`. That is, with

```
parser.add_argument('--example', nargs='?', const=1, type=int, default=1)

```

then

```
% test.py 
Namespace(example=1)

```"
"I noticed that the Python 2.7 documentation includes yet another command-line parsing module. In addition to `getopt` and `optparse` we now have `argparse`.

Why has yet another command-line parsing module been created? Why should I use it instead of `optparse`? Are there new features that I should know about?","As of python `2.7`, [`optparse`](https://docs.python.org/2/library/optparse.html) is deprecated, and will hopefully go away in the future.

[`argparse`](https://docs.python.org/3/library/argparse.html) is better for all the reasons listed on its original page (<https://code.google.com/archive/p/argparse/>):

* handling positional arguments
* supporting sub-commands
* allowing alternative option prefixes like `+` and `/`
* handling zero-or-more and one-or-more style arguments
* producing more informative usage messages
* providing a much simpler interface for custom types and actions

More information is also in [PEP 389](http://www.python.org/dev/peps/pep-0389/), which is the vehicle by which `argparse` made it into the standard library."
"Right now I have a central module in a framework that spawns multiple processes using the Python 2.6 [`multiprocessing` module](http://docs.python.org/library/multiprocessing.html?#module-multiprocessing). Because it uses `multiprocessing`, there is module-level multiprocessing-aware log, `LOG = multiprocessing.get_logger()`. Per [the docs](http://docs.python.org/library/multiprocessing.html#logging), this logger (**EDIT**) does **not** have process-shared locks so that you don't garble things up in `sys.stderr` (or whatever filehandle) by having multiple processes writing to it simultaneously.

The issue I have now is that the other modules in the framework are not multiprocessing-aware. The way I see it, I need to make all dependencies on this central module use multiprocessing-aware logging. That's annoying *within* the framework, let alone for all clients of the framework. Are there alternatives I'm not thinking of?","I just now wrote a log handler of my own that just feeds everything to the parent process via a pipe. I've only been testing it for ten minutes but it seems to work pretty well.

(**Note:** This is hardcoded to `RotatingFileHandler`, which is my own use case.)

---

Update: @javier now maintains this approach as a package available on Pypi - see [multiprocessing-logging](https://pypi.python.org/pypi/multiprocessing-logging/) on Pypi, github at <https://github.com/jruere/multiprocessing-logging>
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

---

Update: Implementation!
-----------------------

This now uses a queue for correct handling of concurrency, and also recovers from errors correctly. I've now been using this in production for several months, and the current version below works without issue.

```
from logging.handlers import RotatingFileHandler
import multiprocessing, threading, logging, sys, traceback

class MultiProcessingLog(logging.Handler):
    def __init__(self, name, mode, maxsize, rotate):
        logging.Handler.__init__(self)

        self._handler = RotatingFileHandler(name, mode, maxsize, rotate)
        self.queue = multiprocessing.Queue(-1)

        t = threading.Thread(target=self.receive)
        t.daemon = True
        t.start()

    def setFormatter(self, fmt):
        logging.Handler.setFormatter(self, fmt)
        self._handler.setFormatter(fmt)

    def receive(self):
        while True:
            try:
                record = self.queue.get()
                self._handler.emit(record)
            except (KeyboardInterrupt, SystemExit):
                raise
            except EOFError:
                break
            except:
                traceback.print_exc(file=sys.stderr)

    def send(self, s):
        self.queue.put_nowait(s)

    def _format_record(self, record):
        # ensure that exc_info and args
        # have been stringified.  Removes any chance of
        # unpickleable things inside and possibly reduces
        # message size sent over the pipe
        if record.args:
            record.msg = record.msg % record.args
            record.args = None
        if record.exc_info:
            dummy = self.format(record)
            record.exc_info = None

        return record

    def emit(self, record):
        try:
            s = self._format_record(record)
            self.send(s)
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            self.handleError(record)

    def close(self):
        self._handler.close()
        logging.Handler.close(self)

```"
"I've been trying to figure out a good way to load JSON objects in Python.
I send this json data:

```
{'http://example.org/about': {'http://purl.org/dc/terms/title': [{'type': 'literal', 'value': ""Anna's Homepage""}]}}

```

to the backend where it will be received as a string then I used `json.loads(data)` to parse it.

But each time I got the same exception :

```
ValueError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)

```

I googled it but nothing seems to work besides this solution `json.loads(json.dumps(data))` which personally seems for me not that efficient since it accept any kind of data even the ones that are not in json format.

Any suggestions will be much appreciated.","This is not JSON:

```
{
    'http://example.org/about': {
        'http://purl.org/dc/terms/title': [
            {'type': 'literal', 'value': ""Anna's Homepage""}
        ]
    }
}

```

This is JSON:

```
{
    ""http://example.org/about"": {
        ""http://purl.org/dc/terms/title"": [
            {""type"": ""literal"", ""value"": ""Anna's Homepage""}
        ]
    }
}

```

[JSON specification - RFC7159](https://www.rfc-editor.org/rfc/rfc7159#section-7) states that a string begins and ends with quotation mark. That is **`""`**.  
Single quote **`'`** has no semantic meaning in JSON and is allowed only inside a string."
"In Python, the only way I can find to concatenate two lists is `list.extend`, which modifies the first list. Is there any concatenation function that returns its result without modifying its arguments?",Yes: `list1 + list2`. This gives a new list that is the concatenation of `list1` and `list2`.
"Here is a functional code (create file with success)

```
sys.stdout = open('filename1.xml', 'w')

```

Now I'm trying to name the file with the current date/time (I'm not an expert in Python)

```
filename1 = datetime.now().strftime(""%Y%m%d-%H%M%S"")
sys.stdout = open(filename1 + '.xml', 'w')

```

I want to write out a file name with the exact date and time, it is a xml file, that the program has already create, I just need to name the file. The above code is not working.

The error returned:

```
  File ""./fix.py"", line 226, in <module>
    filenames = datetime.now().strftime(""%Y%m%d-%H%M%S"")
AttributeError: 'module' object has no attribute 'now'

```","While not using `datetime`, this solves your problem (answers your question) of getting a string with the current time and date format you specify:

```
import time
timestr = time.strftime(""%Y%m%d-%H%M%S"")
print timestr

```

yields:

```
20120515-155045

```

so your filename could append or use this string."
"[`asyncio.gather`](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [`asyncio.wait`](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait) seem to have similar uses: I have a bunch of async things that I want to execute/wait for (not necessarily waiting for one to finish before the next one starts).

Since Python 3.11 there is yet another similar feature, [`asyncio.TaskGroup`](https://docs.python.org/3/library/asyncio-task.html#asyncio.TaskGroup).

They use a different syntax, and differ in some details, but it seems very un-pythonic to me to have several functions that have such a huge overlap in functionality.

What am I missing?","Although similar in general cases (""run and get results for many tasks""), each function has some specific functionality for other cases (and see also `TaskGroup` for Python 3.11+ below):

[`asyncio.gather()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather)
========================================================================================

Returns a Future instance, allowing high level grouping of tasks:

```
import asyncio
from pprint import pprint

import random


async def coro(tag):
    print("">"", tag)
    await asyncio.sleep(random.uniform(1, 3))
    print(""<"", tag)
    return tag


loop = asyncio.get_event_loop()

group1 = asyncio.gather(*[coro(""group 1.{}"".format(i)) for i in range(1, 6)])
group2 = asyncio.gather(*[coro(""group 2.{}"".format(i)) for i in range(1, 4)])
group3 = asyncio.gather(*[coro(""group 3.{}"".format(i)) for i in range(1, 10)])

all_groups = asyncio.gather(group1, group2, group3)

results = loop.run_until_complete(all_groups)

loop.close()

pprint(results)

```

All tasks in a group can be cancelled by calling `group2.cancel()` or even `all_groups.cancel()`. See also `.gather(..., return_exceptions=True)`,

[`asyncio.wait()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait)
====================================================================================

Supports waiting to be stopped after the first task is done, or after a specified timeout, allowing lower level precision of operations:

```
import asyncio
import random


async def coro(tag):
    print("">"", tag)
    await asyncio.sleep(random.uniform(0.5, 5))
    print(""<"", tag)
    return tag


loop = asyncio.get_event_loop()

tasks = [coro(i) for i in range(1, 11)]

print(""Get first result:"")
finished, unfinished = loop.run_until_complete(
    asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED))

for task in finished:
    print(task.result())
print(""unfinished:"", len(unfinished))

print(""Get more results in 2 seconds:"")
finished2, unfinished2 = loop.run_until_complete(
    asyncio.wait(unfinished, timeout=2))

for task in finished2:
    print(task.result())
print(""unfinished2:"", len(unfinished2))

print(""Get all other results:"")
finished3, unfinished3 = loop.run_until_complete(asyncio.wait(unfinished2))

for task in finished3:
    print(task.result())

loop.close()

```

---

[`TaskGroup`](https://docs.python.org/3/library/asyncio-task.html#task-groups) (Python 3.11+)
=============================================================================================

**Update**: Python 3.11 introduces [`TaskGroup`s](https://docs.python.org/3/library/asyncio-task.html#task-groups) which can ""automatically"" await more than one task without `gather()` or `await()`:

```
# Python 3.11+ ONLY!
async def main():
    async with asyncio.TaskGroup() as tg:
        task1 = tg.create_task(some_coro(...))
        task2 = tg.create_task(another_coro(...))
    print(""Both tasks have completed now."")

```"
"I have read the official definition of ""raise"", but I still don't quite understand what it does.

In simplest terms, what is ""raise""?

Example usage would help.","It has two purposes.

[jackcogdill has given the first one:](https://stackoverflow.com/a/13957849/20862)

> It's used for raising your own errors.
>
> ```
> if something:
>    raise Exception('My error!')
>
> ```

The second is to reraise the *current* exception in an exception handler, so that it can be handled further up the call stack.

```
try:
  generate_exception()
except SomeException as e:
  if not can_handle(e):
    raise
  handle_exception(e)

```"
"I need to replace all non-ASCII (\x00-\x7F) characters with a space. I'm surprised that this is not dead-easy in Python, unless I'm missing something. The following function simply removes all non-ASCII characters:

```
def remove_non_ascii_1(text):

    return ''.join(i for i in text if ord(i)<128)

```

And this one replaces non-ASCII characters with the amount of spaces as per the amount of bytes in the character code point (i.e. the `â€“` character is replaced with 3 spaces):

```
def remove_non_ascii_2(text):

    return re.sub(r'[^\x00-\x7F]',' ', text)

```

**How can I replace all non-ASCII characters with a single space?**

[Of](https://stackoverflow.com/questions/1342000/how-to-replace-non-ascii-characters-in-string) [the](https://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii) [myriad](https://stackoverflow.com/questions/6609895/efficiently-replace-bad-characters) [of](https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python) [similar](https://stackoverflow.com/questions/15737048/handle-non-ascii-code-string-in-python) [SO](https://stackoverflow.com/questions/8689795/python-remove-non-ascii-characters-but-leave-periods-and-spaces) [questions](https://stackoverflow.com/questions/2921815/help-replacing-non-ascii-character-in-python), [none](https://stackoverflow.com/questions/17273575/python-replace-non-ascii-characters-in-a-list-of-strings) [address](https://stackoverflow.com/questions/16866261/detecting-non-ascii-characters-in-unicode-string) [character](https://stackoverflow.com/questions/3667875/removing-non-ascii-characters-from-any-given-stringtype-in-python) [replacement](https://stackoverflow.com/questions/19000968/what-is-the-correct-way-to-use-unicode-characters-in-a-python-regex) [as](https://stackoverflow.com/questions/10993612/python-removing-xa0-from-string) [opposed](https://stackoverflow.com/questions/3586903/sqlite-remove-non-utf-8-characters) [to](https://stackoverflow.com/questions/15321138/removing-unicode-u2026-like-characters-in-a-string-in-python2-7) [stripping](https://stackoverflow.com/questions/18522127/removing-non-ascii-characters-in-a-csv-file), [and](https://stackoverflow.com/questions/3870084/how-to-decode-a-non-unicode-character-in-python)  additionally address all non-ascii characters not a specific character.","Your `''.join()` expression is *filtering*, removing anything non-ASCII; you could use a conditional expression instead:

```
return ''.join([i if ord(i) < 128 else ' ' for i in text])

```

This handles characters one by one and would still use one space per character replaced.

Your regular expression should just replace *consecutive* non-ASCII characters with a space:

```
re.sub(r'[^\x00-\x7F]+',' ', text)

```

Note the `+` there."
"I am using below referred code to edit a csv using Python. Functions called in the code form upper part of the code.

Problem: I want the below referred code to start editing the csv from 2nd row, I want it to exclude 1st row which contains headers. Right now it is applying the functions on 1st row only and my header row is getting changed.

```
in_file = open(""tmob_notcleaned.csv"", ""rb"")
reader = csv.reader(in_file)
out_file = open(""tmob_cleaned.csv"", ""wb"")
writer = csv.writer(out_file)
row = 1
for row in reader:
    row[13] = handle_color(row[10])[1].replace("" - "","""").strip()
    row[10] = handle_color(row[10])[0].replace(""-"","""").replace(""("","""").replace("")"","""").strip()
    row[14] = handle_gb(row[10])[1].replace(""-"","""").replace("" "","""").replace(""GB"","""").strip()
    row[10] = handle_gb(row[10])[0].strip()
    row[9] = handle_oem(row[10])[1].replace(""Blackberry"",""RIM"").replace(""TMobile"",""T-Mobile"").strip()
    row[15] = handle_addon(row[10])[1].strip()
    row[10] = handle_addon(row[10])[0].replace("" by"","""").replace(""FREE"","""").strip()
    writer.writerow(row)
in_file.close()    
out_file.close()

```

I tried to solve this problem by initializing `row` variable to `1` but it didn't work.

Please help me in solving this issue.","Your `reader` variable is an iterable, by looping over it you retrieve the rows.

To make it skip one item before your loop, simply call [`next(reader, None)`](http://docs.python.org/3/library/functions.html#next) and ignore the return value.

You can also simplify your code a little; use the opened files as context managers to have them closed automatically:

```
with open(""tmob_notcleaned.csv"", ""rb"") as infile, open(""tmob_cleaned.csv"", ""wb"") as outfile:
   reader = csv.reader(infile)
   next(reader, None)  # skip the headers
   writer = csv.writer(outfile)
   for row in reader:
       # process each row
       writer.writerow(row)

# no need to close, the files are closed automatically when you get to this point.

```

If you wanted to write the header to the output file unprocessed, that's easy too, pass the output of `next()` to `writer.writerow()`:

```
headers = next(reader, None)  # returns the headers or `None` if the input is empty
if headers:
    writer.writerow(headers)

```"
"I want to group my dataframe by two columns and then sort the aggregated results within those groups.

```
In [167]: df

Out[167]:
   count     job source
0      2   sales      A
1      4   sales      B
2      6   sales      C
3      3   sales      D
4      7   sales      E
5      5  market      A
6      3  market      B
7      2  market      C
8      4  market      D
9      1  market      E


In [168]: df.groupby(['job','source']).agg({'count':sum})

Out[168]:
               count
job    source       
market A           5
       B           3
       C           2
       D           4
       E           1
sales  A           2
       B           4
       C           6
       D           3
       E           7

```

I would now like to sort the 'count' column in descending order within each of the groups, and then take only the top three rows. To get something like:

```
                count
job     source
market  A           5
        D           4
        B           3
sales   E           7
        C           6
        B           4

```","You could also just do it in one go, by doing the sort first and using head to take the first 3 of each group.

```
In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)

Out[35]: 
   count     job source
4      7   sales      E
2      6   sales      C
1      4   sales      B
5      5  market      A
8      4  market      D
6      3  market      B

```"
"I have always thought that functional programming can be done in Python. Thus, I was surprised that Python didn't get much of a mention in [this](https://stackoverflow.com/questions/1015606/mixing-object-oriented-and-functional-programming) question, and when it was mentioned, it normally wasn't very positive. However, not many reasons were given for this (lack of pattern matching and algebraic data types were mentioned). So my question is: why isn't Python very good for functional programming? Are there more reasons than its lack of pattern matching and algebraic data types? Or are these concepts so important to functional programming that a language that doesn't support them can only be classed as a second rate functional programming language? (Keep in mind that my experience with functional programming is quite limited.)","The question you reference asks which languages promote both OO and functional programming. Python does not *promote* functional programming even though it *works* fairly well.

The best argument *against* functional programming in Python is that imperative/OO use cases are carefully considered by Guido, while functional programming use cases are not. When I write imperative Python, it's one of the prettiest languages I know. When I write functional Python, it becomes as ugly and unpleasant as your average language that doesn't have a [BDFL](http://en.wikipedia.org/wiki/Benevolent_Dictator_for_Life).

Which is not to say that it's bad, just that you have to work harder than you would if you switched to a language that promotes functional programming or switched to writing OO Python.

Here are the functional things I miss in Python:

* [Pattern matching](http://learnyouahaskell.com/syntax-in-functions#pattern-matching)
* [Tail recursion](http://book.realworldhaskell.org/read/functional-programming.html#fp.loop)
* [Large library of list functions](https://hackage.haskell.org/package/base/docs/Data-List.html)
* [Functional dictionary class](https://hackage.haskell.org/package/containers/docs/Data-Map.html)
* [Automatic currying](http://learnyouahaskell.com/higher-order-functions#curried-functions)
* [Concise way to compose functions](http://learnyouahaskell.com/higher-order-functions#composition)
* Lazy lists
* Simple, powerful expression syntax (Python's simple block syntax prevents Guido from adding it)

---

* No pattern matching and no tail recursion mean your basic algorithms have to be written imperatively. Recursion is ugly and slow in Python.
* A small list library and no functional dictionaries mean that you have to write a lot of stuff yourself.
* No syntax for currying or composition means that point-free style is about as full of punctuation as explicitly passing arguments.
* Iterators instead of lazy lists means that you have to know whether you want efficiency or persistence, and to scatter calls to `list` around if you want persistence. (Iterators are use-once)
* Python's simple imperative syntax, along with its simple LL1 parser, mean that a better syntax for if-expressions and lambda-expressions is basically impossible. Guido likes it this way, and I think he's right."
"Is there any way to force install a pip python package ignoring all its dependencies that cannot be satisfied?

(I don't care how ""wrong"" it is to do so, I just need to do it, any logic and reasoning aside...)","pip has a `--no-dependencies` switch. You should use that.

For more information, run `pip install -h`, where you'll see this line:

```
--no-deps, --no-dependencies
                        Ignore package dependencies

```"
"Example:

```
>>> convert('CamelCase')
'camel_case'

```","Camel case to snake case
------------------------

```
import re

name = 'CamelCaseName'
name = re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower()
print(name)  # camel_case_name

```

If you do this many times and the above is slow, compile the regex beforehand:

```
pattern = re.compile(r'(?<!^)(?=[A-Z])')
name = pattern.sub('_', name).lower()

```

Note that this and immediately following regex use a zero-width match, which is not handled correctly by Python 3.6 or earlier. See further below for alternatives that don't use lookahead/lookbehind if you need to support older EOL Python.

If you want to avoid converting `""HTTPHeader""` into `""h_t_t_p_header""`, you can use this variant with regex alternation:

```
pattern = re.compile(r""(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])"")
name = pattern.sub('_', name).lower()

```

See [Regex101.com](https://regex101.com/r/XkJuZY/3) for test cases (that don't include final lowercase).

You can improve readability with `?x` or `re.X`:

```
pattern = re.compile(
    r""""""
        (?<=[a-z])      # preceded by lowercase
        (?=[A-Z])       # followed by uppercase
        |               #   OR
        (?<[A-Z])       # preceded by lowercase
        (?=[A-Z][a-z])  # followed by uppercase, then lowercase
    """""",
    re.X,
)

```

If you use the [`regex`](https://pypi.python.org/pypi/regex) module instead of `re`, you can use the more readable POSIX character classes (which are not limited to ASCII).

```
pattern = re.compile(
    r""""""
        (?<=[[:lower:]])            # preceded by lowercase
        (?=[[:upper:]])             # followed by uppercase
        |                           #   OR
        (?<[[:upper:]])             # preceded by lower
        (?=[[:upper:]][[:lower:]])  # followed by upper then lower
    """""",
    re.X,
)

```

Another way to handle more advanced cases without relying on lookahead/lookbehind, using two substitution passes:

```
def camel_to_snake(name):
    name = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', name).lower()

print(camel_to_snake('camel2_camel2_case'))  # camel2_camel2_case
print(camel_to_snake('getHTTPResponseCode'))  # get_http_response_code
print(camel_to_snake('HTTPResponseCodeXYZ'))  # http_response_code_xyz

```

To add also cases with two underscores or more:

```
def to_snake_case(name):
    name = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    name = re.sub('__([A-Z])', r'_\1', name)
    name = re.sub('([a-z0-9])([A-Z])', r'\1_\2', name)
    return name.lower()

```

Snake case to pascal case
-------------------------

```
name = 'snake_case_name'
name = ''.join(word.title() for word in name.split('_'))
print(name)  # SnakeCaseName

```"
"Is it possible to append to an empty data frame that doesn't contain any indices or columns?

I have tried to do this, but keep getting an empty dataframe at the end.

e.g.

```
import pandas as pd

df = pd.DataFrame()
data = ['some kind of data here' --> I have checked the type already, and it is a dataframe]
df.append(data)

```

The result looks like this:

```
Empty DataFrame
Columns: []
Index: []

```","This should work:

```
>>> df = pd.DataFrame()
>>> data = pd.DataFrame({""A"": range(3)})
>>> df = df.append(data) 
>>> df

   A
0  0
1  1
2  2

```

Since **the `append` doesn't happen in-place**, so you'll have to store the output if you want it:

```
>>> df = pd.DataFrame()
>>> data = pd.DataFrame({""A"": range(3)})
>>> df.append(data)  # without storing
>>> df
Empty DataFrame
Columns: []
Index: []
>>> df = df.append(data)
>>> df
   A
0  0
1  1
2  2

```"
"Given a list `[""foo"", ""bar"", ""baz""]` and an item in the list `""bar""`, how do I get its index `1`?","```
>>> [""foo"", ""bar"", ""baz""].index(""bar"")
1

```

See [the documentation](https://docs.python.org/tutorial/datastructures.html#more-on-lists) for the built-in `.index()` method of the list:

> ```
> list.index(x[, start[, end]])
>
> ```
>
> Return zero-based index in the list of the first item whose value is equal to *x*. Raises a [`ValueError`](https://docs.python.org/library/exceptions.html#ValueError) if there is no such item.
>
> The optional arguments *start* and *end* are interpreted as in the [slice notation](https://docs.python.org/tutorial/introduction.html#lists) and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.

Caveats
-------

### Linear time-complexity in list length

An `index` call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.

This problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the `start` and `end` parameters can be used to narrow the search.

For example:

```
>>> import timeit
>>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000)
9.356267921015387
>>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000)
0.0004404920036904514

```

The second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.

### Only the index of the *first match* is returned

A call to `index` searches through the list in order until it finds a match, and *stops there.* If there could be more than one occurrence of the value, and all indices are needed, `index` cannot solve the problem:

```
>>> [1, 1].index(1) # the `1` index is not found.
0

```

Instead, use a [list comprehension or generator expression to do the search](/questions/34835951/), with [`enumerate` to get indices](/questions/522563/):

```
>>> # A list comprehension gives a list of indices directly:
>>> [i for i, e in enumerate([1, 2, 1]) if e == 1]
[0, 2]
>>> # A generator comprehension gives us an iterable object...
>>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1)
>>> # which can be used in a `for` loop, or manually iterated with `next`:
>>> next(g)
0
>>> next(g)
2

```

The list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.

### Raises an exception if there is no match

As noted in the documentation above, using `.index` will raise an exception if the searched-for value is not in the list:

```
>>> [1, 1].index(2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: 2 is not in list

```

If this is a concern, either [explicitly check first](https://stackoverflow.com/questions/12934190) using `item in my_list`, or handle the exception with `try`/`except` as appropriate.

The explicit check is simple and readable, but it must iterate the list a second time. See [What is the EAFP principle in Python?](https://stackoverflow.com/questions/11360858) for more guidance on this choice."
"Is there a way to conveniently define a C-like structure in Python? I'm tired of writing stuff like:

```
class MyStruct():
    def __init__(self, field1, field2, field3):
        self.field1 = field1
        self.field2 = field2
        self.field3 = field3

```","*Update*: Data Classes
----------------------

With the introduction of [*Data Classes*](https://docs.python.org/3/library/dataclasses.html) in **Python 3.7** we get very close.

The following example is similar to the *NamedTuple* example below, but the resulting object is **mutable** and it allows for default values.

```
from dataclasses import dataclass


@dataclass
class Point:
    x: float
    y: float
    z: float = 0.0


p = Point(1.5, 2.5)

print(p)  # Point(x=1.5, y=2.5, z=0.0)

```

This plays nicely with the new [typing](https://docs.python.org/3/library/typing.html) module in case you want to use more specific type annotations.

I've been waiting desperately for this! If you ask me, *Data Classes* and the new *NamedTuple* declaration, combined with the *typing* module are a godsend!

Improved NamedTuple declaration
-------------------------------

Since **Python 3.6** it became quite simple and beautiful (IMHO), as long as you can live with **immutability**.

A [new way of declaring NamedTuples](https://docs.python.org/3/library/typing.html#typing.NamedTuple) was introduced, which allows for [type annotations](https://docs.python.org/3/library/typing.html) as well:

```
from typing import NamedTuple


class User(NamedTuple):
    name: str


class MyStruct(NamedTuple):
    foo: str
    bar: int
    baz: list
    qux: User


my_item = MyStruct('foo', 0, ['baz'], User('peter'))

print(my_item) # MyStruct(foo='foo', bar=0, baz=['baz'], qux=User(name='peter'))

```"
"I'm trying to use `get_or_create()` for some fields in my forms, but I'm getting a 500 error when I try to do so.

One of the lines looks like this:

```
customer.source = Source.objects.get_or_create(name=""Website"")

```

The error I get for the above code is:

> Cannot assign ""(<Source: Website>, False)"": ""Customer.source"" must be a ""Source"" instance.","From the documentation [get\_or\_create](https://docs.djangoproject.com/en/stable/ref/models/querysets/#get-or-create):

```
# get_or_create() a person with similar first names.

p, created = Person.objects.get_or_create(
    first_name='John',
    last_name='Lennon',
    defaults={'birthday': date(1940, 10, 9)},
)

# get_or_create() didn't have to create an object.
>>> created
False

```

**Explanation:**
Fields to be evaluated for similarity, have to be mentioned outside `defaults`. Rest of the fields have to be included in `defaults`. In case CREATE event occurs, all the fields are taken into consideration.

It looks like you need to be returning into a tuple, instead of a single variable, do like this:

```
customer.source,created = Source.objects.get_or_create(name=""Website"")

```"
"Why in this millennium should Python [PEP-8](http://www.python.org/dev/peps/pep-0008/) specify a [maximum line length](https://www.python.org/dev/peps/pep-0008/#maximum-line-length) of 79 characters?

Pretty much every code editor under the sun can handle longer lines. What to do with wrapping should be the choice of the content consumer, not the responsibility of the content creator.

Are there any (legitimately) good reasons for adhering to 79 characters in this age?","Much of the value of PEP-8 is to stop people arguing about inconsequential formatting rules, and get on with writing good, consistently formatted code. Sure, no one really thinks that 79 is optimal, but there's no obvious gain in changing it to 99 or 119 or whatever your preferred line length is. I think the choices are these: follow the rule and find a worthwhile cause to battle for, or provide some data that demonstrates how readability and productivity vary with line length. The latter would be extremely interesting, and would have a good chance of changing people's minds I think."
"I am starting to depend heavily on the IPython notebook app to develop and document algorithms. It is awesome; but there is something that seems like it should be possible, but I can't figure out how to do it:

I would like to insert a local image into my (local) IPython notebook markdown to aid in documenting an algorithm. I know enough to add something like `<img src=""image.png"">` to the markdown, but that is about as far as my knowledge goes. I assume I could put the image in the directory represented by 127.0.0.1:8888 (or some subdirectory) to be able to access it, but I can't figure out where that directory is. (I'm working on a mac.) So, is it possible to do what I'm trying to do without too much trouble?","Most of the answers given so far go in the wrong direction, suggesting to load additional libraries and use the code instead of markup. In Ipython/Jupyter Notebooks it is very simple. Make sure the cell is indeed in markup and to display a image use:

```
![alt text](imagename.png ""Title"")

```

Further advantage compared to the other methods proposed is that you can display all common file formats including jpg, png, and gif (animations)."
"How do I find out if a key in a dictionary has already been set to a non-None value?

I want to increment the value if there's already one there, or set it to 1 otherwise:

```
my_dict = {}

if my_dict[key] is not None:
  my_dict[key] = 1
else:
  my_dict[key] += 1

```","You are looking for [`collections.defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict) (available for Python 2.5+). This

```
from collections import defaultdict

my_dict = defaultdict(int)
my_dict[key] += 1

```

will do what you want.

For regular Python `dict`s, if there is no value for a given key, you will *not* get `None` when accessing the dict -- a `KeyError` will be raised. So if you want to use a regular `dict`, instead of your code you would use

```
if key in my_dict:
    my_dict[key] += 1
else:
    my_dict[key] = 1

```"
"I've never had to convert time to and from UTC. Recently had a request to have my app be timezone aware, and I've been running myself in circles. Lots of information on converting local time to UTC, which I found fairly elementary (maybe I'm doing that wrong as well), but I can not find any information on easily converting the UTC time to the end-users timezone.

In a nutshell, and android app sends me (appengine app) data and within that data is a timestamp. To store that timestamp to utc time I am using:

```
datetime.utcfromtimestamp(timestamp)

```

That seems to be working. When my app stores the data, it is being store as 5 hours ahead (I am EST -5)

The data is being stored on appengine's BigTable, and when retrieved it comes out as a string like so:

```
""2011-01-21 02:37:21""

```

How do I convert this string to a DateTime in the users correct time zone?

Also, what is the recommended storage for a users timezone information? (How do you typically store tz info ie: ""-5:00"" or ""EST"" etc etc ?) I'm sure the answer to my first question might contain a parameter the answers the second.","If you don't want to provide your own `tzinfo` objects, check out the [python-dateutil](http://niemeyer.net/python-dateutil) library. It provides `tzinfo` implementations on top of a [zoneinfo (Olson) database](http://en.wikipedia.org/wiki/Tz_database) such that you can refer to time zone rules by a somewhat canonical name.

```
from datetime import datetime
from dateutil import tz

# METHOD 1: Hardcode zones:
from_zone = tz.gettz('UTC')
to_zone = tz.gettz('America/New_York')

# METHOD 2: Auto-detect zones:
from_zone = tz.tzutc()
to_zone = tz.tzlocal()

# Since datetime.utcnow() is deprecated since version 3.12 use datetime.now()
# utc = datetime.now()  
utc = datetime.strptime('2011-01-21 02:37:21', '%Y-%m-%d %H:%M:%S')

# Tell the datetime object that it's in UTC time zone since 
# datetime objects are 'naive' by default
utc = utc.replace(tzinfo=from_zone)

# Convert time zone
central = utc.astimezone(to_zone)

```

**Edit** Expanded example to show `strptime` usage

**Edit 2** Fixed API usage to show better entry point method

**Edit 3** Included auto-detect methods for timezones (Yarin)"
"The goal is to create a mock class which behaves like a db resultset.

So for example, if a database query returns, using a dict expression, `{'ab':100, 'cd':200}`, then I would like to see:

```
>>> dummy.ab
100

```

At first I thought maybe I could do it this way:

```
ks = ['ab', 'cd']
vs = [12, 34]
class C(dict):
    def __init__(self, ks, vs):
        for i, k in enumerate(ks):
            self[k] = vs[i]
            setattr(self, k, property(lambda x: vs[i], self.fn_readyonly))

    def fn_readonly(self, v)
        raise ""It is ready only""

if __name__ == ""__main__"":
    c = C(ks, vs)
    print c.ab

```

but `c.ab` returns a property object instead.

Replacing the `setattr` line with `k = property(lambda x: vs[i])` is of no use at all.

So what is the right way to create an instance property at runtime?

P.S. I am aware of an alternative presented in [*How is the `__getattribute__` method used?*](https://stackoverflow.com/questions/371753/how-is-the-getattribute-method-used)","I suppose I should expand this answer, now that I'm older and wiser and know what's going on. Better late than never.

You *can* add a property to a class dynamically. But that's the catch: you have to add it to the *class*.

```
>>> class Foo(object):
...     pass
... 
>>> foo = Foo()
>>> foo.a = 3
>>> Foo.b = property(lambda self: self.a + 1)
>>> foo.b
4

```

A `property` is actually a simple implementation of a thing called a [descriptor](http://docs.python.org/2/reference/datamodel.html#implementing-descriptors). It's an object that provides custom handling for a given attribute, *on a given class*. Kinda like a way to factor a huge `if` tree out of `__getattribute__`.

When I ask for `foo.b` in the example above, Python sees that the `b` defined on the class implements the *descriptor protocol*â€”which just means it's an object with a `__get__`, `__set__`, or `__delete__` method. The descriptor claims responsibility for handling that attribute, so Python calls `Foo.b.__get__(foo, Foo)`, and the return value is passed back to you as the value of the attribute. In the case of `property`, each of these methods just calls the `fget`, `fset`, or `fdel` you passed to the `property` constructor.

Descriptors are really Python's way of exposing the plumbing of its entire OO implementation. In fact, there's another type of descriptor even more common than `property`.

```
>>> class Foo(object):
...     def bar(self):
...         pass
... 
>>> Foo().bar
<bound method Foo.bar of <__main__.Foo object at 0x7f2a439d5dd0>>
>>> Foo().bar.__get__
<method-wrapper '__get__' of instancemethod object at 0x7f2a43a8a5a0>

```

The humble method is just another kind of descriptor. Its `__get__` tacks on the calling instance as the first argument; in effect, it does this:

```
def __get__(self, instance, owner):
    return functools.partial(self.function, instance)

```

Anyway, I suspect this is why descriptors only work on classes: they're a formalization of the stuff that powers classes in the first place. They're even the exception to the rule: you can obviously assign descriptors to a class, and classes are themselves instances of `type`! In fact, trying to read `Foo.bar` still calls `property.__get__`; it's just idiomatic for descriptors to return themselves when accessed as class attributes.

I think it's pretty cool that virtually all of Python's OO system can be expressed in Python. :)

Oh, and I wrote a [wordy blog post about descriptors](http://me.veekun.com/blog/2012/05/23/python-faq-descriptors/) a while back if you're interested."
"I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features.

Currently, I do the following:

```
data = pandas.read_csv('mydata.csv')

```

which gives something like:

```
data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))

```

I'd like to slice this dataframe in two dataframes: one containing the columns `a` and `b` and one containing the columns `c`, `d` and `e`.

It is not possible to write something like

```
observations = data[:'c']
features = data['c':]

```

I'm not sure what the best method is. Do I need a `pd.Panel`?

By the way, I find dataframe indexing pretty inconsistent: `data['a']` is permitted, but `data[0]` is not. On the other side, `data['a':]` is not permitted but `data[0:]` is.
Is there a practical reason for this? This is really confusing if columns are indexed by Int, given that `data[0] != data[0:1]`","2017 Answer - pandas 0.20: .ix is deprecated. Use .loc
======================================================

See the [deprecation in the docs](https://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated)

`.loc` uses label based indexing to select both rows and columns. The labels being the values of the index or the columns. Slicing with `.loc` includes the last element.

> Let's assume we have a DataFrame with the following columns:  
> `foo`, `bar`, `quz`, `ant`, `cat`, `sat`, `dat`.

```
# selects all rows and all columns beginning at 'foo' up to and including 'sat'
df.loc[:, 'foo':'sat']
# foo bar quz ant cat sat

```

`.loc` accepts the same slice notation that Python lists do for both row and columns. Slice notation being `start:stop:step`

```
# slice from 'foo' to 'cat' by every 2nd column
df.loc[:, 'foo':'cat':2]
# foo quz cat

# slice from the beginning to 'bar'
df.loc[:, :'bar']
# foo bar

# slice from 'quz' to the end by 3
df.loc[:, 'quz'::3]
# quz sat

# attempt from 'sat' to 'bar'
df.loc[:, 'sat':'bar']
# no columns returned

# slice from 'sat' to 'bar'
df.loc[:, 'sat':'bar':-1]
sat cat ant quz bar

# slice notation is syntatic sugar for the slice function
# slice from 'quz' to the end by 2 with slice function
df.loc[:, slice('quz',None, 2)]
# quz cat dat

# select specific columns with a list
# select columns foo, bar and dat
df.loc[:, ['foo','bar','dat']]
# foo bar dat

```

You can slice by rows and columns. For instance, if you have 5 rows with labels `v`, `w`, `x`, `y`, `z`

```
# slice from 'w' to 'y' and 'foo' to 'ant' by 3
df.loc['w':'y', 'foo':'ant':3]
#    foo ant
# w
# x
# y

```"
"I am trying to run a Django management command from cron. I am using virtualenv to keep my project sandboxed.

I have seen examples here and elsewhere that show running management commands from within virtualenv's like:

```
0 3 * * * source /home/user/project/env/bin/activate && /home/user/project/manage.py command arg

```

However, even though syslog shows an entry when the task should have started, this task never actually runs (the log file for the script is empty). If I run the line manually from the shell, it works as expected.

The only way I can currently get the command to run via cron, is to break the commands up and put them in a dumb bash wrapper script:

```
#!/bin/sh
source /home/user/project/env/bin/activate
cd /home/user/project/
./manage.py command arg

```

EDIT:

ars came up with a working combination of commands:

```
0 3 * * * cd /home/user/project && /home/user/project/env/bin/python /home/user/project/manage.py command arg

```

At least in my case, invoking the activate script for the virtualenv did nothing. This works, so on with the show.","You should be able to do this by using the `python` in your virtual environment:

```
/home/my/virtual/bin/python /home/my/project/manage.py command arg

```

EDIT: If your django project isn't in the PYTHONPATH, then you'll need to switch to the right directory:

```
cd /home/my/project && /home/my/virtual/bin/python ...

```

You can also try to log the failure from cron:

```
cd /home/my/project && /home/my/virtual/bin/python /home/my/project/manage.py > /tmp/cronlog.txt 2>&1

```

Another thing to try is to make the same change in your `manage.py` script at the very top:

```
#!/home/my/virtual/bin/python

```"
"What's the usage of the tilde operator in Python?

One thing I can think about is do something in both sides of a string or list, such as check if a string is palindromic or not:

```
def is_palindromic(s):
    return all(s[i] == s[~i] for i in range(len(s) / 2)) 

```

Any other good usage?","It is a unary operator (taking a single argument) that is borrowed from C, where all data types are just different ways of interpreting bytes. It is the ""invert"" or ""complement"" operation, in which all the bits of the input data are reversed.

In Python, for integers, the bits of the [twos-complement representation](http://en.wikipedia.org/wiki/Two%27s_complement) of the integer are reversed (as in `b <- b XOR 1` for each individual bit), and the result interpreted again as a twos-complement integer. So for integers, `~x` is equivalent to `(-x) - 1`.

The reified form of the `~` operator is provided as `operator.invert`. To support this operator in your own class, give it an `__invert__(self)` method.

```
>>> import operator
>>> class Foo:
...   def __invert__(self):
...     print 'invert'
...
>>> x = Foo()
>>> operator.invert(x)
invert
>>> ~x
invert

```

Any class in which it is meaningful to have a ""complement"" or ""inverse"" of an instance that is also an instance of the same class is a possible candidate for the invert operator. However, operator overloading can lead to confusion if misused, so be sure that it really makes sense to do so before supplying an `__invert__` method to your class. (Note that byte-strings [ex: `'\xff'`] do not support this operator, even though it is meaningful to invert all the bits of a byte-string.)"
"I have a list of items that likely has some export issues. I would like to get a list of the duplicate items so I can manually compare them. When I try to use pandas [duplicated method](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.duplicated.html), it only returns the first duplicate. Is there a a way to get all of the duplicates and not just the first one?

A small subsection of my dataset looks like this:

```
ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE
1536D,12-Feb-12,""06DA1B3-Lebanon NH"",,15-Feb-12
F15D,18-May-12,""06405B2-Lebanon NH"",,25-Jul-12
8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12
A036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12
8944,19-Feb-12,""06D26AD-Hanover NH"",,4-Feb-12
1004E,8-Jun-12,""06388B2-Lebanon NH"",,24-Dec-11
11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12
30D7,11-Nov-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",30-Nov-11
3AE2,21-Feb-12,""06405B2-Lebanon NH"",,26-Oct-12
B0FE,17-Feb-12,""06D1B9D-Hartland VT"",,16-Feb-12
127A1,11-Dec-11,""064456E-Hanover NH"",""064456E-Hanover NH"",11-Nov-12
161FF,20-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",3-Jul-12
A036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",
475B,25-Sep-12,""06D26AD-Hanover NH"",,5-Nov-12
151A3,7-Mar-12,""06388B2-Lebanon NH"",,16-Nov-12
CA62,3-Jan-12,,,
D31B,18-Dec-11,""06405B2-Lebanon NH"",,9-Jan-12
20F5,8-Jul-12,""0669C50-Randolph VT"",,3-Feb-12
8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12
14E48,1-Aug-12,""06D3206-Hanover NH"",,
177F8,20-Aug-12,""063B208-Randolph VT"",""063B208-Randolph VT"",5-May-12
553E,11-Oct-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",8-Mar-12
12D5F,18-Jul-12,""0649597-White River VT"",""0649597-White River VT"",2-Nov-12
C6DC,13-Apr-12,""06388B2-Lebanon NH"",,
11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12
17B43,11-Aug-12,,,22-Oct-12
A036,11-Aug-12,""06D3206-Hanover NH"",,19-Jun-12

```

My code looks like this currently:

```
df_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols='ID')]

```

There area a couple duplicate items. But, when I use the above code, I only get the first item. In the API reference, I see how I can get the last item, but I would like to have all of them so I can visually inspect them to see why I am getting the discrepancy. So, in this example I would like to get all three A036 entries and both 11795 entries and any other duplicated entries, instead of the just first one. Any help is most appreciated.","Method #1: print all rows where the ID is one of the IDs in duplicated:

```
>>> import pandas as pd
>>> df = pd.read_csv(""dup.csv"")
>>> ids = df[""ID""]
>>> df[ids.isin(ids[ids.duplicated()])].sort_values(""ID"")
       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE
24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12
6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12
18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12
2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12
12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN
3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12
26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12

```

but I couldn't think of a nice way to prevent repeating `ids` so many times. I prefer method #2: `groupby` on the ID.

```
>>> pd.concat(g for _, g in df.groupby(""ID"") if len(g) > 1)
       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE
6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12
24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12
2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12
18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12
3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12
12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN
26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12

```"
"I'm trying to convert a list to a tuple.

Most solutions on Google offer the following code:

```
l = [4,5,6]
tuple(l)

```

However, the code results in an error message when I run it:

> TypeError: 'tuple' object is not callable

How can I fix this problem?","It should work fine. Don't use `tuple`, `list` or other special names as a variable name. It's probably what's causing your problem.

```
>>> l = [4,5,6]
>>> tuple(l)
(4, 5, 6)

>>> tuple = 'whoops'   # Don't do this
>>> tuple(l)
TypeError: 'tuple' object is not callable

```"
"What is the easiest way to remove duplicate columns from a dataframe?

I am reading a text file that has duplicate columns via:

```
import pandas as pd

df=pd.read_table(fname)

```

The column names are:

```
Time, Time Relative, N2, Time, Time Relative, H2, etc...

```

All the Time and Time Relative columns contain the same data. I want:

```
Time, Time Relative, N2, H2

```

All my attempts at dropping, deleting, etc such as:

```
df=df.T.drop_duplicates().T

```

Result in uniquely valued index errors:

```
Reindexing only valid with uniquely valued index objects

```

Sorry for being a Pandas noob. Any Suggestions would be appreciated.

---

**Additional Details**

Pandas version: 0.9.0  
Python Version: 2.7.3  
Windows 7  
(installed via Pythonxy 2.7.3.0)

data file (note: in the real file, columns are separated by tabs, here they are separated by 4 spaces):

```
Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]
2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    
2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    
2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    
... etc ...
2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    
2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005
2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005

```","Here's a one line solution to remove columns based on duplicate **column names**:

```
df = df.loc[:,~df.columns.duplicated()].copy()

```

**How it works:**

Suppose the columns of the data frame are `['alpha','beta','alpha']`

`df.columns.duplicated()` returns a boolean array: a `True` or `False` for each column. If it is `False` then the column name is unique up to that point, if it is `True` then the column name is duplicated earlier. For example, using the given example, the returned value would be `[False,False,True]`.

`Pandas` allows one to index using boolean values whereby it selects only the `True` values. Since we want to keep the unduplicated columns, we need the above boolean array to be flipped (ie `[True, True, False] = ~[False,False,True]`)

Finally, `df.loc[:,[True,True,False]]` selects only the non-duplicated columns using the aforementioned indexing capability.

The final `.copy()` is there to copy the dataframe to (mostly) avoid getting errors about trying to modify an existing dataframe later down the line.

**Note**: the above only checks columns names, *not* column values.

##### To remove duplicated indexes

Since it is similar enough, do the same thing on the index:

```
df = df.loc[~df.index.duplicated(),:].copy()

```

### To remove duplicates by checking values without transposing

**Update and caveat**: please be careful in applying this. Per the counter-example provided by DrWhat in the comments, this solution may **not** have the desired outcome in all cases.

```
df = df.loc[:,~df.apply(lambda x: x.duplicated(),axis=1).all()].copy()

```

This avoids the issue of transposing. Is it fast? No. Does it work? In some cases. Here, try it on this:

```
# create a large(ish) dataframe
ldf = pd.DataFrame(np.random.randint(0,100,size= (736334,1312))) 


#to see size in gigs
#ldf.memory_usage().sum()/1e9 #it's about 3 gigs

# duplicate a column
ldf.loc[:,'dup'] = ldf.loc[:,101]

# take out duplicated columns by values
ldf = ldf.loc[:,~ldf.apply(lambda x: x.duplicated(),axis=1).all()].copy()

```"
Does anybody know how to extract a column from a multi-dimensional array in Python?,"```
>>> import numpy as np
>>> A = np.array([[1,2,3,4],[5,6,7,8]])

>>> A
array([[1, 2, 3, 4],
    [5, 6, 7, 8]])

>>> A[:,2] # returns the third columm
array([3, 7])

```

See also: ""numpy.arange"" and ""reshape"" to allocate memory

Example: (Allocating a array with shaping of matrix (3x4))

```
nrows = 3
ncols = 4
my_array = numpy.arange(nrows*ncols, dtype='double')
my_array = my_array.reshape(nrows, ncols)

```"
"What's the difference of using `List`, `Tuple`, etc. from `typing` module:

```
from typing import Tuple

def f(points: Tuple):
    return map(do_stuff, points)

```

As opposed to referring to Python's types directly:

```
def f(points: tuple):
    return map(do_stuff, points)

```

And when should I use one over the other?","Until Python 3.9 [added support for type hinting using standard collections](https://docs.python.org/3/whatsnew/3.9.html#type-hinting-generics-in-standard-collections), you had to use `typing.Tuple` and `typing.List` if you wanted to document what type the *contents* of the containers needed to be:

```
def f(points: Tuple[float, float]):
    return map(do_stuff, points)

```

Up until Python 3.8, `tuple` and `list` did not support being [used as *generic types*](https://docs.python.org/3/library/typing.html#generics). The above example documents that the function `f` requires the `points` argument to be a tuple with two `float` values.

[`typing.Tuple`](https://docs.python.org/3/library/typing.html#typing.Tuple) is special here in that it lets you specify a specific number of elements expected and the type of each position. Use ellipsis if the length is not set and the type should be repeated: `Tuple[float, ...]` describes a variable-length `tuple` with `float`s.

For [`typing.List`](https://docs.python.org/3/library/typing.html#typing.List) and other sequence types you generally only specify the type for all elements; `List[str]` is a list of strings, of any size. Note that functions should preferentially take [`typing.Sequence`](https://docs.python.org/3/library/typing.html#typing.Sequence) as arguments and `typing.List` is typically only used for return types; generally speaking most functions would take any sequence and only iterate, but when you return a `list`, you really are returning a specific, mutable sequence type.

If you still need to support Python 3.8 or older code, you should always pick the `typing` generics even when you are not currently restricting the contents. It is easier to add that constraint later with a generic type as the resulting change will be smaller.

If you are implementing a custom container type and want that type to support generics, you can [implement a `__class_getitem__` hook](https://docs.python.org/3/reference/datamodel.html#emulating-generic-types) or inherit from [`typing.Generic`](https://docs.python.org/3/library/typing.html#typing.Generic) (which in turn implements `__class_getitem__`)."
"In python, what's the best way to test if a variable contains a list or a tuple? (ie. a collection)

Is `isinstance()` as evil as suggested here? <http://www.canonical.org/~kragen/isinstance/>

Update: the most common reason I want to distinguish a list from a string is when I have some indefinitely deep nested tree / data-structure of lists of lists of lists of strings etc. which I'm exploring with a recursive algorithm and I need to know when I've hit the ""leaf"" nodes.","```
if type(x) is list:
    print 'a list'
elif type(x) is tuple:
    print 'a tuple'
else:
    print 'neither a tuple or a list'

```"
"I want to make some modifications to a few selected tick labels in a plot.

For example, if I do:

```
label = axes.yaxis.get_major_ticks()[2].label
label.set_fontsize(size)
label.set_rotation('vertical')

```

the font size and the orientation of the tick label is changed.

However, if try:

```
label.set_text('Foo')

```

the tick label is *not* modified. Also if I do:

```
print label.get_text()

```

nothing is printed.

Here's some more strangeness. When I tried this:

```
import matplotlib.pyplot as plt
import numpy as np

axes = plt.figure().add_subplot(111)
t = np.arange(0.0, 2.0, 0.01)
s = np.sin(2*np.pi*t)
axes.plot(t, s)
for ticklabel in axes.get_xticklabels():
    print(ticklabel.get_text())

```

Only empty strings are printed, but the plot contains ticks labeled as '0.0', '0.5', '1.0', '1.5', and '2.0'.

[![enter image description here](https://i.sstatic.net/YNEBZ.png)](https://i.sstatic.net/YNEBZ.png)","Caveat: Unless the ticklabels are already set to a string (as is usually the case in e.g. a boxplot), this will not work with any version of matplotlib newer than `1.1.0`. If you're working from the current github master, this won't work. I'm not sure what the problem is yet... It may be an unintended change, or it may not be...

Normally, you'd do something along these lines:

```
import matplotlib.pyplot as plt

fig, ax = plt.subplots()

# We need to draw the canvas, otherwise the labels won't be positioned and 
# won't have values yet.
fig.canvas.draw()

labels = [item.get_text() for item in ax.get_xticklabels()]
labels[1] = 'Testing'

ax.set_xticklabels(labels)

plt.show()

```

![enter image description here](https://i.sstatic.net/5m900.png)

To understand the reason why you need to jump through so many hoops, you need to understand a bit more about how matplotlib is structured.

Matplotlib deliberately avoids doing ""static"" positioning of ticks, etc, unless it's explicitly told to. The assumption is that you'll want to interact with the plot, and so the bounds of the plot, ticks, ticklabels, etc will be dynamically changing.

Therefore, you can't just set the text of a given tick label. By default, it's re-set by the axis's Locator and Formatter every time the plot is drawn.

However, if the Locators and Formatters are set to be static (`FixedLocator` and `FixedFormatter`, respectively), then the tick labels stay the same.

This is what `set_*ticklabels` or `ax.*axis.set_ticklabels` does.

Hopefully that makes it slighly more clear as to why changing an individual tick label is a bit convoluted.

Often, what you actually want to do is just annotate a certain position. In that case, look into `annotate`, instead."
"I try to switch to Homebrew (after using fink and macport) on Mac OS X 10.6.2. I have installed python 2.7 with

```
brew install python 

```

The problem is that, contrary to Macport, it seems that there is no python\_select utility, and my default mac python is always default

```
which python

```

give me

```
/usr/bin/python

```

and `/usr/bin/python` is not a symlink

How can I do to make python brew flavour to be my default python ?","As suggested by the homebrew installer itself, be sure to add this to your `.bashrc` or `.zshrc`:

```
export PATH=""/usr/local/opt/python/libexec/bin:$PATH""

```"
"I have a JSON file I want to convert to a CSV file. How can I do this with Python?

I tried:

```
import json
import csv

f = open('data.json')
data = json.load(f)
f.close()

f = open('data.csv')
csv_file = csv.writer(f)
for item in data:
    csv_file.writerow(item)

f.close()

```

However, it did not work. I am using Django and the error I received is:

```
`file' object has no attribute 'writerow'`

```

I then tried the following:

```
import json
import csv

f = open('data.json')
data = json.load(f)
f.close()

f = open('data.csv')
csv_file = csv.writer(f)
for item in data:
    f.writerow(item)  # ‚Üê changed

f.close()

```

I then get the error:

```
`sequence expected`

```

Sample json file:

```
[
  {
    ""pk"": 22,
    ""model"": ""auth.permission"",
    ""fields"": {
      ""codename"": ""add_logentry"",
      ""name"": ""Can add log entry"",
      ""content_type"": 8
    }
  },
  {
    ""pk"": 23,
    ""model"": ""auth.permission"",
    ""fields"": {
      ""codename"": ""change_logentry"",
      ""name"": ""Can change log entry"",
      ""content_type"": 8
    }
  },
  {
    ""pk"": 24,
    ""model"": ""auth.permission"",
    ""fields"": {
      ""codename"": ""delete_logentry"",
      ""name"": ""Can delete log entry"",
      ""content_type"": 8
    }
  },
  {
    ""pk"": 4,
    ""model"": ""auth.permission"",
    ""fields"": {
      ""codename"": ""add_group"",
      ""name"": ""Can add group"",
      ""content_type"": 2
    }
  },
  {
    ""pk"": 10,
    ""model"": ""auth.permission"",
    ""fields"": {
      ""codename"": ""add_message"",
      ""name"": ""Can add message"",
      ""content_type"": 4
    }
  }
]

```","With the `pandas` [library](https://pandas.pydata.org/docs/dev/index.html), **this is as easy as using two commands!**

```
df = pd.read_json()

```

[read\_json](https://pandas.pydata.org/docs/dev/reference/api/pandas.read_json.html) converts a JSON string to a pandas object (either a series or dataframe). Then:

```
df.to_csv()

```

Which can either return a string or write directly to a csv-file. See the docs for [to\_csv](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_csv.html).

Based on the verbosity of previous answers, we should all thank pandas for the shortcut.

For unstructured JSON see [this answer](https://stackoverflow.com/a/58648286/2861681).

EDIT:
Someone asked for a working minimal example:

```
import pandas as pd

with open('jsonfile.json', encoding='utf-8') as inputfile:
    df = pd.read_json(inputfile)

df.to_csv('csvfile.csv', encoding='utf-8', index=False)

```"
"Assuming `connectionDetails` is a Python dictionary, what's the best, most elegant, most ""pythonic"" way of refactoring code like this?

```
if ""host"" in connectionDetails:
    host = connectionDetails[""host""]
else:
    host = someDefaultValue

```","Like this:

```
host = connectionDetails.get('host', someDefaultValue)

```"
"How would you go about finding out how much memory is being used by an object? I know it is possible to find out how much is used by a block of code, but not by an instantiated object (anytime during its life), which is what I want.","Try this:

```
sys.getsizeof(object)

```

[getsizeof()](https://docs.python.org/3/library/sys.html#sys.getsizeof) Return the size of an object in bytes. It calls the objectâ€™s `__sizeof__` method and adds an additional garbage collector overhead **if** the object is managed by the garbage collector.

[A recursive recipe](https://web.archive.org/web/20230924080253/https://code.activestate.com/recipes/577504/)"
"I'm writing my own container, which needs to give access to a dictionary inside by attribute calls. The typical use of the container would be like this:

```
dict_container = DictContainer()
dict_container['foo'] = bar
...
print dict_container.foo

```

I know that it might be stupid to write something like this, but that's the functionality I need to provide. I was thinking about implementing this in a following way:

```
def __getattribute__(self, item):
    try:
        return object.__getattribute__(item)
    except AttributeError:
        try:
            return self.dict[item]
        except KeyError:
            print ""The object doesn't have such attribute""

```

I'm not sure whether nested try/except blocks are a good practice, so another way would be to use `hasattr()` and `has_key()`:

```
def __getattribute__(self, item):
        if hasattr(self, item):
            return object.__getattribute__(item)
        else:
            if self.dict.has_key(item):
                return self.dict[item]
            else:
                raise AttributeError(""some customised error"")

```

Or to use one of them and one try catch block like this:

```
def __getattribute__(self, item):
    if hasattr(self, item):
        return object.__getattribute__(item)
    else:
        try:
            return self.dict[item]
        except KeyError:
            raise AttributeError(""some customised error"")

```

Which option is most Pythonic and elegant?","Your first example is perfectly fine. Even the official Python documentation recommends this style known as [EAFP](https://docs.python.org/3/glossary.html#term-EAFP).

Personally, I prefer to avoid nesting when it's not necessary:

```
def __getattribute__(self, item):
    try:
        return object.__getattribute__(self, item)
    except AttributeError:
        pass  # Fallback to dict
    try:
        return self.dict[item]
    except KeyError:
        raise AttributeError(""The object doesn't have such attribute"") from None

```

PS. `has_key()` has been deprecated for a long time in Python 2. Use `item in self.dict` instead."
"How can I create an [iterator](https://stackoverflow.com/questions/9884132/what-are-iterator-iterable-and-iteration) in Python?

For example, suppose I have a class whose instances logically ""contain"" some values:

```
class Example:
    def __init__(self, values):
        self.values = values

```

I want to be able to write code like:

```
e = Example([1, 2, 3])
# Each time through the loop, expose one of the values from e.values
for value in e:
    print(""The example object contains"", value)

```

More generally, the iterator should be able to control where the values come from, or even compute them on the fly (rather than considering any particular attribute of the instance).","Iterator objects in python conform to the iterator protocol, which basically means they provide two methods: `__iter__()` and `__next__()`.

* The `__iter__` returns the iterator object and is implicitly called
  at the start of loops.
* The `__next__()` method returns the next value and is implicitly called at each loop increment. This method raises a StopIteration exception when there are no more value to return, which is implicitly captured by looping constructs to stop iterating.

Here's a simple example of a counter:

```
class Counter:
    def __init__(self, low, high):
        self.current = low - 1
        self.high = high

    def __iter__(self):
        return self

    def __next__(self): # Python 2: def next(self)
        self.current += 1
        if self.current < self.high:
            return self.current
        raise StopIteration


for c in Counter(3, 9):
    print(c)

```

This will print:

```
3
4
5
6
7
8

```

This is easier to write using a generator, as covered in a previous answer:

```
def counter(low, high):
    current = low
    while current < high:
        yield current
        current += 1

for c in counter(3, 9):
    print(c)

```

The printed output will be the same. Under the hood, the generator object supports the iterator protocol and does something roughly similar to the class Counter.

David Mertz's article, [Iterators and Simple Generators](https://www.ibm.com/developerworks/library/l-pycon/), is a pretty good introduction."
"This is my declarative model:

```
import datetime
from sqlalchemy import Column, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Test(Base):
    __tablename__ = 'test'

    id = Column(Integer, primary_key=True)
    created_date = DateTime(default=datetime.datetime.utcnow)

```

However, when I try to import this module, I get this error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""orm/models2.py"", line 37, in <module>
    class Test(Base):
  File ""orm/models2.py"", line 41, in Test
    created_date = sqlalchemy.DateTime(default=datetime.datetime.utcnow)
TypeError: __init__() got an unexpected keyword argument 'default'

```

If I use an Integer type, I can set a default value. What's going on?","### Calculate timestamps within your DB, not your client

For sanity, you probably want to have all `datetimes` calculated by your DB server, rather than the application server. Calculating the timestamp in the application can lead to problems because network latency is variable, clients experience slightly different clock drift, and different programming languages occasionally calculate time slightly differently.

SQLAlchemy allows you to do this by passing [`func.now()`](http://docs.sqlalchemy.org/en/latest/core/functions.html#sqlalchemy.sql.functions.now) or [`func.current_timestamp()`](http://docs.sqlalchemy.org/en/latest/core/functions.html#sqlalchemy.sql.functions.current_timestamp) (they are aliases of each other) which tells the DB to calculate the timestamp itself.

### Use SQLALchemy's `server_default`

Additionally, for a default where you're already telling the DB to calculate the value, it's generally better to use [`server_default`](http://docs.sqlalchemy.org/en/latest/core/defaults.html#server-side-defaults) instead of `default`. This tells SQLAlchemy to pass the default value as part of the `CREATE TABLE` statement.

For example, if you write an ad hoc script against this table, using `server_default` means you won't need to worry about manually adding a timestamp call to your script--the database will set it automatically.

### Understanding SQLAlchemy's `onupdate`/`server_onupdate`

SQLAlchemy also supports `onupdate` so that anytime the row is updated it inserts a new timestamp. Again, best to tell the DB to calculate the timestamp itself:

```
from sqlalchemy.sql import func

time_created = Column(DateTime(timezone=True), server_default=func.now())
time_updated = Column(DateTime(timezone=True), onupdate=func.now())

```

There is a [`server_onupdate`](http://docs.sqlalchemy.org/en/latest/core/metadata.html#sqlalchemy.schema.Column.params.server_onupdate) parameter, but unlike `server_default`, it doesn't actually set anything serverside. It just tells SQLalchemy that your database will change the column when an update happens (perhaps you created a [trigger on the column](http://docs.sqlalchemy.org/en/latest/core/defaults.html#triggered-columns) ), so SQLAlchemy will ask for the return value so it can update the corresponding object.

### One other potential gotcha:

You might be surprised to notice that if you make a bunch of changes within a single transaction, they all have the same timestamp. That's because the SQL standard specifies that `CURRENT_TIMESTAMP` returns values based on the start of the transaction.

PostgreSQL provides the non-SQL-standard `statement_timestamp()` and `clock_timestamp()` which *do* change within a transaction. Docs here: <https://www.postgresql.org/docs/current/static/functions-datetime.html#FUNCTIONS-DATETIME-CURRENT>

### UTC timestamp

If you want to use UTC timestamps, a stub of implementation for `func.utcnow()` is provided in [SQLAlchemy documentation](http://docs.sqlalchemy.org/en/latest/core/compiler.html#utc-timestamp-function). You need to provide appropriate driver-specific functions on your own though."
"I'm trying to display a grayscale image using `matplotlib.pyplot.imshow()`. My problem is that the grayscale image is displayed as a colormap. I need it to be grayscale because I want to draw on top of the image with color.

I read in the image and convert to grayscale using PIL's `Image.open().convert(""L"")`

```
image = Image.open(file).convert(""L"")

```

Then I convert the image to a matrix so that I can easily do some image processing using

```
matrix = scipy.misc.fromimage(image, 0)

```

However, when I do

```
figure()  
matplotlib.pyplot.imshow(matrix)  
show()

```

it displays the image using a colormap (i.e. it's not grayscale).

What am I doing wrong here?","The following code will load an image from a file `image.png` and will display it as grayscale.

```
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

fname = 'image.png'
image = Image.open(fname).convert(""L"")
arr = np.asarray(image)
plt.imshow(arr, cmap='gray', vmin=0, vmax=255)
plt.show()

```

If you want to display the inverse grayscale, switch the cmap to `cmap='gray_r'`."
"From my understanding:

An **interpreted** language is a high-level language run and executed by an interpreter (a program which converts the high-level language to machine code and then executing) on the go; it processes the program a little at a time.

A **compiled** language is a high-level language whose code is first converted to machine-code by a compiler (a program which converts the high-level language to machine code) and then executed by an executor (another program for running the code).

Correct me if my definitions are wrong.

Now coming back to Python, I am bit confused about this. Everywhere you learn that Python is an interpreted language, but it's interpreted to some intermediate code (like byte-code or IL) and *not* to the machine code. So which program then executes the IM code? Please help me understand how a Python script is handled and run.","First off, interpreted/compiled is not a property of the language but a property of the implementation. For most languages, most if not all implementations fall in one category, so one might save a few words saying the language is interpreted/compiled too, but it's still an important distinction, both because it aids understanding and because there are quite a few languages with usable implementations of both kinds (mostly in the realm of functional languages, see Haskell and ML). In addition, there are C interpreters and projects that attempt to compile a subset of Python to C or C++ code (and subsequently to machine code).

Second, compilation is not restricted to ahead-of-time compilation to native machine code. A compiler is, more generally, a program that converts a program in one programming language into a program in another programming language (arguably, you can even have a compiler with the same input and output language if significant transformations are applied). And JIT compilers compile to native machine code *at runtime*, which can give speed very close to or even better than ahead of time compilation (depending on the benchmark and the quality of the implementations compared).

But to stop nitpicking and answer the question you meant to ask: Practically (read: using a somewhat popular and mature implementation), Python is **compiled**. Not compiled to machine code ahead of time (i.e. ""compiled"" by the restricted and wrong, but alas common definition), ""only"" compiled to [bytecode](http://en.wikipedia.org/wiki/Bytecode), but it's still compilation with at least some of the benefits. For example, the statement `a = b.c()` is compiled to a byte stream which, when ""disassembled"", looks somewhat like `load 0 (b); load_str 'c'; get_attr; call_function 0; store 1 (a)`. This is a simplification, it's actually less readable and a bit more low-level - you can experiment with the standard library [`dis` module](http://docs.python.org/library/dis.html) and see what the real deal looks like. Interpreting this is faster than interpreting from a higher-level representation.

That bytecode is either interpreted (note that there's a difference, both in theory and in practical performance, between interpreting directly and first compiling to some intermediate representation and interpret that), as with the reference implementation (CPython), or both interpreted and compiled to optimized machine code at runtime, as with [PyPy](http://pypy.org/)."
How can I get the parent class(es) of a Python class?,"Use the following attribute:

```
cls.__bases__

```

From the [docs](http://docs.python.org/library/stdtypes.html#class.__bases__):

> The tuple of base classes of a class object.

Example:

```
>>> str.__bases__
(<class 'object'>,)

```

Another example:

```
>>> class A(object):
...   pass
... 
>>> class B(object):
...   pass
... 
>>> class C(A, B):
...   pass
... 
>>> C.__bases__
(<class '__main__.A'>, <class '__main__.B'>)

```"
"I would like to convert a NumPy array to a unit vector. More specifically, I am looking for an equivalent version of this normalisation function:

```
def normalize(v):
    norm = np.linalg.norm(v)
    if norm == 0: 
       return v
    return v / norm

```

This function handles the situation where vector `v` has the norm value of 0.

Is there any similar functions provided in `sklearn` or `numpy`?","If you're using scikit-learn you can use [`sklearn.preprocessing.normalize`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize):

```
import numpy as np
from sklearn.preprocessing import normalize

x = np.random.rand(1000)*10
norm1 = x / np.linalg.norm(x)
norm2 = normalize(x[:,np.newaxis], axis=0).ravel()
print np.all(norm1 == norm2)
# True

```"
"I am using Ubuntu and have installed Python 2.7.5 and 3.4.0. In Python 2.7.5 I am able to successfully assign a variable `x = Value('i', 2)`, but not in 3.4.0. I am getting:

```
Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/usr/local/lib/python3.4/multiprocessing/context.py"", line 132, in Value
      from .sharedctypes import Value
   File ""/usr/local/lib/python3.4/multiprocessing/sharedctypes.py"", line 10, in <
module>
   import ctypes
   File ""/usr/local/lib/python3.4/ctypes/__init__.py"", line 7, in <module>
      from _ctypes import Union, Structure, Array
ImportError: No module named '_ctypes'

```

I just updated to 3.3.2 through installing the source of 3.4.0. It installed in */usr/local/lib/python3.4*.

Did I update to Python 3.4 correctly?

One thing I noticed that Python 3.4 is installed in *usr/local/lib*, while Python 3.3.2 is still installed in *usr/lib*, so it was not overwritten.","Installing `libffi-dev` and re-installing python3.7 fixed the problem for me.

to cleanly build py 3.7 `libffi-dev` is required or else later stuff will fail

**If using RHEL/Fedora:**

```
sudo yum install libffi-devel

```

or

```
sudo dnf install libffi-devel

```

**If using Debian/Ubuntu:**

```
sudo apt-get install libffi-dev

```"
"Suppose you have a dictionary like:

```
{'a': 1,
 'c': {'a': 2,
       'b': {'x': 5,
             'y' : 10}},
 'd': [1, 2, 3]}

```

How would you go about flattening that into something like:

```
{'a': 1,
 'c_a': 2,
 'c_b_x': 5,
 'c_b_y': 10,
 'd': [1, 2, 3]}

```","Basically the same way you would flatten a nested list, you just have to do the extra work for iterating the dict by key/value, creating new keys for your new dictionary and creating the dictionary at final step.

```
from collections.abc import MutableMapping

def flatten(dictionary, parent_key='', separator='_'):
    items = []
    for key, value in dictionary.items():
        new_key = parent_key + separator + key if parent_key else key
        if isinstance(value, MutableMapping):
            items.extend(flatten(value, new_key, separator=separator).items())
        else:
            items.append((new_key, value))
    return dict(items)

>>> flatten({'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]})
{'a': 1, 'c_a': 2, 'c_b_x': 5, 'd': [1, 2, 3], 'c_b_y': 10}

```"
"I'm trying to use scikit-learn's `LabelEncoder` to encode a pandas `DataFrame` of string labels. As the dataframe has many (50+) columns, I want to avoid creating a `LabelEncoder` object for each column; I'd rather just have one big `LabelEncoder` objects that works across *all* my columns of data.

Throwing the entire `DataFrame` into `LabelEncoder` creates the below error. Please bear in mind that I'm using dummy data here; in actuality I'm dealing with about 50 columns of string labeled data, so need a solution that doesn't reference any columns by name.

```
import pandas
from sklearn import preprocessing 

df = pandas.DataFrame({
    'pets': ['cat', 'dog', 'cat', 'monkey', 'dog', 'dog'], 
    'owner': ['Champ', 'Ron', 'Brick', 'Champ', 'Veronica', 'Ron'], 
    'location': ['San_Diego', 'New_York', 'New_York', 'San_Diego', 'San_Diego', 
                 'New_York']
})

le = preprocessing.LabelEncoder()

le.fit(df)

```

> Traceback (most recent call last):
> File """", line 1, in
> File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit
> y = column\_or\_1d(y, warn=True)
> File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column\_or\_1d
> raise ValueError(""bad input shape {0}"".format(shape))
> ValueError: bad input shape (6, 3)

Any thoughts on how to get around this problem?","You can easily do this though,

```
df.apply(LabelEncoder().fit_transform)

```

EDIT2:

In scikit-learn 0.20, the recommended way is

```
OneHotEncoder().fit_transform(df)

```

as the OneHotEncoder now supports string input.
Applying OneHotEncoder only to certain columns is possible with the ColumnTransformer.

EDIT:

Since this original answer is over a year ago, and generated many upvotes (including a bounty), I should probably extend this further.

For inverse\_transform and transform, you have to do a little bit of hack.

```
from collections import defaultdict
d = defaultdict(LabelEncoder)

```

With this, you now retain all columns `LabelEncoder` as dictionary.

```
# Encoding the variable
fit = df.apply(lambda x: d[x.name].fit_transform(x))

# Inverse the encoded
fit.apply(lambda x: d[x.name].inverse_transform(x))

# Using the dictionary to label future data
df.apply(lambda x: d[x.name].transform(x))

```

MOAR EDIT:

Using Neuraxle's [`FlattenForEach`](https://www.neuraxle.org/stable/api/neuraxle.steps.loop.html#neuraxle.steps.loop.FlattenForEach) step, it's possible to do this as well to use the same [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) on all the flattened data at once:

```
FlattenForEach(LabelEncoder(), then_unflatten=True).fit_transform(df)

```

For using separate `LabelEncoder`s depending for your columns of data, or if only some of your columns of data needs to be label-encoded and not others, then using a [`ColumnTransformer`](https://stackoverflow.com/a/60302366/2476920) is a solution that allows for more control on your column selection and your LabelEncoder instances."
"Are there simple ways to store a dictionary (or multiple dictionaries) in, for example, a JSON or [pickle](https://docs.python.org/3/library/pickle.html) file?

For example, if I have some data like:

```
data = {}
data ['key1'] = ""keyinfo""
data ['key2'] = ""keyinfo2""

```

How can I save it in a file, and then later load it back in to the program from the file?

---

JSON and Pickle can also be used to store more complex structured data. This question may also include answers that are specific to the case of a simple dictionary like the one described. For more general approaches, see [How can I write structured data to a file and then read it back into the same structure later?](https://stackoverflow.com/questions/890485/). Note that the technique of converting the data to storable data is called *serialization*, and re-creating the data structure is called *deserialization*; storing the data for later use is called *persistence*.

See also [What do files actually contain, and how are they ""read""? What is a ""format"" and why should I worry about them?](https://stackoverflow.com/questions/75078605/) for some theory about how files work, and why structured data cannot just be written into and read from files directly.","**[Pickle](http://docs.python.org/library/pickle.html) save:**

```
try:
    import cPickle as pickle
except ImportError:  # Python 3.x
    import pickle

with open('data.p', 'wb') as fp:
    pickle.dump(data, fp, protocol=pickle.HIGHEST_PROTOCOL)

```

See [the pickle module documentation](https://docs.python.org/library/pickle.html#data-stream-format) for additional information regarding the `protocol` argument.

**[Pickle](http://docs.python.org/library/pickle.html) load:**

```
with open('data.p', 'rb') as fp:
    data = pickle.load(fp)

```

---

**[JSON](http://docs.python.org/library/json.html) save:**

```
import json

with open('data.json', 'w') as fp:
    json.dump(data, fp)

```

Supply extra arguments, like `sort_keys` or `indent`, to get a pretty result. The argument *sort\_keys* will sort the keys alphabetically and *indent* will indent your data structure with `indent=N` spaces.

```
json.dump(data, fp, sort_keys=True, indent=4)

```

**[JSON](http://docs.python.org/library/json.html) load:**

```
with open('data.json', 'r') as fp:
    data = json.load(fp)

```"
"I installed python2.7 and pip [following these instructions](http://johnlaudun.org/20150512-installing-and-setting-pip-with-macports/), on MacOS X Yosemite, version 10.10.3.

I can successfully install packages and import them inside my python environment and python scripts. However, any executable associated with a package that can be called from the command line in the terminal are not found.

For example, I tried to install a package called ""rosdep"" [as instructed here](http://wiki.ros.org/jade/Installation/Source). I can run `sudo pip install -U rosdep`
which installs without errors and the corresponding files are located in `/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages`. However, `sudo rosdep init` subsequently reports that the `rosdep` command is not found.

I even tried adding the above `site-packages` path to my `$PATH`, but the executables are still not found on the command line, even though the packages work perfectly from within python.

Why does this happen, and how can I fix it?

---

**See also**: [Unable to import a module that is definitely installed](https://stackoverflow.com/questions/14295680) - these two problems often have the same root cause.","I know the question asks about macOS, but here is a solution for **Linux** users who arrive here via Google.

I was having the issue described in this question, having installed the [pdfx](https://github.com/metachris/pdfx) package via pip.

When I ran it however, nothing...

```
pip list | grep pdfx
pdfx (1.3.0)

```

Yet:

```
which pdfx
pdfx not found

```

The problem on Linux is that `pip install ...` drops scripts into `~/.local/bin` and this is not on the default Debian/Ubuntu `$PATH`.

Here's a GitHub issue going into more detail: <https://github.com/pypa/pip/issues/3813>

To fix, just add `~/.local/bin` to your `$PATH`, for example by adding the following line to your `.bashrc` file:

```
export PATH=""$HOME/.local/bin:$PATH""

```

After that, restart your shell and things should work as expected."
"`random.randrange(start, stop)` only takes integer arguments. So how would I get a random number between two float values?","Use [random.uniform(a, b)](http://docs.python.org/library/random.html#random.uniform):

```
>>> import random
>>> random.uniform(1.5, 1.9)
1.8733202628557872

```"
"I have a list of dicts like this:

```
[{'value': 'apple', 'blah': 2}, 
 {'value': 'banana', 'blah': 3} , 
 {'value': 'cars', 'blah': 4}]

```

I want `['apple', 'banana', 'cars']`

Whats the best way to do this?","Assuming every dict has a `value` key, you can write (assuming your list is named `l`)

```
[d['value'] for d in l]

```

If `value` might be missing, you can use

```
[d['value'] for d in l if 'value' in d]

```"
"I have a list that contains many sub-lists of 3 elements each, like:

```
my_list = [[""a"", ""b"", 0], [""c"", ""d"", 0], [""e"", ""f"", 0], .....]

```

The last element of each sub-list is a sort of flag, which is initially 0 for each sub-list. As my algorithm progresses, I want to check whether this flag is 0 for at least one element. Currently I use a while loop, like so:

```
def check(list_):
    for item in list_:
        if item[2] == 0:
            return True
    return False

```

The overall algorithm loops as long as that condition is satisfied, and sets some of the flags in each iteration:

```
while check(my_list):
    for item in my_list:
        if condition:
            item[2] = 1
        else:
            do_sth()

```

Because it [causes problems](https://stackoverflow.com/questions/6260089/) to remove elements from the list while iterating over it, I use these flags to keep track of elements that have already been processed.

How can I simplify or speed up the code?

---

See also [Pythonic way of checking if a condition holds for any element of a list](https://stackoverflow.com/questions/1342601/) for checking the condition for any element. Keep in mind that ""any"" and ""all"" checks are related [through De Morgan's law](https://stackoverflow.com/questions/2168603/), just as ""or"" and ""and"" are related.

Existing answers here use the built-in function `all` to do the iteration. See [How do Python's any and all functions work?](https://stackoverflow.com/questions/19389490) for an explanation of `all` and its counterpart, `any`.

If the condition you want to check is ""is found in another container"", see [How to check if all of the following items are in a list?](https://stackoverflow.com/questions/3931541/) and its counterpart, [How to check if one of the following items is in a list?](https://stackoverflow.com/questions/740287). Using `any` and `all` will work, but more efficient solutions are possible.","The best answer here is to use [`all()`](http://docs.python.org/library/functions.html#all), which is the builtin for this situation. We combine this with a [generator expression](https://www.youtube.com/watch?v=t85uBptTDYY) to produce the result you want cleanly and efficiently. For example:

```
>>> items = [[1, 2, 0], [1, 2, 0], [1, 2, 0]]
>>> all(flag == 0 for (_, _, flag) in items)
True
>>> items = [[1, 2, 0], [1, 2, 1], [1, 2, 0]]
>>> all(flag == 0 for (_, _, flag) in items)
False

```

Note that `all(flag == 0 for (_, _, flag) in items)` is directly equivalent to `all(item[2] == 0 for item in items)`, it's just a little nicer to read in this case.

And, for the filter example, a list comprehension (of course, you could use a generator expression where appropriate):

```
>>> [x for x in items if x[2] == 0]
[[1, 2, 0], [1, 2, 0]]

```

If you want to check at least one element is 0, the better option is to use [`any()`](http://docs.python.org/library/functions.html#any) which is more readable:

```
>>> any(flag == 0 for (_, _, flag) in items)
True

```"
"What I'm trying to do is fairly simple when we're dealing with a local file, but the problem comes when I try to do this with a remote URL.

Basically, I'm trying to create a PIL image object from a file pulled from a URL. Sure, I could always just fetch the URL and store it in a temp file, then open it into an image object, but that feels very inefficient.

Here's what I have:

```
Image.open(urlopen(url))

```

It flakes out complaining that `seek()` isn't available, so then I tried this:

```
Image.open(urlopen(url).read())

```

But that didn't work either. Is there a Better Way to do this, or is writing to a temporary file the accepted way of doing this sort of thing?","In Python3 the StringIO and cStringIO modules are gone.

In Python3 you should use:

```
from PIL import Image
import requests
from io import BytesIO

response = requests.get(url)
img = Image.open(BytesIO(response.content))

```"
"I need to compare two lists in order to create a new list of specific elements found in one list but not in the other. For example:

```
main_list = []
list_1 = [""a"", ""b"", ""c"", ""d"", ""e""]
list_2 = [""a"", ""f"", ""c"", ""m""] 

```

I want to loop through `list_1` and append to `main_list` all the elements from `list_2` that are not found in `list_1`.

The result should be:

```
main_list = [""f"", ""m""]

```

How can I do it with Python?","You can use sets:

```
main_list = list(set(list_2) - set(list_1))

```

Output:

```
>>> list_1=[""a"", ""b"", ""c"", ""d"", ""e""]
>>> list_2=[""a"", ""f"", ""c"", ""m""]
>>> set(list_2) - set(list_1)
set(['m', 'f'])
>>> list(set(list_2) - set(list_1))
['m', 'f']

```

Per @JonClements' comment, here is a tidier version:

```
>>> list_1=[""a"", ""b"", ""c"", ""d"", ""e""]
>>> list_2=[""a"", ""f"", ""c"", ""m""]
>>> list(set(list_2).difference(list_1))
['m', 'f']

```"
"I tried this code in Python 2:

```
def NewFunction():
    return 'Â£'

```

But I get an error message that says:

```
SyntaxError: Non-ASCII character '\xa3' in file '...' but no encoding declared;
see http://www.python.org/peps/pep-0263.html for details

```

Similarly, in Python 3, if I write the same code and save it with Latin-1 encoding, I get:

```
SyntaxError: Non-UTF-8 code starting with '\xa3' in file ... on line 2, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details

```

How can I use a pound sign in string literals in my code?

---

See also: [Correct way to define Python source code encoding](https://stackoverflow.com/questions/728891) for details about whether an encoding declaration is needed and how it should be written. Please use that question to close duplicates asking about how to write the declaration, and this one for questions asking about resolving the error.","I'd recommend reading that PEP the error gives you. The problem is that your code is trying to use the ASCII encoding, but the pound symbol is not an ASCII character. Try using UTF-8 encoding. You can start by putting `# -*- coding: utf-8 -*-` at the top of your .py file. To get more advanced, you can also define encodings on a string by string basis in your code. However, if you are trying to put the pound sign literal in to your code, you'll need an encoding that supports it for the entire file."
"I have an integer value `x`, and I need to check if it is between a `start` and `end` values, so I write the following statements:

```
if x >= start and x <= end:
    # do stuff

```

This statement gets underlined, and the tooltip tells me that I must

> simplify chained comparison

As far as I can tell, that comparison is about as simple as they come. What have I missed here?","In Python you can [""chain"" comparison operations](https://docs.python.org/3/reference/expressions.html#comparisons) which just means they are ""and""ed together. In your case, it'd be like this:

```
if start <= x <= end:

```

Reference: <https://docs.python.org/3/reference/expressions.html#comparisons>"
"I have a list of variable length and am trying to find a way to test if the list item currently being evaluated is the longest string contained in the list. And I am using Python 2.6.1

For example:

```
mylist = ['abc','abcdef','abcd']

for each in mylist:
    if condition1:
        do_something()
    elif ___________________: #else if each is the longest string contained in mylist:
        do_something_else()

```

Surely there's a simple list comprehension that's short and elegant that I'm overlooking?","From the [Python documentation](http://docs.python.org/whatsnew/2.5.html#other-language-changes) itself, you can use [`max`](http://docs.python.org/library/functions.html#max):

```
>>> mylist = ['123','123456','1234']
>>> print max(mylist, key=len)
123456

```"
Is there a way to have IPython automatically reload all changed code? Either before each line is executed in the shell or failing that when it is specifically requested to. I'm doing a lot of exploratory programming using IPython and SciPy and it's quite a pain to have to manually reload each module whenever I change it.,"For IPython version 3.1, 4.x, and 5.x

```
%load_ext autoreload
%autoreload 2

```

Then your module will be **auto-reloaded** by default. This is the doc:

```
File:       ...my/python/path/lib/python2.7/site-packages/IPython/extensions/autoreload.py

Docstring:
``autoreload`` is an IPython extension that reloads modules
automatically before executing the line of code typed.

This makes for example the following workflow possible:

.. sourcecode:: ipython

   In [1]: %load_ext autoreload

   In [2]: %autoreload 2

   In [3]: from foo import some_function

   In [4]: some_function()
   Out[4]: 42

   In [5]: # open foo.py in an editor and change some_function to return 43

   In [6]: some_function()
   Out[6]: 43

The module was reloaded without reloading it explicitly, and the
object imported with ``from foo import ...`` was also updated.

```

There is a trick: when you **forget all** of the above when using `ipython`, just try:

```
import autoreload
?autoreload
# Then you get all the above

```"
"I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting a memory error:

```
MemoryError                               Traceback (most recent call last)
<ipython-input-58-67a72687871b> in <module>()
----> 1 data=pd.read_csv('aphro.csv',sep=';')

...

MemoryError: 

```

Any help on this?","The error shows that the machine does not have enough memory to read the entire
CSV into a DataFrame at one time. Assuming you do not need the entire dataset in
memory all at one time, one way to avoid the problem would be to [process the CSV in
chunks](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html) (by specifying the `chunksize` parameter):

```
chunksize = 10 ** 6
for chunk in pd.read_csv(filename, chunksize=chunksize):
    # chunk is a DataFrame. To ""process"" the rows in the chunk:
    for index, row in chunk.iterrows():
        print(row)

```

The `chunksize` parameter specifies the number of rows per chunk.
(The last chunk may contain fewer than `chunksize` rows, of course.)

---

### pandas >= 1.2

`read_csv` with `chunksize` returns a context manager, to be used like so:

```
chunksize = 10 ** 6
with pd.read_csv(filename, chunksize=chunksize) as reader:
    for chunk in reader:
        process(chunk)

```

See [GH38225](https://github.com/pandas-dev/pandas/pull/38225)"
"What is the pythonic way of writing the following code?

```
extensions = ['.mp3','.avi']
file_name = 'test.mp3'

for extension in extensions:
    if file_name.endswith(extension):
        #do stuff

```

I have a vague memory that the explicit declaration of the `for` loop can be avoided and be written in the `if` condition. Is this true?","Though not widely known, [str.endswith](https://docs.python.org/3/library/stdtypes.html#str.endswith) also accepts a tuple. You don't need to loop.

```
>>> 'test.mp3'.endswith(('.mp3', '.avi'))
True

```"
"I have a list `l`:

```
l = [22, 13, 45, 50, 98, 69, 43, 44, 1]

```

For numbers above 45 inclusive, I would like to add 1; and for numbers less than it, 5.

I tried

```
[x+1 for x in l if x >= 45 else x+5]

```

But it gives me a syntax error. How can I achieve an `if` â€“ `else` like this in a list comprehension?","```
>>> l = [22, 13, 45, 50, 98, 69, 43, 44, 1]
>>> [x+1 if x >= 45 else x+5 for x in l]
[27, 18, 46, 51, 99, 70, 48, 49, 6]

```

Do-something if `<condition>`, else do-something else."
"I have a a dictionary mapping keywords to the repetition of the keyword, but I only want a list of distinct words so I wanted to count the number of keywords. Is there a way to count the number of keywords or is there another way I should look for distinct words?","```
len(yourdict.keys())

```

or just

```
len(yourdict)

```

If you like to count unique words in the file, you could just use [`set`](http://docs.python.org/library/sets.html#module-sets) and do like

```
len(set(open(yourdictfile).read().split()))

```"
"I recently discovered [Conda](http://conda.pydata.org/docs/index.html) after I was having trouble installing SciPy, specifically on a Heroku app that I am developing.

With Conda you create environments, very similar to what [virtualenv](https://virtualenv.readthedocs.org/en/latest/) does. My questions are:

1. If I use Conda will it replace the need for virtualenv? If not, how do I use the two together? Do I install virtualenv in Conda, or Conda in virtualenv?
2. Do I still need to use pip? If so, will I still be able to install packages with pip in an isolated environment?","1. Conda replaces virtualenv. In my opinion it is better. It is not limited to Python but can be used for other languages too. In my experience it provides a much smoother experience, especially for scientific packages. The first time I got MayaVi properly installed on Mac was with `conda`.
2. You can still use `pip`. In fact, `conda` installs `pip` in each new environment. It knows about pip-installed packages.

For example:

```
conda list

```

lists all installed packages in your current environment.
Conda-installed packages show up like this:

```
sphinx_rtd_theme          0.1.7                    py35_0    defaults

```

and the ones installed via `pip` have the `<pip>` marker:

```
wxpython-common           3.0.0.0                   <pip>

```"
"My python script uses subprocess to call a linux utility that is very noisy. I want to store all of the output to a log file and show some of it to the user. I thought the following would work, but the output doesn't show up in my application until the utility has produced a significant amount of output.

```
# fake_utility.py, just generates lots of output over time
import time
i = 0
    while True:
        print(hex(i)*512)
        i += 1
        time.sleep(0.5)

```

In the parent process:

```
import subprocess

proc = subprocess.Popen(['python', 'fake_utility.py'], stdout=subprocess.PIPE)
for line in proc.stdout:
    # the real code does filtering here
    print(""test:"", line.rstrip())

```

The behavior I really want is for the filter script to print each line as it is received from the subprocess, like [`tee`](http://unixhelp.ed.ac.uk/CGI/man-cgi?tee) does but within Python code.

What am I missing? Is this even possible?

---","I think the problem is with the statement `for line in proc.stdout`, which reads the entire input before iterating over it. The solution is to use `readline()` instead:

```
#filters output
import subprocess
proc = subprocess.Popen(['python','fake_utility.py'],stdout=subprocess.PIPE)
while True:
  line = proc.stdout.readline()
  if not line:
    break
  #the real code does filtering here
  print ""test:"", line.rstrip()

```

Of course you still have to deal with the subprocess' buffering.

Note: [according to the documentation](http://docs.python.org/library/stdtypes.html#file.next) the solution with an iterator should be equivalent to using `readline()`, except for the read-ahead buffer, but (or exactly because of this) the proposed change did produce different results for me (Python 2.5 on Windows XP)."
"**Original Question**

I am currently engaged in teaching my brother to program. He is a total beginner, but very smart. (And he actually wants to learn). I've noticed that some of our sessions have gotten bogged down in minor details, and I don't feel I've been very organized. (*But the answers to this post have helped a lot.*)

What can I do better to teach him effectively? Is there a logical order that I can use to run through concept by concept? Are there complexities I should avoid till later?

The language we are working with is [Python](http://www.python.org), but advice in any language is welcome.

---

**How to Help**

If you have good ones please add the following in your answer:

* Beginner Exercises and Project Ideas
* Resources for teaching beginners
* Screencasts / blog posts / free e-books
* Print books that are good for beginners

Please describe the resource *with a link to it* so I can take a look. I want everyone to know that I have definitely been using some of these ideas. Your submissions will be aggregated in this post.

---

**Online Resources** for teaching beginners:

* [A Gentle Introduction to Programming Using Python](http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-189January--IAP--2008/CourseHome/)
* [How to Think Like a Computer Scientist](http://openbookproject.net/thinkcs/python/english2e/index.html)
* [Alice: a 3d program for beginners](http://www.alice.org/)
* [Scratch (A system to develop programming skills)](http://scratch.mit.edu/)
* [How To Design Programs](http://www.htdp.org/)
* [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html)
* [Learn To Program](http://pine.fm/LearnToProgram/)
* [Robert Read's How To Be a Programmer](http://samizdat.mines.edu/howto/HowToBeAProgrammer.html)
* [Microsoft XNA](http://creators.xna.com/)
* [Spawning the Next Generation of Hackers](http://vodpod.com/watch/914464-inspirational-oscon-keynote)
* [*COMP1917 Higher Computing* lectures by Richard Buckland](http://deimos3.apple.com/WebObjects/Core.woa/Browse/unsw.edu.au.1504975442.01504975444) (requires iTunes)
* [Dive into Python](http://diveintopython.net/)
* [Python Wikibook](http://en.wikibooks.org/wiki/Programming:Python)
* [Project Euler](http://projecteuler.net/) - sample problems (mostly mathematical)
* [pygame](http://www.pygame.org/) - an easy python library for creating games
* [Invent Your Own Computer Games With Python](http://inventwithpython.com/IYOCGwP_book1.pdf)
* [Foundations of Programming](http://codebetter.com/blogs/karlseguin/archive/2008/06/24/foundations-of-programming-ebook.aspx) for a next step beyond basics.
* [Squeak by Example](http://www.iam.unibe.ch/~scg/SBE/)
* [Snake Wrangling For Kids](http://www.briggs.net.nz/log/writing/snake-wrangling-for-kids/) (It's not just for kids!)

---

**Recommended Print Books** for teaching beginners

* [Accelerated C++](http://www.acceleratedcpp.com/)
* [Python Programming for the Absolute Beginner](https://rads.stackoverflow.com/amzn/click/com/1598631128)
* [Code by Charles Petzold](https://rads.stackoverflow.com/amzn/click/com/0735611319)
* [Python Programming: An Introduction to Computer Science 2nd Edition](https://rads.stackoverflow.com/amzn/click/com/1590282418)","I've had to work with several beginner (never wrote a line of code) programmers, and I'll be doing an after school workshop with high school students this fall. This is the closest thing I've got to documentation. It's still a work in progress, but I hope it helps.

1) **FizzBuzz.** Start with command line programs. You can write some fun games, or tools, very quickly, and you learn all of the language features very quickly without having to learn the GUI tools first. These early apps should be simple enough that you won't need to use any real debugging tools to make them work.

If nothing else things like FizzBuzz are good projects. Your first few apps should not have to deal with DBs, file system, configuration, ect. These are concepts which just confuse most people, and when you're just learning the syntax and basic framework features you really don't need more complexity.

Some projects:

* Hello World!
* Take the year of my birth, and calculate my age (just (now - then) no month corrections). *(simple math, input, output)*
* Ask for a direction(Up, down, left, right), then tell the user their fate (fall in a hole, find a cake, ect). *(Boolean logic)*
* FizzBuzz, but count once every second. *(Loops, timers, and more logic)*
* Depending on their age some really like an app which calls the users a random insult at some interval. *(Loops, arrays, timers, and random if you make the interval random)*

2) **Simple Project** Once they have a good grasp of language features, you can start a project(simple, fun games work good.). You should try to have the first project be able to be completed within 6-12 hours. Don't spend time to architect it early. Let them design it even if it sucks. If it falls apart, talk about what happened and why it failed, then pick another topic and start again.

This is where you start introducing the debugging capabilities of your tools. Even if you can see the problem by reading the code you should teach them how to use the tools, and then show them how you could see it. That serves the dual purpose of teaching the debugging tools and teaching how to ID errors without tools.

Once, or if, the project gets functional you can use it to introduce refactoring tools. Its good if you can then expand the project with some simple features which you never planned for. This usually means refactoring and significant debugging, since very few people write even half decent code their first time.

Some projects:

* Hangman game
* Experimenting with robotics([Vex](http://www.vexlabs.com) and [Mindstorms](http://mindstorms.lego.com) are options)

3) **Real Project**  Start a real project which may take some time. Use proper source control, and make a point to have a schedule. Run this project like a real project, if nothing else its good experience having to deal with the tools.

Obviously you need to adjust this for each person. The most important thing I've found is to make even the first simple apps apply to what the person is interested in.

Some projects:

* Tetris
* Text file based blog engine
* More advanced robotics work"
"I've looked [through the docs](http://www.sqlalchemy.org/docs/orm/query.html) and I cant seem to find out how to do an OR query in SQLAlchemy. I just want to do this query.

```
SELECT address FROM addressbook WHERE city='boston' AND (lastname='bulger' OR firstname='whitey')

```

Should be something like

```
addr = session.query(AddressBook).filter(City == ""boston"").filter(????)

```","From the [tutorial](http://www.sqlalchemy.org/docs/orm/tutorial.html#common-filter-operators):

```
from sqlalchemy import or_
filter(or_(User.name == 'ed', User.name == 'wendy'))

```"
"I'm trying to install OpenCV for Python through [Anaconda](https://en.wikipedia.org/wiki/Anaconda_(Python_distribution)), but I can't seem to figure this out.

I tried

```
conda install opencv
conda install cv2

```

I also tried searching

```
conda search cv

```

No cigar. I ran across this which lists `opencv` as an included package:

<http://docs.continuum.io/anaconda/pkgs.html>

After running `conda info` I noticed my version is 3.4.1, but I couldn't seem to find any information about this version online. I'm pretty confused about this.

Am I missing something pretty obvious here? If `opencv` was available for a previous version of Anaconda, then why wouldn't it be available for the newer version? And why does that link only show me documentation for version 1.9.2?","You can install it using binstar:

```
conda install -c menpo opencv

```"
"I'm trying to use `matplotlib` to read in an RGB image and convert it to grayscale.

In matlab I use this:

```
img = rgb2gray(imread('image.png'));

```

In the [matplotlib tutorial](http://matplotlib.sourceforge.net/users/image_tutorial.html) they don't cover it. They just read in the image

```
import matplotlib.image as mpimg
img = mpimg.imread('image.png')

```

and then they slice the array, but that's not the same thing as converting RGB to grayscale from what I understand.

```
lum_img = img[:,:,0]

```

I find it hard to believe that numpy or matplotlib doesn't have a built-in function to convert from rgb to gray. Isn't this a common operation in image processing?

I wrote a very simple function that works with the image imported using `imread` in 5 minutes. It's horribly inefficient, but that's why I was hoping for a professional implementation built-in.

Sebastian has improved my function, but I'm still hoping to find the built-in one.

matlab's (NTSC/PAL) implementation:

```
import numpy as np

def rgb2gray(rgb):

    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b

    return gray

```","How about doing it with [Pillow](https://pillow.readthedocs.io/en/latest/):

```
from PIL import Image
img = Image.open('image.png').convert('L')
img.save('greyscale.png')

```

---

If an alpha (transparency) channel is present in the input image and should be preserved, use mode `LA`:

```
img = Image.open('image.png').convert('LA')

```

---

Using matplotlib and [the formula](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert)

```
Y' = 0.2989 R + 0.5870 G + 0.1140 B 

```

you could do:

```
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def rgb2gray(rgb):
    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])

img = mpimg.imread('image.png')     
gray = rgb2gray(img)    
plt.imshow(gray, cmap=plt.get_cmap('gray'), vmin=0, vmax=1)
plt.show()

```"
"In Django, what's the difference between the following two:

```
Article.objects.values_list('comment_id', flat=True).distinct()

```

versus:

```
Article.objects.values('comment_id').distinct()

```

My goal is to get a list of unique comment ids under each `Article`. I've read the documentation (and in fact have used both approaches). The results overtly seem similar.","The [`values()`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#values) method returns a QuerySet containing dictionaries:

```
<QuerySet [{'comment_id': 1}, {'comment_id': 2}]>

```

The [`values_list()`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#values-list) method returns a QuerySet containing tuples:

```
<QuerySet [(1,), (2,)]>

```

If you are using `values_list()` with a single field, you can use `flat=True` to return a QuerySet of single values instead of 1-tuples:

```
<QuerySet [1, 2]>

```"
"I need to verify if a list is a subset of another - a boolean return is all I seek.

Is testing equality on the smaller list after an intersection the fastest way to do this? Performance is of utmost importance given the number of datasets that need to be compared.

Adding further facts based on discussions:

1. Will either of the lists be the same for many tests? It does as one of them is a static lookup table.
2. Does it need to be a list? It does not - the static lookup table can be anything that performs best. The dynamic one is a dict from which we extract the keys to perform a static lookup on.

What would be the optimal solution given the scenario?","```
>>> a = [1, 3, 5]
>>> b = [1, 3, 5, 8]
>>> c = [3, 5, 9]
>>> set(a) <= set(b)
True
>>> set(c) <= set(b)
False

>>> a = ['yes', 'no', 'hmm']
>>> b = ['yes', 'no', 'hmm', 'well']
>>> c = ['sorry', 'no', 'hmm']
>>> 
>>> set(a) <= set(b)
True
>>> set(c) <= set(b)
False

```"
"I received some text that is encoded, but I don't know what charset was used. Is there a way to determine the encoding of a text file using Python? [How can I detect the encoding/codepage of a text file](https://stackoverflow.com/questions/90838/how-can-i-detect-the-encoding-codepage-of-a-text-file) deals with C#.","EDIT: chardet seems to be unmantained but most of the answer applies. Check <https://pypi.org/project/charset-normalizer/> for an alternative

Correctly detecting the encoding all times is **impossible**.

(From chardet FAQ:)

> However, some encodings are optimized
> for specific languages, and languages
> are not random. Some character
> sequences pop up all the time, while
> other sequences make no sense. A
> person fluent in English who opens a
> newspaper and finds “txzqJv 2!dasd0a
> QqdKjvz” will instantly recognize that
> that isn't English (even though it is
> composed entirely of English letters).
> By studying lots of “typical” text, a
> computer algorithm can simulate this
> kind of fluency and make an educated
> guess about a text's language.

There is the [chardet](http://pypi.python.org/pypi/chardet) library that uses that study to try to detect encoding. chardet is a port of the auto-detection code in Mozilla.

You can also use [UnicodeDammit](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#unicode-dammit). It will try the following methods:

* An encoding discovered in the document itself: for instance, in an XML declaration or (for HTML documents) an http-equiv META tag. If Beautiful Soup finds this kind of encoding within the document, it parses the document again from the beginning and gives the new encoding a try. The only exception is if you explicitly specified an encoding, and that encoding actually worked: then it will ignore any encoding it finds in the document.
* An encoding sniffed by looking at the first few bytes of the file. If an encoding is detected at this stage, it will be one of the UTF-\* encodings, EBCDIC, or ASCII.
* An encoding sniffed by the [chardet](http://pypi.python.org/pypi/chardet) library, if you have it installed.
* UTF-8
* Windows-1252"
"I have a dataframe like this:

```
        0          1     2
0   354.7      April   4.0
1    55.4     August   8.0
2   176.5   December  12.0
3    95.5   February   2.0
4    85.6    January   1.0
5     152       July   7.0
6   238.7       June   6.0
7   104.8      March   3.0
8   283.5        May   5.0
9   278.8   November  11.0
10  249.6    October  10.0
11  212.7  September   9.0

```

As you can see, months are not in calendar order. So I created a second column to get the month number corresponding to each month (1-12). From there, how can I sort this dataframe according to calendar months' order?","Use [`sort_values`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) to sort the df by a specific column's values:

```
In [18]:
df.sort_values('2')

Out[18]:
        0          1     2
4    85.6    January   1.0
3    95.5   February   2.0
7   104.8      March   3.0
0   354.7      April   4.0
8   283.5        May   5.0
6   238.7       June   6.0
5   152.0       July   7.0
1    55.4     August   8.0
11  212.7  September   9.0
10  249.6    October  10.0
9   278.8   November  11.0
2   176.5   December  12.0

```

If you want to sort by two columns, pass a list of column labels to `sort_values` with the column labels ordered according to sort priority. If you use `df.sort_values(['2', '0'])`, the result would be sorted by column `2` then column `0`. Granted, this does not really make sense for this example because each value in `df['2']` is unique."
"I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person.

How can I ""join"" together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name?

The `join()` function in pandas specifies that I need a multiindex, but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index.","[Zero's answer](https://stackoverflow.com/a/23671390/366309) is basically a `reduce` operation. If I had more than a handful of dataframes, I'd put them in a list like this (generated via list comprehensions or loops or whatnot):

```
dfs = [df0, df1, df2, ..., dfN]

```

Assuming they have a common column, like `name` in your example, I'd do the following:

```
import functools as ft
df_final = ft.reduce(lambda left, right: pd.merge(left, right, on='name'), dfs)

```

That way, your code should work with whatever number of dataframes you want to merge."
"I downloaded `Graphviz 2.38` MSI version and installed under folder `C:\Python34`, then I run `pip install Graphviz`, everything went well. In system's path I added `C:\Python34\bin`. When I tried to run a test script, in line `filename=dot.render(filename='test')`, I got a message

```
 RuntimeError: failed to execute ['dot', '-Tpdf', '-O', 'test'], make sure the Graphviz executables are on your systems' path

```

I tried to put `""C:\Python34\bin\dot.exe""` in system's path, but it didn't work, and I even created a new environment variable `""GRAPHVIZ_DOT""` with value `""C:\Python34\bin\dot.exe""`, still not working. I tried to uninstall Graphviz and `pip uninstall graphviz`, then reinstall it and pip install again, but nothing works.

The whole traceback message is:

```
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\graphviz\files.py"", line 220, in render
    proc = subprocess.Popen(cmd, startupinfo=STARTUPINFO)
  File ""C:\Python34\lib\subprocess.py"", line 859, in __init__
    restore_signals, start_new_session)
  File ""C:\Python34\lib\subprocess.py"", line 1112, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Documents\Kissmetrics\curves and lines\eventNodes.py"", line 56, in <module>
    filename=dot.render(filename='test')
  File ""C:\Python34\lib\site-packages\graphviz\files.py"", line 225, in render
    'are on your systems\' path' % cmd)
RuntimeError: failed to execute ['dot', '-Tpdf', '-O', 'test'], make sure the Graphviz executables are on your systems' path

```

Does anybody have any experience with it?","You should install the graphviz package in your system (not just the python package). On **Ubuntu** you should try:

```
sudo apt-get install graphviz

```"
"I have this code using Pandas in Python:

```
all_data = {}
for ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:
    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')

prices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  
returns = prices.pct_change()

```

I know I can run a regression like this:

```
regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()

```

but **how can I do this for each column in the dataframe**? Specifically, how can I **iterate over columns**, in order to run the regression on each?

Specifically, I want to regress each other ticker symbol (FIUIX, FSAIX and FSAVX) on FSTMX, and store the residuals for each regression.

I've tried various versions of the following, but nothing I've tried gives the desired result:

```
resids = {}
for k in returns.keys():
    reg = sm.OLS(returns[k],returns.FSTMX).fit()
    resids[k] = reg.resid

```

Is there something wrong with the `returns[k]` part of the code? How can I use the `k` value to access a column? Or else is there a simpler approach?","Old answer:

```
for column in df:
    print(df[column])

```

The previous answer still works, but was added around the time of pandas 0.16.0. Better versions are available.

Now you can do:

```
for series_name, series in df.items():
    print(series_name)
    print(series)

```"
"I want to replace whitespace with underscore in a string to create nice URLs. So that for example:

```
""This should be connected"" 

```

Should become

```
""This_should_be_connected"" 

```

I am using Python with Django. Can this be solved using regular expressions?","You don't need regular expressions. Python has a built-in string method that does what you need:

```
mystring.replace("" "", ""_"")

```"
"I've got a python project with a configuration file in the project root.
The configuration file needs to be accessed in a few different files throughout the project.

So it looks something like: `<ROOT>/configuration.conf`
`<ROOT>/A/a.py`, `<ROOT>/A/B/b.py` (when b,a.py access the configuration file).

What's the best / easiest way to get the path to the project root and the configuration file without depending on which file inside the project I'm in? i.e without using `../../`? It's okay to assume that we know the project root's name.","You can do this how Django does it: **define a variable to the Project Root from a file that is in the top-level of the project.** For example, if this is what your project structure looks like:

```
project/
    configuration.conf
    definitions.py
    main.py
    utils.py

```

In `definitions.py` you can define (this requires `import os`):

```
ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) # This is your Project Root

```

Thus, with the **Project Root** known, you can **create a variable that points to the location of the configuration** (this can be defined anywhere, but a logical place would be to put it in a location where constants are defined - e.g. `definitions.py`):

```
CONFIG_PATH = os.path.join(ROOT_DIR, 'configuration.conf')  # requires `import os`

```

Then, you can easily access the constant (in any of the other files) with the import statement (e.g. in `utils.py`): `from definitions import CONFIG_PATH`."
"I'd really like to be able to print out valid SQL for my application, including values, rather than bind parameters, but it's not obvious how to do this in SQLAlchemy (by design, I'm fairly sure).

Has anyone solved this problem in a general way?","In the vast majority of cases, the ""stringification"" of a SQLAlchemy statement or query is as simple as:

```
print(str(statement))

```

This applies both to an ORM `Query` as well as any `select()` or other statement.

**Note**: the following detailed answer is being maintained on the [sqlalchemy documentation](http://docs.sqlalchemy.org/en/latest/faq/sqlexpressions.html).

To get the statement as compiled to a specific dialect or engine, if the statement itself is not already bound to one you can pass this in to [compile()](http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#sqlalchemy.sql.expression.ClauseElement.compile):

```
print(statement.compile(someengine))

```

or without an engine:

```
from sqlalchemy.dialects import postgresql
print(statement.compile(dialect=postgresql.dialect()))

```

When given an ORM `Query` object, in order to get at the `compile()` method we only need access the [.statement](http://docs.sqlalchemy.org/en/latest/orm/query.html?highlight=query.statement#sqlalchemy.orm.query.Query.statement) accessor first:

```
statement = query.statement
print(statement.compile(someengine))

```

with regards to the original stipulation that bound parameters are to be ""inlined"" into the final string, the challenge here is that SQLAlchemy normally is not tasked with this, as this is handled appropriately by the Python DBAPI, not to mention bypassing bound parameters is probably the most widely exploited security holes in modern web applications. SQLAlchemy has limited ability to do this stringification in certain circumstances such as that of emitting DDL. In order to access this functionality one can use the 'literal\_binds' flag, passed to `compile_kwargs`:

```
from sqlalchemy.sql import table, column, select

t = table('t', column('x'))

s = select([t]).where(t.c.x == 5)

print(s.compile(compile_kwargs={""literal_binds"": True}))

```

the above approach has the caveats that it is only supported for basic
types, such as ints and strings, and furthermore if a `bindparam`
without a pre-set value is used directly, it won't be able to
stringify that either.

To support inline literal rendering for types not supported, implement
a `TypeDecorator` for the target type which includes a
`TypeDecorator.process_literal_param` method:

```
from sqlalchemy import TypeDecorator, Integer


class MyFancyType(TypeDecorator):
    impl = Integer

    def process_literal_param(self, value, dialect):
        return ""my_fancy_formatting(%s)"" % value

from sqlalchemy import Table, Column, MetaData

tab = Table('mytable', MetaData(), Column('x', MyFancyType()))

print(
    tab.select().where(tab.c.x > 5).compile(
        compile_kwargs={""literal_binds"": True})
)

```

producing output like:

```
SELECT mytable.x
FROM mytable
WHERE mytable.x > my_fancy_formatting(5)

```"
"I'm trying to get the content of [App Store > Business](https://itunes.apple.com/in/genre/ios-business/id6000?mt=8):

```
import requests
from lxml import html

page = requests.get(""https://itunes.apple.com/in/genre/ios-business/id6000?mt=8"")
tree = html.fromstring(page.text)

flist = []
plist = []
for i in range(0, 100):
    app = tree.xpath(""//div[@class='column first']/ul/li/a/@href"")
    ap = app[0]
    page1 = requests.get(ap)

```

When I try the `range` with `(0,2)` it works, but when I put the `range` in `100`s it shows this error:

```
Traceback (most recent call last):
  File ""/home/preetham/Desktop/eg.py"", line 17, in <module>
    page1 = requests.get(ap)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 55, in get
    return request('get', url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 383, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 378, in send
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)

```","Just use `requests` features:

```
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


session = requests.Session()
retry = Retry(connect=3, backoff_factor=0.5)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
session.mount('https://', adapter)

session.get(url)

```

This will `GET` the URL and retry 3 times in case of `requests.exceptions.ConnectionError`. `backoff_factor` will help to apply delays between attempts to avoid failing again in case of periodic request quota.

Take a look at [`urllib3.util.retry.Retry`](http://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry), it has many options to simplify retries."
"```
mydict = {""key1"":""value1"", ""key2"":""value2""}

```

The regular way to lookup a dictionary value in a Django template is `{{ mydict.key1 }}`, `{{ mydict.key2 }}`. What if the key is a loop variable? ie:

```
{% for item in list %} # where item has an attribute NAME
  {{ mydict.item.NAME }} # I want to look up mydict[item.NAME]
{% endfor %}

```

`mydict.item.NAME` fails. How to fix this?","Write a [custom template filter](https://docs.djangoproject.com/en/dev/howto/custom-template-tags/#writing-custom-template-filters):

```
from django.template.defaulttags import register
...
@register.filter
def get_item(dictionary, key):
    return dictionary.get(key)

```

(I use `.get` so that if the key is absent, it returns none. If you do `dictionary[key]` it will raise a `KeyError` then.)

usage:

```
{{ mydict|get_item:item.NAME }}

```"
"I have a situation wherein sometimes when I read a `csv` from `df` I get an unwanted index-like column named `unnamed:0`.

`file.csv`

```
,A,B,C
0,1,2,3
1,4,5,6
2,7,8,9

```

The CSV is read with this:

```
pd.read_csv('file.csv')

   Unnamed: 0  A  B  C
0           0  1  2  3
1           1  4  5  6
2           2  7  8  9

```

This is very annoying! Does anyone have an idea on how to get rid of this?","It's the index column, pass `pd.to_csv(..., index=False)` to not write out an unnamed index column in the first place, see the [`to_csv()` docs](http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.DataFrame.to_csv.html).

Example:

```
In [37]:
df = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))
pd.read_csv(io.StringIO(df.to_csv()))

Out[37]:
   Unnamed: 0         a         b         c
0           0  0.109066 -1.112704 -0.545209
1           1  0.447114  1.525341  0.317252
2           2  0.507495  0.137863  0.886283
3           3  1.452867  1.888363  1.168101
4           4  0.901371 -0.704805  0.088335

```

compare with:

```
In [38]:
pd.read_csv(io.StringIO(df.to_csv(index=False)))

Out[38]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335

```

You could also optionally tell `read_csv` that the first column is the index column by passing `index_col=0`:

```
In [40]:
pd.read_csv(io.StringIO(df.to_csv()), index_col=0)

Out[40]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335

```"
"I have a pandas dataframe. I want to print the unique values of one of its columns in ascending order. This is how I am doing it:

```
import pandas as pd
df = pd.DataFrame({'A':[1,1,3,2,6,2,8]})
a = df['A'].unique()
print a.sort()

```

The problem is that I am getting a `None` for the output.","[`sorted(iterable)`](https://docs.python.org/3/library/functions.html#sorted): Return a new sorted list from the items in *iterable*.
  
  
**CODE**

```
import pandas as pd
df = pd.DataFrame({'A':[1,1,3,2,6,2,8]})
a = df['A'].unique()
print(sorted(a))

```

**OUTPUT**

```
[1, 2, 3, 6, 8]

```"
"In Python, calling e.g. `temp = open(filename,'r').readlines()` results in a list in which each element is a line from the file. However, these strings have a newline character at the end, which I don't want.

How can I get the data without the newlines?","You can read the whole file and split lines using [`str.splitlines`](https://docs.python.org/library/stdtypes.html#str.splitlines):

```
temp = file.read().splitlines()

```

Or you can strip the newline by hand:

```
temp = [line[:-1] for line in file]

```

**Note:** this last solution only works if the file ends with a newline, otherwise the last line will lose a character.

This assumption is true in most cases (especially for files created by text editors, which often *do* add an ending newline anyway).

If you want to avoid this you can add a newline at the end of file:

```
with open(the_file, 'r+') as f:
    f.seek(-1, 2)  # go at the end of the file
    if f.read(1) != '\n':
        # add missing newline if not already present
        f.write('\n')
        f.flush()
        f.seek(0)
    lines = [line[:-1] for line in f]

```

Or a simpler alternative is to `strip` the newline instead:

```
[line.rstrip('\n') for line in file]

```

Or even, although pretty unreadable:

```
[line[:-(line[-1] == '\n') or len(line)+1] for line in file]

```

Which exploits the fact that the return value of `or` isn't a boolean, but the object that was evaluated true or false.

---

The `readlines` method is actually equivalent to:

```
def readlines(self):
    lines = []
    for line in iter(self.readline, ''):
        lines.append(line)
    return lines

# or equivalently

def readlines(self):
    lines = []
    while True:
        line = self.readline()
        if not line:
            break
        lines.append(line)
    return lines

```

Since `readline()` keeps the newline also `readlines()` keeps it.

**Note:** for symmetry to `readlines()` the [`writelines()`](https://docs.python.org/library/io.html#io.IOBase.writelines) method does *not* add ending newlines, so `f2.writelines(f.readlines())` produces an exact copy of `f` in `f2`."
"I am trying my very first formal python program using Threading and Multiprocessing on a windows machine. I am unable to launch the processes though, with python giving the following message. The thing is, I am not launching my threads in the **main** module. The threads are handled in a separate module inside a class.

**EDIT**: By the way this code runs fine on ubuntu. Not quite on windows

```
RuntimeError: 
            Attempt to start a new process before the current process
            has finished its bootstrapping phase.
            This probably means that you are on Windows and you have
            forgotten to use the proper idiom in the main module:
                if __name__ == '__main__':
                    freeze_support()
                    ...
            The ""freeze_support()"" line can be omitted if the program
            is not going to be frozen to produce a Windows executable.

```

My original code is pretty long, but I was able to reproduce the error in an abridged version of the code. It is split in two files, the first is the main module and does very little other than import the module which handles processes/threads and calls a method. The second module is where the meat of the code is.

---

**testMain.py:**

```
import parallelTestModule

extractor = parallelTestModule.ParallelExtractor()
extractor.runInParallel(numProcesses=2, numThreads=4)

```

---

**parallelTestModule.py:**

```
import multiprocessing
from multiprocessing import Process
import threading

class ThreadRunner(threading.Thread):
    """""" This class represents a single instance of a running thread""""""
    def __init__(self, name):
        threading.Thread.__init__(self)
        self.name = name
    def run(self):
        print self.name,'\n'

class ProcessRunner:
    """""" This class represents a single instance of a running process """"""
    def runp(self, pid, numThreads):
        mythreads = []
        for tid in range(numThreads):
            name = ""Proc-""+str(pid)+""-Thread-""+str(tid)
            th = ThreadRunner(name)
            mythreads.append(th) 
        for i in mythreads:
            i.start()
        for i in mythreads:
            i.join()

class ParallelExtractor:    
    def runInParallel(self, numProcesses, numThreads):
        myprocs = []
        prunner = ProcessRunner()
        for pid in range(numProcesses):
            pr = Process(target=prunner.runp, args=(pid, numThreads)) 
            myprocs.append(pr) 
#        if __name__ == 'parallelTestModule':    #This didnt work
#        if __name__ == '__main__':              #This obviously doesnt work
#        multiprocessing.freeze_support()        #added after seeing error to no avail
        for i in myprocs:
            i.start()

        for i in myprocs:
            i.join()

```","On Windows the subprocesses will import (i.e. execute) the main module at start. You need to insert an `if __name__ == '__main__':` guard in the main module to avoid creating subprocesses recursively.

Modified `testMain.py`:

```
import parallelTestModule

if __name__ == '__main__':    
    extractor = parallelTestModule.ParallelExtractor()
    extractor.runInParallel(numProcesses=2, numThreads=4)

```"
"Is there a built-in nCr (n choose r) function included in the Python `math` library like the one shown below?

[![n choose k formula](https://i.sstatic.net/KmC1gm.jpg)](https://i.sstatic.net/KmC1g.jpg)

I understand that the computation can be programmed, but I thought I'd check to see if it's built-in before I do.","On Python **3.8**+, use [`math.comb`](https://docs.python.org/3/library/math.html#math.comb):

```
>>> from math import comb
>>> comb(10, 3)
120

```

For older versions of Python, you can use the following program:

```
import operator as op
from functools import reduce

def ncr(n, r):
    r = min(r, n-r)
    numer = reduce(op.mul, range(n, n-r, -1), 1)
    denom = reduce(op.mul, range(1, r+1), 1)
    return numer // denom  # or / in Python 2

```"
"Mock has a [helpful `assert_called_with()` method](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called_with). However, as far as I understand this only checks the *last* call to a method.  
If I have code that calls the mocked method 3 times successively, each time with different parameters, how can I assert these 3 calls with their specific parameters?","`assert_has_calls` is another approach to this problem.

From the docs:

> **assert\_has\_calls** *(calls, any\_order=False)*
>
> assert the mock has been
> called with the specified calls. The mock\_calls list is checked for
> the calls.
>
> If any\_order is False (the default) then the calls must be sequential.
> There can be extra calls before or after the specified calls.
>
> If any\_order is True then the calls can be in any order, but they must
> all appear in mock\_calls.

Example:

```
>>> from unittest.mock import call, Mock
>>> mock = Mock(return_value=None)
>>> mock(1)
>>> mock(2)
>>> mock(3)
>>> mock(4)
>>> calls = [call(2), call(3)]
>>> mock.assert_has_calls(calls)
>>> calls = [call(4), call(2), call(3)]
>>> mock.assert_has_calls(calls, any_order=True)

```

Source: <https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_has_calls>"
"I want to inherit from a class in a file that lies in a directory above the current one.

Is it possible to relatively import that file?","`from ..subpkg2 import mod`

Per the Python docs: When inside a package hierarchy, use two dots, as the [import statement](http://docs.python.org/reference/simple_stmts.html#the-import-statement) doc says:

> When specifying what module to import you do not have to specify the absolute name of the module. When a module or package is contained within another package it is possible to make a relative import within the same top package without having to mention the package name. By using leading dots in the specified module or package after `from` you can specify how high to traverse up the current package hierarchy without specifying exact names. One leading dot means the current package where the module making the import exists. **Two dots means up one package level**. Three dots is up two levels, etc. So if you execute `from . import mod` from a module in the `pkg` package then you will end up importing `pkg.mod`. If you execute `from ..subpkg2 import mod` from within `pkg.subpkg1` you will import `pkg.subpkg2.mod`. The specification for relative imports is contained within [PEP 328](http://www.python.org/dev/peps/pep-0328/).

[PEP 328](http://www.python.org/dev/peps/pep-0328/) deals with absolute/relative imports."
"I am trying to append a dictionary to a DataFrame object, but I get the following error:

> AttributeError: 'DataFrame' object has no attribute 'append'

As far as I know, DataFrame does have the method ""append"".

Code snippet:

```
df = pd.DataFrame(df).append(new_row, ignore_index=True)

```

I was expecting the dictionary `new_row` to be added as a new row.

How can I fix it?","As of pandas 2.0, `append` (previously deprecated) [was removed](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes).

You need to use [`concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) instead (for most applications):

```
df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

```

[As noted by @cottontail](https://stackoverflow.com/a/76132725/16343464), it's also possible to use [`loc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html), although this only works if the new index is not already present in the DataFrame (typically, this will be the case if the index is a `RangeIndex`:

```
df.loc[len(df)] = new_row # only use with a RangeIndex!

```

#### Why was it removed?

We frequently see new users of [pandas](/questions/tagged/pandas ""show questions tagged 'pandas'"") try to code like they would do it in pure Python. They use [`iterrows`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html) to access items in a loop (see [here](/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas) why you shouldn't), or `append` in a way that is similar to python [`list.append`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists).

However, as noted in pandas' issue [#35407](https://github.com/pandas-dev/pandas/issues/35407), **pandas's `append` and `list.append` are really not the same thing**. `list.append` is in place, while pandas's `append` creates a new DataFrame:

> I think that we should deprecate Series.append and DataFrame.append.
> They're making an analogy to list.append, but it's a poor analogy
> since the behavior isn't (and can't be) in place. The data for the
> index and values needs to be copied to create the result.
>
> These are also apparently popular methods. DataFrame.append is around
> the 10th most visited page in our API docs.
>
> Unless I'm mistaken, users are always better off building up a list of
> values and passing them to the constructor, or building up a list of
> NDFrames followed by a single concat.

As a consequence, while `list.append` is [amortized O(1)](/questions/33044883/why-is-the-time-complexity-of-pythons-list-append-method-o1) at each step of the loop, **pandas' `append` is `O(n)`, making it inefficient when repeated insertion is performed**.

#### What if I need to repeat the process?

Using `append` or `concat` repeatedly is not a good idea (this has a [quadratic behavior](https://stackoverflow.com/questions/36489576/why-does-concatenation-of-dataframes-get-exponentially-slower) as it creates a new DataFrame for each step).

In such case, the new items should be collected in a list, and at the end of the loop converted to `DataFrame` and eventually concatenated to the original `DataFrame`.

```
lst = []

for new_row in items_generation_logic:
    lst.append(new_row)

# create extension
df_extended = pd.DataFrame(lst, columns=['A', 'B', 'C'])
# or columns=df.columns if identical columns

# concatenate to original
out = pd.concat([df, df_extended])

```"
"Is there a list somewhere of recommendations of different Python-based REST frameworks for use on the serverside to write your own RESTful APIs? Preferably with pros and cons.

Please feel free to add recommendations here. :)","Something to be careful about when designing a RESTful API is the conflation of GET and POST, as if they were the same thing. It's easy to make this mistake with [Django](http://www.djangoproject.com/)'s [function-based views](https://docs.djangoproject.com/en/dev/topics/http/views/) and [CherryPy](http://www.cherrypy.org/)'s default dispatcher, although both frameworks now provide a way around this problem ([class-based views](https://docs.djangoproject.com/en/dev/topics/class-based-views/) and [MethodDispatcher](http://docs.cherrypy.org/dev/refman/_cpdispatch.html#cherrypy._cpdispatch.MethodDispatcher), respectively).

[HTTP-verbs are very important](http://en.wikipedia.org/wiki/Representational_State_Transfer#RESTful_web_services) in REST, and unless you're very careful about this, you'll end up falling into a [REST anti-pattern](http://www.infoq.com/articles/rest-anti-patterns).

Some frameworks that get it right are [web.py](http://webpy.org/), [Flask](http://flask.pocoo.org) and [Bottle](http://bottlepy.org). When combined with the [mimerender](https://github.com/martinblech/mimerender) library (full disclosure: I wrote it), they allow you to write nice RESTful webservices:

```
import web
import json
from mimerender import mimerender

render_xml = lambda message: '<message>%s</message>'%message
render_json = lambda **args: json.dumps(args)
render_html = lambda message: '<html><body>%s</body></html>'%message
render_txt = lambda message: message

urls = (
    '/(.*)', 'greet'
)
app = web.application(urls, globals())

class greet:
    @mimerender(
        default = 'html',
        html = render_html,
        xml  = render_xml,
        json = render_json,
        txt  = render_txt
    )
    def GET(self, name):
        if not name: 
            name = 'world'
        return {'message': 'Hello, ' + name + '!'}

if __name__ == ""__main__"":
    app.run()

```

The service's logic is implemented only once, and the correct representation selection (Accept header) + dispatch to the proper render function (or template) is done in a tidy, transparent way.

```
$ curl localhost:8080/x
<html><body>Hello, x!</body></html>

$ curl -H ""Accept: application/html"" localhost:8080/x
<html><body>Hello, x!</body></html>

$ curl -H ""Accept: application/xml"" localhost:8080/x
<message>Hello, x!</message>

$ curl -H ""Accept: application/json"" localhost:8080/x
{'message':'Hello, x!'}

$ curl -H ""Accept: text/plain"" localhost:8080/x
Hello, x!

```

**Update (April 2012)**: added information about Django's class-based views, CherryPy's MethodDispatcher and Flask and Bottle frameworks. Neither existed back when the question was asked."
"I want to do something like:

```
foo = {
    'foo': 1,
    'zip': 2,
    'zam': 3,
    'bar': 4
}

if (""foo"", ""bar"") in foo:
    #do stuff

```

How do I check whether both `foo` and `bar` are in dict `foo`?","Well, you could do this:

```
>>> if all(k in foo for k in (""foo"",""bar"")):
...     print ""They're there!""
...
They're there!

```"
"Without using `groupby` how would I filter out data without `NaN`?

Let say I have a matrix where customers will fill in `'N/A','n/a'` or any of its variations and others leave it blank:

```
import pandas as pd
import numpy as np


df = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],
                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],
                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})

nbs = df['name'].str.extract('^(N/A|NA|na|n/a)')
nms=df[(df['name'] != nbs) ]

```

output:

```
>>> nms
  movie    name  rating
0   thg    John       3
1   thg     NaN       4
3   mol  Graham     NaN
4   lob     NaN     NaN
5   lob     NaN     NaN

```

How would I filter out `NaN` values so I can get results to work with like this:

```
  movie    name  rating
0   thg    John       3
3   mol  Graham     NaN

```

I am guessing I need something like `~np.isnan` but the tilda does not work with strings.","**Simplest of all solutions:**

```
filtered_df = df[df['name'].notnull()]

```

Thus, it filters out only rows that doesn't have NaN values in 'name' column.

*For multiple columns:*

```
filtered_df = df[df[['name', 'country', 'region']].notnull().all(1)]

```"
"When I render a page using the Django template renderer, I can pass in a dictionary variable containing various values to manipulate them in the page using `{{ myVar }}`.

Is there a way to access the same variable in JavaScript, `<script></script>` (perhaps using the [DOM](https://en.wikipedia.org/wiki/Document_Object_Model); I don't know how Django makes the variables accessible)? I want to be able to look up details using an [Ajax](https://en.wikipedia.org/wiki/Ajax_%28programming%29) lookup based on the values contained in the variables passed in.","The `{{variable}}` is substituted directly into the HTML. Do a view source; it isn't a ""variable"" or anything like it. It's just rendered text.

Having said that, you can put this kind of substitution into your JavaScript.

```
<script type=""text/javascript"">
    var a = ""{{someDjangoVariable}}"";
</script>

```

This gives you ""dynamic"" JavaScript code."
"My editor warns me when I compare `my_var == None`, but no warning when I use `my_var is None`.

I did a test in the Python shell and determined both are valid syntax, but my editor seems to be saying that `my_var is None` is preferred.

Is this the case, and if so, why?","### Summary:

Use `is` when you want to check against an object's *identity* (e.g. checking to see if `var` is `None`). Use `==` when you want to check *equality* (e.g. Is `var` equal to `3`?).

### Explanation:

You can have custom classes where `my_var == None` will return `True`

e.g:

```
class Negator(object):
    def __eq__(self,other):
        return not other

thing = Negator()
print thing == None    #True
print thing is None    #False

```

`is` checks for object *identity*. There is only 1 object `None`, so when you do `my_var is None`, you're checking whether they actually are the same object (not just *equivalent* objects)

In other words, `==` is a check for equivalence (which is defined from object to object) whereas `is` checks for object identity:

```
lst = [1,2,3]
lst == lst[:]  # This is True since the lists are ""equivalent""
lst is lst[:]  # This is False since they're actually different objects

```"
"What is the difference between the `search()` and `match()` functions in the Python `re` module?

I've read the [Python 2 documentation](https://docs.python.org/2/library/re.html?highlight=matching%20searching#search-vs-match) ([Python 3 documentation](https://docs.python.org/3/library/re.html#search-vs-match)), but I never seem to remember it.","`re.match` is anchored at the beginning of the string. That has nothing to do with newlines, so it is not the same as using `^` in the pattern.

As the [re.match documentation](http://docs.python.org/2/library/re.html#re.match) says:

> If zero or more characters at the
> **beginning of string** match the regular expression pattern, return a
> corresponding `MatchObject` instance.
> Return `None` if the string does not
> match the pattern; note that this is
> different from a zero-length match.
>
> Note: If you want to locate a match
> anywhere in string, use `search()`
> instead.

`re.search` searches the entire string, as [the documentation says](http://docs.python.org/2/library/re.html#re.search):

> **Scan through string** looking for a
> location where the regular expression
> pattern produces a match, and return a
> corresponding `MatchObject` instance.
> Return `None` if no position in the
> string matches the pattern; note that
> this is different from finding a
> zero-length match at some point in the
> string.

So if you need to match at the beginning of the string, or to match the entire string use `match`. It is faster. Otherwise use `search`.

The documentation has a [specific section for `match` vs. `search`](http://docs.python.org/2/library/re.html#search-vs-match) that also covers multiline strings:

> Python offers two different primitive
> operations based on regular
> expressions: `match` checks for a match
> **only at the beginning** of the string,
> while `search` checks for a match
> **anywhere** in the string (this is what
> Perl does by default).
>
> Note that `match` may differ from `search`
> even when using a regular expression
> beginning with `'^'`: `'^'` matches only
> at the start of the string, or in
> `MULTILINE` mode also immediately
> following a newline. The “`match`”
> operation succeeds *only if the pattern
> matches at the **start** of the string*
> regardless of mode, or at the starting
> position given by the optional `pos`
> argument regardless of whether a
> newline precedes it.

Now, enough talk. Time to see some example code:

```
# example code:
string_with_newlines = """"""something
someotherthing""""""

import re

print re.match('some', string_with_newlines) # matches
print re.match('someother', 
               string_with_newlines) # won't match
print re.match('^someother', string_with_newlines, 
               re.MULTILINE) # also won't match
print re.search('someother', 
                string_with_newlines) # finds something
print re.search('^someother', string_with_newlines, 
                re.MULTILINE) # also finds something

m = re.compile('thing$', re.MULTILINE)

print m.match(string_with_newlines) # no match
print m.match(string_with_newlines, pos=4) # matches
print m.search(string_with_newlines, 
               re.MULTILINE) # also matches

```"
"I want to call a C library from a Python application. I don't want to wrap the whole API, only the functions and datatypes that are relevant to my case. As I see it, I have three choices:

1. Create an actual extension module in C. Probably overkill, and I'd also like to avoid the overhead of learning extension writing.
2. Use [Cython](http://www.cython.org/) to expose the relevant parts from the C library to Python.
3. Do the whole thing in Python, using [`ctypes`](http://docs.python.org/library/ctypes.html) to communicate with the external library.

I'm not sure whether 2) or 3) is the better choice. The advantage of 3) is that `ctypes` is part of the standard library, and the resulting code would be pure Python – although I'm not sure how big that advantage actually is.

Are there more advantages / disadvantages with either choice? Which approach do you recommend?

---

**Edit:** Thanks for all your answers, they provide a good resource for anyone looking to do something similar. The decision, of course, is still to be made for the single case—there's no one ""This is the right thing"" sort of answer. For my own case, I'll probably go with ctypes, but I'm also looking forward to trying out Cython in some other project.

With there being no single true answer, accepting one is somewhat arbitrary; I chose FogleBird's answer as it provides some good insight into ctypes and it currently also is the highest-voted answer. However, I suggest to read all the answers to get a good overview.

Thanks again.","Warning: a Cython core developer's opinion ahead.

I almost always recommend Cython over ctypes. The reason is that it has a much smoother upgrade path. If you use ctypes, many things will be simple at first, and it's certainly cool to write your FFI code in plain Python, without compilation, build dependencies and all that. However, at some point, you will almost certainly find that you have to call into your C library a lot, either in a loop or in a longer series of interdependent calls, and you would like to speed that up. That's the point where you'll notice that you can't do that with ctypes. Or, when you need callback functions and you find that your Python callback code becomes a bottleneck, you'd like to speed it up and/or move it down into C as well. Again, you cannot do that with ctypes. So you have to switch languages at that point and start rewriting parts of your code, potentially reverse engineering your Python/ctypes code into plain C, thus spoiling the whole benefit of writing your code in plain Python in the first place.

With Cython, OTOH, you're completely free to make the wrapping and calling code as thin or thick as you want. You can start with simple calls into your C code from regular Python code, and Cython will translate them into native C calls, without any additional calling overhead, and with an extremely low conversion overhead for Python parameters. When you notice that you need even more performance at some point where you are making too many expensive calls into your C library, you can start annotating your surrounding Python code with static types and let Cython optimise it straight down into C for you. Or, you can start rewriting parts of your C code in Cython in order to avoid calls and to specialise and tighten your loops algorithmically. And if you need a fast callback, just write a function with the appropriate signature and pass it into the C callback registry directly. Again, no overhead, and it gives you plain C calling performance. And in the much less likely case that you really cannot get your code fast enough in Cython, you can still consider rewriting the truly critical parts of it in C (or C++ or Fortran) and call it from your Cython code naturally and natively. But then, this really becomes the last resort instead of the only option.

So, ctypes is nice to do simple things and to quickly get something running. However, as soon as things start to grow, you'll most likely come to the point where you notice that you'd better used Cython right from the start."
"Django has some good automatic serialization of ORM models returned from DB to JSON format.

How to serialize SQLAlchemy query result to JSON format?

I tried `jsonpickle.encode` but it encodes query object itself.
I tried `json.dumps(items)` but it returns

```
TypeError: <Product('3', 'some name', 'some desc')> is not JSON serializable

```

Is it really so hard to serialize SQLAlchemy ORM objects to JSON /XML? Isn't there any default serializer for it? It's very common task to serialize ORM query results nowadays.

What I need is just to return JSON or XML data representation of SQLAlchemy query result.

SQLAlchemy objects query result in JSON/XML format is needed to be used in javascript datagird (JQGrid <http://www.trirand.com/blog/>)","You could just output your object as a dictionary:

```
class User:
    def as_dict(self):
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}

```

And then you use `User.as_dict()` to serialize your object.

As explained in [How to convert SQLAlchemy row object to a Python dict?](https://stackoverflow.com/questions/1958219/convert-sqlalchemy-row-object-to-python-dict)"
"When I run the program, Pandas gives 'Future warning' like below every time.

```
D:\Python\lib\site-packages\pandas\core\frame.py:3581: 
FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward
  "" from pandas 0.11 onward"", FutureWarning) 

```

I got the message, but I just want to stop Pandas showing such message again and again. Is there any builtin parameter that I can set to make Pandas stop popping up the 'Future warning'?","Found this on [github](https://github.com/pydata/pandas/issues/2841)...

```
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import pandas

```"
"I have a list of lists. For example,

```
[
[0,1,'f'],
[4,2,'t'],
[9,4,'afsd']
]

```

If I wanted to sort the outer list by the string field of the inner lists, how would you do that in python?","This is a job for [itemgetter](http://docs.python.org/library/operator.html#operator.itemgetter)

```
>>> from operator import itemgetter
>>> L=[[0, 1, 'f'], [4, 2, 't'], [9, 4, 'afsd']]
>>> sorted(L, key=itemgetter(2))
[[9, 4, 'afsd'], [0, 1, 'f'], [4, 2, 't']]

```

It is also possible to use a lambda function here, however the lambda function is slower in this simple case"
"I have a bunch of datetime objects and I want to calculate the number of seconds since a fixed time in the past for each one (for example since January 1, 1970).

```
import datetime
t = datetime.datetime(2009, 10, 21, 0, 0)

```

This seems to be only differentiating between dates that have different days:

```
t.toordinal()

```

How does one convert a `datetime` object to seconds?","For the special date of January 1, 1970 there are multiple options.

For any other starting date you need to get the difference between the two dates in seconds. Subtracting two dates gives a `timedelta` object, which as of Python 2.7 has a `total_seconds()` function.

```
>>> (t-datetime.datetime(1970,1,1)).total_seconds()
1256083200.0

```

The starting date is usually specified in UTC, so for proper results the `datetime` you feed into this formula should be in UTC as well. If your `datetime` isn't in UTC already, you'll need to convert it before you use it, or attach a `tzinfo` class that has the proper offset.

As noted in the comments, if you have a `tzinfo` attached to your `datetime` then you'll need one on the starting date as well or the subtraction will fail; for the example above I would add `tzinfo=pytz.utc` if using Python 2 or `tzinfo=timezone.utc` if using Python 3."
"I'm trying to use pip to install a package. I try to run `pip install` from the Python shell, but I get a `SyntaxError`. Why do I get this error? How do I use pip to install the package?

```
>>> pip install selenium
  File ""<stdin>"", line 1
    pip install selenium
              ^
SyntaxError: invalid syntax

```","pip is run from the command line, not the Python interpreter. It is a program that **installs** modules, so you can use them from Python. Once you have installed the module, then you can open the Python shell and do `import selenium`.

The Python shell is not a command line, it is an interactive interpreter. You type Python code into it, not commands."
"After these instructions in the Python interpreter one gets a window with a plot:

```
from matplotlib.pyplot import *
plot([1,2,3])
show()
# other code

```

Unfortunately, I don't know how to continue to interactively explore the figure created by `show()` while the program does further calculations.

Is it possible at all? Sometimes calculations are long and it would help if they would proceed during examination of intermediate results.","Use `matplotlib`'s calls that won't block:

Using `draw()`:

```
from matplotlib.pyplot import plot, draw, show
plot([1,2,3])
draw()
print('continue computation')

# at the end call show to ensure window won't close.
show()

```

Using interactive mode:

```
from matplotlib.pyplot import plot, ion, show
ion() # enables interactive mode
plot([1,2,3]) # result shows immediatelly (implicit draw())

print('continue computation')

# at the end call show to ensure window won't close.
show()

```"
"How do I make a `for` loop or a list comprehension so that every iteration gives me two elements?

```
l = [1,2,3,4,5,6]

for i,k in ???:
    print str(i), '+', str(k), '=', str(i+k)

```

Output:

```
1+2=3
3+4=7
5+6=11

```","Starting with Python 3.12, you can use the [`batched()`](https://docs.python.org/3/library/itertools.html#itertools.batched) function provided by the [`itertools`](https://docs.python.org/3/library/itertools.html) module:

```
from itertools import batched

for x, y in batched(l, n=2):
    print(""%d + %d = %d"" % (x, y, x + y))

```

Otherwise, you need a **`pairwise()`** (or **`grouped()`**) implementation.

```
def pairwise(iterable):
    ""s -> (s0, s1), (s2, s3), (s4, s5), ...""
    a = iter(iterable)
    return zip(a, a)

for x, y in pairwise(l):
   print(""%d + %d = %d"" % (x, y, x + y))

```

Or, more generally:

```
def grouped(iterable, n):
    ""s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), (s2n,s2n+1,s2n+2,...s3n-1), ...""
    return zip(*[iter(iterable)]*n)

for x, y in grouped(l, 2):
   print(""%d + %d = %d"" % (x, y, x + y))

```

In Python 2, you should import [`izip`](https://docs.python.org/2/library/itertools.html#itertools.izip) as a replacement for Python 3's built-in [`zip()`](https://docs.python.org/3/library/functions.html#zip) function.

All credit to [martineau](https://stackoverflow.com/users/355230/martineau) for [his answer](https://stackoverflow.com/questions/4356329/creating-a-python-dictionary-from-a-line-of-text/4356415#4356415) to [my question](https://stackoverflow.com/q/4356329/78845), I have found this to be very efficient as it only iterates once over the list and does not create any unnecessary lists in the process.

**N.B**: This should not be confused with the [**`pairwise`** recipe](https://docs.python.org/3/library/itertools.html#itertools-recipes) in Python's own [**`itertools`** documentation](https://docs.python.org/3/library/itertools.html), which yields `s -> (s0, s1), (s1, s2), (s2, s3), ...`, as pointed out by [@lazyr](https://stackoverflow.com/users/566644/lazyr) in the comments.

Little addition for those who would like to do type checking with **mypy** on Python 3:

```
from typing import Iterable, Tuple, TypeVar

T = TypeVar(""T"")

def grouped(iterable: Iterable[T], n=2) -> Iterable[Tuple[T, ...]]:
    """"""s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), ...""""""
    return zip(*[iter(iterable)] * n)

```"
"This is my error:

```
(mysite)zjm1126@zjm1126-G41MT-S2:~/zjm_test/mysite$ pip install lxml
Downloading/unpacking lxml
  Running setup.py egg_info for package lxml
    Building lxml version 2.3.
    Building without Cython.
    ERROR: /bin/sh: xslt-config: not found

    ** make sure the development packages of libxml2 and libxslt are installed **

    Using build configuration of libxslt
Installing collected packages: lxml
  Running setup.py install for lxml
    Building lxml version 2.3.
    Building without Cython.
    ERROR: /bin/sh: xslt-config: not found

    ** make sure the development packages of libxml2 and libxslt are installed **

    Using build configuration of libxslt
    building 'lxml.etree' extension
    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.6 -c src/lxml/lxml.etree.c -o build/temp.linux-i686-2.6/src/lxml/lxml.etree.o -w
    src/lxml/lxml.etree.c:4: fatal error: Python.h: 没有那个文件或目录
    compilation terminated.
    error: command 'gcc' failed with exit status 1
    Complete output from command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/lxml/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-jOhgvD-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6:
    Building lxml version 2.3.

Building without Cython.

ERROR: /bin/sh: xslt-config: not found



** make sure the development packages of libxml2 and libxslt are installed **



Using build configuration of libxslt

running install

running build

running build_py

running build_ext

building 'lxml.etree' extension

gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.6 -c src/lxml/lxml.etree.c -o build/temp.linux-i686-2.6/src/lxml/lxml.etree.o -w

src/lxml/lxml.etree.c:4: fatal error: Python.h: 没有那个文件或目录

compilation terminated.

error: command 'gcc' failed with exit status 1

----------------------------------------
Command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/lxml/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-jOhgvD-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6 failed with error code 1
Storing complete log in /home/zjm1126/.pip/pip.log

```

What can I do?

**updated:**

```
(mysite)zjm1126@zjm1126-G41MT-S2:~/zjm_test/mysite$ pip install lxml
Downloading/unpacking lxml
  Running setup.py egg_info for package lxml
    Building lxml version 2.3.
    Building without Cython.
    Using build configuration of libxslt 1.1.26
    Building against libxml2/libxslt in the following directory: /usr/lib
Installing collected packages: lxml
  Running setup.py install for lxml
    Building lxml version 2.3.
    Building without Cython.
    Using build configuration of libxslt 1.1.26
    Building against libxml2/libxslt in the following directory: /usr/lib
    building 'lxml.etree' extension
    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/libxml2 -I/usr/include/python2.6 -c src/lxml/lxml.etree.c -o build/temp.linux-i686-2.6/src/lxml/lxml.etree.o -w
    src/lxml/lxml.etree.c:4: fatal error: Python.h: 没有那个文件或目录
    compilation terminated.
    error: command 'gcc' failed with exit status 1
    Complete output from command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/lxml/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-NJw2ws-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6:
    Building lxml version 2.3.

Building without Cython.

Using build configuration of libxslt 1.1.26

Building against libxml2/libxslt in the following directory: /usr/lib

running install

running build

running build_py

running build_ext

building 'lxml.etree' extension

gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/libxml2 -I/usr/include/python2.6 -c src/lxml/lxml.etree.c -o build/temp.linux-i686-2.6/src/lxml/lxml.etree.o -w

src/lxml/lxml.etree.c:4: fatal error: Python.h: 没有那个文件或目录

compilation terminated.

error: command 'gcc' failed with exit status 1

----------------------------------------
Command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/lxml/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-NJw2ws-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6 failed with error code 1
Storing complete log in /home/zjm1126/.pip/pip.log

```

the log:

```
------------------------------------------------------------
/home/zjm1126/zjm_test/mysite/bin/pip run on Thu Mar  3 17:07:27 2011
Downloading/unpacking mysql-python
  Running setup.py egg_info for package mysql-python
    running egg_info
    creating pip-egg-info/MySQL_python.egg-info
    writing pip-egg-info/MySQL_python.egg-info/PKG-INFO
    writing top-level names to pip-egg-info/MySQL_python.egg-info/top_level.txt
    writing dependency_links to pip-egg-info/MySQL_python.egg-info/dependency_links.txt
    writing pip-egg-info/MySQL_python.egg-info/PKG-INFO
    writing top-level names to pip-egg-info/MySQL_python.egg-info/top_level.txt
    writing dependency_links to pip-egg-info/MySQL_python.egg-info/dependency_links.txt
    writing manifest file 'pip-egg-info/MySQL_python.egg-info/SOURCES.txt'
    warning: manifest_maker: standard file '-c' not found
    reading manifest file 'pip-egg-info/MySQL_python.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching 'MANIFEST'
    warning: no files found matching 'ChangeLog'
    warning: no files found matching 'GPL'
    writing manifest file 'pip-egg-info/MySQL_python.egg-info/SOURCES.txt'
Installing collected packages: mysql-python
  Running setup.py install for mysql-python
    Running command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-XuVIux-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-i686-2.6
    copying _mysql_exceptions.py -> build/lib.linux-i686-2.6
    creating build/lib.linux-i686-2.6/MySQLdb
    copying MySQLdb/__init__.py -> build/lib.linux-i686-2.6/MySQLdb
    copying MySQLdb/converters.py -> build/lib.linux-i686-2.6/MySQLdb
    copying MySQLdb/connections.py -> build/lib.linux-i686-2.6/MySQLdb
    copying MySQLdb/cursors.py -> build/lib.linux-i686-2.6/MySQLdb
    copying MySQLdb/release.py -> build/lib.linux-i686-2.6/MySQLdb
    copying MySQLdb/times.py -> build/lib.linux-i686-2.6/MySQLdb
    creating build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/__init__.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/CR.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/FIELD_TYPE.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/ER.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/FLAG.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/REFRESH.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    copying MySQLdb/constants/CLIENT.py -> build/lib.linux-i686-2.6/MySQLdb/constants
    running build_ext
    building '_mysql' extension
    creating build/temp.linux-i686-2.6
    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -Dversion_info=(1,2,3,'final',0) -D__version__=1.2.3 -I/usr/include/mysql -I/usr/include/python2.6 -c _mysql.c -o build/temp.linux-i686-2.6/_mysql.o -DBIG_JOINS=1 -fno-strict-aliasing -DUNIV_LINUX -DUNIV_LINUX
    In file included from _mysql.c:29:
    pymemcompat.h:10: fatal error: Python.h: 没有那个文件或目录
    compilation terminated.
    error: command 'gcc' failed with exit status 1
    Complete output from command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-XuVIux-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6:
    running install

running build

running build_py

creating build

creating build/lib.linux-i686-2.6

copying _mysql_exceptions.py -> build/lib.linux-i686-2.6

creating build/lib.linux-i686-2.6/MySQLdb

copying MySQLdb/__init__.py -> build/lib.linux-i686-2.6/MySQLdb

copying MySQLdb/converters.py -> build/lib.linux-i686-2.6/MySQLdb

copying MySQLdb/connections.py -> build/lib.linux-i686-2.6/MySQLdb

copying MySQLdb/cursors.py -> build/lib.linux-i686-2.6/MySQLdb

copying MySQLdb/release.py -> build/lib.linux-i686-2.6/MySQLdb

copying MySQLdb/times.py -> build/lib.linux-i686-2.6/MySQLdb

creating build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/__init__.py -> build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/CR.py -> build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/FIELD_TYPE.py -> build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/ER.py -> build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/FLAG.py -> build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/REFRESH.py -> build/lib.linux-i686-2.6/MySQLdb/constants

copying MySQLdb/constants/CLIENT.py -> build/lib.linux-i686-2.6/MySQLdb/constants

running build_ext

building '_mysql' extension

creating build/temp.linux-i686-2.6

gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -Dversion_info=(1,2,3,'final',0) -D__version__=1.2.3 -I/usr/include/mysql -I/usr/include/python2.6 -c _mysql.c -o build/temp.linux-i686-2.6/_mysql.o -DBIG_JOINS=1 -fno-strict-aliasing -DUNIV_LINUX -DUNIV_LINUX

In file included from _mysql.c:29:

pymemcompat.h:10: fatal error: Python.h: 没有那个文件或目录

compilation terminated.

error: command 'gcc' failed with exit status 1

----------------------------------------
Command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-XuVIux-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6 failed with error code 1
Exception information:
Traceback (most recent call last):
  File ""/home/zjm1126/zjm_test/mysite/lib/python2.6/site-packages/pip-0.8.1-py2.6.egg/pip/basecommand.py"", line 130, in main
    self.run(options, args)
  File ""/home/zjm1126/zjm_test/mysite/lib/python2.6/site-packages/pip-0.8.1-py2.6.egg/pip/commands/install.py"", line 228, in run
    requirement_set.install(install_options, global_options)
  File ""/home/zjm1126/zjm_test/mysite/lib/python2.6/site-packages/pip-0.8.1-py2.6.egg/pip/req.py"", line 1043, in install
    requirement.install(install_options, global_options)
  File ""/home/zjm1126/zjm_test/mysite/lib/python2.6/site-packages/pip-0.8.1-py2.6.egg/pip/req.py"", line 559, in install
    cwd=self.source_dir, filter_stdout=self._filter_install, show_stdout=False)
  File ""/home/zjm1126/zjm_test/mysite/lib/python2.6/site-packages/pip-0.8.1-py2.6.egg/pip/__init__.py"", line 249, in call_subprocess
    % (command_desc, proc.returncode))
InstallationError: Command /home/zjm1126/zjm_test/mysite/bin/python -c ""import setuptools;__file__='/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py';execfile(__file__)"" install --single-version-externally-managed --record /tmp/pip-XuVIux-record/install-record.txt --install-headers /home/zjm1126/zjm_test/mysite/include/site/python2.6 failed with error code 1

```","> \*\* make sure the development packages of libxml2 and libxslt are installed \*\*

From the [lxml documentation](https://lxml.de/installation.html#requirements), assuming you are running a Debian-based distribution :

```
sudo apt-get install libxml2-dev libxslt-dev python-dev

```

For Debian based systems, it should be enough to install the known build dependencies of `python-lxml` or `python3-lxml`, e.g.

```
sudo apt-get install build-dep python3-lxml

```"
"I want to dynamically query Google Maps through the Google Directions API. As an example, this request calculates the route from Chicago, IL to Los Angeles, CA via two waypoints in Joplin, MO and Oklahoma City, OK:

<http://maps.googleapis.com/maps/api/directions/json?origin=Chicago,IL&destination=Los+Angeles,CA&waypoints=Joplin,MO|Oklahoma+City,OK&sensor=false>

It returns a result [in the JSON format](https://developers.google.com/maps/documentation/directions/intro?csw=1#Waypoints).

How can I do this in Python? I want to send such a request, receive the result and parse it.","I recommend using the awesome [requests](https://requests.readthedocs.io/en/master/) library:

```
import requests

url = 'http://maps.googleapis.com/maps/api/directions/json'

params = dict(
    origin='Chicago,IL',
    destination='Los+Angeles,CA',
    waypoints='Joplin,MO|Oklahoma+City,OK',
    sensor='false'
)

resp = requests.get(url=url, params=params)
data = resp.json() # Check the JSON Response Content documentation below

```

JSON Response Content: <https://requests.readthedocs.io/en/master/user/quickstart/#json-response-content>"
"I have a file. In Python, I would like to take its creation time, and convert it to an [ISO time (ISO 8601) string](http://en.wikipedia.org/wiki/ISO_8601) **while preserving the fact that it was created in the [Eastern Time Zone](https://en.wikipedia.org/wiki/Eastern_Time_Zone) (ET)**.

How do I take the file's ctime and convert it to an ISO time string that indicates the Eastern Time Zone (and takes into account daylight savings time, if necessary)?","Local to ISO 8601:

```
import datetime
datetime.datetime.now().isoformat()
>>> '2024-08-01T14:38:32.499588'

```

UTC to ISO 8601:

```
import datetime
datetime.datetime.now(datetime.timezone.utc).isoformat()
>>> '2024-08-01T04:38:47.731215+00:00'

```

Local to ISO 8601 without microsecond:

```
import datetime
datetime.datetime.now().replace(microsecond=0).isoformat()
>>> '2024-08-01T14:38:57'

```

UTC to ISO 8601 with timezone information (Python 3):

```
import datetime
datetime.datetime.now(datetime.timezone.utc).isoformat()
>>> '2024-08-01T04:39:06.274874+00:00'

```

Local to ISO 8601 with timezone information (Python 3):

```
import datetime
datetime.datetime.now().astimezone().isoformat()
>>> '2024-08-01T14:39:16.698776+10:00'

```

Local to ISO 8601 with local timezone information without microsecond (Python 3):

```
import datetime
datetime.datetime.now().astimezone().replace(microsecond=0).isoformat()
>>> '2024-08-01T14:39:28+10:00'

```

**Notice there is a bug when using `astimezone()` on `utcnow()`. This gives an incorrect result:**

```
datetime.datetime.utcnow().astimezone().isoformat() #Incorrect result, do not use.

```

**`.utcnow()` is deprecated, use `.now(datetime.timezome.utc)` instead.**

For Python 2, see and use [pytz](https://pypi.python.org/pypi/pytz/ ""pytz"")."
"I have a column in python `pandas` DataFrame that has boolean `True`/`False` values, but for further calculations I need `1`/`0` representation. Is there a quick `pandas`/`numpy` way to do that?","A succinct way to convert a single column of boolean values to a column of integers 1 or 0:

```
df[""somecolumn""] = df[""somecolumn""].astype(int)

```"
"When I try to install the [Cryptography](https://cryptography.io) package for Python through either `pip install cryptography` or by downloading the package from [their site](https://github.com/pyca/cryptography) and running `python setup.py`, I get the following error:

---

```
D:\Anaconda\Scripts\pip-script.py run on 02/27/14 16:13:17
Downloading/unpacking cryptography
  Getting page https://pypi.python.org/simple/cryptography/
  URLs to search for versions for cryptography:
  * https://pypi.python.org/simple/cryptography/
  Analyzing links from page https://pypi.python.org/simple/cryptography/
    Skipping https://pypi.python.org/packages/cp26/c/cryptography/cryptography-0.2-cp26-none-win32.whl#md5=13e5c4b19520e7dc6f07c6502b3f74e2 (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp26/c/cryptography/cryptography-0.2.1-cp26-none-win32.whl#md5=00e733648ee5cdb9e58876238b1328f8 (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp27/c/cryptography/cryptography-0.2-cp27-none-win32.whl#md5=013ccafa6a5a3ea92c73f2c1c4879406 (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp27/c/cryptography/cryptography-0.2.1-cp27-none-win32.whl#md5=127d6a5dc687250721f892d55720a06c (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp32/c/cryptography/cryptography-0.2-cp32-none-win32.whl#md5=051424a36e91039807b72f112333ded3 (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp32/c/cryptography/cryptography-0.2.1-cp32-none-win32.whl#md5=53f6f57db8e952d64283baaa14cbde3d (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp33/c/cryptography/cryptography-0.2-cp33-none-win32.whl#md5=302812c1c1a035cf9ba3292f8dbf3f9e (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Skipping https://pypi.python.org/packages/cp33/c/cryptography/cryptography-0.2.1-cp33-none-win32.whl#md5=81acca90caf8a45f2ca73f3f9859fae4 (from https://pypi.python.org/simple/cryptography/) because it is not compatible with this Python
    Found link https://pypi.python.org/packages/source/c/cryptography/cryptography-0.1.tar.gz#md5=bdc1c5fe069deca7467b71a0cc538f17 (from https://pypi.python.org/simple/cryptography/), version: 0.1
    Found link https://pypi.python.org/packages/source/c/cryptography/cryptography-0.2.1.tar.gz#md5=872fc04268dadc66a0305ae5ab1c123b (from https://pypi.python.org/simple/cryptography/), version: 0.2.1
    Found link https://pypi.python.org/packages/source/c/cryptography/cryptography-0.2.tar.gz#md5=8a3d21e837a21e1b7634ee1f22b06bb6 (from https://pypi.python.org/simple/cryptography/), version: 0.2
  Using version 0.2.1 (newest of versions: 0.2.1, 0.2, 0.1)
  Downloading from URL https://pypi.python.org/packages/source/c/cryptography/cryptography-0.2.1.tar.gz#md5=872fc04268dadc66a0305ae5ab1c123b (from https://pypi.python.org/simple/cryptography/)
  Running setup.py (path:c:\users\paco\appdata\local\temp\pip_build_Paco\cryptography\setup.py) egg_info for package cryptography
    In file included from c/_cffi_backend.c:7:0:
    c/misc_win32.h:225:23: error: two or more data types in declaration specifiers
    c/misc_win32.h:225:1: warning: useless type name in empty declaration [enabled by default]
    c/_cffi_backend.c: In function 'convert_array_from_object':
    c/_cffi_backend.c:1105:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1105:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:1130:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1130:30: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:1150:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1150:30: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'convert_struct_from_object':
    c/_cffi_backend.c:1183:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1183:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:1196:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1196:30: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'cdata_repr':
    c/_cffi_backend.c:1583:13: warning: unknown conversion type character 'L' in format [-Wformat]
    c/_cffi_backend.c:1583:13: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:1595:9: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1595:9: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'cdataowning_repr':
    c/_cffi_backend.c:1647:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1647:30: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function '_cdata_get_indexed_ptr':
    c/_cffi_backend.c:1820:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1820:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1820:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function '_cdata_getslicearg':
    c/_cffi_backend.c:1872:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1872:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1872:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'cdata_ass_slice':
    c/_cffi_backend.c:1951:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1951:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1951:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:1969:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1969:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1969:30: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:1983:22: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:1983:22: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'cdata_call':
    c/_cffi_backend.c:2367:30: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:2367:30: warning: format '%s' expects argument of type 'char *', but argument 3 has type 'Py_ssize_t' [-Wformat]
    c/_cffi_backend.c:2367:30: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'cast_to_integer_or_char':
    c/_cffi_backend.c:2916:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:2916:26: warning: format '%s' expects argument of type 'char *', but argument 3 has type 'Py_ssize_t' [-Wformat]
    c/_cffi_backend.c:2916:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c:2928:26: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:2928:26: warning: format '%s' expects argument of type 'char *', but argument 3 has type 'Py_ssize_t' [-Wformat]
    c/_cffi_backend.c:2928:26: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'new_array_type':
    c/_cffi_backend.c:3480:9: warning: unknown conversion type character 'l' in format [-Wformat]
    c/_cffi_backend.c:3480:9: warning: too many arguments for format [-Wformat-extra-args]
    c/_cffi_backend.c: In function 'b_complete_struct_or_union':
    c/_cffi_backend.c:3878:22: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:3878:22: warning: unknown conversion type character 'z' in format [-Wformat]
    c/_cffi_backend.c:3878:22: warning: too many arguments for format [-Wformat-extra-args]
    Traceback (most recent call last):
      File ""<string>"", line 17, in <module>
      File ""c:\users\paco\appdata\local\temp\pip_build_Paco\cryptography\setup.py"", line 113, in <module>
        ""build"": cffi_build,
      File ""D:\Anaconda\lib\distutils\core.py"", line 112, in setup
        _setup_distribution = dist = klass(attrs)
      File ""build\bdist.win-amd64\egg\setuptools\dist.py"", line 239, in __init__
      File ""build\bdist.win-amd64\egg\setuptools\dist.py"", line 264, in fetch_build_eggs
      File ""build\bdist.win-amd64\egg\pkg_resources.py"", line 580, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""build\bdist.win-amd64\egg\pkg_resources.py"", line 818, in best_match
        return self.obtain(req, installer) # try and download/install
      File ""build\bdist.win-amd64\egg\pkg_resources.py"", line 830, in obtain
        return installer(requirement)
      File ""build\bdist.win-amd64\egg\setuptools\dist.py"", line 314, in fetch_build_egg
      File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 593, in easy_install

      File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 623, in install_item

      File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 809, in install_eggs

      File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 1015, in build_and_install

      File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 1003, in run_setup

    distutils.errors.DistutilsError: Setup script exited with error: command 'gcc' failed with exit status 1
    Complete output from command python setup.py egg_info:
    In file included from c/_cffi_backend.c:7:0:

c/misc_win32.h:225:23: error: two or more data types in declaration specifiers

c/misc_win32.h:225:1: warning: useless type name in empty declaration [enabled by default]

c/_cffi_backend.c: In function 'convert_array_from_object':

c/_cffi_backend.c:1105:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1105:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:1130:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1130:30: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:1150:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1150:30: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'convert_struct_from_object':

c/_cffi_backend.c:1183:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1183:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:1196:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1196:30: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'cdata_repr':

c/_cffi_backend.c:1583:13: warning: unknown conversion type character 'L' in format [-Wformat]

c/_cffi_backend.c:1583:13: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:1595:9: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1595:9: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'cdataowning_repr':

c/_cffi_backend.c:1647:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1647:30: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function '_cdata_get_indexed_ptr':

c/_cffi_backend.c:1820:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1820:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1820:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function '_cdata_getslicearg':

c/_cffi_backend.c:1872:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1872:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1872:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'cdata_ass_slice':

c/_cffi_backend.c:1951:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1951:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1951:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:1969:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1969:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1969:30: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:1983:22: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:1983:22: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'cdata_call':

c/_cffi_backend.c:2367:30: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:2367:30: warning: format '%s' expects argument of type 'char *', but argument 3 has type 'Py_ssize_t' [-Wformat]

c/_cffi_backend.c:2367:30: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'cast_to_integer_or_char':

c/_cffi_backend.c:2916:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:2916:26: warning: format '%s' expects argument of type 'char *', but argument 3 has type 'Py_ssize_t' [-Wformat]

c/_cffi_backend.c:2916:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c:2928:26: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:2928:26: warning: format '%s' expects argument of type 'char *', but argument 3 has type 'Py_ssize_t' [-Wformat]

c/_cffi_backend.c:2928:26: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'new_array_type':

c/_cffi_backend.c:3480:9: warning: unknown conversion type character 'l' in format [-Wformat]

c/_cffi_backend.c:3480:9: warning: too many arguments for format [-Wformat-extra-args]

c/_cffi_backend.c: In function 'b_complete_struct_or_union':

c/_cffi_backend.c:3878:22: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:3878:22: warning: unknown conversion type character 'z' in format [-Wformat]

c/_cffi_backend.c:3878:22: warning: too many arguments for format [-Wformat-extra-args]

Traceback (most recent call last):

  File ""<string>"", line 17, in <module>

  File ""c:\users\paco\appdata\local\temp\pip_build_Paco\cryptography\setup.py"", line 113, in <module>

    ""build"": cffi_build,

  File ""D:\Anaconda\lib\distutils\core.py"", line 112, in setup

    _setup_distribution = dist = klass(attrs)

  File ""build\bdist.win-amd64\egg\setuptools\dist.py"", line 239, in __init__

  File ""build\bdist.win-amd64\egg\setuptools\dist.py"", line 264, in fetch_build_eggs

  File ""build\bdist.win-amd64\egg\pkg_resources.py"", line 580, in resolve

    dist = best[req.key] = env.best_match(req, ws, installer)

  File ""build\bdist.win-amd64\egg\pkg_resources.py"", line 818, in best_match

    return self.obtain(req, installer) # try and download/install

  File ""build\bdist.win-amd64\egg\pkg_resources.py"", line 830, in obtain

    return installer(requirement)

  File ""build\bdist.win-amd64\egg\setuptools\dist.py"", line 314, in fetch_build_egg

  File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 593, in easy_install



  File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 623, in install_item



  File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 809, in install_eggs



  File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 1015, in build_and_install



  File ""build\bdist.win-amd64\egg\setuptools\command\easy_install.py"", line 1003, in run_setup



distutils.errors.DistutilsError: Setup script exited with error: command 'gcc' failed with exit status 1

----------------------------------------
Cleaning up...
  Removing temporary dir c:\users\paco\appdata\local\temp\pip_build_Paco...
Command python setup.py egg_info failed with error code 1 in c:\users\paco\appdata\local\temp\pip_build_Paco\cryptography
Exception information:
Traceback (most recent call last):
  File ""D:\Anaconda\lib\site-packages\pip-1.5.4-py2.7.egg\pip\basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""D:\Anaconda\lib\site-packages\pip-1.5.4-py2.7.egg\pip\commands\install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""D:\Anaconda\lib\site-packages\pip-1.5.4-py2.7.egg\pip\req.py"", line 1229, in prepare_files
    req_to_install.run_egg_info()
  File ""D:\Anaconda\lib\site-packages\pip-1.5.4-py2.7.egg\pip\req.py"", line 325, in run_egg_info
    command_desc='python setup.py egg_info')
  File ""D:\Anaconda\lib\site-packages\pip-1.5.4-py2.7.egg\pip\util.py"", line 697, in call_subprocess
    % (command_desc, proc.returncode, cwd))
InstallationError: Command python setup.py egg_info failed with error code 1 in c:\users\paco\appdata\local\temp\pip_build_Paco\cryptography

```

I found other egg\_info error posts ([here](https://stackoverflow.com/questions/17886647/cant-install-via-pip-because-of-egg-info-error) and [here](https://stackoverflow.com/questions/11425106/python-pip-install-fails-invalid-command-egg-info)) but the solutions there provided wouldn't solve my problem. Also, I am able to install other packages through PIP.

PIP version 1.5.4 setuptools version 2.2","I had a similar issue, and found I was simply missing a dependency (libssl-dev, for me). As referenced in <https://cryptography.io/en/latest/installation/>, ensure that all dependencies are met:

### On Windows

If you’re on Windows you’ll need to make sure you have OpenSSL installed. There are pre-compiled binaries available. If your installation is in an unusual location set the LIB and INCLUDE environment variables to include the
corresponding locations. For example:

```
C:\> \path\to\vcvarsall.bat x86_amd64
C:\> set LIB=C:\OpenSSL-1.0.1f-64bit\lib;%LIB%
C:\> set INCLUDE=C:\OpenSSL-1.0.1f-64bit\include;%INCLUDE%
C:\> pip install cryptography

```

### Building cryptography on Linux

cryptography should build very easily on Linux provided you have a C compiler, headers for Python (if you’re
not using pypy), and headers for the OpenSSL and libffi libraries available on your system.

For **Debian and Ubuntu**, the following command will ensure that the required dependencies are installed:

```
sudo apt-get install build-essential libssl-dev libffi-dev python-dev

```

For **Fedora and RHEL-derivatives**, the following command will ensure that the required dependencies are installed:

```
sudo yum install gcc libffi-devel python-devel openssl-devel

```

You should now be able to build and install cryptography with the usual.

```
pip install cryptography

```"
"I want to loop through a Python list and process 2 list items at a time. Something like this in another language:

```
for(int i = 0; i < list.length(); i+=2)
{
   // do something with list[i] and list[i + 1]
}

```

What's the best way to accomplish this?","You can use a `range` with a step size of 2:

**Python 2**

```
for i in xrange(0,10,2):
  print(i)

```

**Python 3**

```
for i in range(0,10,2):
  print(i)

```

**Note:** Use `xrange` in Python 2 instead of `range` because it is more efficient as it generates an iterable object, and not the whole list."
"I have a dictionary of points, say:

```
>>> points={'a':(3,4), 'b':(1,2), 'c':(5,5), 'd':(3,3)}

```

I want to create a new dictionary with all the points whose x and y value is smaller than 5, i.e. points 'a', 'b' and 'd'.

According to the [the book](http://docs.python.org/library/stdtypes.html#mapping-types-dict), each dictionary has the `items()` function, which returns a list of `(key, pair)` tuple:

```
>>> points.items()
[('a', (3, 4)), ('c', (5, 5)), ('b', (1, 2)), ('d', (3, 3))]

```

So I have written this:

```
>>> for item in [i for i in points.items() if i[1][0]<5 and i[1][1]<5]:
...     points_small[item[0]]=item[1]
...
>>> points_small
{'a': (3, 4), 'b': (1, 2), 'd': (3, 3)}

```

Is there a more elegant way? I was expecting Python to have some super-awesome `dictionary.filter(f)` function...","You can use a dict comprehension:

```
{k: v for k, v in points.items() if v[0] < 5 and v[1] < 5}

```

And in Python 2, starting from 2.7:

```
{k: v for k, v in points.iteritems() if v[0] < 5 and v[1] < 5}

```"
"Is it not possible to define multiple constructors in Python, with different signatures? If not, what's the general way of getting around it?

For example, let's say you wanted to define a class `City`.

I'd like to be able to say `someCity = City()` or `someCity = City(""Berlin"")`, where the first just gives a default name value, and the second defines it.","Unlike Java, you cannot define multiple constructors. However, you can define a default value if one is not passed.

```
def __init__(self, city=""Berlin""):
  self.city = city

```"
"I have a situation with some code where `eval()` came up as a possible solution. Now I have never had to use `eval()` before but, I have come across plenty of information about the potential danger it can cause. That said, I'm very wary about using it.

My situation is that I have input being given by a user:

```
datamap = input('Provide some data here: ')

```

Where `datamap` needs to be a dictionary. I searched around and found that `eval()` could work this out. I thought that I might be able to check the type of the input before trying to use the data and that would be a viable security precaution.

```
datamap = eval(input('Provide some data here: ')
if not isinstance(datamap, dict):
    return

```

I read through the docs and I am still unclear if this would be safe or not. Does eval evaluate the data as soon as its entered or after the `datamap` variable is called?

Is the `ast` module's `.literal_eval()` the only safe option?","`datamap = eval(input('Provide some data here: '))` means that you actually evaluate the code *before* you deem it to be unsafe or not. It evaluates the code as soon as the function is called. See also [the dangers of `eval`](http://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html).

[`ast.literal_eval`](https://docs.python.org/3/library/ast.html#ast.literal_eval) raises an exception if the input isn't a valid Python datatype, so the code won't be executed if it's not.

Use `ast.literal_eval` whenever you need [`eval`](https://docs.python.org/3/library/functions.html#eval). You shouldn't usually evaluate literal Python statements."
"I have a class with two class methods (using the `classmethod()` function) for getting and setting what is essentially a static variable. I tried to use the `property()` function with these, but it results in an error. I was able to reproduce the error with the following in the interpreter:

```
class Foo(object):
    _var = 5
    @classmethod
    def getvar(cls):
        return cls._var
    @classmethod
    def setvar(cls, value):
        cls._var = value
    var = property(getvar, setvar)

```

I can demonstrate the class methods, but they don't work as properties:

```
>>> f = Foo()
>>> f.getvar()
5
>>> f.setvar(4)
>>> f.getvar()
4
>>> f.var
Traceback (most recent call last):
  File ""<stdin>"", line 1, in ?
TypeError: 'classmethod' object is not callable
>>> f.var=5
Traceback (most recent call last):
  File ""<stdin>"", line 1, in ?
TypeError: 'classmethod' object is not callable

```

Is it possible to use the `property()` function with `@classmethod` decorated functions?","### 3.8 < Python < 3.11

Can use both decorators together. See [this answer](https://stackoverflow.com/a/64738850/674039).

### Python 2 and python 3 (works in 3.9-3.10 too)

A property is created on a class but affects an instance. So if you want a `classmethod` property, create the property on the metaclass.

```
>>> class foo(object):
...     _var = 5
...     class __metaclass__(type):  # Python 2 syntax for metaclasses
...         pass
...     @classmethod
...     def getvar(cls):
...         return cls._var
...     @classmethod
...     def setvar(cls, value):
...         cls._var = value
...     
>>> foo.__metaclass__.var = property(foo.getvar.im_func, foo.setvar.im_func)
>>> foo.var
5
>>> foo.var = 3
>>> foo.var
3

```

But since you're using a metaclass anyway, it will read better if you just move the classmethods in there.

```
>>> class foo(object):
...     _var = 5
...     class __metaclass__(type):  # Python 2 syntax for metaclasses
...         @property
...         def var(cls):
...             return cls._var
...         @var.setter
...         def var(cls, value):
...             cls._var = value
... 
>>> foo.var
5
>>> foo.var = 3
>>> foo.var
3

```

or, using Python 3's `metaclass=...` syntax, and the metaclass defined outside of the `foo` class body, and the metaclass responsible for setting the initial value of `_var`:

```
>>> class foo_meta(type):
...     def __init__(cls, *args, **kwargs):
...         cls._var = 5
...     @property
...     def var(cls):
...         return cls._var
...     @var.setter
...     def var(cls, value):
...         cls._var = value
...
>>> class foo(metaclass=foo_meta):
...     pass
...
>>> foo.var
5
>>> foo.var = 3
>>> foo.var
3

```"
"I am diving into Python and I have a question about foreach iteration. I am new to Python and I have some experience in C#. So I am wondering, if there is some equivalent function in Python for iteration all over all items in my collection, e.g.

```
pets = ['cat', 'dog', 'fish']
marks = [ 5, 4, 3, 2, 1]

```

or something like this.","Sure. A for loop.

```
for f in pets:
    print f

```"
"I use SQLAlchemy and there are at least three entities: `engine`, `session` and `connection`, which have `execute` method, so if I e.g. want to select all records from `table` I can do this on the **Engine level**:

```
engine.execute(select([table])).fetchall()

```

and on the **Connection level**:

```
connection.execute(select([table])).fetchall()

```

and even on the **Session level**:

```
session.execute(select([table])).fetchall()

```

- the results will be the same.

As I understand it, if someone uses `engine.execute` it creates `connection`, opens `session` (Alchemy takes care of it for you) and executes the query. But is there a global difference between these three ways of performing such a task?","### Running .execute()

When executing a plain `SELECT * FROM tablename`, there's no difference in the result provided.

The differences between these three objects do become important depending on the context that the `SELECT` statement is used in or, more commonly, when you want to do other things like `INSERT`, `DELETE`, etc.

### When to use Engine, Connection, Session generally

* **Engine** is the lowest level object used by SQLAlchemy. It [maintains a pool of connections](http://docs.sqlalchemy.org/en/latest/core/pooling.html) available for use whenever the application needs to talk to the database. `.execute()` is a convenience method that first calls `conn = engine.connect(close_with_result=True)` and the then `conn.execute()`. The close\_with\_result parameter means the connection is closed automatically. (I'm slightly paraphrasing the source code, but essentially true). *edit: [Here's the source code for engine.execute](https://github.com/sqlalchemy/sqlalchemy/blob/d514c032cd0349afc93f89d5b99835198ae70112/lib/sqlalchemy/engine/base.py#L2165-L2166)*

  You can use engine to execute raw SQL.

  ```
    result = engine.execute('SELECT * FROM tablename;')
    # what engine.execute() is doing under the hood:
    conn = engine.connect(close_with_result=True)
    result = conn.execute('SELECT * FROM tablename;')

    # after you iterate over the results, the result and connection get closed
    for row in result:
        print(result['columnname']

    # or you can explicitly close the result, which also closes the connection
    result.close()

  ```

  This is covered in the docs under [basic usage](http://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage).
* **Connection** is (as we saw above) the thing that actually does the work of executing a SQL query. You should do this whenever you want greater control over attributes of the connection, when it gets closed, etc. An important example of this is a [transaction](http://docs.sqlalchemy.org/en/rel_1_0/core/connections.html#using-transactions), which lets you decide when to commit your changes to the database (if at all). In normal use, changes are auto-committed. With the use of transactions, you could (for example) run several different SQL statements and if something goes wrong with one of them you could undo all the changes at once.

  ```
    connection = engine.connect()
    trans = connection.begin()
    try:
        connection.execute(text(""INSERT INTO films VALUES ('Comedy', '82 minutes');""))
        connection.execute(text(""INSERT INTO datalog VALUES ('added a comedy');""))
        trans.commit()
    except Exception:
        trans.rollback()
        raise

  ```

  This would let you undo both changes if one failed, like if you forgot to create the datalog table.

  So if you're executing raw SQL code and need control, use connections
* **Sessions** are used for the Object Relationship Management (ORM) aspect of SQLAlchemy (in fact you can see this from how they're imported: `from sqlalchemy.orm import sessionmaker`). They use connections and transactions under the hood to run their automatically-generated SQL statements. `.execute()` is a convenience function that passes through to whatever the session is bound to (usually an engine, but can be a connection).

  If you're using the ORM functionality, use a session. If you're only doing straight SQL queries not bound to objects, you're probably better off using connections directly."
"I'm a bit miffed by the python package installation process. Specifically, what's the difference between packages installed in the dist-packages directory and the site-packages directory?","`dist-packages` is a Debian-specific convention that is also present in its derivatives, like Ubuntu. Modules are installed to `dist-packages` when they come from the Debian package manager into this location:

```
/usr/lib/python2.7/dist-packages

```

Since `easy_install` and `pip` are installed from the package manager, they also use `dist-packages`, but they put packages here:

```
/usr/local/lib/python2.7/dist-packages

```

From the [Debian Python Wiki](http://wiki.debian.org/Python):

> dist-packages instead of site-packages. Third party Python software
> installed from Debian packages goes into dist-packages, not
> site-packages. This is to reduce conflict between the system Python,
> and any from-source Python build you might install manually.

This means that if you manually compile and install Python interpreter from source, it uses the **`site-packages`** directory. This allows you to keep the two installations separate, especially since Debian and Ubuntu rely on the system version of Python for many system utilities."
"Given a Python object of any kind, is there an easy way to get the list of all methods that this object has?

Or if this is not possible, is there at least an easy way to check if it has a particular method, other than checking if an error occurs when the method is called?","**For many objects**, you can use this code, replacing 'object' with the object you're interested in:

```
object_methods = [method_name for method_name in dir(object)
                  if callable(getattr(object, method_name))]

```

I discovered it at [diveintopython.net](https://web.archive.org/web/20180901124519/http://www.diveintopython.net/power_of_introspection/index.html) (now archived), that should provide some further details!

**If you get an `AttributeError`, you can use this instead**:

`getattr()` is intolerant of pandas style Python 3.6 abstract virtual sub-classes. This code does the same as above and ignores exceptions.

```
import pandas as pd
df = pd.DataFrame([[10, 20, 30], [100, 200, 300]],
                  columns=['foo', 'bar', 'baz'])
def get_methods(object, spacing=20):
  methodList = []
  for method_name in dir(object):
    try:
        if callable(getattr(object, method_name)):
            methodList.append(str(method_name))
    except Exception:
        methodList.append(str(method_name))
  processFunc = (lambda s: ' '.join(s.split())) or (lambda s: s)
  for method in methodList:
    try:
        print(str(method.ljust(spacing)) + ' ' +
              processFunc(str(getattr(object, method).__doc__)[0:90]))
    except Exception:
        print(method.ljust(spacing) + ' ' + ' getattr() failed')

get_methods(df['foo'])

```"
"I want to take an integer (that will be <= 255), to a hex string representation

e.g.: I want to pass in `65` and get out `'\x41'`, or `255` and get `'\xff'`.

I've tried doing this with the `struct.pack('c',`65`)`, but that chokes on anything above `9` since it wants to take in a single character string.","You are looking for the `chr` function.

You seem to be mixing decimal representations of integers and hex representations of integers, so it's not entirely clear what you need. Based on the description you gave, I think one of these snippets shows what you want.

```
>>> chr(0x65) == '\x65'
True


>>> hex(65)
'0x41'
>>> chr(65) == '\x41'
True

```

Note that this is quite different from *a string containing an integer as hex*. If that is what you want, use the `hex` builtin."
"I am trying to find the largest cube root that is a whole number, that is less than 12,000.

```
processing = True
n = 12000
while processing:
    n -= 1
    if n ** (1/3) == #checks to see if this has decimals or not

```

I am not sure how to check if it is a whole number or not though! I could convert it to a string then use indexing to check the end values and see whether they are zero or not, that seems rather cumbersome though. Is there a simpler way?","To check if a float value is a whole number, use the [`float.is_integer()` method](http://docs.python.org/library/stdtypes.html#float.is_integer):

```
>>> (1.0).is_integer()
True
>>> (1.555).is_integer()
False

```

The method was added to the `float` type in Python 2.6.

Take into account that in Python 2, `1/3` is `0` (floor division for integer operands!), and that floating point arithmetic can be imprecise (a `float` is an approximation using binary fractions, *not* a precise real number). But adjusting your loop a little this gives:

```
>>> for n in range(12000, -1, -1):
...     if (n ** (1.0/3)).is_integer():
...         print n
... 
27
8
1
0

```

which means that anything over 3 cubed, (including 10648) was missed out due to the aforementioned imprecision:

```
>>> (4**3) ** (1.0/3)
3.9999999999999996
>>> 10648 ** (1.0/3)
21.999999999999996

```

You'd have to check for numbers **close** to the whole number instead, or not use `float()` to find your number. Like rounding down the cube root of `12000`:

```
>>> int(12000 ** (1.0/3))
22
>>> 22 ** 3
10648

```

If you are using Python 3.5 or newer, you can use the [`math.isclose()` function](https://docs.python.org/library/math.html#math.isclose) to see if a floating point value is within a configurable margin:

```
>>> from math import isclose
>>> isclose((4**3) ** (1.0/3), 4)
True
>>> isclose(10648 ** (1.0/3), 22)
True

```

For older versions, the naive implementation of that function (skipping error checking and ignoring infinity and NaN) as [mentioned in PEP485](https://www.python.org/dev/peps/pep-0485/#proposed-implementation):

```
def isclose(a, b, rel_tol=1e-9, abs_tol=0.0):
    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)

```"
"I have a Model as follows:

```
class TankJournal(models.Model):
    user = models.ForeignKey(User)
    tank = models.ForeignKey(TankProfile)
    ts = models.IntegerField(max_length=15)
    title = models.CharField(max_length=50)
    body = models.TextField()

```

I also have a model form for the above model as follows:

```
class JournalForm(ModelForm):
    tank = forms.IntegerField(widget=forms.HiddenInput()) 

    class Meta:
        model = TankJournal
        exclude = ('user','ts')

```

I want to know how to set the default value for that tank hidden field. Here is my function to show/save the form so far:

```
def addJournal(request, id=0):
    if not request.user.is_authenticated():
        return HttpResponseRedirect('/')

    # checking if they own the tank
    from django.contrib.auth.models import User
    user = User.objects.get(pk=request.session['id'])

    if request.method == 'POST':
        form = JournalForm(request.POST)
        if form.is_valid():
            obj = form.save(commit=False)

            # setting the user and ts
            from time import time
            obj.ts = int(time())
            obj.user = user

            obj.tank = TankProfile.objects.get(pk=form.cleaned_data['tank_id'])

            # saving the test
            obj.save()

    else:
        form = JournalForm()

    try:
        tank = TankProfile.objects.get(user=user, id=id)
    except TankProfile.DoesNotExist:
        return HttpResponseRedirect('/error/')

```","You can use [`Form.initial`](https://docs.djangoproject.com/en/stable/ref/forms/api/#initial-form-values).

You have two options either populate the value when calling form constructor:

```
form = JournalForm(initial={'tank': 123})

```

or set the value in the form definition:

```
tank = forms.IntegerField(widget=forms.HiddenInput(), initial=123) 

```"
"Is it a linked list, an array? I searched around and only found people guessing. My C knowledge isn't good enough to look at the source code.","The C code is pretty simple, actually. Expanding one macro and pruning some irrelevant comments, the basic structure is in [`listobject.h`](https://github.com/python/cpython/blob/5c22476c01622f11b7745ee693f8b296a9d6a761/Include/listobject.h#L22), which defines a list as:

```
typedef struct {
    PyObject_HEAD
    Py_ssize_t ob_size;

    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */
    PyObject **ob_item;

    /* ob_item contains space for 'allocated' elements.  The number
     * currently in use is ob_size.
     * Invariants:
     *     0 <= ob_size <= allocated
     *     len(list) == ob_size
     *     ob_item == NULL implies ob_size == allocated == 0
     */
    Py_ssize_t allocated;
} PyListObject;

```

`PyObject_HEAD` contains a reference count and a type identifier. So, it's a vector/array that overallocates. The code for resizing such an array when it's full is in [`listobject.c`](https://github.com/python/cpython/blob/5c22476c01622f11b7745ee693f8b296a9d6a761/Objects/listobject.c#L22). It doesn't actually double the array, but grows by allocating

```
new_allocated = (newsize >> 3) + (newsize < 9 ? 3 : 6);
new_allocated += newsize;

```

to the capacity each time, where `newsize` is the requested size (not necessarily `allocated + 1` because you can `extend` by an arbitrary number of elements instead of `append`'ing them one by one).

See also the [Python FAQ](https://docs.python.org/3/faq/design.html#how-are-lists-implemented-in-cpython)."
"If I add a subtitle to my matplotlib figure it gets overlaid by the subplot's titles. Does anybody know how to easily take care of that? I tried the `tight_layout()` function, but it only makes things worse.

Example:

```
import numpy as np
import matplotlib.pyplot as plt

f = np.random.random(100)
g = np.random.random(100)
fig = plt.figure()
fig.suptitle('Long Suptitle', fontsize=24)
plt.subplot(121)
plt.plot(f)
plt.title('Very Long Title 1', fontsize=20)
plt.subplot(122)
plt.plot(g)
plt.title('Very Long Title 2', fontsize=20)
plt.tight_layout()
plt.show()

```","You can adjust the subplot geometry in the very `tight_layout` call as follows:

```
fig.tight_layout(rect=[0, 0.03, 1, 0.95])

```

As it's stated in the documentation (<https://matplotlib.org/stable/users/explain/axes/tight_layout_guide.html>):

> `tight_layout()` only considers ticklabels, axis labels, and titles. Thus, other artists may be clipped and also may overlap."
"I want to get the latest version of Python to use [f-strings](https://en.wikipedia.org/wiki/Python_(programming_language)#Expressions) in my code. Currently my version is (`python -V`):

```
Python 3.5.2 :: Anaconda 4.2.0 (x86_64)

```

How would I upgrade to Python 3.6?","Anaconda had not updated Python internally to 3.6, but later versions of Anaconda has a Python 3.6 version [here](https://www.continuum.io/downloads).

a) Method 1

1. If you wanted to update, you will type `conda update python`
2. To update Anaconda, type `conda update conda`
3. If you want to upgrade between major python versions, like 3.5 to 3.6, you'll have to do

   ```
   conda install python=$pythonversion$

   ```

b) Method 2 - Create a new environment (the better method)

```
conda create --name py36 python=3.6

```

c) To get the absolute latest Python (3.6.5 at time of writing)

```
conda create --name py365 python=3.6.5 --channel conda-forge

```

You can see all this from [here](http://conda.pydata.org/docs/using/pkgs.html#package-update).

Also, refer to this for force [upgrading](https://www.scivision.dev/switch-anaconda-latest-python-3/)."
"I want to set up a complete Python IDE in Sublime Text 2.

I want to know how to run the Python code from within the editor. Is it done using build system? How do I do it ?","Tools -> Build System -> (choose) Python then:

**To Run:**

```
      Tools -> Build

      -or-

      Ctrl + B

      CMD + B  (OSX)

```

*This would start your file in the console which should be at the bottom of the editor.*

**To Stop:**

```
       Ctrl + Break or Tools -> Cancel Build

       Fn + C (OSX)

```

You can find out where your `Break` key is here: <http://en.wikipedia.org/wiki/Break_key>.

Note: `CTRL + C` will **NOT** work.

**What to do when `Ctrl + Break` does not work:**

Go to:

> Preferences -> Key Bindings - User

and paste the line below:

```
{""keys"": [""ctrl+shift+c""], ""command"": ""exec"", ""args"": {""kill"": true} } 

```

Now, you can use `ctrl+shift+c` instead of `CTRL+BREAK`"
"Is there a javascript equivalent of Python's zip function? That is, given multiple arrays of equal lengths create an array of pairs.

For instance, if I have three arrays that look like this:

```
var array1 = [1, 2, 3];
var array2 = ['a','b','c'];
var array3 = [4, 5, 6];

```

The output array should be:

```
var outputArray = [[1,'a',4], [2,'b',5], [3,'c',6]]

```","**2016 update:**

Here's a snazzier Ecmascript 6 version:

```
zip= rows=>rows[0].map((_,c)=>rows.map(row=>row[c]))

```

Illustration equiv. to ***Python***{`zip(*args)`}:

```
> zip([['row0col0', 'row0col1', 'row0col2'],
       ['row1col0', 'row1col1', 'row1col2']]);
[[""row0col0"",""row1col0""],
 [""row0col1"",""row1col1""],
 [""row0col2"",""row1col2""]]

```

(and FizzyTea points out that ES6 has variadic argument syntax, so the following function definition will act like python, but see below for disclaimer... this will not be its own inverse so `zip(zip(x))` will not equal `x`; though as Matt Kramer points out `zip(...zip(...x))==x` (like in regular python `zip(*zip(*x))==x`))

Alternative definition equiv. to ***Python***{`zip`}:

```
> zip = (...rows) => [...rows[0]].map((_,c) => rows.map(row => row[c]))
> zip( ['row0col0', 'row0col1', 'row0col2'] ,
       ['row1col0', 'row1col1', 'row1col2'] );
             // note zip(row0,row1), not zip(matrix)
same answer as above

```

(Do note that the `...` syntax may have performance issues at this time, and possibly in the future, so if you use the second answer with variadic arguments, you may want to perf test it. That said it's been quite a while since it's been in the standard.)

Make sure to note the addendum if you wish to use this on strings (perhaps there's a better way to do it now with es6 iterables).

---

Here's a oneliner:

```
function zip(arrays) {
    return arrays[0].map(function(_,i){
        return arrays.map(function(array){return array[i]})
    });
}

// > zip([[1,2],[11,22],[111,222]])
// [[1,11,111],[2,22,222]]]

// If you believe the following is a valid return value:
//   > zip([])
//   []
// then you can special-case it, or just do
//  return arrays.length==0 ? [] : arrays[0].map(...)

```

---

The above assumes that the arrays are of equal size, as they should be. It also assumes you pass in a single list of lists argument, unlike Python's version where the argument list is variadic. **If you want all of these** ""features"", see below. It takes just about 2 extra lines of code.

The following will mimic Python's `zip` behavior on edge cases where the arrays are not of equal size, silently pretending the longer parts of arrays don't exist:

```
function zip() {
    var args = [].slice.call(arguments);
    var shortest = args.length==0 ? [] : args.reduce(function(a,b){
        return a.length<b.length ? a : b
    });

    return shortest.map(function(_,i){
        return args.map(function(array){return array[i]})
    });
}

// > zip([1,2],[11,22],[111,222,333])
// [[1,11,111],[2,22,222]]]

// > zip()
// []

```

This will mimic Python's `itertools.zip_longest` behavior, inserting `undefined` where arrays are not defined:

```
function zip() {
    var args = [].slice.call(arguments);
    var longest = args.reduce(function(a,b){
        return a.length>b.length ? a : b
    }, []);

    return longest.map(function(_,i){
        return args.map(function(array){return array[i]})
    });
}

// > zip([1,2],[11,22],[111,222,333])
// [[1,11,111],[2,22,222],[null,null,333]]

// > zip()
// []

```

If you use these last two version (variadic aka. multiple-argument versions), then zip is no longer its own inverse. To mimic the `zip(*[...])` idiom from Python, you will need to do `zip.apply(this, [...])` when you want to invert the zip function or if you want to similarly have a variable number of lists as input.

---

**addendum**:

To make this handle any iterable (e.g. in Python you can use `zip` on strings, ranges, map objects, etc.), you could define the following:

```
function iterView(iterable) {
    // returns an array equivalent to the iterable
}

```

However if you write `zip` in the following [way](https://stackoverflow.com/a/13735425/711085), even that won't be necessary:

```
function zip(arrays) {
    return Array.apply(null,Array(arrays[0].length)).map(function(_,i){
        return arrays.map(function(array){return array[i]})
    });
}

```

Demo:

```
> JSON.stringify( zip(['abcde',[1,2,3,4,5]]) )
[[""a"",1],[""b"",2],[""c"",3],[""d"",4],[""e"",5]]

```

(Or you could use a `range(...)` Python-style function if you've written one already. Eventually you will be able to use ECMAScript array comprehensions or generators.)"
"I installed Anaconda3 4.4.0 (32 bit) on my Windows 7 Professional machine and imported NumPy and Pandas on Jupyter notebook so I assume Python was installed correctly. But when I type `conda list` and `conda --version` in command prompt, it says `conda is not recognized as internal or external command.`

I have set environment variable for Anaconda3; `Variable Name: Path`, `Variable Value: C:\Users\dipanwita.neogy\Anaconda3`

How do I make it work?","I was faced with the same issue in windows 10, Updating the environment variable following steps, it's working fine.

I know It is a lengthy answer for the simple environment setups, I thought it's may be useful for the new window 10 users.

**1) Open Anaconda Prompt:**

[![enter image description here](https://i.sstatic.net/HtHFu.png)](https://i.sstatic.net/HtHFu.png)

2) Check Conda Installed Location.

```
where conda

```

[![enter image description here](https://i.sstatic.net/0NGNJ.png)](https://i.sstatic.net/0NGNJ.png)

**3) Open Advanced System Settings**

[![enter image description here](https://i.sstatic.net/W1loq.png)](https://i.sstatic.net/W1loq.png)

**4) Click on Environment Variables**

[![enter image description here](https://i.sstatic.net/TUBh9.png)](https://i.sstatic.net/TUBh9.png)

**5) Edit Path**

[![enter image description here](https://i.sstatic.net/OytYj.png)](https://i.sstatic.net/OytYj.png)

**6) Add New Path**

```
 C:\Users\RajaRama\Anaconda3\Scripts

 C:\Users\RajaRama\Anaconda3

 C:\Users\RajaRama\Anaconda3\Library\bin

```

[![enter image description here](https://i.sstatic.net/9WbkZ.png)](https://i.sstatic.net/9WbkZ.png)

**7) Open Command Prompt and Check Versions**

8) After 7th step type
conda install anaconda-navigator in cmd then press y

[![enter image description here](https://i.sstatic.net/jAshq.png)](https://i.sstatic.net/jAshq.png)"
"Besides writing a function, is the any short way to check if one of multiple items is in a list?

The following did not work (expected `True` to print in both cases):

```
>>> a = [2,3,4]
>>> print (1 or 2) in a
False
>>> print (2 or 1) in a
True

```","```
>>> L1 = [2,3,4]
>>> L2 = [1,2]
>>> [i for i in L1 if i in L2]
[2]


>>> S1 = set(L1)
>>> S2 = set(L2)
>>> S1.intersection(S2)
set([2])

```

Both empty lists and empty sets are False, so you can use the value directly as a truth value."
"```
d = {'x': 1, 'y': 2, 'z': 3}

for key in d:
    print(key, 'corresponds to', d[key])

```

How does Python recognize that it needs only to read the `key` from the dictionary? Is `key` a special keyword, or is it simply a variable?","`key` is just a variable name.

```
for key in d:

```

will simply loop over the keys in the dictionary, rather than the keys and values. To loop over both key and value you can use the following:

For Python 3.x:

```
for key, value in d.items():

```

For Python 2.x:

```
for key, value in d.iteritems():

```

To test for yourself, change the word `key` to `poop`.

In Python 3.x, `iteritems()` was replaced with simply `items()`, which returns a set-like view backed by the dict, like `iteritems()` but even better.
This is also available in 2.7 as `viewitems()`.

The operation `items()` will work for both 2 and 3, but in 2 it will return a list of the dictionary's `(key, value)` pairs, which will not reflect changes to the dict that happen after the `items()` call. If you want the 2.x behavior in 3.x, you can call `list(d.items())`."
"Just like C, you can break a long line into multiple short lines. But in [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29), if I do this, there will be an indent error... Is it possible?","From [PEP 8 - Style Guide for Python Code](http://www.python.org/dev/peps/pep-0008/):

> The preferred way of wrapping long lines is by using Python's implied line
> continuation inside parentheses, brackets and braces. If necessary, you
> can add an extra pair of parentheses around an expression, but sometimes
> using a backslash looks better. Make sure to indent the continued line
> appropriately.

Example of implicit line continuation:

```
a = (
    '1'
    + '2'
    + '3'
    - '4'
)


b = some_function(
    param1=foo(
        ""a"", ""b"", ""c""
    ),
    param2=bar(""d""),
)

```

On the topic of line breaks around a binary operator, it goes on to say:

> For decades the recommended style was to break after binary operators.
> But this can hurt readability in two ways: the operators tend to get scattered across different columns on the screen, and each operator is moved away from its operand and onto the previous line.

> In Python code, it is permissible to break before or after a binary operator, as long as the convention is consistent locally. For new code Knuth's style (line breaks *before* the operator) is suggested.

Example of explicit line continuation:

```
a = '1'   \
    + '2' \
    + '3' \
    - '4'

```"
"I just need a python script that copies text to the clipboard.

After the script gets executed i need the output of the text to be pasted to another source.
Is it possible to write a python script that does this job?","See [Pyperclip](https://pypi.org/project/pyperclip/). Example (taken from Pyperclip site):

```
import pyperclip
pyperclip.copy('The text to be copied to the clipboard.')
spam = pyperclip.paste()

```

Also, see [Xerox](https://github.com/kennethreitz/xerox). But it appears to have more dependencies."
"How can I test if a string contains only whitespace?

Example strings:

* `"" ""` (space, space, space)
* `"" \t \n ""` (space, tab, space, newline, space)
* `""\n\n\n\t\n""` (newline, newline, newline, tab, newline)","Use the [`str.isspace()`](https://docs.python.org/3/library/stdtypes.html#str.isspace) method:

> Return `True` if there are only whitespace characters in the string and there is at least one character, `False` otherwise.
>
> A character is whitespace if in the Unicode character database (see [unicodedata](https://docs.python.org/3/library/unicodedata.html#module-unicodedata)), either its general category is Zs (“Separator, space”), or its bidirectional class is one of WS, B, or S.

Combine that with a special case for handling the empty string.

Alternatively, you could use [`str.strip()`](https://docs.python.org/3/library/stdtypes.html#str.strip) and check if the result is empty."
"I have been using the introductory example of matrix multiplication in TensorFlow.

```
matrix1 = tf.constant([[3., 3.]])
matrix2 = tf.constant([[2.],[2.]])
product = tf.matmul(matrix1, matrix2)

```

When I print the product, it is displaying it as a `Tensor` object:

```
<tensorflow.python.framework.ops.Tensor object at 0x10470fcd0>

```

But how do I know the value of `product`?

The following doesn't help:

```
print product
Tensor(""MatMul:0"", shape=TensorShape([Dimension(1), Dimension(1)]), dtype=float32)

```

I know that graphs run on `Sessions`, but isn't there any way I can check the output of a `Tensor` object without running the graph in a `session`?","The easiest[A] way to evaluate the actual value of a `Tensor` object is to pass it to the `Session.run()` method, or call `Tensor.eval()` when you have a default session (i.e. in a `with tf.Session():` block, or see below). In general[B], you cannot print the value of a tensor without running some code in a session.

If you are experimenting with the programming model, and want an easy way to evaluate tensors, the [`tf.InteractiveSession`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/InteractiveSession) lets you open a session at the start of your program, and then use that session for all `Tensor.eval()` (and `Operation.run()`) calls. This can be easier in an interactive setting, such as the shell or an IPython notebook, when it's tedious to pass around a `Session` object everywhere. For example, the following works in a Jupyter notebook:

```
with tf.Session() as sess:  print(product.eval()) 

```

This might seem silly for such a small expression, but one of the key ideas in Tensorflow 1.x is *deferred execution*: it's very cheap to build a large and complex expression, and when you want to evaluate it, the back-end (to which you connect with a `Session`) is able to schedule its execution more efficiently (e.g. executing independent parts in parallel and using GPUs).

---

[A]: To print the value of a tensor without returning it to your Python program, you can use the [`tf.print()`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/print) operator, as [Andrzej suggests in another answer](https://stackoverflow.com/a/36296783/3574081). According to the official documentation:

> To make sure the operator runs, users need to pass the produced op to `tf.compat.v1.Session`'s run method, or to use the op as a control dependency for executed ops by specifying with `tf.compat.v1.control_dependencies([print_op]`), which is printed to standard output.

Also note that:

> In Jupyter notebooks and colabs, `tf.print` prints to the notebook cell outputs. It will not write to the notebook kernel's console logs.

[B]: You *might* be able to use the [`tf.get_static_value()`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/get_static_value) function to get the constant value of the given tensor if its value is efficiently calculable."
"From the [Udacity's deep learning class](https://www.udacity.com/course/viewer#!/c-ud730/l-6370362152/m-6379811820), the softmax of `y_i` is simply the exponential divided by the sum of exponential of the whole Y vector:

[![enter image description here](https://i.sstatic.net/iP8Du.png)](https://i.sstatic.net/iP8Du.png)

Where `S(y_i)` is the softmax function of `y_i` and `e` is the exponential and `j` is the no. of columns in the input vector Y.

I've tried the following:

```
import numpy as np

def softmax(x):
    """"""Compute softmax values for each sets of scores in x.""""""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

scores = [3.0, 1.0, 0.2]
print(softmax(scores))

```

which returns:

```
[ 0.8360188   0.11314284  0.05083836]

```

But the suggested solution was:

```
def softmax(x):
    """"""Compute softmax values for each sets of scores in x.""""""
    return np.exp(x) / np.sum(np.exp(x), axis=0)

```

which produces the **same output as the first implementation**, even though the first implementation explicitly takes the difference of each column and the max and then divides by the sum.

Can someone show mathematically why? Is one correct and the other one wrong?

Are the implementation similar in terms of code and time complexity? Which is more efficient?","They're both correct, but yours is preferred from the point of view of numerical stability.

You start with

```
e ^ (x - max(x)) / sum(e^(x - max(x))

```

By using the fact that a^(b - c) = (a^b)/(a^c) we have

```
= e ^ x / (e ^ max(x) * sum(e ^ x / e ^ max(x)))

= e ^ x / sum(e ^ x)

```

Which is what the other answer says. You could replace max(x) with any variable and it would cancel out."
"When I try this code:

```
a, b, c = (1, 2, 3)

def test():
    print(a)
    print(b)
    print(c)
    c += 1
test()

```

I get an error from the `print(c)` line that says:

```
UnboundLocalError: local variable 'c' referenced before assignment

```

or in some older versions:

```
UnboundLocalError: 'c' not assigned

```

If I comment out `c += 1`, all the `print`s are successful.

I don't understand: why does printing `a` and `b` work, if `c` does not? How did `c += 1` cause `print(c)` to fail, even when it comes later in the code?

It seems like the assignment `c += 1` creates a **local** variable `c`, which takes precedence over the global `c`. But how can a variable ""steal"" scope before it exists? Why is `c` apparently local here?

---

See also [How to use a global variable in a function?](https://stackoverflow.com/questions/423379/) for questions that are simply about how to reassign a global variable from within a function, and [Is it possible to modify a variable in python that is in an outer (enclosing), but not global, scope?](https://stackoverflow.com/questions/8447947) for reassigning from an enclosing function (closure).

See [Why isn't the 'global' keyword needed to access a global variable?](https://stackoverflow.com/questions/4693120) for cases where OP *expected* an error but *didn't* get one, from simply accessing a global without the `global` keyword.

See [How can a name be ""unbound"" in Python? What code can cause an `UnboundLocalError`?](https://stackoverflow.com/questions/22101836/) for cases where OP *expected* the variable to be local, but has a logical error that prevents assignment in every case.

See [How can ""NameError: free variable 'var' referenced before assignment in enclosing scope"" occur in real code?](https://stackoverflow.com/questions/24707379/) for a related problem caused by the `del` keyword.","Python treats variables in functions differently depending on whether you assign values to them from inside or outside the function. **If a variable is assigned within a function, it is treated by default as a local variable**. Therefore, when you uncomment the line, you are trying to reference the local variable `c` before any value has been assigned to it.

If you want the variable `c` to refer to the global `c = 3` assigned before the function, put

```
global c

```

as the first line of the function.

As for python 3, there is now

```
nonlocal c

```

that you can use to refer to the nearest enclosing function scope that has a `c` variable."
"Recently I started using Python3 and it's lack of `xrange` hurts.

Simple example:

1. Python2:

   ```
   from time import time as t
   def count():
     st = t()
     [x for x in xrange(10000000) if x%4 == 0]
     et = t()
     print et-st
   count()

   ```
2. Python3:

   ```
   from time import time as t

   def xrange(x):

       return iter(range(x))

   def count():
       st = t()
       [x for x in xrange(10000000) if x%4 == 0]
       et = t()
       print (et-st)
   count()

   ```

The results are, respectively:

1. `1.53888392448`
2. `3.215819835662842`

Why is that? I mean, why `xrange` has been removed? It's such a great tool to learn. For the beginners, just like myself, like we all were at some point. Why remove it? Can somebody point me to the proper PEP, I can't find it.","Some performance measurements, using `timeit` instead of trying to do it manually with `time`.

First, Apple 2.7.2 64-bit:

```
In [37]: %timeit collections.deque((x for x in xrange(10000000) if x%4 == 0), maxlen=0)
1 loops, best of 3: 1.05 s per loop

```

Now, python.org 3.3.0 64-bit:

```
In [83]: %timeit collections.deque((x for x in range(10000000) if x%4 == 0), maxlen=0)
1 loops, best of 3: 1.32 s per loop

In [84]: %timeit collections.deque((x for x in xrange(10000000) if x%4 == 0), maxlen=0)
1 loops, best of 3: 1.31 s per loop

In [85]: %timeit collections.deque((x for x in iter(range(10000000)) if x%4 == 0), maxlen=0) 
1 loops, best of 3: 1.33 s per loop

```

Apparently, 3.x `range` really is a bit slower than 2.x `xrange`. And the OP's `xrange` function has nothing to do with it. (Not surprising, as a one-time call to the `__iter__` slot isn't likely to be visible among 10000000 calls to whatever happens in the loop, but someone brought it up as a possibility.)

But it's only 30% slower. How did the OP get 2x as slow? Well, if I repeat the same tests with 32-bit Python, I get 1.58 vs. 3.12. So my guess is that this is yet another of those cases where 3.x has been optimized for 64-bit performance in ways that hurt 32-bit.

But does it really matter? Check this out, with 3.3.0 64-bit again:

```
In [86]: %timeit [x for x in range(10000000) if x%4 == 0]
1 loops, best of 3: 3.65 s per loop

```

So, building the `list` takes more than twice as long than the entire iteration.

And as for ""consumes much more resources than Python 2.6+"", from my tests, it looks like a 3.x `range` is exactly the same size as a 2.x `xrange`â€”and, even if it were 10x as big, building the unnecessary list is still about 10000000x more of a problem than anything the range iteration could possibly do.

And what about an explicit `for` loop instead of the C loop inside `deque`?

```
In [87]: def consume(x):
   ....:     for i in x:
   ....:         pass
In [88]: %timeit consume(x for x in range(10000000) if x%4 == 0)
1 loops, best of 3: 1.85 s per loop

```

So, almost as much time wasted in the `for` statement as in the actual work of iterating the `range`.

If you're worried about optimizing the iteration of a range object, you're probably looking in the wrong place.

---

Meanwhile, you keep asking why `xrange` was removed, no matter how many times people tell you the same thing, but I'll repeat it again: It was not removed: it was renamed to `range`, and the 2.x `range` is what was removed.

Here's some proof that the 3.3 `range` object is a direct descendant of the 2.x `xrange` object (and not of the 2.x `range` function): the source to [3.3 `range`](http://hg.python.org/cpython/file/3.3/Objects/rangeobject.c) and [2.7 `xrange`](http://hg.python.org/cpython/file/2.7/Objects/rangeobject.c). You can even see the [change history](http://hg.python.org/cpython/diff/a6eb6acfe04a/Objects/rangeobject.c) (linked to, I believe, the change that replaced the last instance of the string ""xrange"" anywhere in the file).

So, why is it slower?

Well, for one, they've added a lot of new features. For another, they've done all kinds of changes all over the place (especially inside iteration) that have minor side effects. And there'd been a lot of work to dramatically optimize various important cases, even if it sometimes slightly pessimizes less important cases. Add this all up, and I'm not surprised that iterating a `range` as fast as possible is now a bit slower. It's one of those less-important cases that nobody would ever care enough to focus on. No one is likely to ever have a real-life use case where this performance difference is the hotspot in their code."
"There is a `--user` option for pip which can install a Python package per user:

```
pip install --user [python-package-name]

```

I used this option to install a package on a server for which I do not have root access. What I need now is to uninstall the installed package on the current user. I tried to execute this command:

```
pip uninstall --user [python-package-name]

```

But I got:

```
no such option: --user

```

How can I uninstall a package that I installed with `pip install --user`, other than manually finding and deleting the package?

I've found this article

[pip cannot uninstall from per-user site-packages directory](https://stackoverflow.com/questions/33412974/uninstall-python-package-per-user)

which describes that uninstalling packages from user directory does not supported. According to the article if it was implemented correctly then with

```
pip uninstall [package-name]

```

the package that was installed will be also searched in user directories. But a problem still remains for me. What if the same package was installed both system-wide and per-user?
What if someone needs to target a specific user directory?","Having tested this using Python 3.5 and pip 7.1.2 on Linux, the situation appears to be this:

* `pip install --user somepackage` installs to `$HOME/.local`, and uninstalling it does work using `pip uninstall somepackage`.
* This is true whether or not `somepackage` is also installed system-wide at the same time.
* If the package is installed at both places, only the local one will be uninstalled. To uninstall the package system-wide using `pip`, first uninstall it locally, then run the same uninstall command again, with `root` privileges.
* In addition to the predefined user install directory, `pip install --target somedir somepackage` will install the package into `somedir`. There is no way to uninstall a package from such a place using `pip`. (But there is a somewhat old unmerged pull request on Github that implements `pip uninstall --target`.)
* Since the only places `pip` will ever uninstall from are system-wide and predefined user-local, you need to run `pip uninstall` as the respective user to uninstall from a given user's local install directory."
"I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:

```
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen(""http://en.wikipedia.org""+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href'] 
                print(newPage) 
                pages.add(newPage) 
                getLinks(newPage)
getLinks("""")

```

The error is:

```
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1049)>

```

Btw,I was also practicing scrapy, but kept getting the problem: command not found: scrapy (I tried all sorts of solutions online but none works... really frustrating)","Once upon a time I stumbled with this issue. If you're using macOS go to Macintosh HD > Applications > Python3.6 folder (or whatever version of python you're using) > double click on ""Install Certificates.command"" file. :D"
"I'm working on a certain program where I need to do different things depending on the extension of the file. Could I just use this?

```
if m == *.mp3
   ...
elif m == *.flac
   ...

```","Assuming `m` is a string, you can use `endswith`:

```
if m.endswith('.mp3'):
...
elif m.endswith('.flac'):
...

```

To be case-insensitive, and to eliminate a potentially large else-if chain:

```
m.lower().endswith(('.png', '.jpg', '.jpeg'))

```"
"I am attempting to insert data from a dictionary into a database. I want to iterate over the values and format them accordingly, depending on the data type. Here is a snippet of the code I am using:

```
def _db_inserts(dbinfo):
    try:
        rows = dbinfo['datarows']

        for row in rows:
            field_names = "","".join([""'{0}'"".format(x) for x in row.keys()])
            value_list = row.values()

            for pos, value in enumerate(value_list):
                if isinstance(value, str):
                    value_list[pos] = ""'{0}'"".format(value)
                elif isinstance(value, datetime):
                    value_list[pos] = ""'{0}'"".format(value.strftime('%Y-%m-%d'))

            values = "","".join(value_list)

            sql = ""INSERT INTO table_foobar ({0}) VALUES ({1})"".format(field_names, values)

    except Exception as e:
        print 'BARFED with msg:',e

```

When I run the algo using some sample data (see below), I get the error:

> TypeError: sequence item 0: expected string, int found

An example of a value\_list data which gives the above error is:

```
value_list = [377, -99999, -99999, 'f', -99999, -99999, -99999, 1108.0999999999999, 0, 'f', -99999, 0, 'f', -99999, 'f', -99999, 1108.0999999999999, -99999, 'f', -99999, 'f', -99999, 'f', 'f', 0, 1108.0999999999999, -99999, -99999, 'f', 'f', 'f', -99999, 'f', '1984-04-02', -99999, 'f', -99999, 'f', 1108.0999999999999] 

```

What am I doing wrong?","`string.join` connects elements inside list of strings, not ints.

Use this generator expression instead :

```
values = ','.join(str(v) for v in value_list)

```"
"What is the most efficient way to map a function over a numpy array? I am currently doing:

```
import numpy as np 

x = np.array([1, 2, 3, 4, 5])

# Obtain array of square of each element in x
squarer = lambda t: t ** 2
squares = np.array([squarer(xi) for xi in x])

```

However, this is probably very inefficient, since I am using a list comprehension to construct the new array as a Python list before converting it back to a numpy array. Can we do better?","I've tested all suggested methods plus `np.array(list(map(f, x)))` with [`perfplot`](https://github.com/nschloe/perfplot) (a small project of mine).

> Message #1: If you can use numpy's native functions, do that.

If the function you're trying to vectorize already *is* vectorized (like the `x**2` example in the original post), using that is *much* faster than anything else (note the log scale):

[![enter image description here](https://i.sstatic.net/IDYsC.png)](https://i.sstatic.net/IDYsC.png)

If you actually need vectorization, it doesn't really matter much which variant you use.

[![enter image description here](https://i.sstatic.net/V30x6.png)](https://i.sstatic.net/V30x6.png)

---

Code to reproduce the plots:

```
import numpy as np
import perfplot
import math


def f(x):
    # return math.sqrt(x)
    return np.sqrt(x)


vf = np.vectorize(f)


def array_for(x):
    return np.array([f(xi) for xi in x])


def array_map(x):
    return np.array(list(map(f, x)))


def fromiter(x):
    return np.fromiter((f(xi) for xi in x), x.dtype)


def vectorize(x):
    return np.vectorize(f)(x)


def vectorize_without_init(x):
    return vf(x)


b = perfplot.bench(
    setup=np.random.rand,
    n_range=[2 ** k for k in range(20)],
    kernels=[
        f,
        array_for,
        array_map,
        fromiter,
        vectorize,
        vectorize_without_init,
    ],
    xlabel=""len(x)"",
)
b.save(""out1.svg"")
b.show()

```"
"I am trying to add a new row to my old CSV file. Basically, it gets updated each time I run the Python script.

Right now I am storing the old CSV rows values in a list and then deleting the CSV file and creating it again with the new list value.

I wanted to know are there any better ways of doing this.","```
with open('document.csv','a') as fd:
    fd.write(myCsvRow)

```

Opening a file with the `'a'` parameter allows you to append to the end of the file instead of simply overwriting the existing content. Try that."
"I installed Python Anaconda on Mac (OS Mavericks). I wanted to revert to the default version of Python on my Mac. What's the best way to do this? Should I delete the `~/anaconda` directory? Any other changes required?

Currently when I run `which python` I get this path:

`/Users/username/anaconda/bin/python`","From the [docs](https://conda.io/docs/install/full.html#os-x-anaconda-install):

> To uninstall Anaconda open a terminal window and remove the entire
> anaconda install directory: `rm -rf ~/anaconda`. You may also edit
> `~/.bash_profile` and remove the anaconda directory from your `PATH`
> environment variable, and remove the hidden `.condarc` file and
> `.conda` and `.continuum` directories which may have been created in
> the home directory with `rm -rf ~/.condarc ~/.conda ~/.continuum`.

Further notes:

* Python3 installs may use a `~/anaconda3` dir instead of `~/anaconda`.
* You might also have a `~/.anaconda` hidden directory that may be removed.
* Depending on how you installed, it is possible that the `PATH` is modified in one of your runcom files, and not in your shell profile. So, for example if you are using bash, be sure to check your `~/.bashrc` if you don't find the `PATH` modified in `~/.bash_profile`."
How to call a shell script from python code?,"The [subprocess](http://docs.python.org/library/subprocess.html) module will help you out.

Blatantly trivial example:

```
>>> import subprocess
>>> subprocess.call(['sh', './test.sh']) # Thanks @Jim Dennis for suggesting the []
0 
>>> 

```

Where `test.sh` is a simple shell script and `0` is its return value for this run."
What do I have to do in Python to figure out which encoding a string has?,"In Python 3, all strings are sequences of Unicode characters. There is a `bytes` type that holds raw bytes.

In Python 2, a string may be of type `str` or of type `unicode`. You can tell which using code something like this:

```
def whatisthis(s):
    if isinstance(s, str):
        print ""ordinary string""
    elif isinstance(s, unicode):
        print ""unicode string""
    else:
        print ""not a string""

```

This does not distinguish ""Unicode or ASCII""; it only distinguishes Python types. A Unicode string may consist of purely characters in the ASCII range, and a bytestring may contain ASCII, encoded Unicode, or even non-textual data."
"I have a pandas dataframe in the following format:

```
df = pd.DataFrame([
    [1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], 
    list('AAABBBBABCBDDD'), 
    [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], 
    ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],
    ['1','3','3','2','4','2','5','3','6','3','5','1','1','1']
]).T
df.columns = ['col1','col2','col3','col4','col5']

```

df:

```
   col1 col2 col3     col4 col5
0   1.1    A  1.1    x/y/z    1
1   1.1    A  1.7      x/y    3
2   1.1    A  2.5  x/y/z/n    3
3   2.6    B  2.6      x/u    2
4   2.5    B  3.3        x    4
5   3.4    B  3.8    x/u/v    2
6   2.6    B    4    x/y/z    5
7   2.6    A  4.2        x    3
8   3.4    B  4.3  x/u/v/b    6
9   3.4    C  4.5        -    3
10  2.6    B  4.6      x/y    5
11  1.1    D  4.7    x/y/z    1
12  1.1    D  4.7        x    1
13  3.3    D  4.8  x/u/v/w    1

```

I want to get the count by each row like following. Expected Output:

```
col5 col2 count
1    A      1
     D      3
2    B      2
etc...

```

How to get my expected output? And I want to find largest count for each 'col2' value?","You are looking for [`size`](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation):

```
In [11]: df.groupby(['col5', 'col2']).size()
Out[11]:
col5  col2
1     A       1
      D       3
2     B       2
3     A       3
      C       1
4     B       1
5     B       2
6     B       1
dtype: int64

```

---

To get the same answer as waitingkuo (the ""second question""), but slightly cleaner, is to groupby the level:

```
In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max()
Out[12]:
col2
A       3
B       2
C       1
D       3
dtype: int64

```"
"In Python, what is the difference between `json.load()` and `json.loads()`?

I guess that the *load()* function must be used with a file object (I need thus to use a context manager) while the *loads()* function take the path to the file as a string. It is a bit confusing.

Does the letter ""**s**"" in `json.loads()` stand for **string**?","Yes, `s` stands for string. The `json.loads` function does not take the file path, but the file contents as a string. Look at [the documentation](https://docs.python.org/3/library/json.html#json.loads).

Simple example:

```
with open(""file.json"") as f:
  data = json.load(f)  # ok

  data = json.loads(f)  # not ok, f is not a string but a file

```

```
text = '{""a"": 1, ""b"": 2}'  # a string with json encoded data
data = json.loads(text) 

```"
"I'm using a python script as a driver for a hydrodynamics code. When it comes time to run the simulation, I use `subprocess.Popen` to run the code, collect the output from `stdout` and `stderr` into a `subprocess.PIPE` --- then I can print (and save to a log-file) the output information, and check for any errors. The problem is, I have no idea how the code is progressing. If I run it directly from the command line, it gives me output about what iteration its at, what time, what the next time-step is, etc.

**Is there a way to both store the output (for logging and error checking), and also produce a live-streaming output?**

The relevant section of my code:

```
ret_val = subprocess.Popen( run_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True )
output, errors = ret_val.communicate()
log_file.write(output)
print output
if( ret_val.returncode ):
    print ""RUN failed\n\n%s\n\n"" % (errors)
    success = False

if( errors ): log_file.write(""\n\n%s\n\n"" % errors)

```

Originally I was piping the `run_command` through `tee` so that a copy went directly to the log-file, and the stream still output directly to the terminal -- but that way I can't store any errors (to my knowlege).

---

My temporary solution so far:

```
ret_val = subprocess.Popen( run_command, stdout=log_file, stderr=subprocess.PIPE, shell=True )
while not ret_val.poll():
    log_file.flush()

```

then, in another terminal, run `tail -f log.txt` (s.t. `log_file = 'log.txt'`).","TLDR for Python 3:

```
import subprocess
import sys

with open(""test.log"", ""wb"") as f:
    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)
    for c in iter(lambda: process.stdout.read(1), b""""):
        sys.stdout.buffer.write(c)
        f.buffer.write(c)

```

---

You have two ways of doing this, either by creating an iterator from the `read` or `readline` functions and do:

```
import subprocess
import sys

# replace ""w"" with ""wb"" for Python 3
with open(""test.log"", ""w"") as f:
    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)
    # replace """" with b'' for Python 3
    for c in iter(lambda: process.stdout.read(1), """"):
        sys.stdout.write(c)
        f.write(c)

```

or

```
import subprocess
import sys

# replace ""w"" with ""wb"" for Python 3
with open(""test.log"", ""w"") as f:
    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)
    # replace """" with b"""" for Python 3
    for line in iter(process.stdout.readline, """"):
        sys.stdout.write(line)
        f.write(line)

```

Or you can create a `reader` and a `writer` file. Pass the `writer` to the [`Popen`](https://docs.python.org/3/library/subprocess.html#subprocess.Popen) and read from the `reader`

```
import io
import time
import subprocess
import sys

filename = ""test.log""
with io.open(filename, ""wb"") as writer, io.open(filename, ""rb"", 1) as reader:
    process = subprocess.Popen(command, stdout=writer)
    while process.poll() is None:
        sys.stdout.write(reader.read())
        time.sleep(0.5)
    # Read the remaining
    sys.stdout.write(reader.read())

```

This way you will have the data written in the `test.log` as well as on the standard output.

The only advantage of the file approach is that your code doesn't block. So you can do whatever you want in the meantime and read whenever you want from the `reader` in a non-blocking way. When you use `PIPE`, `read` and `readline` functions will block until either one character is written to the pipe or a line is written to the pipe respectively."
"In Java, I can override the `toString()` method of my class. Then Java's print function prints the string representation of the object defined by its `toString()`. Is there a Python equivalent to Java's `toString()`?

For example, I have a PlayCard class. I have an instance c of PlayCard. Now:

```
>>> print(c)
<__main__.Card object at 0x01FD5D30>

```

But what I want is something like:

```
>>> print(c)
Aâ™£

```

How do I customize the string representation of my class instances?

I'm using Python 3.x","The closest equivalent to Java's `toString` is to implement `__str__` for your class. Put this in your class definition:

```
def __str__(self):
     return ""foo""

```

You may also want to implement `__repr__` to aid in debugging.

See here for more information:

* [Special Method Names - Basic Customization](http://docs.python.org/reference/datamodel.html#customization)"
"Does anyone know why Python's `list.append` method is not called `list.push`, given that there's already a `list.pop` that removes and returns the last element (indexed at `-1`) and `list.append` semantic is consistent with that usage?","Because ""append"" existed long before ""pop"" was thought of. [Python 0.9.1](http://www.dalkescientific.com/writings/diary/archive/2009/03/27/python_0_9_1p1.html) supported list.append in early 1991. By comparison, here's part of a [discussion on comp.lang.python](http://groups.google.com/group/comp.lang.python/browse_thread/thread/48a26add2d996268/57ab953cb73bc9c7?q=list+pop+group:comp.lang.python+author:guido#) about adding pop in 1997. Guido wrote:

> To implement a stack, one would need
> to add a list.pop() primitive (and
> no, I'm not against this particular
> one on the basis of any principle).
> list.push() could be added for
> symmetry with list.pop() but I'm not
> a big fan of multiple names for the
> same operation -- sooner or later
> you're going to read code that uses
> the other one, so you need to learn
> both, which is more cognitive load.

You can also see he discusses the idea of if push/pop/put/pull should be at element [0] or after element [-1] where he posts a reference to Icon's list:

> I stil think that all this is best
> left out of the list object
> implementation -- if you need a stack,
> or a queue, with particular
> semantics, write a little class that
> uses a lists

In other words, for stacks implemented directly as Python lists, which already supports fast append(), and del list[-1], it makes sense that list.pop() work by default on the last element. Even if other languages do it differently.

Implicit here is that most people need to append to a list, but many fewer have occasion to treat lists as stacks, which is why list.append came in so much earlier."
"There has to be an easier way to do this. I have objects that want to be refreshed every so often, so I want to record when they were created, check against the current timestamp, and refresh as necessary.

datetime.datetime has proven to be difficult, and I don't want to dive into the ctime library. Is there anything easier for this sort of thing?","if you want to compute differences between two known dates, use `total_seconds` like this:

```
import datetime as dt

a = dt.datetime(2013,12,30,23,59,59)
b = dt.datetime(2013,12,31,23,59,59)

(b-a).total_seconds()

```

86400.0

```
#note that seconds doesn't give you what you want:
(b-a).seconds

```

0"
"I'm trying to understand how Python packages work. Presumably `eggs` are some sort of packaging mechanism, but what would be a quick overview of what role they play and may be some information on why they're useful and how to create them?","> \*Note: [Egg packaging has been superseded by Wheel packaging.\*](https://packaging.python.org/en/latest/discussions/package-formats/)

Same concept as a `.jar` file in Java, it is a `.zip` file with some metadata files renamed `.egg`, for distributing code as bundles.

[Specifically: The Internal Structure of Python Eggs](http://svn.python.org/projects/sandbox/trunk/setuptools/doc/formats.txt)

> A ""Python egg"" is a logical structure embodying the release of a
> specific version of a Python project, comprising its code, resources,
> and metadata. There are multiple formats that can be used to
> physically encode a Python egg, and others can be developed. However,
> a key principle of Python eggs is that they should be discoverable and
> importable. That is, it should be possible for a Python application to
> easily and efficiently find out what eggs are present on a system, and
> to ensure that the desired eggs' contents are importable.
>
> The `.egg` format is well-suited to distribution and the easy
> uninstallation or upgrades of code, since the project is essentially
> self-contained within a single directory or file, unmingled with any
> other projects' code or resources. It also makes it possible to have
> multiple versions of a project simultaneously installed, such that
> individual programs can select the versions they wish to use."
"I am having trouble in using inheritance with Python. While the concept seems too easy for me in Java yet up till now I have been unable to understand in Python which is surprising to me at least.

I have a prototype which follow:

```
class Shape():
   def __init__(self, shape_name):
       self.shape = shape_name

class Rectangle(Shape):
   def __init__(self, name):
       self.shape = name

```

In the above code how can I make an abstract method that would need to be implemented for all the subclasses?","Before abc was introduced you would see this frequently.

```
class Base(object):
    def go(self):
        raise NotImplementedError(""Please Implement this method"")


class Specialized(Base):
    def go(self):
        print ""Consider me implemented""

```"
"I'm talking about doing something like:

```
for(i=n; i>=1; --i) {
   //do something with i
}

```

I can think of some ways to do so in python (creating a list of `range(1,n+1)` and reverse it, using `while` and `--i`, ...) but I wondered if there's a more elegant way to do it. Is there?

EDIT:
Some suggested I use xrange() instead of range() since range returns a list while xrange returns an iterator. But in Python 3 (which I happen to use) range() returns an iterator and xrange doesn't exist.","`range()` and `xrange()` take a third parameter that specifies a step. So you can do the following.

```
range(10, 0, -1)

```

Which gives

```
[10, 9, 8, 7, 6, 5, 4, 3, 2, 1] 

```

But for iteration, you should really be using `xrange` instead. So,

```
xrange(10, 0, -1)

```

> Note for Python 3 users: There are no separate `range` and `xrange` functions in Python 3, there is just `range`, which follows the design of Python 2's `xrange`."
"I have a time in UTC from which I want the number of seconds since epoch.

I am using strftime to convert it to the number of seconds. Taking 1st April 2012 as an example.

```
>>>datetime.datetime(2012,04,01,0,0).strftime('%s')
'1333234800'

```

1st of April 2012 UTC from epoch is 1333238400 but this above returns 1333234800 which is different by 1 hour.

So it looks like that strftime is taking my system time into account and applies a timezone shift somewhere. I thought datetime was purely naive?

How can I get around that? If possible avoiding to import other libraries unless standard. (I have portability concerns).","In Python 3.3+ you can use [`timestamp()`](https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp):

```
>>> datetime.datetime(2012,4,1,0,0).timestamp()
1333234800.0

```

In Python 3.2 or earlier, you could do it explicitly:

```
 >>> (datetime.datetime(2012,4,1,0,0) - datetime.datetime(1970,1,1)).total_seconds()
 1333238400.0

```

---

**Why you should not use `datetime.strftime('%s')`**

Python doesn't actually support %s as an argument to strftime (if you check at <http://docs.python.org/library/datetime.html#strftime-and-strptime-behavior> it's not in the list), the only reason it's working is because Python is passing the information to your system's strftime, which uses your local timezone.

```
>>> datetime.datetime(2012,04,01,0,0).strftime('%s')
'1333234800'

```"
How can I skip the header row and start reading a file from line2?,"```
with open(fname) as f:
    next(f)
    for line in f:
        #do something

```"
"This is obviously simple, but as a numpy newbe I'm getting stuck.

I have a CSV file that contains 3 columns, the State, the Office ID, and the Sales for that office.

I want to calculate the percentage of sales per office in a given state (total of all percentages in each state is 100%).

```
df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
                   'office_id': list(range(1, 7)) * 2,
                   'sales': [np.random.randint(100000, 999999)
                             for _ in range(12)]})

df.groupby(['state', 'office_id']).agg({'sales': 'sum'})

```

This returns:

```
                  sales
state office_id        
AZ    2          839507
      4          373917
      6          347225
CA    1          798585
      3          890850
      5          454423
CO    1          819975
      3          202969
      5          614011
WA    2          163942
      4          369858
      6          959285

```

I can't seem to figure out how to ""reach up"" to the `state` level of the `groupby` to total up the `sales` for the entire `state` to calculate the fraction.","Update 2022-03
==============

[This answer](https://stackoverflow.com/a/57359372/3393459) by [caner](https://stackoverflow.com/users/448625/caner) using `transform` looks much better than my original answer!

```
df['sales'] / df.groupby('state')['sales'].transform('sum')

```

Thanks to [this comment](https://stackoverflow.com/questions/23377108/pandas-percentage-of-total-with-groupby/23377232?noredirect=1#comment109237809_23377108) by [Paul Rougieux](https://stackoverflow.com/users/2641825/paul-rougieux) for surfacing it.

Original Answer (2014)
======================

[Paul H's answer](https://stackoverflow.com/a/23377155/3393459) is right that you will have to make a second `groupby` object, but you can calculate the percentage in a simpler way -- just `groupby` the `state_office` and divide the `sales` column by its sum. Copying the beginning of Paul H's answer:

```
# From Paul H
import numpy as np
import pandas as pd
np.random.seed(0)
df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
                   'office_id': list(range(1, 7)) * 2,
                   'sales': [np.random.randint(100000, 999999)
                             for _ in range(12)]})
state_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
# Change: groupby state_office and divide by sum
state_pcts = state_office.groupby(level=0).apply(lambda x:
                                                 100 * x / float(x.sum()))

```

Returns:

```
                     sales
state office_id           
AZ    2          16.981365
      4          19.250033
      6          63.768601
CA    1          19.331879
      3          33.858747
      5          46.809373
CO    1          36.851857
      3          19.874290
      5          43.273852
WA    2          34.707233
      4          35.511259
      6          29.781508

```"
"I have a list of tuples `l = [(1,2), (3,4), (8,9)]`. How can I, succinctly and Pythonically, unzip this list into two independent lists, to get `[ [1, 3, 8], [2, 4, 9] ]`?

In other words, how do I get the inverse of what [`zip`](http://docs.python.org/library/functions.html?highlight=zip#zip) does?","Use `zip(*list)`:

```
>>> l = [(1,2), (3,4), (8,9)]
>>> list(zip(*l))
[(1, 3, 8), (2, 4, 9)]

```

The [`zip()` function](https://docs.python.org/3/library/functions.html#zip) pairs up the elements from all inputs, starting with the first values, then the second, etc. By using `*l` you [apply all tuples in `l` as *separate arguments* to the `zip()` function](https://stackoverflow.com/questions/36901), so `zip()` pairs up `1` with `3` with `8` first, then `2` with `4` and `9`. Those happen to correspond nicely with the columns, or the *transposition* of `l`.

`zip()` produces tuples; if you must have mutable list objects, just `map()` the tuples to lists or use a list comprehension to produce a list of lists:

```
map(list, zip(*l))          # keep it a generator
[list(t) for t in zip(*l)]  # consume the zip generator into a list of lists

```"
"I have `dt = datetime(2013,9,1,11)`, and I would like to get a Unix timestamp of this datetime object.

When I do `(dt - datetime(1970,1,1)).total_seconds()` I got the timestamp `1378033200`.

When converting it back using `datetime.fromtimestamp` I got `datetime.datetime(2013, 9, 1, 6, 0)`.

The hour doesn't match. What did I miss here?","If you want to convert a python datetime to seconds since epoch you should do it explicitly:

```
>>> import datetime
>>> datetime.datetime(2012, 04, 01, 0, 0).strftime('%s')
'1333234800'
>>> (datetime.datetime(2012, 04, 01, 0, 0) - datetime.datetime(1970, 1, 1)).total_seconds()
1333238400.0

```

In Python 3.3+ you can use [`timestamp()`](https://docs.python.org/3/library/datetime.html?highlight=re#datetime.datetime.timestamp) instead:

```
>>> import datetime
>>> datetime.datetime(2012, 4, 1, 0, 0).timestamp()
1333234800.0

```"
"Is there a convenient way to calculate percentiles for a sequence or single-dimensional `numpy` array?

I am looking for something similar to Excel's percentile function.","NumPy has [`np.percentile()`](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html).

```
import numpy as np
a = np.array([1,2,3,4,5])
p = np.percentile(a, 50)  # return 50th percentile, i.e. median.

```

```
>>> print(p)
3.0

```

---

SciPy has [`scipy.stats.scoreatpercentile()`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.scoreatpercentile.html#scipy.stats.scoreatpercentile), in addition to many [other statistical goodies](http://docs.scipy.org/doc/scipy/reference/stats.html)."
"Python works on multiple platforms and can be used for desktop and web applications, thus I conclude that there is some way to compile it into an executable for Mac, Windows and Linux.

The problem being I have no idea where to start or how to write a GUI with it, can anybody shed some light on this and point me in the right direction please?","First you will need some GUI library with Python bindings and then (if you want) some program that will convert your python scripts into standalone executables.

Cross-platform GUI libraries with Python bindings (Windows, Linux, Mac)
-----------------------------------------------------------------------

Of course, there are many, but the most popular that I've seen in wild are:

* [Tkinter](http://wiki.python.org/moin/TkInter) - based on [Tk GUI toolkit](http://www.tcl.tk/).
  > De-facto standard GUI library for python, free for commercial projects.
* [WxPython](http://www.wxpython.org/) - based on [WxWidgets](http://www.wxwidgets.org/).
  > Popular, and free for commercial projects.
* [Qt](https://www.qt.io) using the [PyQt bindings](https://riverbankcomputing.com/software/pyqt/intro) or [Qt for Python](https://www.qt.io/qt-for-python).
  > The former is not free for commercial projects. The latter is less mature, but can be used for free.
  >
  > Qt itself supposedly supports `Android` and `iOS` as well, but achiving same with it's bindings should be tricky.
* [Kivy](https://github.com/kivy/kivy)
  written in Python for Python (update 2023).
  > Supposedly supports `Android` and `iOS` as well.

> **Note** that users of `WxWidgets` (hence `WxPython` users), often need to use `WxQt` as well, because `WxWidgets`'s own GUI is not yet at `Qt`'s level (at time of writting).

Complete list is at <http://wiki.python.org/moin/GuiProgramming>

Stand-alone/ single executables
-------------------------------

**For all platforms:**

* [PyInstaller](https://www.pyinstaller.org/) - The most active (which could also be used with `PyQt`)
* [fbs](https://build-system.fman.io) - if you chose Qt above (commercial, with free plan)

**For Windows:**

* [py2exe](http://www.py2exe.org/) - used to be the most popular

**For Linux:**

* [Freeze](http://wiki.python.org/moin/Freeze) - works the same way like py2exe but targets Linux platform

**For MacOS:**

* [py2app](https://pythonhosted.org/py2app/) - again, works like py2exe but targets Mac OS"
"Is it possible to stop execution of a python script at any line with a command?

Like

```
some code

quit() # quit at this point

some more code (that's not executed)

```","[sys.exit()](https://docs.python.org/2/library/sys.html#sys.exit) will do exactly what you want.

```
import sys
sys.exit(""Error message"")

```"
"I have a method that sometimes returns a NoneType value. So how can I question a variable that is a NoneType? I need to use ***if*** method, for example

```
if not new:
    new = '#'

```

I know that is the wrong way and I hope you understand what I meant.","> So how can I question a variable that is a NoneType?

Use `is` operator, like this

```
if variable is None:

```

**Why this works?**

Since [`None`](https://docs.python.org/2/library/constants.html#None) is the sole singleton object of [`NoneType`](https://docs.python.org/2/library/types.html#types.NoneType) in Python, we can use `is` operator to check if a variable has `None` in it or not.

Quoting from [`is` docs](https://docs.python.org/2/reference/expressions.html#is),

> The operators `is` and `is not` test for object identity: `x is y` is true if and only if `x` and `y` are the same object. `x is not y` yields the inverse truth value.

Since there can be only one instance of `None`, `is` would be the preferred way to check `None`.

---

**Hear it from the horse's mouth**

Quoting [Python's Coding Style Guidelines - PEP-008](https://www.python.org/dev/peps/pep-0008/#programming-recommendations) (jointly defined by Guido himself),

> Comparisons to singletons like **`None` should always be done with `is` or `is not`**, **never the equality operators**."
"I've got a dataframe called `data`. How would I rename the only one column header? For example `gdp` to `log(gdp)`?

```
data =
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7
5   4    8    3
6   8    2    8
7   9    9   10
8   6    6    4
9  10   10    7

```","```
data.rename(columns={'gdp':'log(gdp)'}, inplace=True)

```

The [`rename`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html#pandas.DataFrame.rename) show that it accepts a dict as a param for `columns` so you just pass a dict with a single entry.

Also see [related](https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas)"
"I would like to know how i can initialize an array(or list), yet to be populated with values, to have a defined size.

For example in C:

```
int x[5]; /* declared without adding elements*/

```

How do I do that in Python?","You can use:

```
>>> lst = [None] * 5
>>> lst
[None, None, None, None, None]

```"
"I'm very new to Python and multithreaded programming in general. Basically, I have a script that will copy files to another location. I would like this to be placed in another thread so I can output `....` to indicate that the script is still running.

The problem that I am having is that if the files cannot be copied it will throw an exception. This is OK if running in the main thread; however, having the following code does not work:

```
try:
    threadClass = TheThread(param1, param2, etc.)
    threadClass.start()   ##### **Exception takes place here**
except:
    print ""Caught an exception""

```

In the thread class itself, I tried to re-throw the exception, but it does not work. I have seen people on here ask similar questions, but they all seem to be doing something more specific than what I am trying to do (and I don't quite understand the solutions offered). I have seen people mention the usage of `sys.exc_info()`, however I do not know where or how to use it.

**Edit:** The code for the thread class is below:

```
class TheThread(threading.Thread):
    def __init__(self, sourceFolder, destFolder):
        threading.Thread.__init__(self)
        self.sourceFolder = sourceFolder
        self.destFolder = destFolder
    
    def run(self):
        try:
           shul.copytree(self.sourceFolder, self.destFolder)
        except:
           raise

```","The problem is that `thread_obj.start()` returns immediately. The child thread that you spawned executes in its own context, with its own stack. Any exception that occurs there is in the context of the child thread, and it is in its own stack. One way I can think of right now to communicate this information to the parent thread is by using some sort of message passing, so you might look into that.

Try this on for size:

```
import sys
import threading
import queue


class ExcThread(threading.Thread):

    def __init__(self, bucket):
        threading.Thread.__init__(self)
        self.bucket = bucket

    def run(self):
        try:
            raise Exception('An error occured here.')
        except Exception:
            self.bucket.put(sys.exc_info())


def main():
    bucket = queue.Queue()
    thread_obj = ExcThread(bucket)
    thread_obj.start()

    while True:
        try:
            exc = bucket.get(block=False)
        except queue.Empty:
            pass
        else:
            exc_type, exc_obj, exc_trace = exc
            # deal with the exception
            print exc_type, exc_obj
            print exc_trace

        thread_obj.join(0.1)
        if thread_obj.isAlive():
            continue
        else:
            break


if __name__ == '__main__':
    main()

```"
"I am trying to get data from the web using python. I imported urllib.request package for it but while executing, I get error:

```
certificate verify failed: unable to get local issuer certificate (_ssl.c:1045)

```

* I am using Python 3.7 on Mac OS High Sierra.
* I am trying to get CSV file from:
  <https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv>

When I changed the URL to 'http' - I am able to get data. But, I believe, this avoids checking SSL certificate.

So I checked on the internet and found one solution:
Run `/Applications/Python\ 3.7/Install\ Certificates.command`

This solved my problem. But I have no knowledge on SSL and the likes. Can you help me understand what it actually did to solve my issue.

If possible, please recommend me any good resource to learn about the security and certificates. I am new to this.

Thanks!

Note: I did go through the link - [openssl, python requests error: ""certificate verify failed""](https://stackoverflow.com/questions/22027418/openssl-python-requests-error-certificate-verify-failed)

My question differs from the one in link because, I want to know what actually happens when I install `certifi` package or run `Install\ Certificates.command` to fix the error. I have a poor understanding of securities.","For anyone who still wonders on how to fix this, i got mine by installing the ""`Install Certificates.command`""

Here is how I did,

[![Install Certificates.commad location](https://i.sstatic.net/dV0eJ.png)](https://i.sstatic.net/dV0eJ.png)

Just double click on that file wait for it to install and in my case, you will be ready to go"
"I have a numerical list:

```
myList = [1, 2, 3, 100, 5]

```

Now if I sort this list to obtain `[1, 2, 3, 5, 100]`.
What I want is the indices of the elements from the
original list in the sorted order i.e. `[0, 1, 2, 4, 3]`
--- ala MATLAB's sort function that returns both
values and indices.","If you are using numpy, you have the argsort() function available:

```
>>> import numpy
>>> numpy.argsort(myList)
array([0, 1, 2, 4, 3])

```

<http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html>

This returns the arguments that would sort the array or list."
"How do I test the following code with [`unittest.mock`](https://docs.python.org/3/library/unittest.mock.html):

```
def testme(filepath):
    with open(filepath) as f:
        return f.read()

```","Python 3
========

Patch [`builtins.open`](https://docs.python.org/3/library/builtins.html) and use [`mock_open`](https://docs.python.org/3/library/unittest.mock.html#mock-open), which is part of the [`mock`](https://docs.python.org/3/library/unittest.mock.html#module-unittest.mock) framework. [`patch`](https://docs.python.org/3/library/unittest.mock.html#patch) used as a [context manager](https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers) returns the object used to replace the patched one:

```
from unittest.mock import patch, mock_open
with patch(""builtins.open"", mock_open(read_data=""data"")) as mock_file:
    assert open(""path/to/open"").read() == ""data""
mock_file.assert_called_with(""path/to/open"")

```

If you want to use `patch` as a decorator, using `mock_open()`'s result as the `new=` argument to `patch` can be a little bit weird. Instead, use `patch`'s `new_callable=` argument and remember that every extra argument that `patch` doesn't use will be passed to the `new_callable` function, as described in the [`patch` documentation](https://docs.python.org/3/library/unittest.mock.html#patch):

> `patch()` takes arbitrary keyword arguments. These will be passed to the `Mock` (or *new\_callable*) on construction.

```
@patch(""builtins.open"", new_callable=mock_open, read_data=""data"")
def test_patch(mock_file):
    assert open(""path/to/open"").read() == ""data""
    mock_file.assert_called_with(""path/to/open"")

```

Remember that in this case `patch` will pass the mocked object as an argument to your test function.

Python 2
========

You need to patch `__builtin__.open` instead of `builtins.open` and `mock` is not part of `unittest`, you need to `pip install` and import it separately:

```
from mock import patch, mock_open
with patch(""__builtin__.open"", mock_open(read_data=""data"")) as mock_file:
    assert open(""path/to/open"").read() == ""data""
mock_file.assert_called_with(""path/to/open"")

```"
"The new version of Pandas uses [the following interface](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.excel.read_excel.html#pandas.io.excel.read_excel) to load Excel files:

```
read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])

```

but what if I don't know the sheets that are available?

For example, I am working with excel files that the following sheets

> Data 1, Data 2 ..., Data N, foo, bar

but I don't know `N` a priori.

Is there any way to get the list of sheets from an excel document in Pandas?","You can still use the [ExcelFile](http://pandas.pydata.org/pandas-docs/dev/io.html#excel-files) class (and the `sheet_names` attribute):

```
xl = pd.ExcelFile('foo.xls')

xl.sheet_names  # see all sheet names

xl.parse(sheet_name)  # read a specific sheet to DataFrame

```

*see [docs for parse](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.ExcelFile.parse.html) for more options...*"
"When I do a pip freeze I see large number of Python packages that I didn't explicitly install, e.g.

```
$ pip freeze
Cheetah==2.4.3
GnuPGInterface==0.3.2
Landscape-Client==11.01
M2Crypto==0.20.1
PAM==0.4.2
PIL==1.1.7
PyYAML==3.09
Twisted-Core==10.2.0
Twisted-Web==10.2.0
(etc.)

```

Is there a way for me to determine why pip installed these particular dependent packages? In other words, how do I determine the parent package that had these packages as dependencies?

For example, I might want to use Twisted and I don't want to depend on a package until I know more about not accidentally uninstalling it or upgrading it.","You could try [pipdeptree](https://pypi.org/project/pipdeptree/), which displays dependencies as a tree structure e.g.:

```
$ pipdeptree
Lookupy==0.1
wsgiref==0.1.2
argparse==1.2.1
psycopg2==2.5.2
Flask-Script==0.6.6
  - Flask [installed: 0.10.1]
    - Werkzeug [required: >=0.7, installed: 0.9.4]
    - Jinja2 [required: >=2.4, installed: 2.7.2]
      - MarkupSafe [installed: 0.18]
    - itsdangerous [required: >=0.21, installed: 0.23]
alembic==0.6.2
  - SQLAlchemy [required: >=0.7.3, installed: 0.9.1]
  - Mako [installed: 0.9.1]
    - MarkupSafe [required: >=0.9.2, installed: 0.18]
ipython==2.0.0
slugify==0.0.1
redis==2.9.1

```

To install it, run:

```
pip install pipdeptree

```

As noted by [@Esteban](https://stackoverflow.com/users/685109/esteban) in the comments you can also list the tree in reverse with `-r` or for a single package with `-p <package_name>`. So to find which module(s) Werkzeug is a dependency for, you could run:

```
$ pipdeptree -r -p Werkzeug
Werkzeug==0.11.15
  - Flask==0.12 [requires: Werkzeug>=0.7]

```"
"I'm working with a Boolean index in Pandas.

The question is why the statement:

```
a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]

```

works fine whereas

```
a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]

```

exits with error?

Example:

```
a = pd.DataFrame({'x':[1,1],'y':[10,20]})

In: a[(a['x']==1)&(a['y']==10)]
Out:    x   y
     0  1  10

In: a[(a['x']==1) and (a['y']==10)]
Out: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()

```","When you say

```
(a['x']==1) and (a['y']==10)

```

You are implicitly asking Python to convert `(a['x']==1)` and `(a['y']==10)` to Boolean values.

NumPy arrays (of length greater than 1) and Pandas objects such as Series do not have a Boolean value -- in other words, they raise

> ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().

when used as a Boolean value. That's because it's [unclear when it should be True or False](https://pandas.pydata.org/pandas-docs/dev/user_guide/gotchas.html#using-if-truth-statements-with-pandas). Some users might assume they are True if they have non-zero length, like a Python list. Others might desire for it to be True only if **all** its elements are True. Others might want it to be True if **any** of its elements are True.

Because there are so many conflicting expectations, the designers of NumPy and Pandas refuse to guess, and instead raise a ValueError.

Instead, you must be explicit, by calling the `empty()`, `all()` or `any()` method to indicate which behavior you desire.

In this case, however, it looks like you do not want Boolean evaluation, you want **element-wise** logical-and. That is what the `&` binary operator performs:

```
(a['x']==1) & (a['y']==10)

```

returns a boolean array.

---

By the way, as [alexpmil notes](https://stackoverflow.com/questions/21415661/logic-operator-for-boolean-indexing-in-pandas/#comment77317569_21415990),
the parentheses are mandatory since `&` has a higher [operator precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence) than `==`.

Without the parentheses,

```
a['x']==1 & a['y']==10

```

would be evaluated as

```
a['x'] == (1 & a['y']) == 10

```

which would in turn be equivalent to the chained comparison

```
(a['x'] == (1 & a['y'])) and ((1 & a['y']) == 10)

```

That is an expression of the form `Series and Series`.
The use of `and` with two Series would again trigger the same `ValueError` as above. That's why the parentheses are mandatory."
How would you get only the first line of a file as a string with Python?,"Use the [`.readline()`](https://docs.python.org/3/library/io.html#io.IOBase.readline) method:

```
with open('myfile.txt') as f:
    first_line = f.readline()

```

Note that unless it is the only line in the file, the string returned from `f.readline()` will contain a trailing newline. You may wish to use

```
with open('myfile.txt') as f:
    first_line = f.readline().strip('\n')

```

instead, to remove the newline."
"I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.

I expect to be able to do this (per [this answer](https://stackoverflow.com/questions/11881165/slice-pandas-dataframe-by-row)):

```
df[(len(df['column name']) < 2)]

```

but I just get the error:

```
KeyError: u'no item named False'

```

What am I doing wrong?

(Note: I know I can use `df.dropna()` to get rid of rows that contain any `NaN`, but I didn't see how to remove rows based on a conditional expression.)","To directly answer this question's original title ""How to delete rows from a pandas DataFrame based on a conditional expression"" (which I understand is not necessarily the OP's problem but could help other users coming across this question) one way to do this is to use the [drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method:

```
df = df.drop(some labels)
df = df.drop(df[<some boolean condition>].index)

```

**Example**

To remove all rows where column 'score' is < 50:

```
df = df.drop(df[df.score < 50].index)

```

In place version (as pointed out in comments)

```
df.drop(df[df.score < 50].index, inplace=True)

```

**Multiple conditions**

(see [Boolean Indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing))

> The operators are: `|` for `or`, `&` for `and`, and `~` for `not`. These must be
> grouped by using parentheses.

To remove all rows where column 'score' is < 50 and > 20

```
df = df.drop(df[(df.score < 50) & (df.score > 20)].index)

```"
"This is a question I have wondered about for quite some time, yet I have never found a suitable solution. If I run a script and I come across, let's say an IndexError, python prints the line, location and quick description of the error and exits. Is it possible to automatically start pdb when an error is encountered? I am not against having an extra import statement at the top of the file, nor a few extra lines of code.","```
python -m pdb -c continue myscript.py
# or
python -m pdb -c continue -m myscript

```

If you don't provide the `-c continue` flag then you'll need to enter 'c' (for Continue) when execution begins. Then it will run to the error point and give you control there. As [mentioned by eqzx](https://stackoverflow.com/questions/242485/starting-python-debugger-automatically-on-error#comment73725114_2438834), this flag is a new addition in python 3.2 so entering 'c' is required for earlier Python versions (see <https://docs.python.org/3/library/pdb.html>)."
"I've got some Python code that runs through a list of strings and converts them to integers or floating point numbers if possible. Doing this for integers is pretty easy

```
if element.isdigit():
  newelement = int(element)

```

Floating point numbers are more difficult. Right now I'm using `partition('.')` to split the string and checking to make sure that one or both sides are digits.

```
partition = element.partition('.')
if (partition[0].isdigit() and partition[1] == '.' and partition[2].isdigit()) 
    or (partition[0] == '' and partition[1] == '.' and partition[2].isdigit()) 
    or (partition[0].isdigit() and partition[1] == '.' and partition[2] == ''):
  newelement = float(element)

```

This works, but obviously the if statement for that is a bit of a bear. The other solution I considered is to just wrap the conversion in a try/catch block and see if it succeeds, as described in [this question](https://stackoverflow.com/questions/354038/checking-if-string-is-a-number-python).

Anyone have any other ideas? Opinions on the relative merits of the partition and try/catch approaches?","I would just use..

```
try:
    float(element)
except ValueError:
    print(""Not a float"")

```

..it's simple, and it works. Note that it will still throw OverflowError if element is e.g. 1<<1024.

Another option would be a regular expression:

```
import re
if re.match(r'^-?\d+(?:\.\d+)$', element) is None:
    print(""Not float"")

```"
"I would like to print a specific Python dictionary key:

```
mydic = {
    ""key_name"": ""value""
}

```

Now I can check if `mydic.has_key('key_name')`, but what I would like to do is print the name of the key `'key_name'`. Of course I could use `mydic.items()`, but I don't want **all** the keys listed, merely one specific key. For instance I'd expect something like this (in pseudo-code):

```
print(""the key name is"", mydic['key_name'].name_the_key(), ""and its value is"", mydic['key_name'])

```

Is there any `name_the_key()` method to print a key name?

---

**Edit:**
OK, thanks a lot guys for your reactions! :) I realise my question is not well formulated and trivial. I just got confused because I realised `'key_name'` and `mydic['key_name']` are two different things and I thought it would be incorrect to print the `'key_name'` out of the dictionary context. But indeed I can simply use the `'key_name'` to refer to the key! :)","A dictionary has, by definition, an arbitrary number of keys. There is no ""the key"". You have the `keys()` method, which gives you a python `list` of all the keys, and you have the `iteritems()` method, which returns key-value pairs, so

```
for key, value in mydic.iteritems() :
    print key, value

```

Python 3 version:

```
for key, value in mydic.items() :
    print (key, value)

```

So you have a handle on the keys, but they only really mean sense if coupled to a value. I hope I have understood your question."
"I am new to Django and pretty new to Ajax. I am working on a project where I need to integrate the two. I believe that I understand the principles behind them both, but have not found a good explanation of the two together.

Could someone give me a quick explanation of how the codebase must change with the two of them integrating together?

For example, can I still use the `HttpResponse` with Ajax, or do my responses have to change with the use of Ajax? If so, could you please provide an example of how the responses to the requests must change? If it makes any difference, the data I am returning is JSON.","Even though this isn't entirely in the SO spirit, I love this question, because I had the same trouble when I started, so I'll give you a quick guide. Obviously you don't understand the principles behind them (don't take it as an offense, but if you did you wouldn't be asking).

Django is **server-side**. It means, say a client goes to a URL, you have a function inside `views` that renders what he sees and returns a response in HTML. Let's break it up into examples:

**views.py:**

```
def hello(request):
    return HttpResponse('Hello World!')

def home(request):
    return render_to_response('index.html', {'variable': 'world'})

```

**index.html:**

```
<h1>Hello {{ variable }}, welcome to my awesome site</h1>

```

**urls.py:**

```
url(r'^hello/', 'myapp.views.hello'),
url(r'^home/', 'myapp.views.home'),

```

That's an example of the simplest of usages. Going to `127.0.0.1:8000/hello` means a request to the `hello()` function, going to `127.0.0.1:8000/home` will return the `index.html` and replace all the variables as asked (you probably know all this by now).

Now let's talk about **AJAX**. AJAX calls are client-side code that does asynchronous requests. That sounds complicated, but it simply means it does a request for you in the background and then handles the response. So when you do an AJAX call for some URL, you get the same data you would get as a user going to that place.

For example, an AJAX call to `127.0.0.1:8000/hello` will return the same thing it would as if you visited it. Only this time, you have it inside a JavaScript function and you can deal with it however you'd like. Let's look at a simple use case:

```
$.ajax({
    url: '127.0.0.1:8000/hello',
    type: 'get', // This is the default though, you don't actually need to always mention it
    success: function(data) {
        alert(data);
    },
    failure: function(data) { 
        alert('Got an error dude');
    }
}); 

```

The general process is this:

1. The call goes to the URL `127.0.0.1:8000/hello` as if you opened a new tab and did it yourself.
2. If it succeeds (status code 200), do the function for success, which will alert the data received.
3. If fails, do a different function.

Now what would happen here? You would get an alert with 'hello world' in it. What happens if you do an AJAX call to home? Same thing, you'll get an alert stating `<h1>Hello world, welcome to my awesome site</h1>`.

In other words - there's nothing new about AJAX calls. They are just a way for you to let the user get data and information without leaving the page, and it makes for a smooth and very neat design of your website. A few guidelines you should take note of:

1. **Learn jQuery**. I cannot stress this enough. You're gonna have to understand it a little to know how to handle the data you receive. You'll also need to understand some basic JavaScript syntax (not far from python, you'll get used to it). I strongly recommend [Envato's video tutorials for jQuery](https://tutsplus.com/course/30-days-to-learn-jquery/), they are great and will put you on the right path.
2. **When to use JSON?**. You're going to see a lot of examples where the data sent by the Django views is in JSON. I didn't go into detail on that, because it isn't important *how* to do it (there are plenty of explanations abound) and a lot more important *when*. And the answer to that is - JSON data is serialized data. That is, data you can manipulate. Like I mentioned, an AJAX call will fetch the response as if the user did it himself. Now say you don't want to mess with all the html, and instead want to send data (a list of objects perhaps). JSON is good for this, because it sends it as an object (JSON data looks like a python dictionary), and then you can iterate over it or do something else that removes the need to sift through useless html.
3. **Add it last**. When you build a web app and want to implement AJAX - do yourself a favor. First, build the entire app completely devoid of any AJAX. See that everything is working. Then, and only then, start writing the AJAX calls. That's a good process that helps you learn a lot as well.
4. **Use chrome's developer tools**. Since AJAX calls are done in the background it's sometimes very hard to debug them. You should use the chrome developer tools (or similar tools such as firebug) and `console.log` things to debug. I won't explain in detail, just google around and find out about it. It would be very helpful to you.
5. **CSRF awareness**. Finally, remember that post requests in Django require the `csrf_token`. With AJAX calls, a lot of times you'd like to send data without refreshing the page. You'll probably face some trouble before you'd finally remember that - wait, you forgot to send the `csrf_token`. This is a known beginner roadblock in AJAX-Django integration, but after you learn how to make it play nice, it's easy as pie.

That's everything that comes to my head. It's a vast subject, but yeah, there's probably not enough examples out there. Just work your way there, slowly, you'll get it eventually."
"I would like to know how to I exit from Python without having an traceback dump on the output.

I still want want to be able to return an error code but I do not want to display the traceback log.

I want to be able to exit using `exit(number)` without trace but in case of an Exception (not an exit) I want the trace.","You are presumably encountering an exception and the program is exiting because of this (with a traceback). The first thing to do therefore is to catch that exception, before exiting cleanly (maybe with a message, example given).

Try something like this in your `main` routine:

```
import sys, traceback

def main():
    try:
        do main program stuff here
        ....
    except KeyboardInterrupt:
        print ""Shutdown requested...exiting""
    except Exception:
        traceback.print_exc(file=sys.stdout)
    sys.exit(0)

if __name__ == ""__main__"":
    main()

```"
"I am practicing using type hints in Python 3.5. One of my colleague uses `typing.Dict`:

```
import typing


def change_bandwidths(new_bandwidths: typing.Dict,
                      user_id: int,
                      user_name: str) -> bool:
    print(new_bandwidths, user_id, user_name)
    return False


def my_change_bandwidths(new_bandwidths: dict,
                         user_id: int,
                         user_name: str) ->bool:
    print(new_bandwidths, user_id, user_name)
    return True


def main():
    my_id, my_name = 23, ""Tiras""
    simple_dict = {""Hello"": ""Moon""}
    change_bandwidths(simple_dict, my_id, my_name)
    new_dict = {""new"": ""energy source""}
    my_change_bandwidths(new_dict, my_id, my_name)

if __name__ == ""__main__"":
    main()

```

Both of them work just fine, there doesn't appear to be a difference.

I have read the [`typing` module documentation](https://docs.python.org/3/library/typing.html).

Between `typing.Dict` or `dict` which one should I use in the program?","Note: `typing.Dict` has been deprecated as of Python 3.9, because the `dict` type itself can be used as a generic type directly (together with [other standard containers](https://www.python.org/dev/peps/pep-0585/)). You can do the same in Python 3.7 or 3.8 if you use a [`from __future__ import annotations`](https://www.python.org/dev/peps/pep-0563/) directive. My answer was originally written for much older Python 3 releases.

---

There is no real difference between using a plain `typing.Dict` and `dict`, no.

However, `typing.Dict` is a [*Generic type*](https://docs.python.org/3/library/typing.html#generics) that lets you specify the type of the keys and values *too*, making it more flexible:

```
def change_bandwidths(new_bandwidths: typing.Dict[str, str],
                      user_id: int,
                      user_name: str) -> bool:

```

As such, it could well be that at some point in your project lifetime you want to define the dictionary argument a little more precisely, at which point expanding `typing.Dict` to `typing.Dict[key_type, value_type]` is a 'smaller' change than replacing `dict`.

You can make this even more generic by using [`Mapping`](https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping) or [`MutableMapping`](https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping) types here; since your function doesn't need to *alter* the mapping, I'd stick with `Mapping`. A `dict` is one mapping, but you could create other objects that also satisfy the mapping interface, and your function might well still work with those:

```
from collections.abc import Mapping

def change_bandwidths(new_bandwidths: Mapping[str, str],
                      user_id: int,
                      user_name: str) -> bool:

```

Now you are clearly telling other users of this function that your code won't actually *alter* the `new_bandwidths` mapping passed in.

Your actual implementation is merely expecting an object that is printable. That may be a test implementation, but as it stands your code would continue to work if you used `new_bandwidths: object`, because *any* object in Python is printable."
"I'm trying to convert a Python dictionary into a Python list, in order to perform some calculations.

```
#My dictionary
dict = {}
dict['Capital']=""London""
dict['Food']=""Fish&Chips""
dict['2012']=""Olympics""

#lists
temp = []
dictList = []

#My attempt:
for key, value in dict.iteritems():
    aKey = key
    aValue = value
    temp.append(aKey)
    temp.append(aValue)
    dictList.append(temp) 
    aKey = """"
    aValue = """"

```

That's my attempt at it... but I can't work out what's wrong?","```
dict.items()

```

Does the trick.

(For Python 2 only)"
Is there a SciPy function or NumPy function or module for Python that calculates the running mean of a 1D array given a specific window?,"> **NOTE:** More [efficient solutions](https://stackoverflow.com/a/33055571/365102) may include [`scipy.ndimage.uniform_filter1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.uniform_filter1d.html) (see [this answer](https://stackoverflow.com/a/43200476/675674)), or using newer libraries including [talib](http://ta-lib.github.io/ta-lib-python/)'s [`talib.MA`](https://hexdocs.pm/talib/TAlib.Indicators.MA.html).

---

Use [`np.convolve`](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html):

```
np.convolve(x, np.ones(N)/N, mode='valid')

```

### Explanation

The running mean is a case of the mathematical operation of [convolution](https://en.wikipedia.org/wiki/Convolution). For the running mean, you slide a window along the input and compute the mean of the window's contents. For discrete 1D signals, convolution is the same thing, except instead of the mean you compute an arbitrary linear combination, i.e., multiply each element by a corresponding coefficient and add up the results. Those coefficients, one for each position in the window, are sometimes called the convolution *kernel*. The arithmetic mean of N values is `(x_1 + x_2 + ... + x_N) / N`, so the corresponding kernel is `(1/N, 1/N, ..., 1/N)`, and that's exactly what we get by using `np.ones(N)/N`.

### Edges

The `mode` argument of `np.convolve` specifies how to handle the edges. I chose the `valid` mode here because I think that's how most people expect the running mean to work, but you may have other priorities. Here is a plot that illustrates the difference between the modes:

```
import numpy as np
import matplotlib.pyplot as plt
modes = ['full', 'same', 'valid']
for m in modes:
    plt.plot(np.convolve(np.ones(200), np.ones(50)/50, mode=m));
plt.axis([-10, 251, -.1, 1.1]);
plt.legend(modes, loc='lower center');
plt.show()

```

![Running mean convolve modes](https://i.sstatic.net/IMt8g.png)"
"I was trying to create migrations within an existing app using the makemigrations command but it outputs ""No changes detected"".

Usually I create new apps using the `startapp` command but did not use it for this app when I created it.

After debugging, I found that it is not creating a migration because the `migrations` package/folder is missing from an app.

Would it be better if it creates the folder if it is not there or am I missing something?","To create initial migrations for an app, run `makemigrations` and specify the app name. The migrations folder will be created.

```
./manage.py makemigrations <myapp>

```

Your app must be included in `INSTALLED_APPS` first (inside settings.py)."
"I am using the Boto 3 python library, and want to connect to AWS CloudFront.
I need to specify the correct AWS Profile (AWS Credentials), but looking at the official documentation, I see no way to specify it.

I am initializing the client using the code:
`client = boto3.client('cloudfront')`

However, this results in it using the default profile to connect.
I couldn't find a method where I can specify which profile to use.","I think the docs aren't wonderful at exposing how to do this. It has been a supported feature for some time, however, and there are some details in this [pull request](https://github.com/boto/boto3/pull/69).

So there are three different ways to do this:

Option A) Create a new session with the profile

```
    dev = boto3.session.Session(profile_name='dev')

```

Option B) Change the profile of the default session in code

```
    boto3.setup_default_session(profile_name='dev')

```

Option C) Change the profile of the default session with an environment variable

```
    $ AWS_PROFILE=dev ipython
    >>> import boto3
    >>> s3dev = boto3.resource('s3')

```"
"I have a function that analyzes a CSV file with Pandas and produces a dict with summary information. I want to return the results as a response from a Flask view. How do I return a JSON response?

```
@app.route(""/summary"")
def summary():
    d = make_summary()
    # send it back as json

```","A view can directly return a Python dict or list and Flask will call [`jsonify`](https://flask.palletsprojects.com/api/#flask.json.jsonify) automatically.

```
@app.route(""/summary"")
def summary():
    d = make_summary()
    return d

```

For older Flask versions, or to return a different JSON-serializable object, import and use [`jsonify`](https://flask.palletsprojects.com/api/#flask.json.jsonify).

```
from flask import jsonify

@app.route(""/summary"")
def summary():
    d = make_summary()
    return jsonify(d)

```"
"In the [Python documentation](https://docs.python.org/2/library/threading.html#thread-objects)
it says:

> A thread can be flagged as a ""daemon thread"". The significance of this
> flag is that the entire Python program exits when only daemon threads
> are left. The initial value is inherited from the creating thread.

Does anyone have a clearer explanation of what that means or a practical example showing where you would set threads as `daemonic`?

Clarify it for me: so the only situation you wouldn't set threads as `daemonic`, is when you want them to continue running after the main thread exits?","Some threads do background tasks, like sending keepalive packets, or performing periodic garbage collection, or whatever. These are only useful when the main program is running, and it's okay to kill them off once the other, non-daemon, threads have exited.

Without daemon threads, you'd have to keep track of them, and tell them to exit, before your program can completely quit. By setting them as daemon threads, you can let them run and forget about them, and when your program quits, any daemon threads are killed automatically."
"I've got a pandas DataFrame filled mostly with real numbers, but there is a few `nan` values in it as well.

How can I replace the `nan`s with averages of columns where they are?

This question is very similar to this one: [numpy array: replace nan values with average of columns](https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns) but, unfortunately, the solution given there doesn't work for a pandas DataFrame.","You can simply use [`DataFrame.fillna`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna) to fill the `nan`'s directly:

```
In [27]: df 
Out[27]: 
          A         B         C
0 -0.166919  0.979728 -0.632955
1 -0.297953 -0.912674 -1.365463
2 -0.120211 -0.540679 -0.680481
3       NaN -2.027325  1.533582
4       NaN       NaN  0.461821
5 -0.788073       NaN       NaN
6 -0.916080 -0.612343       NaN
7 -0.887858  1.033826       NaN
8  1.948430  1.025011 -2.982224
9  0.019698 -0.795876 -0.046431

In [28]: df.mean()
Out[28]: 
A   -0.151121
B   -0.231291
C   -0.530307
dtype: float64

In [29]: df.fillna(df.mean())
Out[29]: 
          A         B         C
0 -0.166919  0.979728 -0.632955
1 -0.297953 -0.912674 -1.365463
2 -0.120211 -0.540679 -0.680481
3 -0.151121 -2.027325  1.533582
4 -0.151121 -0.231291  0.461821
5 -0.788073 -0.231291 -0.530307
6 -0.916080 -0.612343 -0.530307
7 -0.887858  1.033826 -0.530307
8  1.948430  1.025011 -2.982224
9  0.019698 -0.795876 -0.046431

```

The docstring of `fillna` says that `value` should be a scalar or a dict, however, it seems to work with a `Series` as well. If you want to pass a dict, you could use `df.mean().to_dict()`."
"I am new to django-1.6. When I run the django server with `DEBUG = True`, it's running perfectly. But when I change `DEBUG` to `False` in the settings file, then the server stopped and it gives the following error on the command prompt:

```
CommandError: You must set settings.ALLOWED_HOSTS if DEBUG is False.

```

After I changed `ALLOWED_HOSTS` to `[""http://127.0.0.1:8000"",]`, in the browser I get the error:

```
Bad Request (400)

```

Is it possible to run Django without debug mode?","The [`ALLOWED_HOSTS` list](https://docs.djangoproject.com/en/stable/ref/settings/#allowed-hosts) should contain fully qualified *host names*, **not** urls. Leave out the port and the protocol. If you are using `127.0.0.1`, I would add `localhost` to the list too:

```
ALLOWED_HOSTS = ['127.0.0.1', 'localhost']

```

You could also use `*` to match *any* host:

```
ALLOWED_HOSTS = ['*']

```

Quoting the documentation:

> Values in this list can be fully qualified names (e.g. `'www.example.com'`), in which case they will be matched **against the requestâ€™s `Host` header** exactly (case-insensitive, **not including port**). A value beginning with a period can be used as a subdomain wildcard: `'.example.com'` will match `example.com`, `www.example.com`, and any other subdomain of `example.com`. A value of `'*'` will match anything; in this case you are responsible to provide your own validation of the `Host` header (perhaps in a middleware; if so this middleware must be listed first in `MIDDLEWARE_CLASSES`).

*Bold emphasis mine*.

The status 400 response you get is due to a [`SuspiciousOperation` exception](https://docs.djangoproject.com/en/stable/ref/exceptions/#suspiciousoperation) being raised when your host header doesn't match any values in that list."
"I have a Python script that needs to execute an external program, but for some reason fails.

If I have the following script:

```
import os;
os.system(""C:\\Temp\\a b c\\Notepad.exe"");
raw_input();

```

Then it fails with the following error:

> 'C:\Temp\a' is not recognized as an internal or external command, operable program or batch file.

If I escape the program with quotes:

```
import os;
os.system('""C:\\Temp\\a b c\\Notepad.exe""');
raw_input();

```

Then it works. However, if I add a parameter, it stops working again:

```
import os;
os.system('""C:\\Temp\\a b c\\Notepad.exe"" ""C:\\test.txt""');
raw_input();

```

What is the right way to execute a program and wait for it to complete? I do not need to read output from it, as it is a visual program that does a job and then just exits, but I need to wait for it to complete.

Also note, moving the program to a non-spaced path is not an option either.

---

This does not work either:

```
import os;
os.system(""'C:\\Temp\\a b c\\Notepad.exe'"");
raw_input();

```

Note the swapped single/double quotes.

With or without a parameter to Notepad here, it fails with the error message

> The filename, directory name, or volume label syntax is incorrect.","[`subprocess.call`](http://docs.python.org/2/library/subprocess.html#using-the-subprocess-module) will avoid problems with having to deal with quoting conventions of various shells. It accepts a list, rather than a string, so arguments are more easily delimited. i.e.

```
import subprocess
subprocess.call(['C:\\Temp\\a b c\\Notepad.exe', 'C:\\test.txt'])

```"
"I have a nested dictionary. Is there only one way to get values out safely?

```
try:
    example_dict['key1']['key2']
except KeyError:
    pass

```

Or maybe python has a method like `get()` for nested dictionary ?","You could use `get` twice:

```
example_dict.get('key1', {}).get('key2')

```

This will return `None` if either `key1` or `key2` does not exist.

Note that this could still raise an `AttributeError` if `example_dict['key1']` exists but is not a dict (or a dict-like object with a `get` method). The `try..except` code you posted would raise a `TypeError` instead if `example_dict['key1']` is unsubscriptable.

Another difference is that the `try...except` short-circuits immediately after the first missing key. The chain of `get` calls does not.

---

If you wish to preserve the syntax, `example_dict['key1']['key2']` but do not want it to ever raise KeyErrors, then you could use the [Hasher recipe](https://stackoverflow.com/a/3405143/190597):

```
class Hasher(dict):
    # https://stackoverflow.com/a/3405143/190597
    def __missing__(self, key):
        value = self[key] = type(self)()
        return value

example_dict = Hasher()
print(example_dict['key1'])
# {}
print(example_dict['key1']['key2'])
# {}
print(type(example_dict['key1']['key2']))
# <class '__main__.Hasher'>

```

Note that this returns an empty Hasher when a key is missing.

Since `Hasher` is a subclass of `dict` you can use a Hasher in much the same way you could use a `dict`. All the same methods and syntax is available, Hashers just treat missing keys differently.

You can convert a regular `dict` into a `Hasher` like this:

```
hasher = Hasher(example_dict)

```

and convert a `Hasher` to a regular `dict` just as easily:

```
regular_dict = dict(hasher)

```

---

Another alternative is to hide the ugliness in a helper function:

```
def safeget(dct, *keys):
    for key in keys:
        try:
            dct = dct[key]
        except KeyError:
            return None
    return dct

```

So the rest of your code can stay relatively readable:

```
safeget(example_dict, 'key1', 'key2')

```"
"How can I make anaconda environment file which could be use on other computers?

I exported my anaconda python environment to YML using `conda env export > environment.yml`. The exported `environment.yml` contains this line `prefix: /home/superdev/miniconda3/envs/juicyenv` which maps to my anaconda's location which will be different on other's pcs.","I can't find anything in the `conda` specs which allows you to export an environment file without the `prefix: ...` line. However, like [Alex pointed out](https://stackoverflow.com/questions/41274007/anaconda-export-environment-file/41274348#comment69750196_41274007) in the comments, conda doesn't seem to care about the prefix line when creating an environment from the file.

With that in mind, if you want the other user to have no knowledge of your default install path, you can remove the prefix line with `grep` before writing to `environment.yml`.

```
conda env export | grep -v ""^prefix: "" > environment.yml

```

Either way, the other user then runs:

```
conda env create -f environment.yml

```

and the environment will get installed in their default conda environment path.

If you want to specify a different install path than the default for your system (not related to 'prefix' in the environment.yml), just use the `-p` flag followed by the required path.

```
conda env create -f environment.yml -p /home/user/anaconda3/envs/env_name

```

Note that Conda recommends creating the `environment.yml` by hand, which is especially important if you are wanting to share your environment across platforms (Windows/Linux/Mac). In this case, you can just leave out the `prefix` line."
"I want to execute a curl command in Python.

Usually, I just need to enter the command in the terminal and press the return key. However, I don't know how it works in Python.

The command shows below:

```
curl -d @request.json --header ""Content-Type: application/json"" https://www.googleapis.com/qpxExpress/v1/trips/search?key=mykeyhere

```

There is a `request.json` file to be sent to get a response.

I searched a lot and got confused. I tried to write a piece of code, although I could not fully understand it and it didn't work.

```
import pycurl
import StringIO

response = StringIO.StringIO()
c = pycurl.Curl()
c.setopt(c.URL, 'https://www.googleapis.com/qpxExpress/v1/trips/search?key=mykeyhere')
c.setopt(c.WRITEFUNCTION, response.write)
c.setopt(c.HTTPHEADER, ['Content-Type: application/json','Accept-Charset: UTF-8'])
c.setopt(c.POSTFIELDS, '@request.json')
c.perform()
c.close()
print response.getvalue()
response.close()

```

The error message is `Parse Error`. How to get a response from the server correctly?","For the sake of simplicity, you should consider using the [Requests](https://requests.readthedocs.io/) library.

An example with JSON response content would be something like:

```
import requests
r = requests.get('https://github.com/timeline.json')
r.json()

```

If you look for further information, in the [Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/) section, they have lots of working examples.

For your specific curl translation:

```
import requests

url = 'https://www.googleapis.com/qpxExpress/v1/trips/search?key=mykeyhere'
payload = open(""request.json"")
headers = {'content-type': 'application/json', 'Accept-Charset': 'UTF-8'}
r = requests.post(url, data=payload, headers=headers)

```"
"I was studying the python threading and came across [`join()`](http://docs.python.org/2/library/threading.html#threading.Thread.join).

The author told that if thread is in daemon mode then i need to use `join()` so that thread can finish itself before main thread terminates.

but I have also seen him using `t.join()` even though `t` was not `daemon`

example code is this

```
import threading
import time
import logging

logging.basicConfig(level=logging.DEBUG,
                    format='(%(threadName)-10s) %(message)s',
                    )

def daemon():
    logging.debug('Starting')
    time.sleep(2)
    logging.debug('Exiting')

d = threading.Thread(name='daemon', target=daemon)
d.setDaemon(True)

def non_daemon():
    logging.debug('Starting')
    logging.debug('Exiting')

t = threading.Thread(name='non-daemon', target=non_daemon)

d.start()
t.start()

d.join()
t.join()

```

i don't know what is use of `t.join()` as it is not daemon and i can see no change even if i remove it","A somewhat clumsy ascii-art to demonstrate the mechanism:
The `join()` is presumably called by the main-thread. It could also be called by another thread, but would needlessly complicate the diagram.

`join`-calling should be placed in the track of the main-thread, but to express thread-relation and keep it as simple as possible, I choose to place it in the child-thread instead.

```
    without join:
    +---+---+------------------                     main-thread
        |   |
        |   +...........                            child-thread(short)
        +..................................         child-thread(long)
    
    with join
    +---+---+------------------***********+###      main-thread
        |   |                             |
        |   +...........join()            |         child-thread(short)
        +......................join()......         child-thread(long)

    with join and daemon thread
    +-+--+---+------------------***********+###     parent-thread
      |  |   |                             |
      |  |   +...........join()            |        child-thread(short)
      |  +......................join()......        child-thread(long)
      +,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,     child-thread(long + daemonized)

    '-' main-thread/parent-thread/main-program execution
    '.' child-thread execution
    '#' optional parent-thread execution after join()-blocked parent-thread could 
        continue
    '*' main-thread 'sleeping' in join-method, waiting for child-thread to finish
    ',' daemonized thread - 'ignores' lifetime of other threads;
        terminates when main-programs exits; is normally meant for 
        join-independent tasks

```

So the reason you don't see any changes is because your main-thread does nothing after your `join`.
You could say `join` is (only) relevant for the execution-flow of the main-thread.

If, for example, you want to concurrently download a bunch of pages to concatenate them into a single large page, you may start concurrent downloads using threads, but need to wait until the last page/thread is finished before you start assembling a single page out of many. That's when you use `join()`."
"Inspired by the question series 'Hidden features of ...', I am curious to hear about your favorite Django tips or lesser known but useful features you know of.

* Please, include only one tip per answer.
* Add Django version requirements if there are any.","I'm just going to start with a tip from myself :)

**Use os.path.dirname() in settings.py to avoid hardcoded dirnames.**

Don't hardcode path's in your settings.py if you want to run your project in different locations. Use the following code in settings.py if your templates and static files are located within the Django project directory:

```
# settings.py
import os
PROJECT_DIR = os.path.dirname(__file__)
...
STATIC_DOC_ROOT = os.path.join(PROJECT_DIR, ""static"")
...
TEMPLATE_DIRS = (
    os.path.join(PROJECT_DIR, ""templates""),
)

```

Credits: I got this tip from the screencast '[Django From the Ground Up](http://thisweekindjango.com/screencasts/episode/10/django-ground-episodes-1-and-2/)'."
"I've been able to verify that the `findUniqueWords` does result in a sorted `list`. However, it does not return the list. Why?

```
def findUniqueWords(theList):
    newList = []
    words = []

    # Read a line at a time
    for item in theList:

        # Remove any punctuation from the line
        cleaned = cleanUp(item)

        # Split the line into separate words
        words = cleaned.split()

        # Evaluate each word
        for word in words:

            # Count each unique word
            if word not in newList:
                newList.append(word)

    answer = newList.sort()
    return answer

```","`list.sort` sorts the list in place, i.e. it doesn't return a new list. Just write

```
newList.sort()
return newList

```"
"What the difference is between `flush()` and `commit()` in SQLAlchemy?

I've read the docs, but am none the wiser - they seem to assume a pre-understanding that I don't have.

I'm particularly interested in their impact on memory usage. I'm loading some data into a database from a series of files (around 5 million rows in total) and my session is occasionally falling over - it's a large database and a machine with not much memory.

I'm wondering if I'm using too many `commit()` and not enough `flush()` calls - but without really understanding what the difference is, it's hard to tell!","A Session object is basically an ongoing transaction of changes to a database (update, insert, delete). These operations aren't persisted to the database until they are committed (if your program aborts for some reason in mid-session transaction, any uncommitted changes within are lost).

The session object registers transaction operations with `session.add()`, but doesn't yet communicate them to the database until `session.flush()` is called.

`session.flush()` communicates a series of operations to the database (insert, update, delete). The database maintains them as pending operations in a transaction. The changes aren't persisted permanently to disk, or visible to other transactions until the database receives a COMMIT for the current transaction (which is what `session.commit()` does).

`session.commit()` commits (persists) those changes to the database.

`flush()` is *always* called as part of a call to `commit()` ([1](http://docs.sqlalchemy.org/en/latest/orm/session_basics.html#committing)).

When you use a Session object to query the database, the query will return results both from the database and from the flushed parts of the uncommitted transaction it holds. By default, Session objects `autoflush` their operations, but this can be disabled.

Hopefully this example will make this clearer:

```
#---
s = Session()

s.add(Foo('A')) # The Foo('A') object has been added to the session.
                # It has not been committed to the database yet,
                #   but is returned as part of a query.
print 1, s.query(Foo).all()
s.commit()

#---
s2 = Session()
s2.autoflush = False

s2.add(Foo('B'))
print 2, s2.query(Foo).all() # The Foo('B') object is *not* returned
                             #   as part of this query because it hasn't
                             #   been flushed yet.
s2.flush()                   # Now, Foo('B') is in the same state as
                             #   Foo('A') was above.
print 3, s2.query(Foo).all() 
s2.rollback()                # Foo('B') has not been committed, and rolling
                             #   back the session's transaction removes it
                             #   from the session.
print 4, s2.query(Foo).all()

#---
Output:
1 [<Foo('A')>]
2 [<Foo('A')>]
3 [<Foo('A')>, <Foo('B')>]
4 [<Foo('A')>]

```"
"I've a csv file without header, with a DateTime index. I want to rename the index and column name, but with df.rename() only the column name is renamed. Bug? I'm on version 0.12.0

```
In [2]: df = pd.read_csv(r'D:\Data\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )

In [3]: df.head()
Out[3]: 
                   1
0                   
2002-06-18  0.112000
2002-06-22  0.190333
2002-06-26  0.134000
2002-06-30  0.093000
2002-07-04  0.098667

In [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)

In [5]: df.head()
Out[5]: 
                  SM
0                   
2002-06-18  0.112000
2002-06-22  0.190333
2002-06-26  0.134000
2002-06-30  0.093000
2002-07-04  0.098667

```","The `rename` method takes a dictionary for the index which applies to index *values*.  
You want to rename to index level's name:

```
df.index.names = ['Date']

```

*A good way to think about this is that columns and index are the same type of object (`Index` or `MultiIndex`), and you can interchange the two via transpose.*

This is a little bit confusing since the index names have a similar meaning to columns, so here are some more examples:

```
In [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))

In [2]: df
Out[2]: 
   A  B  C
0  1  2  3
1  4  5  6

In [3]: df1 = df.set_index('A')

In [4]: df1
Out[4]: 
   B  C
A      
1  2  3
4  5  6

```

You can see the rename on the index, which can change the *value* 1:

```
In [5]: df1.rename(index={1: 'a'})
Out[5]: 
   B  C
A      
a  2  3
4  5  6

In [6]: df1.rename(columns={'B': 'BB'})
Out[6]: 
   BB  C
A       
1   2  3
4   5  6

```

Whilst renaming the level names:

```
In [7]: df1.index.names = ['index']
        df1.columns.names = ['column']

```

Note: this attribute is just a list, and you could do the renaming as a list comprehension/map.

```
In [8]: df1
Out[8]: 
column  B  C
index       
1       2  3
4       5  6

```"
"I'm trying to write PEP-8 compliant code for a domestic project and I have a line with an f-string that is more than 80 characters long:

```
def __str__(self):
    return f'{self.data} - {self.time},\nTags: {self.tags},\nText: {self.text}'

```

I'm trying to split it into different lines in the most *Pythonic* way but the only answer that actually works is an error for my linter.

Working code:

```
def __str__(self):
    return f'{self.date} - {self.time},\nTags:' + \
    f' {self.tags},\nText: {self.text}'

```

Output:

```
2017-08-30 - 17:58:08.307055,
Tags: test tag,
Text: test text

```

The linter thinks that I'm not respecting [E122](https://www.flake8rules.com/rules/E122.html) from [PEP-8](https://peps.python.org/pep-0008/), is there a way to get the string right and the code compliant?","From [Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/):

> The preferred way of wrapping long lines is by using Python's implied
> line continuation inside parentheses, brackets and braces.

Given this, the following would solve your problem in a PEP-8 compliant way.

```
return (
    f'{self.date} - {self.time}\n'
    f'Tags: {self.tags}\n'
    f'Text: {self.text}'
)

```

Python strings will automatically concatenate when not separated by a comma, so you do not need to explicitly call `join()`."
"In Python, is there a way to check if a string is valid JSON before trying to parse it?

For example working with things like the Facebook Graph API, sometimes it returns JSON, sometimes it could return an image file.","You can try to do `json.loads()`, which will throw a `ValueError` if the string you pass can't be decoded as JSON.

In general, the ""[Pythonic](https://docs.python.org/2/glossary.html#term-pythonic)"" philosophy for this kind of situation is called [EAFP](https://docs.python.org/2/glossary.html#term-eafp), for *Easier to Ask for Forgiveness than Permission*."
"Often I find myself wanting to get the first object from a queryset in Django, or return `None` if there aren't any. There are lots of ways to do this which all work. But I'm wondering which is the most performant.

```
qs = MyModel.objects.filter(blah = blah)
if qs.count() > 0:
    return qs[0]
else:
    return None

```

Does this result in two database calls? That seems wasteful. Is this any faster?

```
qs = MyModel.objects.filter(blah = blah)
if len(qs) > 0:
    return qs[0]
else:
    return None

```

Another option would be:

```
qs = MyModel.objects.filter(blah = blah)
try:
    return qs[0]
except IndexError:
    return None

```

This generates a single database call, which is good. But requires creating an exception object a lot of the time, which is a very memory-intensive thing to do when all you really need is a trivial if-test.

How can I do this with just a single database call and without churning memory with exception objects?",[Django 1.6 (released Nov 2013)](https://www.djangoproject.com/weblog/2013/nov/06/django-16-released/) introduced the [convenience methods](https://docs.djangoproject.com/en/dev/ref/models/querysets/#first) `first()` and `last()` which swallow the resulting exception and return `None` if the queryset returns no objects.
"Is it possible, using Python, to merge separate PDF files?

Assuming so, I need to extend this a little further. I am hoping to loop through folders in a directory and repeat this procedure.

And I may be pushing my luck, but is it possible to exclude a page that is contained in each of the PDFs (my report generation always creates an extra blank page)?","You can use [pypdf](https://github.com/py-pdf/pypdf)'s [`PdfMerger`](https://web.archive.org/web/20231128132534/https://pypdf.readthedocs.io/en/stable/modules/PdfMerger.html) class.

**File Concatenation**

You can simply [concatenate](https://en.wikipedia.org/wiki/Concatenation) files by using the [`append`](https://web.archive.org/web/20231128132534/https://pypdf.readthedocs.io/en/stable/modules/PdfMerger.html#pypdf.PdfMerger.append) method.

```
from pypdf import PdfMerger

pdfs = ['file1.pdf', 'file2.pdf', 'file3.pdf', 'file4.pdf']

merger = PdfMerger()

for pdf in pdfs:
    merger.append(pdf)

merger.write(""result.pdf"")
merger.close()

```

You can pass file handles instead of file paths if you want.

**File Merging**

If you want more fine grained control of merging there is a [`merge`](https://web.archive.org/web/20231128132534/https://pypdf.readthedocs.io/en/stable/modules/PdfMerger.html#pypdf.PdfMerger.merge) method of the `PdfMerger`, which allows you to specify an insertion point in the output file, meaning you can insert the pages anywhere in the file. The `append` method can be thought of as a `merge` where the insertion point is the end of the file.

E.g.

```
merger.merge(2, pdf)

```

Here we insert the whole PDF into the output but at page 2.

**Page Ranges**

If you wish to control which pages are appended from a particular file, you can use the `pages` keyword argument of `append` and `merge`, passing a tuple in the form `(start, stop[, step])` (like the regular `range` function).

E.g.

```
merger.append(pdf, pages=(0, 3))    # first 3 pages
merger.append(pdf, pages=(0, 6, 2)) # pages 1,3, 5

```

If you specify an invalid range you will get an `IndexError`.

**Note:** also to avoid files being left open, the `PdfMerger`s close method should be called when the merged file has been written. This ensures all files are closed (input and output) in a timely manner. It's a shame that `PdfMerger` isn't implemented as a context manager, so we can use the `with` keyword, avoid the explicit close call and get some easy exception safety.

You might also want to look at the [`pdfly cat`](https://github.com/py-pdf/pdfly) command provided by the pypdf developers. You can potentially avoid the need to write code altogether.

The pypdf documentation also [includes](https://pypdf.readthedocs.io/en/stable/user/merging-pdfs.html) some example code demonstrating merging.

**PyMuPdf**

Another library perhaps worth a look is [PyMuPdf](https://github.com/pymupdf/PyMuPDF). Merging is equally simple.

From the command line:

```
python -m fitz join -o result.pdf file1.pdf file2.pdf file3.pdf

```

and from code

```
import fitz

result = fitz.open()

for pdf in ['file1.pdf', 'file2.pdf', 'file3.pdf']:
    with fitz.open(pdf) as mfile:
        result.insert_pdf(mfile)
    
result.save(""result.pdf"")

```

With plenty of options, detailed in the projects [wiki](https://github.com/pymupdf/PyMuPDF/wiki).

Note: In older versions of PyMuPDF `insert_pdf` was `insertPDF`"
"How can I format a float so that it doesn't contain trailing zeros? In other words, I want the resulting string to be as short as possible.

For example:

```
3 -> ""3""
3. -> ""3""
3.0 -> ""3""
3.1 -> ""3.1""
3.14 -> ""3.14""
3.140 -> ""3.14""

```","You could use `%g` to achieve this:

```
'%g'%(3.140)

```

or, with Python ≥ 2.6:

```
'{0:g}'.format(3.140)

```

or, with Python ≥ 3.6:

```
f'{3.140:g}'

```

From the [docs for `format`](http://docs.python.org/library/string.html#format-specification-mini-language): `g` causes (among other things)

> insignificant trailing zeros [to be]
> removed from the significand, and the
> decimal point is also removed if there
> are no remaining digits following it."
"```
def insert(array):
    connection=sqlite3.connect('images.db')
    cursor=connection.cursor()
    cnt=0
    while cnt != len(array):
            img = array[cnt]
            print(array[cnt])
            cursor.execute('INSERT INTO images VALUES(?)', (img))
            cnt+= 1
    connection.commit()
    connection.close()

```

When I try `insert(""/gifs/epic-fail-photos-there-i-fixed-it-aww-man-the-tire-pressures-low.gif"")`, I get an error message like in the title (the string is indeed 74 characters long).

What is wrong with the code, and how do I fix it?

---

The same problem occurs with `MySQLdb` and many other popular SQL libraries. See [Why do I get ""TypeError: not all arguments converted during string formatting"" when trying to use a string in a parameterized SQL query?](https://stackoverflow.com/questions/21740359) for details.","You need to pass in a sequence, but you forgot the comma to make your parameters a tuple:

```
cursor.execute('INSERT INTO images VALUES(?)', (img,))

```

Without the comma, `(img)` is just a grouped expression, not a tuple, and thus the `img` string is treated as the input sequence. If that string is 74 characters long, then Python sees that as 74 separate bind values, each one character long.

```
>>> len(img)
74
>>> len((img,))
1

```

If you find it easier to read, you can also use a list literal:

```
cursor.execute('INSERT INTO images VALUES(?)', [img])

```"
"In Python, how do I create a numpy array of arbitrary shape filled with all `True` or all `False`?","**The answer:**

```
numpy.full((2, 2), True)

```

---

**Explanation:**

numpy creates arrays of all ones or all zeros very easily:

e.g. `numpy.ones((2, 2))` or `numpy.zeros((2, 2))`

Since `True` and `False` are represented in Python as `1` and `0`, respectively, we have only to specify this array should be boolean using the optional `dtype` parameter and we are done:

```
numpy.ones((2, 2), dtype=bool)

```

returns:

```
array([[ True,  True],
       [ True,  True]], dtype=bool)

```

**UPDATE: 30 October 2013**

Since numpy [version 1.8](https://pypi.python.org/pypi/numpy/1.8.0), we can use `full` to achieve the same result with syntax that more clearly shows our intent (as fmonegaglia points out):

```
numpy.full((2, 2), True, dtype=bool)

```

**UPDATE: 16 January 2017**

Since at least numpy [version 1.12](https://docs.scipy.org/doc/numpy-1.12.0/reference/index.html), `full` automatically casts to the `dtype` of the second parameter, so we can just write:

```
numpy.full((2, 2), True)

```"
How can I get the index or column of a DataFrame as a NumPy array or Python list?,"To get a NumPy array, you should use the `values` attribute:

```
In [1]: df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=['a', 'b', 'c']); df
   A  B
a  1  4
b  2  5
c  3  6

In [2]: df.index.values
Out[2]: array(['a', 'b', 'c'], dtype=object)

```

This accesses how the data is already stored, so there isn't any need for a conversion.

Note: This attribute is also available for many other pandas objects.

```
In [3]: df['A'].values
Out[3]: Out[16]: array([1, 2, 3])

```

---

To get the index as a list, call `tolist`:

```
In [4]: df.index.tolist()
Out[4]: ['a', 'b', 'c']

```

And similarly, for columns."
"I'm trying to figure out how to add multiple columns to pandas simultaneously with Pandas. I would like to do this in one step rather than multiple repeated steps.

```
import pandas as pd

data = {'col_1': [0, 1, 2, 3],
        'col_2': [4, 5, 6, 7]}
df = pd.DataFrame(data)

```

I thought this would work here...

```
df[['column_new_1', 'column_new_2', 'column_new_3']] = [np.nan, 'dogs', 3]

```","I would have expected your syntax to work too. The problem arises because when you create new columns with the column-list syntax (`df[[new1, new2]] = ...`), pandas requires that the right hand side be a DataFrame (note that it doesn't actually matter if the columns of the DataFrame have the same names as the columns you are creating).

Your syntax works fine for assigning scalar values to *existing* columns, and pandas is also happy to assign scalar values to a new column using the single-column syntax (`df[new1] = ...`). So the solution is either to convert this into several single-column assignments, or create a suitable DataFrame for the right-hand side.

Here are several approaches that *will* work:

```
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'col_1': [0, 1, 2, 3],
    'col_2': [4, 5, 6, 7]
})

```

Then one of the following:

### 1) Three assignments in one, using iterator unpacking

```
df['column_new_1'], df['column_new_2'], df['column_new_3'] = np.nan, 'dogs', 3

```

### 2) Use `DataFrame()` to expand a single row to match the index

```
df[['column_new_1', 'column_new_2', 'column_new_3']] = pd.DataFrame([[np.nan, 'dogs', 3]], index=df.index)

```

### 3) Combine with a temporary DataFrame using [`pd.concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)

```
df = pd.concat(
    [
        df,
        pd.DataFrame(
            [[np.nan, 'dogs', 3]], 
            index=df.index, 
            columns=['column_new_1', 'column_new_2', 'column_new_3']
        )
    ], axis=1
)

```

### 4) Combine with a temporary DataFrame using [`.join`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)

This is similar to 3, but may be less efficient.

```
df = df.join(pd.DataFrame(
    [[np.nan, 'dogs', 3]], 
    index=df.index, 
    columns=['column_new_1', 'column_new_2', 'column_new_3']
))

```

### 5) Use a dictionary instead of the lists used in 3 and 4

This is a more ""natural"" way to create the temporary DataFrame than the previous two. Note that in [Python 3.5 or earlier](https://stackoverflow.com/q/39980323/3830997), the new columns will be sorted alphabetically.

```
df = df.join(pd.DataFrame(
    {
        'column_new_1': np.nan,
        'column_new_2': 'dogs',
        'column_new_3': 3
    }, index=df.index
))

```

### 6) Use [`.assign()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html) with multiple column arguments

This may be the winner in Python 3.6+. But like the previous one, the new columns will be sorted alphabetically in earlier versions of Python.

```
df = df.assign(column_new_1=np.nan, column_new_2='dogs', column_new_3=3)

```

### 7) Create new columns, then assign all values at once

Based on [this answer](https://stackoverflow.com/a/44951376/3830997). This is interesting, but I don't know when it would be worth the trouble.

```
new_cols = ['column_new_1', 'column_new_2', 'column_new_3']
new_vals = [np.nan, 'dogs', 3]
df = df.reindex(columns=df.columns.tolist() + new_cols)   # add empty cols
df[new_cols] = new_vals  # multi-column assignment works for existing cols

```

### 8) Three separate assignments

In the end, it's hard to beat this.

```
df['column_new_1'] = np.nan
df['column_new_2'] = 'dogs'
df['column_new_3'] = 3

```

Note: many of these options have already been covered in other questions:

* [Add multiple columns to DataFrame and set them equal to an existing column](https://stackoverflow.com/q/43415798/3830997)
* [Is it possible to add several columns at once to a pandas DataFrame?](https://stackoverflow.com/q/19866377/3830997)
* [Add multiple empty columns to pandas DataFrame](https://stackoverflow.com/q/30926670/3830997)"
"What is the `related_name` argument useful for on `ManyToManyField` and `ForeignKey` fields? For example, given the following code, what is the effect of `related_name='maps'`?

```
class Map(db.Model):
    members = models.ManyToManyField(User, related_name='maps',
                                     verbose_name=_('members'))

```","The `related_name` attribute specifies the name of the reverse relation from the `User` model back to your model.

If you don't specify a `related_name`, Django automatically creates one using the name of your model with the suffix `_set`, for instance `User.map_set.all()`.

If you *do* specify, e.g. `related_name=maps` on the `User` model, `User.map_set` will still work, but the `User.maps.` syntax is obviously a bit cleaner and less clunky; so for example, if you had a user object `current_user`, you could use `current_user.maps.all()` to get all instances of your `Map` model that have a relation to `current_user`.

The [Django documentation](http://docs.djangoproject.com/en/dev/topics/db/queries/#backwards-related-objects) has more details.

To disable creating the backwards relationship entirely, [set `related_name` to `""+""`](https://docs.djangoproject.com/en/5.0/ref/models/fields/#django.db.models.ForeignKey.related_name)."
"I'm a new Python programmer who is making the leap from 2.6.4 to 3.1.1. Everything has gone fine until I tried to use the 'else if' statement. The interpreter gives me a syntax error after the 'if' in 'else if' for a reason I can't seem to figure out.

```
def function(a):
    if a == '1':
        print ('1a')
    else if a == '2'
        print ('2a')
    else print ('3a')

function(input('input:'))

```

I'm probably missing something very simple; however, I haven't been able to find the answer on my own.","In python ""else if"" is spelled ""elif"".  
Also, you need a colon after the `elif` and the `else`.

Simple answer to a simple question. I had the same problem, when I first started (in the last couple of weeks).

So your code should read:

```
def function(a):
    if a == '1':
        print('1a')
    elif a == '2':
        print('2a')
    else:
        print('3a')

function(input('input:'))

```"
"I have the following code:

```
r = numpy.zeros(shape = (width, height, 9))

```

It creates a `width x height x 9` matrix filled with zeros. Instead, I'd like to know if there's a function or way to initialize them instead to `NaN`s in an easy way.","You rarely need loops for vector operations in numpy.
You can create an uninitialized array and assign to all entries at once:

```
>>> a = numpy.empty((3,3,))
>>> a[:] = numpy.nan
>>> a
array([[ NaN,  NaN,  NaN],
       [ NaN,  NaN,  NaN],
       [ NaN,  NaN,  NaN]])

```

---

I have timed the alternatives `a[:] = numpy.nan` here and `a.fill(numpy.nan)` as posted by Blaenk:

```
$ python -mtimeit ""import numpy as np; a = np.empty((100,100));"" ""a.fill(np.nan)""
10000 loops, best of 3: 54.3 usec per loop
$ python -mtimeit ""import numpy as np; a = np.empty((100,100));"" ""a[:] = np.nan"" 
10000 loops, best of 3: 88.8 usec per loop

```

The timings show a preference for `ndarray.fill(..)` as the faster alternative. OTOH, I like numpy's convenience implementation where you can assign values to whole slices at the time, the code's intention is very clear.

Note that `ndarray.fill` performs its operation in-place, so `numpy.empty((3,3,)).fill(numpy.nan)` will instead return `None`."
"I have to create an ""Expires"" value 5 minutes in the future, but I have to supply it in UNIX Timestamp format. I have this so far, but it seems like a hack.

```
def expires():
    '''return a UNIX style timestamp representing 5 minutes from now'''
    epoch = datetime.datetime(1970, 1, 1)
    seconds_in_a_day = 60 * 60 * 24
    five_minutes = datetime.timedelta(seconds=5*60)
    five_minutes_from_now = datetime.datetime.now() + five_minutes
    since_epoch = five_minutes_from_now - epoch
    return since_epoch.days * seconds_in_a_day + since_epoch.seconds

```

Is there a module or function that does the timestamp conversion for me?","Another way is to use [`calendar.timegm`](http://docs.python.org/3.3/library/calendar.html#calendar.timegm):

```
future = datetime.datetime.utcnow() + datetime.timedelta(minutes=5)
return calendar.timegm(future.timetuple())

```

It's also more portable than `%s` flag to `strftime` (which doesn't work on Windows)."
"Before I re-invent this particular wheel, has anybody got a nice routine for calculating the size of a directory using Python? It would be very nice if the routine would format the size nicely in Mb/Gb etc.","This walks all sub-directories; summing file sizes:

```
import os

def get_size(start_path = '.'):
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(start_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            # skip if it is symbolic link
            if not os.path.islink(fp):
                total_size += os.path.getsize(fp)

    return total_size

print(get_size(), 'bytes')

```

And a oneliner for fun using [os.listdir](http://docs.python.org/library/os.html?highlight=shutil#os.listdir) (*Does not include sub-directories*):

```
import os
sum(os.path.getsize(f) for f in os.listdir('.') if os.path.isfile(f))

```

Reference:

* [os.path.getsize](http://docs.python.org/library/os.path.html#os.path.getsize) - Gives the size in bytes
* [os.walk](http://docs.python.org/library/os.html?highlight=shutil#os.walk)
* [os.path.islink](https://docs.python.org/3/library/os.path.html#os.path.islink)

**Updated**
To use *os.path.getsize*, this is clearer than using the os.stat().st\_size method.

*Thanks to [ghostdog74](https://stackoverflow.com/a/1777225) for pointing this out!*

[os.stat](http://docs.python.org/library/os.html?highlight=shutil#os.stat) - *st\_size* Gives the size in bytes. Can also be used to get file size and other file related information.

```
import os

nbytes = sum(d.stat().st_size for d in os.scandir('.') if d.is_file())

```

**Update 2018**

If you use Python 3.4 or previous then you may consider using the more efficient `walk` method provided by the third-party [`scandir`](https://pypi.org/project/scandir/) package. In Python 3.5 and later, this package has been incorporated into the standard library and `os.walk` has received the corresponding increase in performance.

**Update 2019**

Recently I've been using `pathlib` more and more, here's a `pathlib` solution:

```
from pathlib import Path

root_directory = Path('.')
sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())

```"
"When would the `-e`, or `--editable` option be useful with `pip install`?

For some projects the last line in requirements.txt is `-e .`. What does it do exactly?","As the man page says it:

```
-e,--editable <path/url>
     Install a project in editable mode (i.e.  setuptools ""develop mode"") from a local project path or a VCS url.

```

So you would use this when trying to install a package locally, most often in the case when you are developing it on your system. It will just link the package to the original location, basically meaning any changes to the original package would reflect directly in your environment.

Some nuggets around the same [here](http://pip-python3.readthedocs.org/en/latest/reference/pip_install.html#vcs-support) and [here](https://stackoverflow.com/a/20043907/1860929).

An example run can be:

```
pip install -e .

```

or

```
pip install -e ~/ultimate-utils/ultimate-utils-proj-src/

```

note the second is the full path to where the `setup.py` would be at."
"After much searching I couldn't find out how to use smtplib.sendmail to send to multiple recipients. The problem was every time the mail would be sent the mail headers would appear to contain multiple addresses, but in fact only the first recipient would receive the email.

The problem seems to be that the [`email.Message`](http://docs.python.org/library/email.message.html#email.message.Message) module expects something different than the [`smtplib.sendmail()`](http://docs.python.org/library/smtplib.html#smtplib.SMTP.sendmail) function.

In short, to send to multiple recipients you should set the header to be a string of comma delimited email addresses. The `sendmail()` parameter `to_addrs` however should be a list of email addresses.

```
from email.MIMEMultipart import MIMEMultipart
from email.MIMEText import MIMEText
import smtplib

msg = MIMEMultipart()
msg[""Subject""] = ""Example""
msg[""From""] = ""me@example.com""
msg[""To""] = ""malcom@example.com,reynolds@example.com,firefly@example.com""
msg[""Cc""] = ""serenity@example.com,inara@example.com""
body = MIMEText(""example email body"")
msg.attach(body)
smtp = smtplib.SMTP(""mailhost.example.com"", 25)
smtp.sendmail(msg[""From""], msg[""To""].split("","") + msg[""Cc""].split("",""), msg.as_string())
smtp.quit()

```","This **really works**, I spent a lot of time trying multiple variants.

```
import smtplib
from email.mime.text import MIMEText

s = smtplib.SMTP('smtp.uk.xensource.com')
s.set_debuglevel(1)
msg = MIMEText(""""""body"""""")
sender = 'me@example.com'
recipients = ['john.doe@example.com', 'john.smith@example.co.uk']
msg['Subject'] = ""subject line""
msg['From'] = sender
msg['To'] = "", "".join(recipients)
s.sendmail(sender, recipients, msg.as_string())

```"
"I have a simple factorplot

```
import seaborn as sns
g = sns.factorplot(""name"", ""miss_ratio"", ""policy"", dodge=.2, 
    linestyles=[""none"", ""none"", ""none"", ""none""], data=df[df[""level""] == 2])

```

![enter image description here](https://i.sstatic.net/gg7aD.png)

The problem is that the x labels all run together, making them unreadable. How do you rotate the text so that the labels are readable?","I had a problem with the answer by @mwaskorn, namely that

```
g.set_xticklabels(rotation=30)

```

fails, because this also requires the labels. A bit easier than the answer by @Aman is to just add

```
plt.xticks(rotation=30)

```"
"When I type `pip3 install --upgrade tensorflow-gpu` in cmd, both administrative and normal I get this:

```
Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\users\\dylan\\appdata\\local\\programs\\python\\python35\\Lib\\site-packages\\numpy\\.libs\\libopenblas.BNVRK7633HSX7YVO2TADGR4A5KEKXJAW.gfortran-win_amd64.dll'
Consider using the `--user` option or check the permissions.

```

How can I upgrade the library?","Just type the command you want execute with the user permission, if you don't want to change the permission:

```
pip3 install --upgrade tensorflow-gpu --user

```"
"I've successfully converted something of `26 Sep 2012` format to `26-09-2012` using:

```
datetime.strptime(request.POST['sample_date'],'%d %b %Y')

```

However, I don't know how to set the hour and minute of something like the above to 11:59. Does anyone know how to do this?

Note, this can be a future date or any random one, not just the current date.","Use [`datetime.replace`](https://docs.python.org/3.5/library/datetime.html#datetime.datetime.replace):

```
from datetime import datetime
dt = datetime.strptime('26 Sep 2012', '%d %b %Y')
newdatetime = dt.replace(hour=11, minute=59)

```

Also worth noting: `datetime.replace` returns a new copy of the `datetime` (since `datetime` is immutable): it is like `str.replace` in that regard."
"I'm trying to upload an image via the Django admin and then view that image either in a page on the frontend or just via a URL.

Note this is all on my local machine.

My settings are as follows:

```
MEDIA_ROOT = '/home/dan/mysite/media/'

MEDIA_URL = '/media/'

```

I have set the upload\_to parameter to 'images' and the file has been correctly uploaded to the directory:

```
'/home/dan/mysite/media/images/myimage.png'

```

However, when I try to access the image at the following URL:

```
http://127.0.0.1:8000/media/images/myimage.png

```

I get a 404 error.

Do I need to setup specific URLconf patters for uploaded media?

Any advice appreciated.

Thanks.","**UPDATE for Django >= 1.7**

Per Django 2.1 documentation: [Serving files uploaded by a user during development](https://docs.djangoproject.com/en/2.1/howto/static-files/#serving-files-uploaded-by-a-user-during-development)

```
from django.conf import settings
from django.conf.urls.static import static

urlpatterns = patterns('',
    # ... the rest of your URLconf goes here ...
) + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)

```

You no longer need `if settings.DEBUG` as Django will handle ensuring this is only used in Debug mode.

---

**ORIGINAL answer for Django <= 1.6**

Try putting this into your *urls.py*

```
from django.conf import settings

# ... your normal urlpatterns here

if settings.DEBUG:
    # static files (images, css, javascript, etc.)
    urlpatterns += patterns('',
        (r'^media/(?P<path>.*)$', 'django.views.static.serve', {
        'document_root': settings.MEDIA_ROOT}))

```

With this you can serve the static media from Django when `DEBUG = True` (when you run on local computer) but you can let your web server configuration serve static media when you go to production and `DEBUG = False`"
"I'm having trouble parsing HTML elements with ""class"" attribute using Beautifulsoup. The code looks like this

```
soup = BeautifulSoup(sdata)
mydivs = soup.findAll('div')
for div in mydivs: 
    if (div[""class""] == ""stylelistrow""):
        print div

```

I get an error on the same line ""after"" the script finishes.

```
File ""./beautifulcoding.py"", line 130, in getlanguage
  if (div[""class""] == ""stylelistrow""):
File ""/usr/local/lib/python2.6/dist-packages/BeautifulSoup.py"", line 599, in __getitem__
   return self._getAttrMap()[key]
KeyError: 'class'

```

How do I get rid of this error?","You can refine your search to only find those divs with a given class using BS3:

```
mydivs = soup.find_all(""div"", {""class"": ""stylelistrow""})

```"
"PIL does support JPEG in my system.

Whenever I do an upload, my code is failing with:

```
File ""PIL/Image.py"", line 375, in _getdecoder
    raise IOError(""decoder %s not available"" % decoder_name)
IOError: decoder jpeg not available

```

How can I resolve this?","libjpeg-dev is required to be able to process jpegs with pillow (or PIL), so you need to install it and then recompile pillow. It also seems that libjpeg8-dev is needed on Ubuntu 14.04

If you're still using PIL then you should really be using pillow these days though, so first `pip uninstall PIL` before following these instructions to switch, or if you have a good reason for sticking with PIL then replace ""pillow"" with ""PIL"" in the below).

On Ubuntu:

```
# install libjpeg-dev with apt
sudo apt-get install libjpeg-dev
# if you're on Ubuntu 14.04, also install this
sudo apt-get install libjpeg8-dev

# reinstall pillow
pip install --no-cache-dir -I pillow

```

If that doesn't work, try one of the below, depending on whether you are on 64bit or 32bit Ubuntu.

For Ubuntu x64:

```
sudo ln -s /usr/lib/x86_64-linux-gnu/libjpeg.so /usr/lib
sudo ln -s /usr/lib/x86_64-linux-gnu/libfreetype.so /usr/lib
sudo ln -s /usr/lib/x86_64-linux-gnu/libz.so /usr/lib

```

Or for Ubuntu 32bit:

```
sudo ln -s /usr/lib/i386-linux-gnu/libjpeg.so /usr/lib/
sudo ln -s /usr/lib/i386-linux-gnu/libfreetype.so.6 /usr/lib/
sudo ln -s /usr/lib/i386-linux-gnu/libz.so /usr/lib/

```

Then reinstall pillow:

```
pip install --no-cache-dir -I pillow

```

(Edits to include feedback from comments. Thanks Charles Offenbacher for pointing out this differs for 32bit, and t-mart for suggesting use of `--no-cache-dir`)."
"In Java the numeric types all descend from Number so I would use

```
(x instanceof Number).

```

What is the python equivalent?","Test if your variable is an instance of [`numbers.Number`](https://docs.python.org/library/numbers.html#numbers.Number):

```
>>> import numbers
>>> import decimal
>>> [isinstance(x, numbers.Number) for x in (0, 0.0, 0j, decimal.Decimal(0))]
[True, True, True, True]

```

This uses [ABCs](https://docs.python.org/glossary.html#term-abstract-base-class) and will work for all built-in number-like classes, and also for all third-party classes if they are worth their salt (registered as subclasses of the `Number` ABC).

However, in many cases you shouldn't worry about checking types manually - Python is [duck typed](https://en.wikipedia.org/wiki/Duck_typing) and mixing somewhat compatible types usually works, yet it will barf an error message when some operation doesn't make sense (`4 - ""1""`), so manually checking this is rarely really needed. It's just a bonus. You can add it when finishing a module to avoid pestering others with implementation details.

This works [starting with Python 2.6](https://docs.python.org/whatsnew/2.6.html#pep-3141-a-type-hierarchy-for-numbers). On older versions you're pretty much limited to checking for a few hardcoded types."
"I have this `try` block in my code:

```
try:
    do_something_that_might_raise_an_exception()
except ValueError as err:
    errmsg = 'My custom error message.'
    raise ValueError(errmsg)

```

Strictly speaking, I am actually raising *another* `ValueError`, not the `ValueError` thrown by `do_something...()`, which is referred to as `err` in this case. How do I attach a custom message to `err`? I try the following code but fails due to `err`, a `ValueError` **instance**, not being callable:

```
try:
    do_something_that_might_raise_an_exception()
except ValueError as err:
    errmsg = 'My custom error message.'
    raise err(errmsg)

```","### raise from

We can chain the exceptions using [raise from](https://docs.python.org/3/reference/simple_stmts.html#raise).

```
try:
    1 / 0
except ZeroDivisionError as e:
    raise Exception('Smelly socks') from e

```

In this case, the exception your caller would catch has the line number of the place where we raise our exception.

```
Traceback (most recent call last):
  File ""test.py"", line 2, in <module>
    1 / 0
ZeroDivisionError: division by zero

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""test.py"", line 4, in <module>
    raise Exception('Smelly socks') from e
Exception: Smelly socks

```

Notice the bottom exception only has the stacktrace from where we raised our exception. Your caller could still get the original exception by accessing the `__cause__` attribute of the exception they catch.

### with\_traceback

Or you can use [`with_traceback`](https://docs.python.org/3/library/exceptions.html#BaseException.with_traceback).

```
try:
    1 / 0
except ZeroDivisionError as e:
    raise Exception('Smelly socks').with_traceback(e.__traceback__)

```

Using this form, the exception your caller would catch has the traceback from where the original error occurred.

```
Traceback (most recent call last):
  File ""test.py"", line 2, in <module>
    1 / 0
ZeroDivisionError: division by zero

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 4, in <module>
    raise Exception('Smelly socks').with_traceback(e.__traceback__)
  File ""test.py"", line 2, in <module>
    1 / 0
Exception: Smelly socks

```

Notice the bottom exception has the line where we performed the invalid division as well as the line where we reraise the exception."
"Suppose I need to refer to the path `C:\meshes\as`. If I try writing that directly, like `""C:\meshes\as""`, I encounter problems - either some exception, or the path just doesn't work. Is this because `\` is acting as an escape character? How should I write the paths?","You can use always:

```
'C:/mydir'

```

This works both in Linux and Windows.

Another possibility is:

```
'C:\\mydir'

```

If you have problems with some names you can also try raw string literals:

```
r'C:\mydir'

```

However, the best practice is to use the [`os.path`](https://docs.python.org/3/library/os.path.html) module functions that always joins with the correct path separator (`os.path.sep`) for your OS:

```
os.path.join(mydir, myfile)

```

From python 3.4 you can also use the [`pathlib`](https://docs.python.org/3/library/pathlib.html) module. This is equivalent to the above:

```
pathlib.Path(mydir, myfile)

```

or:

```
pathlib.Path(mydir) / myfile

```"
"Lets assume we have a dataset which might be given approximately by:

```
import numpy as np
x = np.linspace(0,2*np.pi,100)
y = np.sin(x) + np.random.random(100) * 0.2

```

Therefore we have a variation of 20% of the dataset. My first idea was to use the `UnivariateSpline` function of scipy, but the problem is that this does not consider the small noise well. If you consider the frequencies, the background is much smaller than the signal, so a spline only of the cutoff might be an idea, but that would involve a back and forth Fourier transformation, which might result in bad behavior.
Another way would be a moving average, but this would also need the right choice of delay.

Any hints/books or links on how to tackle this problem?

![example](https://i.sstatic.net/bcaCP.png)","I prefer a [Savitzky-Golay filter](http://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter_for_smoothing_and_differentiation). [It's available in scipy here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html). It uses least squares to regress a small window of your data onto a polynomial, then uses the polynomial to estimate the point in the center of the window. Finally the window is shifted forward by one data point and the process repeats. This continues until every point has been optimally adjusted relative to its neighbors. It works great even with noisy samples from non-periodic and non-linear sources.

Here is a [thorough cookbook example](https://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay), although this is outdated now. Note: I left out the code for defining the `savitzky_golay()` function because you can copy/paste it from the cookbook example I linked above.

```
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0,2*np.pi,100)
y = np.sin(x) + np.random.random(100) * 0.2
yhat = savitzky_golay(y, 51, 3) # window size 51, polynomial order 3

plt.plot(x,y)
plt.plot(x,yhat, color='red')
plt.show()

```

![optimally smoothing a noisy sinusoid](https://i.sstatic.net/SbvSL.png)

**UPDATE:** It has come to my attention that the cookbook example I linked to has been taken down. Fortunately, the Savitzky-Golay filter has been incorporated [into the SciPy library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html), as pointed out by [@dodohjk](https://stackoverflow.com/users/2068580/dodohjk) (thanks [@bicarlsen](https://stackoverflow.com/users/2961550/bicarlsen) for the updated link).
To adapt the above code by using SciPy source, type:

```
from scipy.signal import savgol_filter
yhat = savgol_filter(y, 51, 3) # window size 51, polynomial order 3

```"
"I have a class like the following:

```
class User:
    def __init__(self):
        self.data = []
        self.other_data = []

    def doSomething(self, source):
        // if source = 'other_data' how to access self.other_data

```

I want to pass a string for the source variable in `doSomething` and access the class member of the same name.

I have tried `getattr` which only works on functions (from what I can tell) as well as having `User` extend `dict` and using `self.__getitem__`, but that doesn't work either. What is the best way to do this?","`x = getattr(self, source)` will work just perfectly if `source` names ANY attribute of self, including the `other_data` in your example."
"How do I take multiple lists and put them as different columns in a python dataframe? I tried [this solution](https://stackoverflow.com/questions/29014618/read-lists-into-columns-of-pandas-dataframe) but had some trouble.

Attempt 1:

* Have three lists, and zip them together and use that `res = zip(lst1,lst2,lst3)`
* Yields just one column

Attempt 2:

```
percentile_list = pd.DataFrame({'lst1Tite' : [lst1],
                                'lst2Tite' : [lst2],
                                'lst3Tite' : [lst3] }, 
                                columns=['lst1Tite','lst1Tite', 'lst1Tite'])

```

* yields either one row by 3 columns (the way above) or if I transpose it is 3 rows and 1 column

How do I get a 100 row (length of each independent list) by 3 column (three lists) pandas dataframe?","I think you're almost there, try removing the extra square brackets around the `lst`'s (Also you don't need to specify the column names when you're creating a dataframe from a dict like this):

```
import pandas as pd
lst1 = range(100)
lst2 = range(100)
lst3 = range(100)
percentile_list = pd.DataFrame(
    {'lst1Title': lst1,
     'lst2Title': lst2,
     'lst3Title': lst3
    })

percentile_list
    lst1Title  lst2Title  lst3Title
0          0         0         0
1          1         1         1
2          2         2         2
3          3         3         3
4          4         4         4
5          5         5         5
6          6         6         6
...

```

If you need a more performant solution you can use `np.column_stack` rather than `zip` as in your first attempt, this has around a 2x speedup on the example here, however comes at bit of a cost of readability in my opinion:

```
import numpy as np
percentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), 
                               columns=['lst1Title', 'lst2Title', 'lst3Title'])

```"
"I have used Matplotlib to plot lines on a figure. Now I would now like to set the style, specifically the marker, for individual points on the line. How do I do this?

To clarify my question, I want to be able to set the style for individual markers on a line, not every marker on said line.","Specify the keyword args `linestyle` and/or `marker` in your call to `plot`.

For example, using a dashed line and blue circle markers:

```
plt.plot(range(10), linestyle='--', marker='o', color='b', label='line with marker')
plt.legend()

```

A shortcut call for the same thing:

```
plt.plot(range(10), '--bo', label='line with marker')
plt.legend()

```

[![enter image description here](https://i.sstatic.net/iRLpX.png)](https://i.sstatic.net/iRLpX.png)

Here is a list of the possible line and marker styles:

```
================    ===============================
character           description
================    ===============================
   -                solid line style
   --               dashed line style
   -.               dash-dot line style
   :                dotted line style
   .                point marker
   ,                pixel marker
   o                circle marker
   v                triangle_down marker
   ^                triangle_up marker
   <                triangle_left marker
   >                triangle_right marker
   1                tri_down marker
   2                tri_up marker
   3                tri_left marker
   4                tri_right marker
   s                square marker
   p                pentagon marker
   *                star marker
   h                hexagon1 marker
   H                hexagon2 marker
   +                plus marker
   x                x marker
   D                diamond marker
   d                thin_diamond marker
   |                vline marker
   _                hline marker
================    ===============================

```

---

*edit:* with an example of marking an arbitrary subset of points, as requested in the comments:

```
import numpy as np
import matplotlib.pyplot as plt

xs = np.linspace(-np.pi, np.pi, 30)
ys = np.sin(xs)
markers_on = [12, 17, 18, 19]
plt.plot(xs, ys, '-gD', markevery=markers_on, label='line with select markers')
plt.legend()
plt.show()

```

[![enter image description here](https://i.sstatic.net/xG4iV.png)](https://i.sstatic.net/xG4iV.png)

This last example using the `markevery` kwarg is possible in since 1.4+, due to the merge of [this feature branch](https://github.com/matplotlib/matplotlib/pull/2662). If you are stuck on an older version of matplotlib, you can still achieve the result by overlaying a scatterplot on the line plot. See the [edit history](https://stackoverflow.com/posts/8409110/revisions) for more details."
"I want to check the operating system (on the computer where the script runs).

I know I can use `os.system('uname -o')` in Linux, but it gives me a message in the console, and I want to write to a variable.

It will be okay if the script can tell if it is Mac, Windows or Linux. How can I check it?","You can use [`sys.platform`](https://docs.python.org/library/sys.html#sys.platform):

```
from sys import platform
if platform == ""linux"" or platform == ""linux2"":
    # linux
elif platform == ""darwin"":
    # OS X
elif platform == ""win32"":
    # Windows...

```

`sys.platform` has finer granularity than `sys.name`.

For the valid values, consult [the documentation](https://docs.python.org/library/sys.html#sys.platform).

See also the answer to [“What OS am I running on?”](https://stackoverflow.com/a/1857/1513933)"
"I would like to put some logging statements within test function to examine some state variables.

I have the following code snippet:

```
import pytest,os
import logging

logging.basicConfig(level=logging.DEBUG)
mylogger = logging.getLogger()

#############################################################################

def setup_module(module):
    ''' Setup for the entire module '''
    mylogger.info('Inside Setup')
    # Do the actual setup stuff here
    pass

def setup_function(func):
    ''' Setup for test functions '''
    if func == test_one:
        mylogger.info(' Hurray !!')

def test_one():
    ''' Test One '''
    mylogger.info('Inside Test 1')
    #assert 0 == 1
    pass

def test_two():
    ''' Test Two '''
    mylogger.info('Inside Test 2')
    pass

if __name__ == '__main__':
    mylogger.info(' About to start the tests ')
    pytest.main(args=[os.path.abspath(__file__)])
    mylogger.info(' Done executing the tests ')

```

I get the following output:

```
[bmaryada-mbp:/Users/bmaryada/dev/platform/main/proto/tests/tpch $]python minitest.py
INFO:root: About to start the tests 
======================================================== test session starts =========================================================
platform darwin -- Python 2.6.2 -- pytest-2.0.0
collected 2 items 

minitest.py ..

====================================================== 2 passed in 0.01 seconds ======================================================
INFO:root: Done executing the tests 

```

Notice that only the logging messages from the `'__name__ == __main__'` block get transmitted to the console.

Is there a way to force `pytest` to emit logging to console from test methods as well?","Since version 3.3, `pytest` supports live logging, meaning that all the log records emitted in tests will be printed to the terminal immediately. The feature is documented under [Live Logs](https://docs.pytest.org/en/latest/how-to/logging.html#live-logs) section. Live logging is disabled by default; to enable it, set `log_cli = 1` in the `pyproject.toml`1 or `pytest.ini`2 config. Live logging supports emitting to terminal and file; the relevant options allow records customizing:

### terminal:

* `log_cli_level`
* `log_cli_format`
* `log_cli_date_format`

### file:

* `log_file`
* `log_file_level`
* `log_file_format`
* `log_file_date_format`

As pointed out by [Kévin Barré](https://stackoverflow.com/users/7505103/k%c3%a9vin-barr%c3%a9) in [this comment](https://stackoverflow.com/questions/4673373/logging-within-py-test-tests/51633600?noredirect=1#comment91175953_51633600), overriding `ini` options from command line can be done via:

> `-o OVERRIDE_INI, --override-ini=OVERRIDE_INI`  
>         override ini option with ""option=value"" style, e.g.  
>         `-o xfail_strict=True -o cache_dir=cache`

So instead of declaring `log_cli` in `pytest.ini`, you can simply call:

```
$ pytest -o log_cli=true ...

```

Examples
--------

Simple test file used for demonstrating:

```
# test_spam.py

import logging

LOGGER = logging.getLogger(__name__)


def test_eggs():
    LOGGER.info('eggs info')
    LOGGER.warning('eggs warning')
    LOGGER.error('eggs error')
    LOGGER.critical('eggs critical')
    assert True

```

As you can see, no extra configuration needed; `pytest` will setup the logger automatically, based on options specified in `pytest.ini` or passed from command line.

### Live logging to terminal, `INFO` level, fancy output

Configuration in `pyproject.toml`:

```
[tool.pytest.ini_options]
log_cli = true
log_cli_level = ""INFO""
log_cli_format = ""%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)""
log_cli_date_format = ""%Y-%m-%d %H:%M:%S""

```

The identical configuration in legacy `pytest.ini`:

```
[pytest]
log_cli = 1
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)
log_cli_date_format=%Y-%m-%d %H:%M:%S

```

Running the test:

```
$ pytest test_spam.py
=============================== test session starts ================================
platform darwin -- Python 3.6.4, pytest-3.7.0, py-1.5.3, pluggy-0.7.1 -- /Users/hoefling/.virtualenvs/stackoverflow/bin/python3.6
cachedir: .pytest_cache
rootdir: /Users/hoefling/projects/private/stackoverflow/so-4673373, inifile: pytest.ini
collected 1 item

test_spam.py::test_eggs
---------------------------------- live log call -----------------------------------
2018-08-01 14:33:20 [    INFO] eggs info (test_spam.py:7)
2018-08-01 14:33:20 [ WARNING] eggs warning (test_spam.py:8)
2018-08-01 14:33:20 [   ERROR] eggs error (test_spam.py:9)
2018-08-01 14:33:20 [CRITICAL] eggs critical (test_spam.py:10)
PASSED                                                                        [100%]

============================= 1 passed in 0.01 seconds =============================

```

### Live logging to terminal and file, only message & `CRITICAL` level in terminal, fancy output in `pytest.log` file

Configuration in `pyproject.toml`:

```
[tool.pytest.ini_options]
log_cli = true
log_cli_level = ""CRITICAL""
log_cli_format = ""%(message)s""

log_file = ""pytest.log""
log_file_level = ""DEBUG""
log_file_format = ""%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)""
log_file_date_format = ""%Y-%m-%d %H:%M:%S""

```

The identical configuration in legacy `pytest.ini`:

```
[pytest]
log_cli = 1
log_cli_level = CRITICAL
log_cli_format = %(message)s

log_file = pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)
log_file_date_format=%Y-%m-%d %H:%M:%S

```

Test run:

```
$ pytest test_spam.py
=============================== test session starts ================================
platform darwin -- Python 3.6.4, pytest-3.7.0, py-1.5.3, pluggy-0.7.1 -- /Users/hoefling/.virtualenvs/stackoverflow/bin/python3.6
cachedir: .pytest_cache
rootdir: /Users/hoefling/projects/private/stackoverflow/so-4673373, inifile: pytest.ini
collected 1 item

test_spam.py::test_eggs
---------------------------------- live log call -----------------------------------
eggs critical
PASSED                                                                        [100%]

============================= 1 passed in 0.01 seconds =============================

$ cat pytest.log
2018-08-01 14:38:09 [    INFO] eggs info (test_spam.py:7)
2018-08-01 14:38:09 [ WARNING] eggs warning (test_spam.py:8)
2018-08-01 14:38:09 [   ERROR] eggs error (test_spam.py:9)
2018-08-01 14:38:09 [CRITICAL] eggs critical (test_spam.py:10)

```

---

1 `pyproject.toml` supported since version 6.0 and is the best option IMO. See [PEP 518](https://www.python.org/dev/peps/pep-0517/) for the specs.

2 Although you can also configure `pytest` in `setup.cfg` under the `[tool:pytest]` section, don't be tempted to do that when you want to provide custom live logging format. Other tools reading `setup.cfg` might treat stuff like `%(message)s` as string interpolation and fail. The best choice is using `pyproject.toml` anyway, but if you are forced to use the legacy ini-style format, stick to `pytest.ini` to avoid errors."
"It seems that python supports many different commands to stop script execution.  
The choices I've found are: `quit()`, `exit()`, `sys.exit()`, `os._exit()`

Have I missed any?
What's the difference between them? When would you use each?","Let me give some information on them:

1. [`quit()`](http://docs.python.org/2/library/constants.html#quit) simply raises the [`SystemExit`](http://docs.python.org/2/library/exceptions.html#exceptions.SystemExit) exception.

   Furthermore, if you print it, it will give a message:

   ```
   >>> print (quit)
   Use quit() or Ctrl-Z plus Return to exit
   >>>

   ```

   This functionality was included to help people who do not know Python. After all, one of the most likely things a newbie will try to exit Python is typing in `quit`.

   Nevertheless, `quit` should **not** be used in production code. This is because it only works if the [`site`](http://docs.python.org/2/library/constants.html#constants-added-by-the-site-module) module is loaded. Instead, this function should only be used in the interpreter.
2. [`exit()`](http://docs.python.org/2/library/constants.html#exit) is an alias for `quit` (or vice-versa). They exist together simply to make Python more user-friendly.

   Furthermore, it too gives a message when printed:

   ```
   >>> print (exit)
   Use exit() or Ctrl-Z plus Return to exit
   >>>

   ```

   However, like `quit`, `exit` is considered bad to use in production code and should be reserved for use in the interpreter. This is because it too relies on the `site` module.
3. [`sys.exit()`](http://docs.python.org/2/library/sys.html#sys.exit) also raises the `SystemExit` exception. This means that it is the same as `quit` and `exit` in that respect.

   Unlike those two however, `sys.exit` is considered good to use in production code. This is because the [`sys`](http://docs.python.org/2/library/sys.html) module will always be there.
4. [`os._exit()`](http://docs.python.org/2/library/os.html#os._exit) exits the program **without calling cleanup handlers, flushing stdio buffers, etc**. Thus, it is not a standard way to exit and should only be used in special cases. The most common of these is in the child process(es) created by [`os.fork`](http://docs.python.org/2/library/os.html#os.fork).

   Note that, of the four methods given, only this one is unique in what it does.

Summed up, all four methods exit the program. However, the first two are considered bad to use in production code and the last is a non-standard, dirty way that is only used in special scenarios. So, if you want to exit a program normally, go with the third method: `sys.exit`.

---

Or, even better in my opinion, you can just do directly what `sys.exit` does behind the scenes and run:

```
raise SystemExit

```

This way, you do not need to import `sys` first.

However, this choice is simply one on style and is purely up to you."
"Optparse, the old version just ignores all unrecognised arguments and carries on. In most situations, this isn't ideal and was changed in argparse. But there are a few situations where you want to ignore any unrecognised arguments and parse the ones you've specified.

For example:

```
parser = argparse.ArgumentParser()
parser.add_argument('--foo', dest=""foo"")
parser.parse_args()

$python myscript.py --foo 1 --bar 2
error: unrecognized arguments: --bar

```

Is there anyway to override this?","Replace

```
args = parser.parse_args()

```

with

```
args, unknown = parser.parse_known_args()

```

For example,

```
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--foo')
args, unknown = parser.parse_known_args(['--foo', 'BAR', 'spam'])
print(args)
# Namespace(foo='BAR')
print(unknown)
# ['spam']

```"
"I tried the following code (`test_seaborn.py`):

```
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
matplotlib.style.use('ggplot')
import seaborn as sns
sns.set()
df = sns.load_dataset('iris')
sns_plot = sns.pairplot(df, hue='species', size=2.5)
fig = sns_plot.get_figure()
fig.savefig(""output.png"")
#sns.plt.show()

```

But I get this error:

```
  Traceback (most recent call last):
  File ""test_searborn.py"", line 11, in <module>
    fig = sns_plot.get_figure()
AttributeError: 'PairGrid' object has no attribute 'get_figure'

```

I expect the final `output.png` will exist and look like this:

[![enter image description here](https://i.sstatic.net/n6uXd.png)](https://i.sstatic.net/n6uXd.png)

How can I resolve the problem?","The following calls allow you to access the figure (Seaborn 0.8.1 compatible):

```
swarm_plot = sns.swarmplot(...)
fig = swarm_plot.get_figure()
fig.savefig(""out.png"") 

```

as seen previously in [this answer](https://stackoverflow.com/a/35670528/1079075).

The suggested solutions are incompatible with Seaborn 0.8.1. They give the following errors because the Seaborn interface has changed:

```
AttributeError: 'AxesSubplot' object has no attribute 'fig'
When trying to access the figure

AttributeError: 'AxesSubplot' object has no attribute 'savefig'
when trying to use the savefig directly as a function

```

---

**UPDATE:**
I have recently used `PairGrid` object from seaborn to generate a plot similar to the one in [this example](http://seaborn.pydata.org/examples/pairgrid_dotplot.html).
In this case, since `GridPlot` is not a plot object like, for example, `sns.swarmplot`, it has no `get_figure()` function.
It is possible to directly access the matplotlib figure by:

```
fig = myGridPlotObject.fig

```"
"I have some Python code that have inconsistent indentation. There is a lot of mixture of tabs and spaces to make the matter even worse, and even space indentation is not preserved.

The code works as expected, but it's difficult to maintain.

How can I fix the indentation (like [HTML Tidy](https://en.wikipedia.org/wiki/HTML_Tidy), but for Python) without breaking the code?

---

Some **editor-specific** advice:

* Vi/Vim:

  + [Replace tabs with spaces in vim](https://stackoverflow.com/questions/426963)
  + [Expand tabs to spaces in vim only in python files?](https://stackoverflow.com/questions/9986475)
  + [Use Vim Retab to solve TabError: inconsistent use of tabs and spaces in indentation?](https://stackoverflow.com/questions/48735671)
  + [How to maintain tabs when pasting in Vim](https://stackoverflow.com/questions/12584465)
* Notepad++:

  + [Convert tabs to spaces in Notepad++](https://stackoverflow.com/questions/455037)
  + [Auto-indent in Notepad++](https://stackoverflow.com/questions/412427)
  + [How do I configure Notepad++ to use spaces instead of tabs?](https://stackoverflow.com/questions/8197812/)
* Emacs:

  + [Indentation not working properly in emacs for python](https://stackoverflow.com/questions/27032218)
  + [Emacs bulk indent for Python](https://stackoverflow.com/questions/2585091)
* Eclipse:

  + [indent python file (with pydev) in eclipse](https://stackoverflow.com/questions/7654267)
* Visual Studio Code:

  + [Visual Studio Code indentation for Python](https://stackoverflow.com/questions/37143985)","Use the `reindent.py` script that you find in the `Tools/scripts/` directory of your Python installation:

> Change Python (.py) files to use
> 4-space indents and no hard tab
> characters. Also trim excess spaces
> and tabs from ends of lines, and
> remove empty lines at the end of
> files. Also ensure the last line ends
> with a newline.

Have a look at that script for detailed usage instructions.

---

NOTE: If your linux distro does not have reindent installed by default with Python:

Many linux distros do not have `reindent` installed by default with `python` --> one easy way to get `reindent` is to do `pip install reindent`.

p.s. An alternative to `pip` is to use your distros package manager (i.e. `apt-get`, `yum`, `dnf`) but then you need to figure out what package has the command line tool because each distro has the tool in a different package."
"Why python 2.7 doesn't include Z character (Zulu or zero offset) at the end of UTC datetime object's isoformat string unlike JavaScript?

```
>>> datetime.datetime.utcnow().isoformat()
'2013-10-29T09:14:03.895210'

```

Whereas in javascript

```
>>> console.log(new Date().toISOString()); 
2013-10-29T09:38:41.341Z

```","### Option: `isoformat()`

Python's `datetime` does not support the [military timezone](https://en.wikipedia.org/wiki/List_of_military_time_zones) suffixes like 'Z' suffix for UTC. The following simple string replacement does the trick:

```
In [1]: import datetime

In [2]: d = datetime.datetime(2014, 12, 10, 12, 0, 0)

In [3]: str(d).replace('+00:00', 'Z')
Out[3]: '2014-12-10 12:00:00Z'

```

`str(d)` is essentially the same as `d.isoformat(sep=' ')`

See: [Datetime, Python Standard Library](https://docs.python.org/2/library/datetime.html#datetime.date.__str__)

### Option: `strftime()`

Or you could use `strftime` to achieve the same effect:

```
In [4]: d.strftime('%Y-%m-%dT%H:%M:%SZ')
Out[4]: '2014-12-10T12:00:00Z'

```

Note: This option works only when you know the date specified is in UTC.

See: [datetime.strftime()](https://docs.python.org/3/library/datetime.html#datetime.datetime.strftime)

---

### Additional: Human Readable Timezone

Going further, you may be interested in displaying human readable timezone information, `pytz` with `strftime` `%Z` timezone flag:

```
In [5]: import pytz

In [6]: d = datetime.datetime(2014, 12, 10, 12, 0, 0, tzinfo=pytz.utc)

In [7]: d
Out[7]: datetime.datetime(2014, 12, 10, 12, 0, tzinfo=<UTC>)

In [8]: d.strftime('%Y-%m-%d %H:%M:%S %Z')
Out[8]: '2014-12-10 12:00:00 UTC'

```"
"I'm familiar with the following questions:

[Matplotlib savefig with a legend outside the plot](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)

[How to put the legend out of the plot](https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot)

It seems that the answers in these questions have the luxury of being able to fiddle with the exact shrinking of the axis so that the legend fits.

Shrinking the axes, however, is not an ideal solution because it makes the data smaller making it actually more difficult to interpret; particularly when its complex and there are lots of things going on ... hence needing a large legend

The example of a complex legend in the documentation demonstrates the need for this because the legend in their plot actually completely obscures multiple data points.

<http://matplotlib.sourceforge.net/users/legend_guide.html#legend-of-complex-plots>

**What I would like to be able to do is dynamically expand the size of the figure box to accommodate the expanding figure legend.**

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(-2*np.pi, 2*np.pi, 0.1)
fig = plt.figure(1)
ax = fig.add_subplot(111)
ax.plot(x, np.sin(x), label='Sine')
ax.plot(x, np.cos(x), label='Cosine')
ax.plot(x, np.arctan(x), label='Inverse tan')
lgd = ax.legend(loc=9, bbox_to_anchor=(0.5,0))
ax.grid('on')

```

Notice how the final label 'Inverse tan' is actually outside the figure box (and looks badly cutoff - not publication quality!)
![enter image description here](https://i.sstatic.net/0XtO2.png)

Finally, I've been told that this is normal behaviour in R and LaTeX, so I'm a little confused why this is so difficult in python... Is there a historical reason? Is Matlab equally poor on this matter?

I have the (only slightly) longer version of this code on pastebin <http://pastebin.com/grVjc007>","*[EDIT - 25th Feb 2025]
My day job is no longer Python, so I'm not following the recent matplotlib developments. Please read all the newer answers here as there look to be some excellent modern suggestions compared to this solution from the ancient history of 2012.*

Sorry EMS, but I actually just got another response from the matplotlib mailling list (Thanks goes out to Benjamin Root).

The code I am looking for is adjusting the savefig call to:

```
fig.savefig('samplefigure', bbox_extra_artists=(lgd,), bbox_inches='tight')
#Note that the bbox_extra_artists must be an iterable

```

This is apparently similar to calling tight\_layout, but instead you allow savefig to consider extra artists in the calculation. This did in fact resize the figure box as desired.

```
import matplotlib.pyplot as plt
import numpy as np

plt.gcf().clear()
x = np.arange(-2*np.pi, 2*np.pi, 0.1)
fig = plt.figure(1)
ax = fig.add_subplot(111)
ax.plot(x, np.sin(x), label='Sine')
ax.plot(x, np.cos(x), label='Cosine')
ax.plot(x, np.arctan(x), label='Inverse tan')
handles, labels = ax.get_legend_handles_labels()
lgd = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.1))
text = ax.text(-0.2,1.05, ""Aribitrary text"", transform=ax.transAxes)
ax.set_title(""Trigonometry"")
ax.grid('on')
fig.savefig('samplefigure', bbox_extra_artists=(lgd,text), bbox_inches='tight')

```

This produces:

![](https://i.sstatic.net/Zs4IQ.png)

[edit] The intent of this question was to completely avoid the use of arbitrary coordinate placements of arbitrary text as was the traditional solution to these problems. Despite this, numerous edits recently have insisted on putting these in, often in ways that led to the code raising an error. I have now fixed the issues and tidied the arbitrary text to show how these are also considered within the bbox\_extra\_artists algorithm.

[edit]
Some of the comments below note that since 2019, the command has been simplified.
*plt.savefig('x.png', bbox\_inches='tight') was sufficient. Thanks for sharing. â€“ mateuszb Jun 27, 2019*"
"Is it guaranteed that `False == 0` and `True == 1`, in Python (assuming that they are not reassigned by the user)? For instance, is it in any way guaranteed that the following code will always produce the same results, whatever the version of Python (both existing and, likely, future ones)?

```
0 == False  # True
1 == True   # True
['zero', 'one'][False]  # is 'zero'

```

Any reference to the official documentation would be much appreciated!

As noted in many answers, `bool` inherits from `int`. The question can therefore be recast as: ""Does the documentation officially say that programmers can rely on booleans inheriting from integers, *with the values `0` and `1`*?"". This question is relevant for writing robust code that won't fail because of implementation details!","In Python 2.x this is *not* guaranteed as it is possible for `True` and `False` to be reassigned. However, even if this happens, boolean True and boolean False are still properly returned for comparisons.

In Python 3.x `True` and `False` are keywords and will always be equal to `1` and `0`.

Under normal circumstances in Python 2, and always in Python 3:

`False` object is of type `bool` which is a subclass of `int`:

```
    object
       |
     int
       |
     bool

```

It is the only reason why in your example, `['zero', 'one'][False]` does work. It would not work with an object which is not a subclass of integer, because list indexing only works with integers, or objects that define a [`__index__`](http://docs.python.org/reference/datamodel.html#object.__index__) method (thanks [mark-dickinson](https://stackoverflow.com/users/270986/mark-dickinson)).

Edit:

It is true of the current python version, and of that of Python 3. The [docs for python 2](https://docs.python.org/2/reference/datamodel.html#index-10) and the [docs for Python 3](https://docs.python.org/3/reference/datamodel.html#index-10) both say:

> There are two types of integers: [...] Integers (int) [...] Booleans (bool)

and in the boolean subsection:

> Booleans: These represent the truth values False and True [...] Boolean values behave like the values 0 and 1, respectively, in almost all contexts, the exception being that when converted to a string, the strings ""False"" or ""True"" are returned, respectively.

There is also, [for Python 2](https://docs.python.org/2/library/stdtypes.html#boolean-values):

> In numeric contexts (for example when used as the argument to an arithmetic operator), they [False and True] behave like the integers 0 and 1, respectively.

So booleans are explicitly considered as integers in Python 2 and 3.

So you're safe until Python 4 comes along. ;-)"
"Suppose I have a list of items, like:

```
['apple', 'red', 'apple', 'red', 'red', 'pear']

```

I want a dictionary that counts how many times each item appears in the list. So for the list above the result should be:

```
{'apple': 2, 'red': 3, 'pear': 1}

```

How can I do this simply in Python?

---

If you are only interested in counting instances of a **single** element in a list, see [How do I count the occurrences of a list item?](https://stackoverflow.com/questions/2600191).","In 2.7 and 3.1, there is the special [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) (`dict` subclass) for this purpose.

```
>>> from collections import Counter
>>> Counter(['apple','red','apple','red','red','pear'])
Counter({'red': 3, 'apple': 2, 'pear': 1})

```"
"Assume we have a data frame in Python Pandas that looks like this:

```
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})

```

Or, in table form:

```
ids    vals
aball   1
bball   2
cnut    3
fball   4

```

How do I filter rows which contain the key word ""ball?"" For example, the output should be:

```
ids    vals
aball   1
bball   2
fball   4

```","```
In [3]: df[df['ids'].str.contains(""ball"")]
Out[3]:
     ids  vals
0  aball     1
1  bball     2
3  fball     4

```"
"I want to output my key value pairs from a python dictionary as such:

```
key1 \t value1
key2 \t value2

```

I thought I could maybe do it like this:

```
for i in d:
    print d.keys(i), d.values(i)

```

but obviously that's not how it goes as the `keys()` and `values()` don't take an argument.","Python 2 and Python 3
---------------------

`i` *is* the key, so you would just need to use it:

```
for i in d:
    print i, d[i]

```

Python 3
--------

`d.items()` returns the iterator; to get a list, you need to pass the iterator to `list()` yourself.

```
for k, v in d.items():
    print(k, v)

```

Python 2
--------

You can get an iterator that contains both keys and values. `d.items()` returns a list of (key, value) tuples, while `d.iteritems()` returns an iterator that provides the same:

```
for k, v in d.iteritems():
    print k, v

```"
"In Java/C# you can easily step through code to trace what might be going wrong, and IDE's make this process very user friendly.

Can you trace through python code in a similar fashion?","Yes! There's a Python debugger called `pdb` just for doing that!

You can launch a Python program through `pdb` via `python -m pdb myscript.py`.

There are a few commands you can then issue, which are documented on the [`pdb`](http://docs.python.org/library/pdb.html) page.

Some useful ones to remember are:

* `b`: set a breakpoint
* `c`: continue debugging until you hit a breakpoint
* `s`: step through the code
* `n`: to go to next line of code
* `l`: list source code for the current file (default: 11 lines including the line being executed)
* `u`: navigate up a stack frame
* `d`: navigate down a stack frame
* `p`: to print the value of an expression in the current context

If you don't want to use a command line debugger, some IDEs like [Pydev](http://pydev.org/), [Wing IDE](https://wingware.com/) or [PyCharm](https://www.jetbrains.com/pycharm/download/) have a GUI debugger. Wing and PyCharm are commercial products, but Wing has a free ""Personal"" edition, and PyCharm has a free community edition."
"I have a pandas dataframe, `df`:

```
   c1   c2
0  10  100
1  11  110
2  12  120

```

How do I iterate over the rows of this dataframe? For every row, I want to access its elements (values in cells) by the name of the columns. For example:

```
for row in df.rows:
    print(row['c1'], row['c2'])

```

---

I found a [similar question](https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas), which suggests using either of these:

* ```
  for date, row in df.T.iteritems():

  ```
* ```
  for row in df.iterrows():

  ```

But I do not understand what the `row` object is and how I can work with it.","[`DataFrame.iterrows`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html#pandas-dataframe-iterrows) is a generator which yields both the index and row (as a Series):

```
import pandas as pd

df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})
df = df.reset_index()  # make sure indexes pair with number of rows

for index, row in df.iterrows():
    print(row['c1'], row['c2'])

```

```
10 100
11 110
12 120

```

---

Obligatory disclaimer from the [documentation](https://pandas.pydata.org/docs/user_guide/basics.html#iteration)

> Iterating through pandas objects is generally **slow**. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:
>
> * Look for a *vectorized* solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, â€¦
> * When you have a function that cannot work on the full DataFrame/Series at once, it is better to use [`apply()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html#pandas.DataFrame.apply ""pandas.DataFrame.apply"") instead of iterating over the values. See the docs on [function application](https://pandas.pydata.org/docs/user_guide/basics.html#basics-apply).
> * If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the [enhancing performance](https://pandas.pydata.org/docs/user_guide/enhancingperf.html#enhancingperf) section for some examples of this approach.

Other answers in this thread delve into greater depth on alternatives to iter\* functions if you are interested to learn more."
"How do I add an item to an existing dictionary in Python? For example, given:

```
default_data = {
    'item1': 1,
    'item2': 2,
}

```

I want to add a new item such that:

```
default_data = default_data + {'item3': 3}

```","```
default_data['item3'] = 3

```

Easy as py.

Another possible solution:

```
default_data.update({'item3': 3})

```

which is nice if you want to insert multiple items at once."
"```
import copy

a = ""deepak""
b = 1, 2, 3, 4
c = [1, 2, 3, 4]
d = {1: 10, 2: 20, 3: 30}

a1 = copy.copy(a)
b1 = copy.copy(b)
c1 = copy.copy(c)
d1 = copy.copy(d)


print(""immutable - id(a)==id(a1)"", id(a) == id(a1))
print(""immutable - id(b)==id(b1)"", id(b) == id(b1))
print(""mutable - id(c)==id(c1)"", id(c) == id(c1))
print(""mutable - id(d)==id(d1)"", id(d) == id(d1))

```

I get the following results:

```
immutable - id(a)==id(a1) True
immutable - id(b)==id(b1) True
mutable - id(c)==id(c1) False
mutable - id(d)==id(d1) False

```

If I perform deepcopy:

```
a1 = copy.deepcopy(a)
b1 = copy.deepcopy(b)
c1 = copy.deepcopy(c)
d1 = copy.deepcopy(d)

```

results are the same:

```
immutable - id(a)==id(a1) True
immutable - id(b)==id(b1) True
mutable - id(c)==id(c1) False
mutable - id(d)==id(d1) False

```

If I work on assignment operations:

```
a1 = a
b1 = b
c1 = c
d1 = d

```

then results are:

```
immutable - id(a)==id(a1) True
immutable - id(b)==id(b1) True
mutable - id(c)==id(c1) True
mutable - id(d)==id(d1) True

```

Can somebody explain what exactly makes a difference between the copies? Is it something related to mutable & immutable objects? If so, can you please explain it to me?","Normal assignment operations will simply point the new variable towards the existing object. The [docs](http://docs.python.org/2/library/copy.html) explain the difference between shallow and deep copies:

> The difference between shallow and deep copying is only relevant for
> compound objects (objects that contain other objects, like lists or
> class instances):
>
> * A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.
> * A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the
>   original.

Here's a little demonstration:

```
import copy

a = [1, 2, 3]
b = [4, 5, 6]
c = [a, b]

```

Using normal assignment operatings to copy:

```
d = c

print id(c) == id(d)          # True - d is the same object as c
print id(c[0]) == id(d[0])    # True - d[0] is the same object as c[0]

```

Using a shallow copy:

```
d = copy.copy(c)

print id(c) == id(d)          # False - d is now a new object
print id(c[0]) == id(d[0])    # True - d[0] is the same object as c[0]

```

Using a deep copy:

```
d = copy.deepcopy(c)

print id(c) == id(d)          # False - d is now a new object
print id(c[0]) == id(d[0])    # False - d[0] is now a new object

```"
"How can I count the number of times a given substring is present within a string in Python?

For example:

```
>>> 'foo bar foo'.numberOfOccurrences('foo')
2

```

---

To get *indices of* the substrings, see [How to find all occurrences of a substring?](https://stackoverflow.com/questions/4664850).","`string.count(substring)`, like in:

```
>>> ""abcdabcva"".count(""ab"")
2

```

This is for **non overlapping** occurrences.  
If you need to count overlapping occurrences, you'd better check the answers [here](https://stackoverflow.com/questions/5616822/python-regex-find-all-overlapping-matches), or just check [my other answer below](https://stackoverflow.com/a/31078260)."
"I'm using **Python 2** to parse JSON from **ASCII encoded** text files.

When loading these files with either [`json`](https://docs.python.org/2/library/json.html) or [`simplejson`](https://pypi.python.org/pypi/simplejson/), all my string values are cast to Unicode objects instead of string objects. The problem is, I have to use the data with some libraries that only accept string objects. I *can't change the libraries* nor update them.

Is it possible to get string objects instead of Unicode ones?

### Example

```
>>> import json
>>> original_list = ['a', 'b']
>>> json_list = json.dumps(original_list)
>>> json_list
'[""a"", ""b""]'
>>> new_list = json.loads(json_list)
>>> new_list
[u'a', u'b']  # I want these to be of type `str`, not `unicode`
```

(One easy and clean solution for 2017 is to use a recent version of Python â€” i.e. **Python 3** and forward.)","While there are some good answers here, I ended up using [PyYAML](http://pyyaml.org/) to parse my JSON files, since it gives the keys and values as `str` type strings instead of the `unicode` type. Because JSON is a subset of [YAML](https://en.wikipedia.org/wiki/YAML), it works nicely:

```
>>> import json
>>> import yaml
>>> list_org = ['a', 'b']
>>> list_dump = json.dumps(list_org)
>>> list_dump
'[""a"", ""b""]'
>>> json.loads(list_dump)
[u'a', u'b']
>>> yaml.safe_load(list_dump)
['a', 'b']

```

### Notes

Some things to note though:

* I get *string objects* because all my entries are **ASCII encoded**. If I would use Unicode encoded entries, I would get them back as *unicode objects* â€” there is no conversion!
* You should (probably always) use PyYAML's `safe_load` function; if you use it to load JSON files, you don't need the ""additional power"" of the `load` function anyway.
* If you want a YAML parser that has more support for the 1.2 version of the spec (and [correctly parses very low numbers](https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number)) try [Ruamel YAML](https://bitbucket.org/ruamel/yaml): `pip install ruamel.yaml` and `import ruamel.yaml as yaml` was all I needed in my tests.

### Conversion

As stated, there isn't any conversion! If you can't be sure to only deal with ASCII values (and you can't be sure most of the time), better use a **conversion function**:

I used the one [from Mark Amery](https://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-ones-from-json-in-python/13105359#13105359) a couple of times now, it works great and is very easy to use. You can also use a similar function as an `object_hook` instead, as it might gain you a performance boost on big files. See the slightly more involved [answer from Mirec Miskuf](https://stackoverflow.com/a/33571117/11666) for that."
Is there a `goto` or any equivalent in Python to be able to jump to a specific line of code?,"No, Python does not support labels and goto. It's a (highly) structured programming language."
"I want to annotate a type of a variable in a `for`-loop. I tried this but it didn't work:

```
for i: int in range(5):
    pass

```

What I expect is working autocomplete in PyCharm 2016.3.2, but using
pre-annotation didn't work:

```
i: int
for i in range(5):
    pass

```

P.S. Pre-annotation works for PyCharm >= 2017.1.","According to [PEP 526](https://www.python.org/dev/peps/pep-0526/#id11), this is not allowed:

> In addition, one **cannot annotate variables used in a `for` or `with`
> statement**; they can be annotated ahead of time, in a similar manner to
> tuple unpacking

Annotate it before the loop:

```
i: int
for i in range(5):
    pass

```

---

*PyCharm 2018.1 and up* now recognizes the type of the variable inside the loop. This was not supported in older PyCharm versions."
"I'm using Visual Studio Code with the inbuilt Debugger in order to debug a Python script.

Following [this guide](https://code.visualstudio.com/docs/python/debugging), I set up the argument in the `launch.json` file:

[![Enter image description here](https://i.sstatic.net/iCrKC.png)](https://i.sstatic.net/iCrKC.png)

But when I press on *Debug*, it says that my argument is not recognized my shell, or rather the `argparser`, says:

> error: unrecognized arguments: --city Auckland

[![Enter image description here](https://i.sstatic.net/YS879.png)](https://i.sstatic.net/YS879.png)

As Visual Studio Code is using PowerShell, let's execute the same file with the same argument:

[![Enter image description here](https://i.sstatic.net/8HJJ5.png)](https://i.sstatic.net/8HJJ5.png)

So: the same file, same path, and same argument. In the terminal it is working, but not in Visual Studio Code's Debugger.

Where am I wrong?","I think the --City and Auckland are used as a single argument. Maybe try separating them like so...

Single argument
---------------

```
""args"": [""--city"",""Auckland""]

```

Multiple arguments and multiple values
--------------------------------------

Such as:

```
--key1 value1 value2 --key2 value3 value4

```

Just put them into the `args` list **one by one in sequence**:

```
""args"": [""--key1"", ""value1"", ""value2"", ""--key2"", ""value3"", ""value4""]

```"
"I am trying to take one string, and append it to every string contained in a list, and then have a new list with the completed strings. Example:

```
list1 = ['foo', 'fob', 'faz', 'funk']
string = 'bar'

*magic*

list2 = ['foobar', 'fobbar', 'fazbar', 'funkbar']

```

I tried for loops, and an attempt at list comprehension, but it was garbage. As always, any help, much appreciated.","The simplest way to do this is with a list comprehension:

```
[s + mystring for s in mylist]

```

Notice that I avoided using builtin names like `list` because that shadows or hides the builtin names, which is very much not good.

Also, if you do not actually need a list, but just need an iterator, a generator expression can be more efficient (although it does not likely matter on short lists):

```
(s + mystring for s in mylist)

```

These are very powerful, flexible, and concise. Every good python programmer should learn to wield them."
"`list.sort()` sorts the list and replaces the original list, whereas `sorted(list)` returns a sorted copy of the list, without changing the original list.

* When is one preferred over the other?
* Which is more efficient? By how much?
* Can a list be reverted to the unsorted state after `list.sort()` has been performed?

---

Please use [Why do these list operations (methods) return None, rather than the resulting list?](https://stackoverflow.com/questions/11205254) to close questions where OP has inadvertently assigned the result of `.sort()`, rather than using `sorted` or a separate statement. Proper debugging would reveal that `.sort()` had returned `None`, at which point ""why?"" is the remaining question.","`sorted()` returns a **new** sorted list, leaving the original list unaffected. `list.sort()` sorts the list **in-place**, mutating the list indices, and returns `None` (like all in-place operations).

`sorted()` works on any iterable, not just lists. Strings, tuples, dictionaries (you'll get the keys), generators, etc., returning a list containing all elements, sorted.

* Use `list.sort()` when you want to mutate the list, `sorted()` when you want a new sorted object back. Use `sorted()` when you want to sort something that is an iterable, not a list *yet*.
* For lists, `list.sort()` is faster than `sorted()` because it doesn't have to create a copy. For any other iterable, you have no choice.
* No, you cannot retrieve the original positions. Once you called `list.sort()` the original order is gone."
"How do you set/get the values of attributes of `t` given by `x`?

```
class Test:
   def __init__(self):
       self.attr1 = 1
       self.attr2 = 2

t = Test()
x = ""attr1""

```

---

Note that the same technique also covers the issue of [Call method from string](https://stackoverflow.com/questions/1855558). Fundamentally, that is two problems: accessing the method (which is just an instance of the same problem here), and calling what was accessed (which is trivial, and works the same way as if it had been accessed normally).

In fact, [Calling a function of a module by using its name (a string)](https://stackoverflow.com/questions/3061) is really the same problem as well - but it may not be obvious that a module is an ""object"" with ""attributes"" that work the same way.","There are built-in functions called [`getattr`](https://docs.python.org/3/library/functions.html#getattr) and [`setattr`](https://docs.python.org/3/library/functions.html#setattr)

```
getattr(object, attrname)
setattr(object, attrname, value)

```

In this case

```
x = getattr(t, 'attr1')
setattr(t, 'attr1', 21)

```"
"How can I replace `foobar` with `foo123bar`?

This doesn't work:

```
>>> re.sub(r'(foo)', r'\1123', 'foobar')
'J3bar'

```

This works:

```
>>> re.sub(r'(foo)', r'\1hi', 'foobar')
'foohibar'

```","The answer is:

```
re.sub(r'(foo)', r'\g<1>123', 'foobar')

```

Relevant excerpt from the [docs](https://docs.python.org/library/re.html#re.sub):

> In addition to character escapes and backreferences as described above, `\g<name>` will use the substring matched by the group named `name`, as defined by the `(?P<name>...)` syntax. `\g<number>` uses the corresponding group number; `\g<2>` is therefore equivalent to `\2`, but isnâ€™t ambiguous in a replacement such as `\g<2>0`. `\20` would be interpreted as a reference to group 20, not a reference to group 2 followed by the literal character `'0'`. The backreference `\g<0>` substitutes in the entire substring matched by the RE."
"Consider the following Python code:

```
import os
print os.getcwd()

```

I use `os.getcwd()` to [get the script file's directory location](http://www.faqs.org/docs/diveintopython/regression_path.html). When I run the script from the command line it gives me the correct path whereas when I run it from a script run by code in a Django view it prints `/`.

How can I get the path to the script from within a script run by a Django view?

**UPDATE:**  
Summing up the answers thus far - `os.getcwd()` and `os.path.abspath()` both give the current working directory which may or may not be the directory where the script resides. In my web host setup `__file__` gives only the filename without the path.

Isn't there any way in Python to (always) be able to receive the path in which the script resides?","You need to call `os.path.realpath` on `__file__`, so that when `__file__` is a filename without the path you still get the dir path:

```
import os
print(os.path.dirname(os.path.realpath(__file__)))

```"
"I'm trying to install pycurl via:

```
sudo pip install pycurl

```

It downloaded fine, but when when it runs setup.py I get the following traceback:

```
Downloading/unpacking pycurl
  Running setup.py egg_info for package pycurl
    Traceback (most recent call last):
      File ""<string>"", line 16, in <module>
      File ""/tmp/pip-build-root/pycurl/setup.py"", line 563, in <module>
        ext = get_extension()
      File ""/tmp/pip-build-root/pycurl/setup.py"", line 368, in get_extension
        ext_config = ExtensionConfiguration()
      File ""/tmp/pip-build-root/pycurl/setup.py"", line 65, in __init__
        self.configure()
      File ""/tmp/pip-build-root/pycurl/setup.py"", line 100, in configure_unix
        raise ConfigurationError(msg)
    __main__.ConfigurationError: Could not run curl-config: [Errno 2] No such file or directory
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""<string>"", line 16, in <module>

  File ""/tmp/pip-build-root/pycurl/setup.py"", line 563, in <module>

    ext = get_extension()

  File ""/tmp/pip-build-root/pycurl/setup.py"", line 368, in get_extension

    ext_config = ExtensionConfiguration()

  File ""/tmp/pip-build-root/pycurl/setup.py"", line 65, in __init__

    self.configure()

  File ""/tmp/pip-build-root/pycurl/setup.py"", line 100, in configure_unix

    raise ConfigurationError(msg)

__main__.ConfigurationError: Could not run curl-config: [Errno 2] No such file or directory

```

Any idea why this is happening and how to get around it","On Debian I needed the following packages to fix this

```
sudo apt install libcurl4-openssl-dev libssl-dev

```"
"I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns.

When I do:

```
import pandas as pd
df = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})
print df
df.fillna(value=0, inplace=True)
print df

```

The output:

```
     a    b    c
0  1.0  4.0  NaN
1  2.0  5.0  NaN
2  3.0  NaN  7.0
3  NaN  6.0  8.0
     a    b    c
0  1.0  4.0  0.0
1  2.0  5.0  0.0
2  3.0  0.0  7.0
3  0.0  6.0  8.0

```

It replaces every `None` with `0`'s. What I want to do is, only replace `None`s in columns `a` and `b`, but not `c`.

What is the best way of doing this?","You can select your desired columns and do it by assignment:

```
df[['a', 'b']] = df[['a','b']].fillna(value=0)

```

The resulting output is as expected:

```
     a    b    c
0  1.0  4.0  NaN
1  2.0  5.0  NaN
2  3.0  0.0  7.0
3  0.0  6.0  8.0

```"
"I need to delete the first three rows of a dataframe in pandas.

I know `df.ix[:-1]` would remove the last row, but I can't figure out how to remove first n rows.","Use [`iloc`](https://pandas.pydata.org/pandas-docs/stable/indexing.html):

```
df = df.iloc[3:]

```

will give you a new df without the first three rows."
"I'm looking for a way to update dict `dictionary1` with the contents of dict `update` wihout overwriting `levelA`

```
dictionary1 = {
    ""level1"": {
        ""level2"": {""levelA"": 0, ""levelB"": 1}
    }
}
update = {
    ""level1"": {
        ""level2"": {""levelB"": 10}
    }
}
dictionary1.update(update)
print(dictionary1)

```

```
{
    ""level1"": {
        ""level2"": {""levelB"": 10}
    }
}

```

I know that update deletes the values in level2 because it's updating the lowest key level1.

How could I tackle this, given that dictionary1 and update can have any length?","@FM's answer has the right general idea, i.e. a recursive solution, but somewhat peculiar coding and at least one bug. I'd recommend, instead:

Python 2:

```
import collections

def update(d, u):
    for k, v in u.iteritems():
        if isinstance(v, collections.Mapping):
            d[k] = update(d.get(k, {}), v)
        else:
            d[k] = v
    return d

```

Python 3:

```
import collections.abc

def update(d, u):
    for k, v in u.items():
        if isinstance(v, collections.abc.Mapping):
            d[k] = update(d.get(k, {}), v)
        else:
            d[k] = v
    return d

```

The bug shows up when the ""update"" has a `k`, `v` item where `v` is a `dict` and `k` is not originally a key in the dictionary being updated -- @FM's code ""skips"" this part of the update (because it performs it on an empty new `dict` which isn't saved or returned anywhere, just lost when the recursive call returns).

My other changes are minor: there is no reason for the `if`/`else` construct when `.get` does the same job faster and cleaner, and `isinstance` is best applied to abstract base classes (not concrete ones) for generality."
"I have a DataFrame with many missing values in columns which I wish to groupby:

```
import pandas as pd
import numpy as np
df = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})

In [4]: df.groupby('b').groups
Out[4]: {'4': [0], '6': [2]}

```

By default `pandas groupby` dropped rows with NaN in the grouped column.

How can I include NaNs values as a group ?","pandas >= 1.1
=============

From pandas 1.1 you have better control over this behavior, [NA values are now allowed in the grouper](https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.1.0.html#allow-na-in-groupby-key) using **`dropna=False`**:

```
pd.__version__
# '1.1.0.dev0+2004.g8d10bfb6f'

# Example from the docs
df

   a    b  c
0  1  2.0  3
1  1  NaN  4
2  2  1.0  3
3  1  2.0  2

# without NA (the default)
df.groupby('b').sum()

     a  c
b        
1.0  2  3
2.0  2  5

```

```
# with NA
df.groupby('b', dropna=False).sum()

     a  c
b        
1.0  2  3
2.0  2  5
NaN  1  4

```"
"How do I pass a class field to a decorator on a class method as an argument? What I want to do is something like:

```
class Client(object):
    def __init__(self, url):
        self.url = url

    @check_authorization(""some_attr"", self.url)
    def get(self):
        do_work()

```

It complains that self does not exist for passing `self.url` to the decorator. Is there a way around this?","Yes. Instead of passing in the instance attribute at class definition time, check it at runtime:

```
def check_authorization(f):
    def wrapper(*args):
        print args[0].url
        return f(*args)
    return wrapper

class Client(object):
    def __init__(self, url):
        self.url = url

    @check_authorization
    def get(self):
        print 'get'

>>> Client('http://www.google.com').get()
http://www.google.com
get

```

The decorator intercepts the method arguments; the first argument is the instance, so it reads the attribute off of that. You can pass in the attribute name as a string to the decorator and use `getattr` if you don't want to hardcode the attribute name:

```
def check_authorization(attribute):
    def _check_authorization(f):
        def wrapper(self, *args):
            print getattr(self, attribute)
            return f(self, *args)
        return wrapper
    return _check_authorization

```"
"Why would you compile a Python script? You can run them directly from the .py file and it works fine, so is there a performance advantage or something?

I also notice that some files in my application get compiled into .pyc while others do not, why is this?","It's compiled to bytecode which can be used much, much, much faster.

The reason some files aren't compiled is that the main script, which you invoke with `python main.py` is recompiled every time you run the script. All imported scripts will be compiled and stored on the disk.

*Important addition by [Ben Blank](https://stackoverflow.com/users/46387/ben-blank):*

> It's worth noting that while running a
> compiled script has a faster *startup*
> time (as it doesn't need to be
> compiled), it doesn't *run* any
> faster."
"In python 3.x, it is common to use return type annotation of a function, such as:

```
def foo() -> str:
    return ""bar""

```

What is the correct annotation for the ""void"" type?

I'm considering 3 options:

1. `def foo() -> None:`
   * not logical IMO, because `None` is not a type,
2. `def foo() -> type(None):`
   * using the best syntax I know for obtaining `NoneType`,
3. `def foo():`
   * omit explicit return type information.

Option 2. seems the most logical to me, but I've already seen some instances of 1.","Use **option 1** for simplicity & adherence to spec.

```
def foo() -> None

```

---

Option 1 & 2 are the 'same' as per [PEP 484 -- Type Hints](https://www.python.org/dev/peps/pep-0484/#using-none), ...

> When used in a type hint, the expression `None` is considered equivalent to `type(None)`.

but the type-hinting specification does not use `type(...)`.

This is why most of the examples use `None` as return type."
"I have some code in a .ipynb file and got it to the point where I don't really need the ""interactive"" feature of IPython Notebook. I would like to just run it straight from a Mac Terminal Command Line.

Basically, if this were just a .py file, I believe I could just do `python filename.py` from the command line. Is there something similar for a .ipynb file?","nbconvert allows you to run notebooks with the `--execute` flag:

```
jupyter nbconvert --execute <notebook>

```

If you want to run a notebook and produce a new notebook, you can add `--to notebook`:

```
jupyter nbconvert --execute --to notebook <notebook>

```

Or if you want to *replace* the existing notebook with the new output:

```
jupyter nbconvert --execute --to notebook --inplace <notebook>

```

Since that's a really long command, you can use an alias:

```
alias nbx=""jupyter nbconvert --execute --to notebook""
nbx [--inplace] <notebook>

```"
"I have a multi-threading Python program, and a utility function, `writeLog(message)`, that writes out a timestamp followed by the message. Unfortunately, the resultant log file gives no indication of which thread is generating which message.

I would like `writeLog()` to be able to add something to the message to identify which thread is calling it. Obviously I could just make the threads pass this information in, but that would be a lot more work. Is there some thread equivalent of `os.getpid()` that I could use?","[`threading.get_ident()`](https://docs.python.org/3/library/threading.html#threading.get_ident) works, or [`threading.current_thread().ident`](https://docs.python.org/3/library/threading.html#threading.current_thread) (or `threading.currentThread().ident` for Python < 2.6)."
"I have a dataframe:

```
   City     Name
0   Seattle    Alice
1   Seattle      Bob
2  Portland  Mallory
3   Seattle  Mallory
4   Seattle      Bob
5  Portland  Mallory

```

I perform the following grouping:

```
g1 = df1.groupby([""Name"", ""City""]).count()

```

which when printed looks like:

```
                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
        Seattle      1     1

```

But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:

```
                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
Mallory Seattle      1     1

```

How do I do it?","`g1` here *is* a DataFrame. It has a hierarchical index, though:

```
In [19]: type(g1)
Out[19]: pandas.core.frame.DataFrame

In [20]: g1.index
Out[20]: 
MultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),
       ('Mallory', 'Seattle')], dtype=object)

```

Perhaps you want something like this?

```
In [21]: g1.add_suffix('_Count').reset_index()
Out[21]: 
      Name      City  City_Count  Name_Count
0    Alice   Seattle           1           1
1      Bob   Seattle           2           2
2  Mallory  Portland           2           2
3  Mallory   Seattle           1           1

```

Or something like:

```
In [36]: DataFrame({'count' : df1.groupby( [ ""Name"", ""City""] ).size()}).reset_index()
Out[36]: 
      Name      City  count
0    Alice   Seattle      1
1      Bob   Seattle      2
2  Mallory  Portland      2
3  Mallory   Seattle      1

```"
"I am installing packages from `requirements.txt`

```
pip install -r requirements.txt

```

The `requirements.txt` file reads:

```
Pillow
lxml
cssselect
jieba
beautifulsoup
nltk

```

`lxml` is the only package failing to install and this leads to everything failing (expected results as pointed out by larsks in the comments). However, after `lxml` fails `pip` still runs through and downloads the rest of the packages.

From what I understand the `pip install -r requirements.txt` command will fail if any of the packages listed in the `requirements.txt` fail to install.

Is there any argument I can pass when running `pip install -r requirements.txt` to tell it to install what it can and skip the packages that it cannot, or to exit as soon as it sees something fail?","Running each line with `pip install` may be a workaround.

```
cat requirements.txt | xargs -n 1 pip install

```

Note: `-a` parameter is not available under MacOS, so old cat is more portable."
Can I run the python interpreter without generating the compiled .pyc files?,"From [""What’s New in Python 2.6 - Interpreter Changes""](http://docs.python.org/dev/whatsnew/2.6.html#interpreter-changes):

> Python can now be prevented from
> writing .pyc or .pyo files by
> supplying the [-B](http://docs.python.org/using/cmdline.html#cmdoption-B) switch to the Python
> interpreter, or by setting the
> [PYTHONDONTWRITEBYTECODE](http://docs.python.org/using/cmdline.html#envvar-PYTHONDONTWRITEBYTECODE) environment
> variable before running the
> interpreter. This setting is available
> to Python programs as the
> [`sys.dont_write_bytecode`](http://docs.python.org/library/sys.html#sys.dont_write_bytecode) variable, and
> Python code can change the value to
> modify the interpreter’s behaviour.

So run your program as `python -B prog.py`.

Update 2010-11-27: Python 3.2 addresses the issue of cluttering source folders with `.pyc` files by introducing a special `__pycache__` subfolder, see [What's New in Python 3.2 - PYC Repository Directories](http://docs.python.org/dev/whatsnew/3.2.html#pep-3147-pyc-repository-directories).

#### NOTE: The default behavior is to generate the bytecode and is done for ""performance"" reasons (for more information see here for [python2](https://www.python.org/dev/peps/pep-0304/) and see here for [python3](https://www.python.org/dev/peps/pep-3147/)).

* The generation of bytecode .pyc files is a form of [caching](https://en.wikipedia.org/wiki/Cache_(computing)) (i.e. greatly improves average performance).
* Configuring python with `PYTHONDONTWRITEBYTECODE=1` can be bad for python performance (for python2 see <https://www.python.org/dev/peps/pep-0304/> and for python3 see <https://www.python.org/dev/peps/pep-3147/> ).
* If you are interested in the performance impact please see here <https://github.com/python/cpython> ."
"What exactly do `*args` and `**kwargs` mean?

According to the Python documentation, from what it seems, it passes in a tuple of arguments.

```
def foo(hello, *args):
    print(hello)

    for each in args:
        print(each)

if __name__ == '__main__':
    foo(""LOVE"", [""lol"", ""lololol""])

```

This prints out:

```
LOVE
['lol', 'lololol']

```

How do you effectively use them?","Putting `*args` and/or `**kwargs` as the last items in your function definition’s argument list allows that function to accept an arbitrary number of arguments and/or keyword arguments.

For example, if you wanted to write a function that returned the sum of all its arguments, no matter how many you supply, you could write it like this:

```
def my_sum(*args):
    return sum(args)

```

It’s probably more commonly used in object-oriented programming, when you’re overriding a function, and want to call the original function with whatever arguments the user passes in.

You don’t actually have to call them `args` and `kwargs`, that’s just a convention. It’s the `*` and `**` that do the magic.

There's a more in-depth look in [the official Python documentation on arbitrary argument lists](https://docs.python.org/3/tutorial/controlflow.html#arbitrary-argument-lists)."
"Is there a way to set up a global variable inside of a module? When I tried to do it the most obvious way as appears below, the Python interpreter said the variable `__DBNAME__` did not exist.

```
...
__DBNAME__ = None

def initDB(name):
    if not __DBNAME__:
        __DBNAME__ = name
    else:
        raise RuntimeError(""Database name has already been set."")
...

```

And after importing the module in a different file

```
...
import mymodule
mymodule.initDB('mydb.sqlite')
...

```

And the traceback was:

`UnboundLocalError: local variable '__DBNAME__' referenced before assignment`

Any ideas? I'm trying to set up a singleton by using a module, as per [this fellow's](https://stackoverflow.com/questions/31875/is-there-a-simple-elegant-way-to-define-singletons-in-python/31887#31887) recommendation.","Here is what is going on.

First, the only global variables Python really has are module-scoped variables. You cannot make a variable that is truly global; all you can do is make a variable in a particular scope. (If you make a variable inside the Python interpreter, and then import other modules, your variable is in the outermost scope and thus global within your Python session.)

All you have to do to make a module-global variable is just assign to a name.

Imagine a file called foo.py, containing this single line:

```
X = 1

```

Now imagine you import it.

```
import foo
print(foo.X)  # prints 1

```

However, let's suppose you want to use one of your module-scope variables as a global inside a function, as in your example. Python's default is to assume that function variables are local. You simply add a `global` declaration in your function, before you try to use the global.

```
def initDB(name):
    global __DBNAME__  # add this line!
    if __DBNAME__ is None: # see notes below; explicit test for None
        __DBNAME__ = name
    else:
        raise RuntimeError(""Database name has already been set."")

```

By the way, for this example, the simple `if not __DBNAME__` test is adequate, because any string value other than an empty string will evaluate true, so any actual database name will evaluate true. But for variables that might contain a number value that might be 0, you can't just say `if not variablename`; in that case, you should explicitly test for `None` using the `is` operator. I modified the example to add an explicit `None` test. The explicit test for `None` is never wrong, so I default to using it.

Finally, as others have noted on this page, two leading underscores signals to Python that you want the variable to be ""private"" to the module. If you ever do an `from mymodule import *`, Python will not import names with two leading underscores into your name space. But if you just do a simple `import mymodule` and then say `dir(mymodule)` you will see the ""private"" variables in the list, and if you explicitly refer to `mymodule.__DBNAME__` Python won't care, it will just let you refer to it. The double leading underscores are a major clue to users of your module that you don't want them rebinding that name to some value of their own.

It is considered best practice in Python not to do `import *`, but to minimize the coupling and maximize explicitness by either using `mymodule.something` or by explicitly doing an import like `from mymodule import something`.

EDIT: If, for some reason, you need to do something like this in a very old version of Python that doesn't have the `global` keyword, there is an easy workaround. Instead of setting a module global variable directly, use a mutable type at the module global level, and store your values inside it.

In your functions, the global variable name will be read-only; you won't be able to rebind the actual global variable name. (If you assign to that variable name inside your function it will only affect the local variable name inside the function.) But you can use that local variable name to access the actual global object, and store data inside it.

You can use a `list` but your code will be ugly:

```
__DBNAME__ = [None] # use length-1 list as a mutable

# later, in code:  
if __DBNAME__[0] is None:
    __DBNAME__[0] = name

```

A `dict` is better. But the most convenient is a class instance, and you can just use a trivial class:

```
class Box:
    pass

__m = Box()  # m will contain all module-level values
__m.dbname = None  # database name global in module

# later, in code:
if __m.dbname is None:
    __m.dbname = name

```

(You don't really need to capitalize the database name variable.)

I like the syntactic sugar of just using `__m.dbname` rather than `__m[""DBNAME""]`; it seems the most convenient solution in my opinion. But the `dict` solution works fine also.

With a `dict` you can use any hashable value as a key, but when you are happy with names that are valid identifiers, you can use a trivial class like `Box` in the above."
"I am trying to pass in a JSON file and convert the data into a dictionary.

So far, this is what I have done:

```
import json
json1_file = open('json1')
json1_str = json1_file.read()
json1_data = json.loads(json1_str)

```

I'm expecting `json1_data` to be a `dict` type but it actually comes out as a `list` type when I check it with `type(json1_data)`.

What am I missing? I need this to be a dictionary so I can access one of the keys.","Your JSON is an array with a single object inside, so when you read it in you get a list with a dictionary inside. You can access your dictionary by accessing item 0 in the list, as shown below:

```
json1_data = json.loads(json1_str)[0]

```

Now you can access the data stored in *datapoints* just as you were expecting:

```
datapoints = json1_data['datapoints']

```

---

> I have one more question if anyone can bite: I am trying to take the average of the first elements in these datapoints(i.e. datapoints[0][0]). Just to list them, I tried doing datapoints[0:5][0] but all I get is the first datapoint with both elements as opposed to wanting to get the first 5 datapoints containing only the first element. Is there a way to do this?

`datapoints[0:5][0]` doesn't do what you're expecting. `datapoints[0:5]` returns a new list slice containing just the first 5 elements, and then adding `[0]` on the end of it will take just the first element *from that resulting list slice*. What you need to use to get the result you want is a [list comprehension](http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions):

```
[p[0] for p in datapoints[0:5]]

```

Here's a simple way to calculate the mean:

```
sum(p[0] for p in datapoints[0:5])/5. # Result is 35.8

```

If you're willing to install [NumPy](http://www.numpy.org/), then it's even easier:

```
import numpy
json1_file = open('json1')
json1_str = json1_file.read()
json1_data = json.loads(json1_str)[0]
datapoints = numpy.array(json1_data['datapoints'])
avg = datapoints[0:5,0].mean()
# avg is now 35.8

```

Using the `,` operator with the slicing syntax for NumPy's arrays has the behavior you were originally expecting with the list slices."
"I have the following plot:

```
import matplotlib.pyplot as plt

fig2 = plt.figure()
ax3 = fig2.add_subplot(2,1,1)
ax4 = fig2.add_subplot(2,1,2)
ax4.loglog(x1, y1)
ax3.loglog(x2, y2)
ax3.set_ylabel('hello')

```

I want to create axes labels and titles that span on both subplots. For example, since both plots have identical axes, I only need one set of `xlabel` and `ylabel`. I do want different titles for each subplot though.

How can I achieve this ?","You can create a big subplot that covers the two subplots and then set the common labels.

```
import random
import matplotlib.pyplot as plt

x = range(1, 101)
y1 = [random.randint(1, 100) for _ in range(len(x))]
y2 = [random.randint(1, 100) for _ in range(len(x))]

fig = plt.figure()
ax = fig.add_subplot(111)    # The big subplot
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)

# Turn off axis lines and ticks of the big subplot
ax.spines['top'].set_color('none')
ax.spines['bottom'].set_color('none')
ax.spines['left'].set_color('none')
ax.spines['right'].set_color('none')
ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)

ax1.loglog(x, y1)
ax2.loglog(x, y2)

# Set common labels
ax.set_xlabel('common xlabel')
ax.set_ylabel('common ylabel')

ax1.set_title('ax1 title')
ax2.set_title('ax2 title')

plt.savefig('common_labels.png', dpi=300)

```

![common_labels.png](https://i.sstatic.net/EHhFk.png)

Another way is using fig.text() to set the locations of the common labels directly.

```
import random
import matplotlib.pyplot as plt

x = range(1, 101)
y1 = [random.randint(1, 100) for _ in range(len(x))]
y2 = [random.randint(1, 100) for _ in range(len(x))]

fig = plt.figure()
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)

ax1.loglog(x, y1)
ax2.loglog(x, y2)

# Set common labels
fig.text(0.5, 0.04, 'common xlabel', ha='center', va='center')
fig.text(0.06, 0.5, 'common ylabel', ha='center', va='center', rotation='vertical')

ax1.set_title('ax1 title')
ax2.set_title('ax2 title')

plt.savefig('common_labels_text.png', dpi=300)

```

![common_labels_text.png](https://i.sstatic.net/J6pVr.png)"
"Python: How to get the caller's method name in the called method?

Assume I have 2 methods:

```
def method1(self):
    ...
    a = A.method2()

def method2(self):
    ...

```

If I don't want to do any change for method1, how to get the name of the caller (in this example, the name is method1) in method2?","[inspect.getframeinfo](http://docs.python.org/library/inspect.html?highlight=inspect#inspect.getframeinfo) and other related functions in `inspect` can help:

```
>>> import inspect
>>> def f1(): f2()
... 
>>> def f2():
...   curframe = inspect.currentframe()
...   calframe = inspect.getouterframes(curframe, 2)
...   print('caller name:', calframe[1][3])
... 
>>> f1()
caller name: f1

```

this introspection is intended to help debugging and development; it's not advisable to rely on it for production-functionality purposes."
"Is there a generally accepted way to comment functions in Python? Is the following acceptable?

```
#########################################################
# Create a new user
#########################################################
def add(self):

```","The correct way to do it is to provide a docstring. That way, `help(add)` will also spit out your comment.

```
def add(self):
    """"""Create a new user.

    Line 2 of comment...
    And so on... 
    """"""

```

That's three double quotes to open the comment and another three double quotes to end it. You can also use any valid Python string. It doesn't need to be multiline and double quotes can be replaced by single quotes.

See: [PEP 257](http://www.python.org/dev/peps/pep-0257/)"
"I am trying to find a simple way of getting a count of the number of elements in a list:

```
MyList = [""a"", ""b"", ""c""]

```

I want to know there are 3 elements in this list.","[`len()`](http://docs.python.org/library/functions.html#len)

```
>>> someList=[]
>>> print len(someList)
0

```"
"I would like to remove the first character of a string.

For example, my string starts with a `:` and I want to remove that only. There are several occurrences of `:` in the string that shouldn't be removed.

I am writing my code in Python.","**python 2.x**

```
s = "":dfa:sif:e""
print s[1:]

```

**python 3.x**

```
s = "":dfa:sif:e""
print(s[1:])

```

**both prints**

```
dfa:sif:e

```"
"I'm using Python's logging module to log some debug strings to a file which works pretty well. Now in addition, I'd like to use this module to also print the strings out to stdout. How do I do this? In order to log my strings to a file I use following code:

```
import logging
import logging.handlers
logger = logging.getLogger("""")
logger.setLevel(logging.DEBUG)
handler = logging.handlers.RotatingFileHandler(
    LOGFILE, maxBytes=(1048576*5), backupCount=7
)
formatter = logging.Formatter(""%(asctime)s - %(name)s - %(levelname)s - %(message)s"")
handler.setFormatter(formatter)
logger.addHandler(handler)

```

and then call a logger function like

```
logger.debug(""I am written to the file"")

```

Thank you for some help here!","Just get a handle to the root logger and add the `StreamHandler`. The `StreamHandler` writes to stderr. Not sure if you really need stdout over stderr, but this is what I use when I setup the Python logger and I also add the `FileHandler` as well. Then all my logs go to both places (which is what it sounds like you want).

```
import logging
logging.getLogger().addHandler(logging.StreamHandler())

```

If you want to output to `stdout` instead of `stderr`, you just need to specify it to the `StreamHandler` constructor.

```
import sys
# ...
logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))

```

You could also add a `Formatter` to it so all your log lines have a common header.

ie:

```
import logging
logFormatter = logging.Formatter(""%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s"")
rootLogger = logging.getLogger()

fileHandler = logging.FileHandler(""{0}/{1}.log"".format(logPath, fileName))
fileHandler.setFormatter(logFormatter)
rootLogger.addHandler(fileHandler)

consoleHandler = logging.StreamHandler()
consoleHandler.setFormatter(logFormatter)
rootLogger.addHandler(consoleHandler)

```

Prints to the format of:

```
2012-12-05 16:58:26,618 [MainThread  ] [INFO ]  my message

```"
"I'm currently working on a map editor for a game in pygame, using tile maps.
The level is built up out of blocks in the following structure (though much larger):

```
level1 = (
         (1,1,1,1,1,1)
         (1,0,0,0,0,1)
         (1,0,0,0,0,1)
         (1,0,0,0,0,1)
         (1,0,0,0,0,1)
         (1,1,1,1,1,1))

```

where ""1"" is a block that's a wall and ""0"" is a block that's empty air.

The following code is basically the one handling the change of block type:

```
clicked = pygame.mouse.get_pressed()
if clicked[0] == 1:
    currLevel[((mousey+cameraY)/60)][((mousex+cameraX)/60)] = 1

```

But since the level is stored in a tuple, I'm unable to change the values of the different blocks. How do I go about changing the different values in the level in an easy manner?","Convert tuple to list:

```
>>> t = ('my', 'name', 'is', 'mr', 'tuple')
>>> t
('my', 'name', 'is', 'mr', 'tuple')
>>> list(t)
['my', 'name', 'is', 'mr', 'tuple']

```

Convert list to tuple:

```
>>> l = ['my', 'name', 'is', 'mr', 'list']
>>> l
['my', 'name', 'is', 'mr', 'list']
>>> tuple(l)
('my', 'name', 'is', 'mr', 'list')

```"
"What does the `/` mean in Python 3.4's `help` output for `range` before the closing parenthesis?

```
>>> help(range)
Help on class range in module builtins:

class range(object)
 |  range(stop) -> range object
 |  range(start, stop[, step]) -> range object
 |  
 |  Return a virtual sequence of numbers from start to stop by step.
 |  
 |  Methods defined here:
 |  
 |  __contains__(self, key, /)
 |      Return key in self.
 |  
 |  __eq__(self, value, /)
 |      Return self==value.

                                        ...

```","It signifies the end of the [*positional only* parameters](http://www.python.org/dev/peps/pep-0436/#functions-with-positional-only-parameters), parameters you *cannot* use as keyword parameters. Before Python 3.8, such parameters could only be specified in the C API.

It means the `key` argument to `__contains__` can only be passed in by position (`range(5).__contains__(3)`), not as a keyword argument (`range(5).__contains__(key=3)`), something you *can* do with positional arguments in pure-python functions.

Also see the [Argument Clinic](https://docs.python.org/3/howto/clinic.html) documentation:

> To mark all parameters as positional-only in Argument Clinic, add a `/` on a line by itself after the last parameter, indented the same as the parameter lines.

and the (very recent addition to) the [Python FAQ](https://docs.python.org/3/faq/programming.html#what-does-the-slash-in-the-parameter-list-of-a-function-mean):

> A slash in the argument list of a function denotes that the parameters prior to it are positional-only. Positional-only parameters are the ones without an externally-usable name. Upon calling a function that accepts positional-only parameters, arguments are mapped to parameters based solely on their position.

The syntax is now part of the Python language specification, [as of version 3.8](https://docs.python.org/3.8/whatsnew/3.8.html#positional-only-parameters), see [PEP 570 â€“ *Python Positional-Only Parameters*](https://www.python.org/dev/peps/pep-0570/). Before PEP 570, the syntax was already reserved for possible future inclusion in Python, see [PEP 457 - *Syntax For Positional-Only Parameters*](https://www.python.org/dev/peps/pep-0457/).

Positional-only parameters can lead to cleaner and clearer APIs, make pure-Python implementations of otherwise C-only modules more consistent and easier to maintain, and because positional-only parameters require very little processing, they lead to faster Python code."
"Currently, in Python, a function's parameters and return types can be type hinted as follows:

```
def func(var1: str, var2: str) -> int:
    return var1.index(var2)

```

Which indicates that the function takes two strings, and returns an integer.

However, this syntax is highly confusing with lambdas, which look like:

```
func = lambda var1, var2: var1.index(var2)

```

I've tried putting in type hints on both parameters and return types, and I can't figure out a way that doesn't cause a syntax error.

Is it possible to type hint a lambda function? If not, are there plans for type hinting lambdas, or any reason (besides the obvious syntax conflict) why not?","You can, sort of, in Python 3.6 and up using [PEP 526 variable annotations](https://www.python.org/dev/peps/pep-0526/). You can annotate the variable you assign the `lambda` result to with the [`typing.Callable` generic](https://docs.python.org/3/library/typing.html#typing.Callable):

```
from typing import Callable

func: Callable[[str, str], int] = lambda var1, var2: var1.index(var2)

```

This doesn't attach the type hinting information to the function object itself, only to the namespace you stored the object in, but this is usually all you need for type hinting purposes.

However, you may as well just use a function statement instead; the only advantage that a `lambda` offers is that you can put a function definition for a simple expression *inside* a larger expression. But the above lambda is not part of a larger expression, it is only ever part of an assignment statement, binding it to a name. That's exactly what a `def func(var1: str, var2: str): return var1.index(var2)` statement would achieve.

Note that you can't annotate `*args` or `**kwargs` arguments separately either, as the documentation for `Callable` states:

> There is no syntax to indicate optional or keyword arguments; such function types are rarely used as callback types.

That limitation does not apply to a [PEP 544 *protocol* with a `__call__` method](https://www.python.org/dev/peps/pep-0544/#callback-protocols); use this if you need a expressive definition of what arguments should be accepted. You need Python 3.8 *or* install the [`typing-extensions` project](https://pypi.org/project/typing-extensions/) for a backport:

```
from typing_extensions import Protocol

class SomeCallableConvention(Protocol):
    def __call__(self, var1: str, var2: str, spam: str = ""ham"") -> int:
        ...

func: SomeCallableConvention = lambda var1, var2, spam=""ham"": var1.index(var2) * spam

```

For the `lambda` expression *itself*, you can't use any annotations (the syntax on which Python's type hinting is built). The syntax is only available for `def` function statements.

From [PEP 3107 - *Function Annotations*](https://www.python.org/dev/peps/pep-3107/#lambda):

> lambda 's syntax does not support annotations. The syntax of lambda could be changed to support annotations, by requiring parentheses around the parameter list. However it [was decided](http://web.archive.org/web/20150922163722/https://mail.python.org/pipermail/python-3000/2006-May/001613.html) not to make this change because:
>
> * It would be an incompatible change.
> * Lambda's are neutered anyway.
> * The lambda can always be changed to a function.

You can still attach the annotations directly to the object, the `function.__annotations__` attribute is a writable dictionary:

```
>>> def func(var1: str, var2: str) -> int:
...     return var1.index(var2)
...
>>> func.__annotations__
{'var1': <class 'str'>, 'return': <class 'int'>, 'var2': <class 'str'>}
>>> lfunc = lambda var1, var2: var1.index(var2)
>>> lfunc.__annotations__
{}
>>> lfunc.__annotations__['var1'] = str
>>> lfunc.__annotations__['var2'] = str
>>> lfunc.__annotations__['return'] = int
>>> lfunc.__annotations__
{'var1': <class 'str'>, 'return': <class 'int'>, 'var2': <class 'str'>}

```

Not that dynamic annotations like these are going to help you when you wanted to run a static analyser over your type hints, of course."
"I tried implementing the formula in *[Finding distances based on Latitude and Longitude](http://andrew.hedges.name/experiments/haversine/)*. The applet does good for the two points I am testing:

![Enter image description here](https://i.sstatic.net/FGED4.png)

Yet my code is not working.

```
from math import sin, cos, sqrt, atan2

R = 6373.0

lat1 = 52.2296756
lon1 = 21.0122287
lat2 = 52.406374
lon2 = 16.9251681

dlon = lon2 - lon1
dlat = lat2 - lat1
a = (sin(dlat/2))**2 + cos(lat1) * cos(lat2) * (sin(dlon/2))**2
c = 2 * atan2(sqrt(a), sqrt(1-a))
distance = R * c

print ""Result"", distance
print ""Should be"", 278.546

```

It returns the distance **5447.05546147**. Why?","The Vincenty distance is now [deprecated since GeoPy version 1.13](https://geopy.readthedocs.io/en/stable/changelog_1xx.html#id12) - [you should use `geopy.distance.distance()`](https://geopy.readthedocs.io/en/stable/#module-geopy.distance) instead!

---

Some previous answers were based on the [haversine formula](https://en.wikipedia.org/wiki/Haversine_formula), which assumes the earth is a sphere, which results in errors of up to about 0.5% (according to `help(geopy.distance)`). The [Vincenty distance](https://en.wikipedia.org/wiki/Vincenty%27s_formulae) uses more accurate ellipsoidal models, such as [WGS-84](https://en.wikipedia.org/wiki/World_Geodetic_System), and is implemented in [geopy](https://pypi.python.org/pypi/geopy). For example,

```
import geopy.distance

coords_1 = (52.2296756, 21.0122287)
coords_2 = (52.406374, 16.9251681)

print(geopy.distance.geodesic(coords_1, coords_2).km)

```

will print the distance of `279.352901604` kilometers using the default ellipsoid WGS-84. (You can also choose `.miles` or one of several other distance units.)"
"I would like to include the current git hash in the output of a Python script (as a the *version number* of the code that generated that output).

How can I access the current git hash in my Python script?","No need to hack around getting data from the `git` command yourself. [GitPython](http://gitpython.readthedocs.io/en/stable/) is a very nice way to do this and a lot of other `git` stuff. It even has ""best effort"" support for Windows.

After `pip install gitpython` you can do

```
import git
repo = git.Repo(search_parent_directories=True)
sha = repo.head.object.hexsha

```

---

Something to consider when using this library. The following is taken from [gitpython.readthedocs.io](https://gitpython.readthedocs.io/en/stable/intro.html#installing-gitpython)

> Leakage of System Resources
>
> GitPython is not suited for long-running processes (like daemons) as it tends to leak system resources. It was written in a time where destructors (as implemented in the `__del__` method) still ran deterministically.
>
> In case you still want to use it in such a context, you will want to search the codebase for `__del__` implementations and call these yourself when you see fit.
>
> Another way assure proper cleanup of resources is to factor out GitPython into a separate process which can be dropped periodically"
"How do you find the median of a list in Python? The list can be of any size and the numbers are not guaranteed to be in any particular order.

If the list contains an even number of elements, the function should return the average of the middle two.

Here are some examples (sorted for display purposes):

```
median([1]) == 1
median([1, 1]) == 1
median([1, 1, 2, 4]) == 1.5
median([0, 2, 5, 6, 8, 9, 9]) == 6
median([0, 0, 0, 0, 4, 4, 6, 8]) == 2

```","Python 3.4 has [`statistics.median`](https://docs.python.org/3/library/statistics.html#statistics.median):

> Return the median (middle value) of numeric data.
>
> When the number of data points is odd, return the middle data point.
> When the number of data points is even, the median is interpolated by taking the average of the two middle values:
>
> ```
> >>> median([1, 3, 5])
> 3
> >>> median([1, 3, 5, 7])
> 4.0
>
> ```

Usage:

```
import statistics

items = [6, 1, 8, 2, 3]

statistics.median(items)
#>>> 3

```

It's pretty careful with types, too:

```
statistics.median(map(float, items))
#>>> 3.0

from decimal import Decimal
statistics.median(map(Decimal, items))
#>>> Decimal('3')

```"
"I want an idiomatic way to find the first element in a list that matches a predicate.

The current code is quite ugly:

```
[x for x in seq if predicate(x)][0]

```

I've thought about changing it to:

```
from itertools import dropwhile
dropwhile(lambda x: not predicate(x), seq).next()

```

But there must be something more elegant... And it would be nice if it returns a `None` value rather than raise an exception if no match is found.

I know I could just define a function like:

```
def get_first(predicate, seq):
    for i in seq:
        if predicate(i): return i
    return None

```

But it is quite tasteless to start filling the code with utility functions like this (and people will probably not notice that they are already there, so they tend to be repeated over time) if there are built ins that already provide the same.","To find the first element in a sequence `seq` that matches a `predicate`:

```
next(x for x in seq if predicate(x))

```

Or simply:

**Python 2**:

```
next(itertools.ifilter(predicate, seq))

```

**Python 3**:

```
next(filter(predicate, seq))

```

These will raise a `StopIteration` exception if the predicate does not match for any element.

---

To return `None` if there is no such element:

```
next((x for x in seq if predicate(x)), None)

```

Or:

```
next(filter(predicate, seq), None)

```"
"I have the following piece of code which fails with the following error:

> RuntimeError: maximum recursion depth exceeded

I attempted to rewrite this to allow for tail call optimization (TCO). I believe that this code would have been successful if a TCO had taken place.

```
def trisum(n, csum):
    if n == 0:
        return csum
    else:
        return trisum(n - 1, csum + n)

print(trisum(1000, 0))

```

Should I conclude that Python does not do any type of TCO, or do I just need to define it differently?","No, and it never will since [Guido van Rossum](https://en.wikipedia.org/wiki/Guido_van_Rossum) prefers to be able to have proper tracebacks:

*[Tail Recursion Elimination](http://neopythonic.blogspot.com.au/2009/04/tail-recursion-elimination.html)* (2009-04-22)

*[Final Words on Tail Calls](http://neopythonic.blogspot.com.au/2009/04/final-words-on-tail-calls.html)* (2009-04-27)

You can manually eliminate the recursion with a transformation like this:

```
>>> def trisum(n, csum):
...     while True:                     # Change recursion to a while loop
...         if n == 0:
...             return csum
...         n, csum = n - 1, csum + n   # Update parameters instead of tail recursion

>>> trisum(1000,0)
500500

```"
"I'm used to the Java model where you can have one public class per file. Python doesn't have this restriction, and I'm wondering what's the best practice for organizing classes.","A Python file is called a ""module"" and it's one way to organize your software so that it makes ""sense"". Another is a directory, called a ""package"".

A module is a distinct thing that may have one or two dozen closely-related classes. The trick is that a module is something you'll import, and you need that import to be perfectly sensible to people who will read, maintain and extend your software.

The rule is this: **a module is the unit of reuse**.

You can't easily reuse a single class. You should be able to reuse a module without any difficulties. Everything in your library (and everything you download and add) is either a module or a package of modules.

For example, you're working on something that reads spreadsheets, does some calculations and loads the results into a database. What do you want your main program to look like?

```
from ssReader import Reader
from theCalcs import ACalc, AnotherCalc
from theDB import Loader

def main( sourceFileName ):
    rdr= Reader( sourceFileName )
    c1= ACalc( options )
    c2= AnotherCalc( options )
    ldr= Loader( parameters )
    for myObj in rdr.readAll():
        c1.thisOp( myObj )
        c2.thatOp( myObj )
        ldr.laod( myObj )

```

Think of the import as the way to organize your code in concepts or chunks. Exactly how many classes are in each import doesn't matter. What matters is the overall organization that you're portraying with your `import` statements."
"I need to install `cv2` for a script that has been written for me. I tried `pip install cv2` and `pip install open_cv` and got the same problem - a warning message from `dist.py` and complains about `zlib` being not found. No cv2 installed. I also tried `pyopenvc` and `pip install opencv-python`.

So, I went to the opencv site and downloaded the relevant exe. Ran it - generated a heap of subdirectories and a make file and stuff.

What do I do now?","Install [`opencv-python`](https://pypi.org/project/opencv-python/) (which is the **official** pre-built OpenCV package for Python) by issuing the following command:

```
pip install opencv-python

```"
"I'm searching for an elegant way to get data using attribute access on a dict with some nested dicts and lists (i.e. javascript-style object syntax).

For example:

```
>>> d = {'a': 1, 'b': {'c': 2}, 'd': [""hi"", {'foo': ""bar""}]}

```

Should be accessible in this way:

```
>>> x = dict2obj(d)
>>> x.a
1
>>> x.b.c
2
>>> x.d[1].foo
bar

```

I think, this is not possible without recursion, but what would be a nice way to get an object style for dicts?","**Update:** In Python 2.6 and onwards, consider whether the [`namedtuple`](https://docs.python.org/2/library/collections.html#collections.namedtuple) data structure suits your needs:

```
>>> from collections import namedtuple
>>> MyStruct = namedtuple('MyStruct', 'a b d')
>>> s = MyStruct(a=1, b={'c': 2}, d=['hi'])
>>> s
MyStruct(a=1, b={'c': 2}, d=['hi'])
>>> s.a
1
>>> s.b
{'c': 2}
>>> s.c
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'MyStruct' object has no attribute 'c'
>>> s.d
['hi']

```

The alternative (original answer contents) is:

```
class Struct:
    def __init__(self, **entries):
        self.__dict__.update(entries)

```

Then, you can use:

```
>>> args = {'a': 1, 'b': 2}
>>> s = Struct(**args)
>>> s
<__main__.Struct instance at 0x01D6A738>
>>> s.a
1
>>> s.b
2

```"
"DataFrame:

```
  c_os_family_ss c_os_major_is l_customer_id_i
0      Windows 7                         90418
1      Windows 7                         90418
2      Windows 7                         90418

```

Code:

```
for name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):
    print name
    print group

```

I'm trying to just loop over the aggregated data, but I get the error:

```
ValueError: too many values to unpack

```

I wish to loop over every group. How do I do it?","`df.groupby('l_customer_id_i').agg(lambda x: ','.join(x))` does already return a dataframe, so you cannot loop over the groups anymore.

In general:

* `df.groupby(...)` returns a `GroupBy` object (a DataFrameGroupBy or SeriesGroupBy), and with this, you can iterate through the groups (as explained in the docs [here](http://pandas.pydata.org/pandas-docs/stable/groupby.html#iterating-through-groups)). You can do something like:

  ```
  grouped = df.groupby('A')

  for name, group in grouped:
      ...

  ```
* When you apply a function on the groupby, in your example `df.groupby(...).agg(...)` (but this can also be `transform`, `apply`, `mean`, ...), you **combine** the result of **applying** the function to the different groups together in one dataframe (the apply and combine step of the 'split-apply-combine' paradigm of groupby). So the result of this will always be again a DataFrame (or a Series depending on the applied function)."
"We have been using [Mock](http://www.voidspace.org.uk/python/mock/) for python for a while.

Now, we have a situation in which we want to mock a function

```
def foo(self, my_param):
    #do something here, assign something to my_result
    return my_result

```

Normally, the way to mock this would be (assuming foo being part of an object)

```
self.foo = MagicMock(return_value=""mocked!"")

```

Even, if i call foo() a couple of times i can use

```
self.foo = MagicMock(side_effect=[""mocked once"", ""mocked twice!""])

```

Now, I am facing a situation in which I want to return a fixed value when the input parameter has a particular value. So if let's say ""my\_param"" is equal to ""something"" then I want to return ""my\_cool\_mock""

This seems to be available on [mockito for python](http://code.google.com/p/mockito-python/)

```
when(dummy).foo(""something"").thenReturn(""my_cool_mock"")

```

I have been searching on how to achieve the same with Mock with no success?

Any ideas?","> If `side_effect_func` is a function then whatever that function returns is
> what calls to the mock return. The `side_effect_func` function is called with
> the same arguments as the mock. This allows you to vary the return
> value of the call dynamically, based on the input:
>
> ```
> >>> def side_effect_func(value):
> ...     return value + 1
> ...
> >>> m = MagicMock(side_effect=side_effect_func)
> >>> m(1)
> 2
> >>> m(2)
> 3
> >>> m.mock_calls
> [call(1), call(2)]
>
> ```

<http://www.voidspace.org.uk/python/mock/mock.html#calling>"
"I want to check my environment for the existence of a variable, say `""FOO""`, in Python. For this purpose, I am using the `os` standard library. After reading the library's documentation, I have figured out 2 ways to achieve my goal:

Method 1:

```
if ""FOO"" in os.environ:
    pass

```

Method 2:

```
if os.getenv(""FOO"") is not None:
    pass

```

I would like to know which method, if either, is a good/preferred conditional and why.","Use the first; it directly tries to check if something is defined in `environ`. Though the second form works equally well, it's lacking semantically since you get a value back if it exists and *only* use it for a comparison.

You're trying to see if something is present *in* `environ`, why would you *get* just to compare it and then *toss it away*?

That's exactly what `getenv` does:

> *Get an environment variable*, return `None` if it doesn't exist. The
> optional second argument can specify an alternate default.

(this also means your check could just be `if getenv(""FOO"")`)

you don't want to *get it*, you want to check for it's existence.

Either way, `getenv` is just a wrapper around `environ.get` but you don't see people checking for membership in mappings with:

```
from os import environ
if environ.get('Foo') is not None:

```

To summarize, use:

```
if ""FOO"" in os.environ:
    pass

```

if you *just* want to check for existence, while, use `getenv(""FOO"")` if you actually want to do something with the value you might get."
"I have some code which will read two strings from the user:

```
name1 = input(""Enter name 1: "")
name2 = input(""Enter name 2: "")

```

Later, I want to format those strings into a longer string for printing:

```
if len(name1) > len(name2):
    print (""'{0}' is longer than '{1}'""% name1, name2)

```

But I get an error message that looks like:

```
Traceback (most recent call last):
  File ""program.py"", line 13, in <module>
    print (""'{0}' is longer than '{1}'""% name1, name2)
TypeError: not all arguments converted during string formatting

```

What is wrong with the code? How should I write this line instead, in order to format the string properly?

---

See also [String formatting: % vs. .format vs. f-string literal](https://stackoverflow.com/questions/5082452/) for in-depth comparison of the most common ways to do this kind of string formatting, and [How do I put a variableâ€™s value inside a string (interpolate it into the string)?](https://stackoverflow.com/questions/2960772) for a general how-to guide for this kind of string construction. See [Printing tuple with string formatting in Python](https://stackoverflow.com/questions/1455602) for another common cause of the error.","Old-style `%` formatting uses `%` codes for formatting:

```
# A single value can be written as is:
'It will cost $%d dollars.' % 95

# Multiple values must be provided as a tuple:
""'%s' is longer than '%s'"" % (name1, name2)

```

New-style `{}` formatting uses `{}` codes and the `.format` method. Make sure not to mix and match - if the ""template"" string contains `{}` placeholders, then call `.format`, don't use `%`.

```
# The values to format are now arguments for a method call,
# so the syntax is the same either way:
'It will cost ${0} dollars.'.format(95)

""'{0}' is longer than '{1}'"".format(name1, name2)

```"
"Consider this - a base class A, class B inheriting from A, class C inheriting from B. What is a generic way to call a parent class initialiser in an initialiser? If this still sounds too vague, here's some code.

```
class A(object):
    def __init__(self):
        print ""Initialiser A was called""

class B(A):
    def __init__(self):
        super(B,self).__init__()
        print ""Initialiser B was called""

class C(B):
    def __init__(self):
        super(C,self).__init__()
        print ""Initialiser C was called""

c = C()

```

This is how I do it now. But it still seems a bit too non-generic - you still must pass a correct type by hand.

Now, I've tried using `self.__class__` as a first argument to super(), but, obviously it doesn't work - if you put it in the initialiser for C - fair enough, B's initialiser gets called. If you do the same in B, ""self"" still points to an instance of C so you end up calling B's initialiser again (this ends in an infinite recursion).

There is no need to think about diamond inheritance for now, I am just interested in solving this specific problem.","Python 3 includes an improved super() which allows use like this:

```
super().__init__(args)

```"
"Given a float between 0 and 1, how to print it as a percentage?

For example, `1/3` should print as `33%`.","Since Python 3.0, [`str.format`](https://docs.python.org/3/library/stdtypes.html#str.format) and [`format`](https://docs.python.org/3/library/functions.html#format) support a [percentage presentation type](https://docs.python.org/3/library/string.html#format-specification-mini-language):

```
>>> f""{1/3:.0%}""
'33%'
>>> ""{:.0%}"".format(1/3)
'33%'
>>> format(1/3, "".0%"")
'33%'

```

> Percentage. Multiplies the number by 100 and displays in fixed (`'f'`) format, followed by a percent sign.

The `.0` part of the format spec `.0%` indicates that you want zero digits of precision after the decimal point, because with `f""{1/3:%}""` you would get the string `'33.333333%'`.

It works with integers, floats, and [decimals](https://docs.python.org/3/library/decimal.html#decimal.Decimal). See [PEP 3101](https://www.python.org/dev/peps/pep-3101/)."
"By default `logging.Formatter('%(asctime)s')` prints with the following format:

```
2011-06-09 10:54:40,638

```

where 638 is the millisecond. I need to change the comma to a dot:

```
2011-06-09 10:54:40.638

```

To format the time I can use:

```
logging.Formatter(fmt='%(asctime)s',datestr=date_format_str)

```

however the [documentation](http://docs.python.org/library/time.html#time.strftime) doesn't specify how to format milliseconds. I've found [this SO question](https://stackoverflow.com/questions/698223/how-can-i-parse-a-time-string-containing-milliseconds-in-it-with-python) which talks about microseconds, but a) I would prefer milliseconds and b) the following doesn't work on Python 2.6 (which I'm working on) due to the `%f`:

```
logging.Formatter(fmt='%(asctime)s',datefmt='%Y-%m-%d,%H:%M:%S.%f')

```","This should work too:

```
logging.Formatter(
    fmt='%(asctime)s.%(msecs)03d',
    datefmt='%Y-%m-%d,%H:%M:%S'
)

```"
"I am trying to read a macro-enabled Excel worksheet using `pandas.read_excel` with the xlrd library. It's running fine in local, but when I try to push the same into PCF, I am getting this error:

```
2020-12-11T21:09:53.441+05:30 [APP/PROC/WEB/0] [ERR] df1=pd.read_excel(os.path.join(APP_PATH, os.path.join(""Data"", ""aug_latest.xlsm"")),sheet_name=None)

2020-12-11T21:09:53.441+05:30 [APP/PROC/WEB/0] [ERR] return open_workbook(filepath_or_buffer)
2020-12-11T21:09:53.441+05:30 [APP/PROC/WEB/0] [ERR] File ""/home/vcap/deps/0/python/lib/python3.8/site-packages/xlrd/__init__.py"", line 170, in open_workbook
2020-12-11T21:09:53.441+05:30 [APP/PROC/WEB/0] [ERR] raise XLRDError(FILE_FORMAT_DESCRIPTIONS[file_format]+'; not supported')
2020-12-11T21:09:53.441+05:30 [APP/PROC/WEB/0] [ERR] xlrd.biffh.XLRDError: Excel xlsx file; not supported

```

How can I resolve this error?","As noted in the [release email](https://groups.google.com/g/python-excel/c/IRa8IWq_4zk/m/Af8-hrRnAgAJ), linked to from the [release tweet](https://twitter.com/chriswithers13/status/1337342648848424962) and noted in large orange warning that appears on the front page of the [documentation](https://xlrd.readthedocs.io/en/latest/), and less orange, but still present, in the [readme on the repository](https://github.com/python-excel/xlrd) and the [release on pypi](https://pypi.org/project/xlrd/):

**xlrd has explicitly removed support for anything other than xls files.**

In your case, the solution is to:

* make sure you are on a recent version of Pandas, at least 1.0.1,
  and preferably the latest release. 1.2 will make his
  even clearer.
* install `openpyxl`: <https://openpyxl.readthedocs.io/en/stable/>
* change your Pandas code to be:

  ```
  df1 = pd.read_excel(
       os.path.join(APP_PATH, ""Data"", ""aug_latest.xlsm""),
       engine='openpyxl',
  )

  ```"
"I am loading a txt file containig a mix of float and string data. I want to store them in an array where I can access each element. Now I am just doing

```
import pandas as pd

data = pd.read_csv('output_list.txt', header = None)
print data

```

Each line in the input file looks like the following:

```
 1 0 2000.0 70.2836942112 1347.28369421 /file_address.txt

```

Now the data are imported as a unique column. How can I divide it, so to store different elements separately (so I can call `data[i,j]`)? And how can I define a header?","You can use:

```
data = pd.read_csv('output_list.txt', sep="" "", header=None)
data.columns = [""a"", ""b"", ""c"", ""etc.""]

```

Add `sep="" ""` in your code, leaving a blank space between the quotes. So pandas can detect spaces between values and sort in columns. Data columns is for naming your columns."
"I need to find ""yesterday's"" date in this format `MMDDYY` in Python.

So for instance, today's date would be represented like this:
111009

I can easily do this for today but I have trouble doing it automatically for ""yesterday"".","Use [`datetime.timedelta()`](https://docs.python.org/library/datetime.html#timedelta-objects)

```
>>> from datetime import date, timedelta
>>> yesterday = date.today() - timedelta(days=1)
>>> yesterday.strftime('%m%d%y')
'110909'

```"
"I'm trying to get the name of the Python script that is currently running.

I have a script called `foo.py` and I'd like to do something like this in order to get the script name:

```
print(Scriptname)

```","You can use [`__file__`](https://docs.python.org/reference/import.html#__file__) to get the name of the current file. When used in the main module, this is the name of the script that was originally invoked.

If you want to omit the directory part (which might be present), you can use `os.path.basename(__file__)`."
"1. How often does Python flush to a file?
2. How often does Python flush to stdout?

I'm unsure about (1).

As for (2), I believe Python flushes to stdout after every new line. But, if you overload stdout to be to a file, does it flush as often?","For file operations, Python uses the operating system's default buffering unless you configure it do otherwise. You can specify a buffer size, unbuffered, or line buffered.

For example, the open function takes a buffer size argument.

<http://docs.python.org/library/functions.html#open>

""The optional buffering argument specifies the fileâ€™s desired buffer size:""

* 0 means unbuffered,
* 1 means line buffered,
* any other positive value means use a buffer of (approximately) that size.
* A negative buffering means to use the system default, which is usually line buffered for tty devices and fully buffered for other files.
* If omitted, the system default is used.

code:

```
bufsize = 0
f = open('file.txt', 'w', buffering=bufsize)

```"
"Is there a way to select random rows from a DataFrame in Pandas.

In R, using the car package, there is a useful function `some(x, n)` which is similar to head but selects, in this example, 10 rows at random from x.

I have also looked at the slicing documentation and there seems to be nothing equivalent.

### Update

Now using version 20. There is a sample method.

```
df.sample(n)

```","With pandas version `0.16.1` and up, there is now a `DataFrame.sample` [method built-in](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html):

```
import pandas

df = pandas.DataFrame(pandas.np.random.random(100))

# Randomly sample 70% of your dataframe
df_percent = df.sample(frac=0.7)

# Randomly sample 7 elements from your dataframe
df_elements = df.sample(n=7)

```

For either approach above, you can get the rest of the rows by doing:

```
df_rest = df.loc[~df.index.isin(df_percent.index)]

```

Per `Pedram`'s comment, if you would like to get reproducible samples, pass the `random_state` parameter.

```
df_percent = df.sample(frac=0.7, random_state=42)

```"
"How can I check whether a numpy array is empty or not?

I used the following code, but this fails if the array contains a zero.

```
if not self.Definition.all():

```

Is this the solution?

```
if self.Definition == array([]):

```","You can always take a look at the `.size` attribute. It is [defined as an integer](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.size.html), and is zero (`0`) when there are no elements in the array:

```
import numpy as np
a = np.array([])

if a.size == 0:
    # Do something when `a` is empty

```"
I'm trying to make a basic Windows application that builds a string out of user input and then adds it to the clipboard. How do I copy a string to the clipboard using Python?,"Actually, `pywin32` and `ctypes` seem to be an overkill for this simple task. [`tkinter`](https://docs.python.org/3/library/tkinter.html) is a cross-platform GUI framework, which ships with Python by default and has clipboard accessing methods along with other cool stuff.

If all you need is to put some text to system clipboard, this will do it:

```
from tkinter import Tk # in Python 2, use ""Tkinter"" instead 
r = Tk()
r.withdraw()
r.clipboard_clear()
r.clipboard_append('i can has clipboardz?')
r.update() # now it stays on the clipboard after the window is closed
r.destroy()

```

And that's all, no need to mess around with platform-specific third-party libraries.

If you are using Python 2, replace `tkinter` with `Tkinter`."
"Given two input lists, how can I create a list of the elements that are common to both inputs?

For example: for inputs `[1,2,3,4,5,6]` and `[3,5,7,9]`, the result should be `[3, 5]`; for inputs `['this','this','n','that']` and `['this','not','that','that']`, the result should be `['this', 'that']`.

---

**See also:**

* [In Python, how do I find common words from two lists while preserving word order?](https://stackoverflow.com/questions/18264471/) (to keep the order)
* [Python -Intersection of multiple lists?](https://stackoverflow.com/questions/3852780/) (for computing the intersection between >=Â 3 lists)
* [Intersection of two lists including duplicates?](https://stackoverflow.com/questions/37645053/) (to keep the duplicate elements)","Use Python's [set intersection](https://docs.python.org/3/library/stdtypes.html#frozenset.intersection):

```
>>> list1 = [1,2,3,4,5,6]
>>> list2 = [3, 5, 7, 9]
>>> list(set(list1).intersection(list2))
[3, 5]

```"
"I've converted my scripts from Python 2.7 to 3.2, and I have a bug.

```
# -*- coding: utf-8 -*-
import time
from datetime import date
from lxml import etree
from collections import OrderedDict

# Create the root element
page = etree.Element('results')

# Make a new document tree
doc = etree.ElementTree(page)

# Add the subelements
pageElement = etree.SubElement(page, 'Country',Tim = 'Now', 
                                      name='Germany', AnotherParameter = 'Bye',
                                      Code='DE',
                                      Storage='Basic')
pageElement = etree.SubElement(page, 'City', 
                                      name='Germany',
                                      Code='PZ',
                                      Storage='Basic',AnotherParameter = 'Hello')
# For multiple multiple attributes, use as shown above

# Save to XML file
outFile = open('output.xml', 'w')
doc.write(outFile) 

```

On the last line, I got this error:

```
builtins.TypeError: must be str, not bytes
File ""C:\PythonExamples\XmlReportGeneratorExample.py"", line 29, in <module>
  doc.write(outFile)
File ""c:\Python32\Lib\site-packages\lxml\etree.pyd"", line 1853, in lxml.etree._ElementTree.write (src/lxml/lxml.etree.c:44355)
File ""c:\Python32\Lib\site-packages\lxml\etree.pyd"", line 478, in lxml.etree._tofilelike (src/lxml/lxml.etree.c:90649)
File ""c:\Python32\Lib\site-packages\lxml\etree.pyd"", line 282, in lxml.etree._ExceptionContext._raise_if_stored (src/lxml/lxml.etree.c:7972)
File ""c:\Python32\Lib\site-packages\lxml\etree.pyd"", line 378, in lxml.etree._FilelikeWriter.write (src/lxml/lxml.etree.c:89527)

```

I've installed Python 3.2, and I've installed lxml-2.3.win32-py3.2.exe.

On Python 2.7, it works.","The outfile should be in binary mode.

```
outFile = open('output.xml', 'wb')

```"
"If I am creating my own class in Python, what function should I define so as to allow the use of the `in` operator, e.g.

```
class MyClass(object):
    ...

m = MyClass()

if 54 in m:
    ...

```

---

See also [What does \_\_contains\_\_ do, what can call \_\_contains\_\_ function](https://stackoverflow.com/questions/1964934) for the corresponding question about what `__contains__` does.","[`MyClass.__contains__(self, item)`](http://docs.python.org/reference/datamodel.html#object.__contains__)"
"Is there a pandas built-in way to apply two different aggregating functions `f1, f2` to the same column `df[""returns""]`, without having to call `agg()` multiple times?

Example dataframe:

```
import pandas as pd
import datetime as dt
import numpy as np

pd.np.random.seed(0)
df = pd.DataFrame({
         ""date""    :  [dt.date(2012, x, 1) for x in range(1, 11)], 
         ""returns"" :  0.05 * np.random.randn(10), 
         ""dummy""   :  np.repeat(1, 10)
}) 

```

The syntactically wrong, but intuitively right, way to do it would be:

```
# Assume `f1` and `f2` are defined for aggregating.
df.groupby(""dummy"").agg({""returns"": f1, ""returns"": f2})

```

Obviously, Python doesn't allow duplicate keys. Is there any other manner for expressing the input to `agg()`? Perhaps a list of tuples `[(column, function)]` would work better, to allow multiple functions applied to the same column? But `agg()` seems like it only accepts a dictionary.

Is there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)","As of 2022-06-20, the below is the accepted practice for aggregations:

```
df.groupby('dummy').agg(
    Mean=('returns', np.mean),
    Sum=('returns', np.sum))

```

see [this answer](https://stackoverflow.com/a/54300159/4909087) for more information.

---

Below the fold included for historical versions of `pandas`.

You can simply pass the functions as a list:

```
In [20]: df.groupby(""dummy"").agg({""returns"": [np.mean, np.sum]})
Out[20]:         
           mean       sum
dummy                    
1      0.036901  0.369012

```

or as a dictionary:

```
In [21]: df.groupby('dummy').agg({'returns':
                                  {'Mean': np.mean, 'Sum': np.sum}})
Out[21]: 
        returns          
           Mean       Sum
dummy                    
1      0.036901  0.369012

```"
"Say I have this:

```
{% if files %}
    Update
{% else %}
    Continue
{% endif %}

```

In PHP, say, I can write a shorthand conditional, like:

```
<?php echo $foo ? 'yes' : 'no'; ?>

```

Is there then a way I can translate this to work in a jinja2 template:

```
'yes' if foo else 'no'

```","Yes, it's possible to use [inline if-expressions](http://jinja.pocoo.org/docs/templates/#if-expression):

```
{{ 'Update' if files else 'Continue' }}

```"
"I'm teaching myself Python and my most recent lesson was that [Python is not Java](http://dirtsimple.org/2004/12/python-is-not-java.html), and so I've just spent a while turning all my Class methods into functions.

I now realise that I don't need to use Class methods for what I would done with `static` methods in Java, but now I'm not sure when I would use them. All the advice I can find about Python Class methods is along the lines of newbies like me should steer clear of them, and the standard documentation is at its most opaque when discussing them.

Does anyone have a good example of using a Class method in Python or at least can someone tell me when Class methods can be sensibly used?","Class methods are for when you need to have methods that aren't specific to any particular instance, but still involve the class in some way. The most interesting thing about them is that they can be overridden by subclasses, something that's simply not possible in Java's static methods or Python's module-level functions.

If you have a class `MyClass`, and a module-level function that operates on MyClass (factory, dependency injection stub, etc), make it a `classmethod`. Then it'll be available to subclasses."
"The [documentation](http://docs.python.org/library/argparse.html) for the [argparse python module](http://www.doughellmann.com/PyMOTW/argparse/), while excellent I'm sure, is too much for my tiny beginner brain to grasp right now. I don't need to do math on the command line or meddle with formatting lines on the screen or change option characters. All I want to do is *""If arg is A, do this, if B do that, if none of the above show help and quit""*.","Here's the way I do it with `argparse` (with multiple args):

```
parser = argparse.ArgumentParser(description='Description of your program')
parser.add_argument('-f','--foo', help='Description for foo argument', required=True)
parser.add_argument('-b','--bar', help='Description for bar argument', required=True)
args = vars(parser.parse_args())

```

`args` will be a dictionary containing the arguments:

```
if args['foo'] == 'Hello':
    # code here

if args['bar'] == 'World':
    # code here

```

In your case simply add only one argument."
"Does any standard ""comes with batteries"" method exist to clear the terminal screen programatically from a Python script, or do I have to use [curses](https://en.wikipedia.org/wiki/Curses_%28programming_library%29) (the libraries, not the words)?","A simple and cross-platform solution would be to use either the `cls` command on Windows, or `clear` on Unix systems. Used with [`os.system`](http://docs.python.org/3/library/os.html#os.system), this makes a nice one-liner:

```
import os
os.system('cls' if os.name == 'nt' else 'clear')

```"
"Restarting the Django server displays the following error:

```
this port is already running....

```

This problem occurs specifically on Ubuntu and not other operating systems. How can I free up the port to restart the server?","A more simple solution just type `sudo fuser -k 8000/tcp`.
This should kill all the processes associated with port 8000.

EDIT:

For osx users you can use `sudo lsof -t -i tcp:8000 | xargs kill -9`"
"I am trying to use pip behind a proxy at work.

One of the answers from [this post](https://stackoverflow.com/questions/9698557/how-to-use-pip-on-windows-behind-an-authenticating-proxy) suggested using [CNTLM](http://cntlm.sourceforge.net/). I installed and configured it per [this other post](https://stackoverflow.com/questions/9181637/how-to-fill-proxy-information-in-cntlm-config-file), but running `cntlm.exe -c cntlm.ini -I -M http://google.com` gave the error `Connection to proxy failed, bailing out`.

I also tried `pip install -â€“proxy=user:pass@localhost:3128` (the default CNTLM port) but that raised `Cannot fetch index base URL http://pypi.python.org/simple/`. Clearly something's up with the proxy.

Does anyone know how to check more definitively whether CNTLM is set up right, or if there's another way around this altogether? I know you can also set the `http_proxy` environment variable as described [here](https://stackoverflow.com/questions/11726881/how-to-set-an-http-proxy-in-python-2-7) but I'm not sure what credentials to put in. The ones from `cntlm.ini`?","With Ubuntu I could not get the proxy option to work as advertised â€“ so the following command did **not** work:

```
sudo pip --proxy http://web-proxy.mydomain.com install somepackage

```

But exporting the `https_proxy` environment variable (note it's `https_proxy` not `http_proxy`) did the trick:

```
export https_proxy=http://web-proxy.mydomain.com

```

Then:

```
sudo -E pip install somepackage

```"
"I'm using a `for` loop to read a file, but I only want to read specific lines, say line `#26` and `#30`. Is there any built-in feature to achieve this?","If the file to read is big, and you don't want to read the whole file in memory at once:

```
fp = open(""file"")
for i, line in enumerate(fp):
    if i == 25:
        # 26th line
    elif i == 29:
        # 30th line
    elif i > 29:
        break
fp.close()

```

Note that `i == n-1` for the `n`th line.

---

In Python 2.6 or later:

```
with open(""file"") as fp:
    for i, line in enumerate(fp):
        if i == 25:
            # 26th line
        elif i == 29:
            # 30th line
        elif i > 29:
            break

```"
"I am reading a csv file into pandas. This csv file consists of four columns and some rows, but does not have a header row, which I want to add. I have been trying the following:

```
Cov = pd.read_csv(""path/to/file.txt"", sep='\t')
Frame = pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])
Frame.to_csv(""path/to/file.txt"", sep='\t')

```

But when I apply the code, I get the following Error:

```
ValueError: Shape of passed values is (1, 1), indices imply (4, 1)

```

What exactly does the error mean? And what would be a clean way in python to add a header row to my csv file/pandas df?","You can use `names` directly in the [`read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)

> names : array-like, default None List of column names to use. If file
> contains no header row, then you should explicitly pass header=None

```
Cov = pd.read_csv(""path/to/file.txt"", 
                  sep='\t', 
                  names=[""Sequence"", ""Start"", ""End"", ""Coverage""])

```"
"How to draw a rectangle on an image, like this:
[![enter image description here](https://i.sstatic.net/KWG46.jpg)](https://i.sstatic.net/KWG46.jpg)

```
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
im = np.array(Image.open('dog.png'), dtype=np.uint8)
plt.imshow(im)

```

To make it clear, I meant to draw a rectangle on top of the image for visualization, not to change the image data.

So using [matplotlib.patches.Patch](https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Patch.html) would be the best option.","You can add a [`Rectangle`](https://matplotlib.org/api/_as_gen/matplotlib.patches.Rectangle.html#matplotlib.patches.Rectangle) patch to the matplotlib Axes.

For example (using the image from the tutorial [here](https://matplotlib.org/stable/tutorials/introductory/images.html#sphx-glr-tutorials-introductory-images-py)):

```
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image

im = Image.open('stinkbug.png')

# Create figure and axes
fig, ax = plt.subplots()

# Display the image
ax.imshow(im)

# Create a Rectangle patch
rect = patches.Rectangle((50, 100), 40, 30, linewidth=1, edgecolor='r', facecolor='none')

# Add the patch to the Axes
ax.add_patch(rect)

plt.show()

```

[![enter image description here](https://i.sstatic.net/4MJtp.png)](https://i.sstatic.net/4MJtp.png)"
"I'm trying to develop a simple web scraper. I want to extract plain text without HTML markup. My code works on plain (static) HTML, but not when content is generated by JavaScript embedded in the page.

In particular, when I use `urllib2.urlopen(request)` to read the page content, it doesn't show anything that would be added by the JavaScript code, because that code *isn't executed* anywhere. Normally it would be run by the web browser, but that isn't a part of my program.

How can I access this dynamic content from within my Python code?

---

See also [Can scrapy be used to scrape dynamic content from websites that are using AJAX?](https://stackoverflow.com/questions/8550114) for answers specific to Scrapy.

See also [How can I scroll a web page using selenium webdriver in python?](https://stackoverflow.com/questions/20986631) for handling a specific sort of dynamic content via Selenium.","EDIT Sept 2021: `phantomjs` isn't maintained any more, either

EDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end.

dryscape isn't maintained anymore and the library dryscape developers recommend is Python 2 only. I have found using Selenium's python library with Phantom JS as a web driver fast enough and easy to get the work done.

Once you have installed [Phantom JS](http://phantomjs.org/download.html), make sure the `phantomjs` binary is available in the current path:

```
phantomjs --version
# result:
2.1.1

```

#Example
To give an example, I created a sample page with following HTML code. ([link](http://avi.im/stuff/js-or-no-js.html)):

```
<!DOCTYPE html>
<html>
<head>
  <meta charset=""utf-8"">
  <title>Javascript scraping test</title>
</head>
<body>
  <p id='intro-text'>No javascript support</p>
  <script>
     document.getElementById('intro-text').innerHTML = 'Yay! Supports javascript';
  </script> 
</body>
</html>

```

without javascript it says: `No javascript support` and with javascript: `Yay! Supports javascript`

#Scraping without JS support:

```
import requests
from bs4 import BeautifulSoup
response = requests.get(my_url)
soup = BeautifulSoup(response.text)
soup.find(id=""intro-text"")
# Result:
<p id=""intro-text"">No javascript support</p>

```

#Scraping with JS support:

```
from selenium import webdriver
driver = webdriver.PhantomJS()
driver.get(my_url)
p_element = driver.find_element_by_id(id_='intro-text')
print(p_element.text)
# result:
'Yay! Supports javascript'

```

---

You can also use Python library [dryscrape](https://github.com/niklasb/dryscrape) to scrape javascript driven websites.

#Scraping with JS support:

```
import dryscrape
from bs4 import BeautifulSoup
session = dryscrape.Session()
session.visit(my_url)
response = session.body()
soup = BeautifulSoup(response)
soup.find(id=""intro-text"")
# Result:
<p id=""intro-text"">Yay! Supports javascript</p>

```"
"When should I use `.eval()`? I understand it is supposed to allow me to ""evaluate my model"". How do I turn it back off for training?

Example training [code](https://github.com/natanielruiz/deep-head-pose/blob/master/code/train_hopenet.py) using `.eval()`.","`model.eval()` is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation, and `.eval()` will do it for you. In addition, the common practice for evaluating/validation is using `torch.no_grad()` in pair with `model.eval()` to turn off gradients computation:

```
# evaluate model:
model.eval()

with torch.no_grad():
    ...
    out_data = model(data)
    ...

```

BUT, don't forget to turn back to `training` mode after eval step:

```
# training step
...
model.train()
...

```"
"In a python script I am writing, I am trying to log events using the logging module. I have the following code to configure my logger:

```
ERROR_FORMAT = ""%(levelname)s at %(asctime)s in %(funcName)s in %(filename) at line %(lineno)d: %(message)s""
DEBUG_FORMAT = ""%(lineno)d in %(filename)s at %(asctime)s: %(message)s""
LOG_CONFIG = {'version':1,
              'formatters':{'error':{'format':ERROR_FORMAT},
                            'debug':{'format':DEBUG_FORMAT}},
              'handlers':{'console':{'class':'logging.StreamHandler',
                                     'formatter':'debug',
                                     'level':logging.DEBUG},
                          'file':{'class':'logging.FileHandler',
                                  'filename':'/usr/local/logs/DatabaseUpdate.log',
                                  'formatter':'error',
                                  'level':logging.ERROR}},
              'root':{'handlers':('console', 'file')}}
logging.config.dictConfig(LOG_CONFIG)

```

When I try to run `logging.debug(""Some string"")`, I get no output to the console, even though [this page in the docs](http://docs.python.org/library/logging.html#logging.debug) says that `logging.debug` should have the root logger output the message. Why is my program not outputting anything, and how can I fix it?","Many years later there seems to still be a usability problem with the Python logger. Here's some explanations with examples:

```
import logging
# This sets the root logger to write to stdout (your console).
# Your script/app needs to call this somewhere at least once.
logging.basicConfig()

# By default the root logger is set to WARNING and all loggers you define
# inherit that value. Here we set the root logger to NOTSET. This logging
# level is automatically inherited by all existing and new sub-loggers
# that do not set a less verbose level.
logging.root.setLevel(logging.NOTSET)

# The following line sets the root logger level as well.
# It's equivalent to both previous statements combined:
logging.basicConfig(level=logging.NOTSET)


# You can either share the `logger` object between all your files or the
# name handle (here `my-app`) and call `logging.getLogger` with it.
# The result is the same.
handle = ""my-app""
logger1 = logging.getLogger(handle)
logger2 = logging.getLogger(handle)
# logger1 and logger2 point to the same object:
# (logger1 is logger2) == True

logger = logging.getLogger(""my-app"")
# Convenient methods in order of verbosity from highest to lowest
logger.debug(""this will get printed"")
logger.info(""this will get printed"")
logger.warning(""this will get printed"")
logger.error(""this will get printed"")
logger.critical(""this will get printed"")


# In large applications where you would like more control over the logging,
# create sub-loggers from your main application logger.
component_logger = logger.getChild(""component-a"")
component_logger.info(""this will get printed with the prefix `my-app.component-a`"")

# If you wish to control the logging levels, you can set the level anywhere 
# in the hierarchy:
#
# - root
#   - my-app
#     - component-a
#

# Example for development:
logger.setLevel(logging.DEBUG)

# If that prints too much, enable debug printing only for your component:
component_logger.setLevel(logging.DEBUG)


# For production you rather want:
logger.setLevel(logging.WARNING)

```

---

A common source of confusion comes from a badly initialised root logger. Consider this:

```
import logging
log = logging.getLogger(""myapp"")
log.warning(""woot"")
logging.basicConfig()
log.warning(""woot"")

```

Output:

```
woot
WARNING:myapp:woot

```

Depending on your runtime environment and logging levels, **the first log line (before basic config) might not show up anywhere**."
"I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.

Any ideas?","```
df = df.replace({np.nan: None})

```

Note: [For pandas versions <1.4,](https://github.com/pandas-dev/pandas/issues/35268) this changes the dtype of **all *affected* columns** to `object`.  
To avoid that, use this syntax instead:

```
df = df.replace(np.nan, None)

```

Note 2: If you don't want to import numpy, `np.nan` can be replaced with native `float('nan')`:

```
df = df.replace({float('nan'): None})

```

Credit goes to this guy here on [this Github issue](https://github.com/pandas-dev/pandas/issues/17494#issuecomment-328966324), [Killian Huyghe](https://stackoverflow.com/users/12094598/killian-huyghe)'s comment and [Matt's answer](https://stackoverflow.com/a/78648622/4960855)."
"A 2D array can be reshaped into a 1D array using `.reshape(-1)`.
For example:

```
>>> a = numpy.array([[1, 2, 3, 4], [5, 6, 7, 8]])
>>> a.reshape(-1)
array([[1, 2, 3, 4, 5, 6, 7, 8]])

```

Usually, `array[-1]` means the last element.
But what does -1 mean here?","The criterion to satisfy for providing the new shape is that *'The new shape should be compatible with the original shape'*

numpy allow us to give one of new shape parameter as -1 (eg: (2,-1) or (-1,3) but not (-1, -1)). It simply means that it is an unknown dimension and we want numpy to figure it out. And numpy will figure this by looking at the *'length of the array and remaining dimensions'* and making sure it satisfies the above mentioned criteria

Now see the example.

```
z = np.array([[1, 2, 3, 4],
         [5, 6, 7, 8],
         [9, 10, 11, 12]])
z.shape
(3, 4)

```

Now trying to reshape with (-1) . Result new shape is (12,) and is compatible with original shape (3,4)

```
z.reshape(-1)
array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])

```

Now trying to reshape with (-1, 1) . We have provided column as 1 but rows as unknown . So we get result new shape as (12, 1).again compatible with original shape(3,4)

```
z.reshape(-1,1)
array([[ 1],
   [ 2],
   [ 3],
   [ 4],
   [ 5],
   [ 6],
   [ 7],
   [ 8],
   [ 9],
   [10],
   [11],
   [12]])

```

The above is consistent with `numpy` advice/error message, to use `reshape(-1,1)` for a single feature; i.e. single column

> Reshape your data using `array.reshape(-1, 1)` if your data has a **single feature**

New shape as (-1, 2). row unknown, column 2. we get result new shape as (6, 2)

```
z.reshape(-1, 2)
array([[ 1,  2],
   [ 3,  4],
   [ 5,  6],
   [ 7,  8],
   [ 9, 10],
   [11, 12]])

```

Now trying to keep column as unknown. New shape as (1,-1). i.e, row is 1, column unknown. we get result new shape as (1, 12)

```
z.reshape(1,-1)
array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])

```

The above is consistent with `numpy` advice/error message, to use `reshape(1,-1)` for a single sample; i.e. single row

> Reshape your data using `array.reshape(1, -1)` if it contains a **single sample**

New shape (2, -1). Row 2, column unknown. we get result new shape as (2,6)

```
z.reshape(2, -1)
array([[ 1,  2,  3,  4,  5,  6],
   [ 7,  8,  9, 10, 11, 12]])

```

New shape as (3, -1). Row 3, column unknown. we get result new shape as (3,4)

```
z.reshape(3, -1)
array([[ 1,  2,  3,  4],
   [ 5,  6,  7,  8],
   [ 9, 10, 11, 12]])

```

And finally, if we try to provide both dimension as unknown i.e new shape as (-1,-1). It will throw an error

```
z.reshape(-1, -1)
ValueError: can only specify one unknown dimension

```"
"I want to be able to output the current loop iteration to my template.

According to [the docs](https://jinja.palletsprojects.com/en/3.0.x/templates/), there is a `loop.counter` variable that I am trying to use:

```
<ul>
{% for user in userlist %}
  <li>
      {{ user }} {{loop.counter}}
  </li>
      {% if loop.counter == 1 %}
          This is the First user
      {% endif %}
{% endfor %}
</ul>

```

But is being outputed to my template. What is the correct syntax?","The counter variable inside the loop is called `loop.index` in Jinja2.

```
>>> from jinja2 import Template

>>> s = ""{% for element in elements %}{{loop.index}} {% endfor %}""
>>> Template(s).render(elements=[""a"", ""b"", ""c"", ""d""])
1 2 3 4

```

In addition to `loop.index`, there is also

* `loop.index0` (index starting at `0`)
* `loop.revindex` (reverse index; ending at `1`)
* `loop.revindex0` (reverse index; ending at `0`)
* Even more at <http://jinja.pocoo.org/docs/templates/>."
"How do I upgrade all my python packages from requirements.txt file using pip command?

tried with below command

```
$ pip install --upgrade -r requirements.txt

```

Since, the python packages are suffixed with the version number (`Django==1.5.1`) they don't seem to upgrade. Is there any better approach than manually editing requirements.txt file?

**EDIT**

As Andy mentioned in his answer packages are pinned to a specific version, hence it is not possible to upgrade packages through pip command.

But, we can achieve this with `pip-tools` using the following command.

```
$ pip-review --auto

```

this will automatically upgrade all packages from requirements.txt (make sure to install `pip-tools` using pip install command).","*I already answered this question [here](https://stackoverflow.com/a/43642193/1552520). Here's my solution:*

Because there was no easy way for upgrading package by package, and updating the requirements.txt file, I wrote this [**pip-upgrader**](https://github.com/simion/pip-upgrader) which **also updates the versions in your `requirements.txt` file** for the packages chosen (or all packages).

**Installation**

```
pip install pip-upgrader

```

**Usage**

Activate your virtualenv (important, because it will also install the new versions of upgraded packages in current virtualenv).

`cd` into your project directory, then run:

```
pip-upgrade

```

**Advanced usage**

If the requirements are placed in a non-standard location, send them as arguments:

```
pip-upgrade path/to/requirements.txt

```

If you already know what package you want to upgrade, simply send them as arguments:

```
pip-upgrade -p django -p celery -p dateutil

```

If you need to upgrade to pre-release / post-release version, add `--prerelease` argument to your command.

Full disclosure: I wrote this package."
"Is output buffering enabled by default in Python's interpreter for `sys.stdout`?

If the answer is positive, what are all the ways to disable it?

Suggestions so far:

1. Use the `-u` command line switch
2. Wrap `sys.stdout` in an object that flushes after every write
3. Set `PYTHONUNBUFFERED` env var
4. `sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)`

Is there any other way to set some global flag in `sys`/`sys.stdout` programmatically during execution?

---

If you just want to flush after a specific write using `print`, see [How can I flush the output of the print function?](https://stackoverflow.com/questions/230751).","From [Magnus Lycka answer on a mailing list](http://mail.python.org/pipermail/tutor/2003-November/026645.html):

> You can skip buffering for a whole
> python process using `python -u`
> or by
> setting the environment variable
> PYTHONUNBUFFERED.
>
> You could also replace sys.stdout with
> some other stream like wrapper which
> does a flush after every call.
>
> ```
> class Unbuffered(object):
>    def __init__(self, stream):
>        self.stream = stream
>    def write(self, data):
>        self.stream.write(data)
>        self.stream.flush()
>    def writelines(self, datas):
>        self.stream.writelines(datas)
>        self.stream.flush()
>    def __getattr__(self, attr):
>        return getattr(self.stream, attr)
>
> import sys
> sys.stdout = Unbuffered(sys.stdout)
> print 'Hello'
>
> ```"
Does `time.time()` in the Python time module return the system's time or the time in UTC?,"The [`time.time()`](https://docs.python.org/library/time.html#time.time) function returns the number of seconds since the epoch, as a float. Note that “[the epoch](https://en.wikipedia.org/wiki/Unix_epoch)” is defined as the start of January 1st, 1970 in UTC. So the epoch is defined in terms of UTC and establishes a global moment in time. No matter where on Earth you are, “seconds past epoch” (`time.time()`) returns the same value at the same moment.

Here is some sample output I ran on my computer, converting it to a string as well.

```
>>> import time
>>> ts = time.time()
>>> ts
1355563265.81
>>> import datetime
>>> datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
'2012-12-15 01:21:05'
>>>

```

The `ts` variable is the time returned in seconds. I then converted it to a human-readable string using the `datetime` library."
"How do I create or use a global variable inside a function?

How do I use a global variable that was defined in one function inside other functions?

---

Failing to use the `global` keyword where appropriate often causes `UnboundLocalError`. The precise rules for this are explained at [UnboundLocalError on local variable when reassigned after first use](https://stackoverflow.com/questions/370357). Generally, please close other questions as a duplicate of *that* question when an explanation is sought, and *this* question when someone simply needs to know the `global` keyword.","You can use a global variable within other functions by declaring it as `global` **within each function that assigns a value to it**:

```
globvar = 0

def set_globvar_to_one():
    global globvar    # Needed to modify global copy of globvar
    globvar = 1

def print_globvar():
    print(globvar)     # No need for global declaration to read value of globvar

set_globvar_to_one()
print_globvar()       # Prints 1

```

Since it's unclear whether `globvar = 1` is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the `global` keyword.

See other answers if you want to share a global variable across modules."
"I have a date `""10/10/11(m-d-y)""` and I want to add 5 days to it using a Python script. Please consider a general solution that works on the month ends also.

I am using following code:

```
import re
from datetime import datetime

StartDate = ""10/10/11""

Date = datetime.strptime(StartDate, ""%m/%d/%y"")

```

`print Date` -> is printing `'2011-10-10 00:00:00'`

Now I want to add 5 days to this date. I used the following code:

```
EndDate = Date.today()+timedelta(days=10)

```

Which returned this error:

```
name 'timedelta' is not defined

```","The previous answers are correct but it's generally a better practice to do:

```
import datetime

```

Then you'll have, using [`datetime.timedelta`](https://docs.python.org/2/library/datetime.html#datetime.timedelta):

```
date_1 = datetime.datetime.strptime(start_date, ""%m/%d/%y"")

end_date = date_1 + datetime.timedelta(days=10)

```"
"Given:

```
test = np.array([[1, 2], [3, 4], [5, 6]])

```

`test[i]` gives the *ith* row (e.g. `[1, 2]`). How do I access the *ith* column? (e.g. `[1, 3, 5]`). Also, would this be an expensive operation?","With:

```
test = np.array([[1, 2], [3, 4], [5, 6]])

```

To access column 0:

```
>>> test[:, 0]
array([1, 3, 5])

```

To access row 0:

```
>>> test[0, :]
array([1, 2])

```

This is covered in Section 1.4 (Indexing) of the [NumPy reference](http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html). This is quick, at least in my experience. It's certainly much quicker than accessing each element in a loop."
"How can I get a list of the values in a dict in Python?

In Java, getting the values of a Map as a List is as easy as doing `list = map.values();`. I'm wondering if there is a similarly simple way in Python to get a list of values from a dict.","[`dict.values`](https://docs.python.org/library/stdtypes.html#dict.values) returns a [*view*](https://docs.python.org/3/library/stdtypes.html#dict-views) of the dictionary's values, so you have to wrap it in [`list`](https://docs.python.org/3/library/stdtypes.html#list):

```
list(d.values())

```"
How do I determine whether a given integer is between two other integers (e.g. greater than/equal to `10000` and less than/equal to `30000`)?,"```
if 10000 <= number <= 30000:
    pass

```

For details, see the [docs](https://docs.python.org/3/reference/expressions.html#comparisons)."
"I want to delete the file `filename` if it exists. Is it proper to say

```
if os.path.exists(filename):
    os.remove(filename)

```

Is there a better way? A one-line way?","A more pythonic way would be:

```
try:
    os.remove(filename)
except OSError:
    pass

```

Although this takes even more lines and looks very ugly, it avoids the unnecessary call to `os.path.exists()` and follows the python convention of overusing exceptions.

It may be worthwhile to write a function to do this for you:

```
import os, errno

def silentremove(filename):
    try:
        os.remove(filename)
    except OSError as e: # this would be ""except OSError, e:"" before Python 2.6
        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory
            raise # re-raise exception if a different error occurred

```"
"How can I use ORDER BY `descending` in a SQLAlchemy query like the following?

This query works, but returns them in ascending order:

```
query = (model.Session.query(model.Entry)
        .join(model.ClassificationItem)
        .join(model.EnumerationValue)
        .filter_by(id=c.row.id)
        .order_by(model.Entry.amount) #Â This row :)
        )

```

If I try:

```
.order_by(desc(model.Entry.amount))

```

then I get: `NameError: global name 'desc' is not defined`.","Just as an FYI, you can also specify those things as column attributes. For instance, I might have done:

```
.order_by(model.Entry.amount.desc())

```

This is handy since it avoids an `import`, and you can use it on other places such as in a relation definition, etc.

For more information, you can refer this [SQLAlchemy 1.4 Documentation](https://docs.sqlalchemy.org/en/14/core/sqlelement.html#sqlalchemy.sql.expression.desc)"
"I'm using the Python bindings to run Selenium WebDriver:

```
from selenium import webdriver
wd = webdriver.Firefox()

```

I know I can grab a webelement like so:

```
elem = wd.find_element_by_css_selector('#my-id')

```

And I know I can get the full page source with...

```
wd.page_source

```

But is there a way to get the ""element source""?

```
elem.source   # <-- returns the HTML as a string

```

The Selenium WebDriver documentation for Python are basically non-existent and I don't see anything in the code that seems to enable that functionality.

What is the best way to access the HTML of an element (and its children)?","You can read the `innerHTML` attribute to get the source of the *content* of the element or `outerHTML` for the source with the current element.

Python:

```
element.get_attribute('innerHTML')

```

Java:

```
elem.getAttribute(""innerHTML"");

```

C#:

```
element.GetAttribute(""innerHTML"");

```

Ruby:

```
element.attribute(""innerHTML"")

```

JavaScript:

```
element.getAttribute('innerHTML');

```

PHP:

```
$element->getAttribute('innerHTML');

```

It was tested and worked with the `ChromeDriver`."
"I am getting a lot of decimals in the output of this code (Fahrenheit to Celsius converter).

My code currently looks like this:

```
def main():
    printC(formeln(typeHere()))

def typeHere():
    global Fahrenheit
    try:
        Fahrenheit = int(raw_input(""Hi! Enter Fahrenheit value, and get it in Celsius!\n""))
    except ValueError:
        print ""\nYour insertion was not a digit!""
        print ""We've put your Fahrenheit value to 50!""
        Fahrenheit = 50
    return Fahrenheit

def formeln(c):
    Celsius = (Fahrenheit - 32.00) * 5.00/9.00
    return Celsius

def printC(answer):
    answer = str(answer)
    print ""\nYour Celsius value is "" + answer + "" C.\n""



main()

```

So my question is, how do I make the program round every answer to the 2nd decimal place?","You can use the [`round`](http://docs.python.org/library/functions.html#round) function, which takes as its first argument the number and the second argument is the precision after the decimal point.

In your case, it would be:

```
answer = str(round(answer, 2))

```"
"How do I increase the figure size for [this figure](https://matplotlib.org/2.0.2/examples/pylab_examples/subplots_demo.html)?

This does nothing:

```
f.figsize(15, 15)

```

Example code from the link:

```
import matplotlib.pyplot as plt
import numpy as np

# Simple data to display in various forms
x = np.linspace(0, 2 * np.pi, 400)
y = np.sin(x ** 2)

plt.close('all')

# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

# Two subplots, the axes array is 1-d
f, axarr = plt.subplots(2, sharex=True)
axarr[0].plot(x, y)
axarr[0].set_title('Sharing X axis')
axarr[1].scatter(x, y)

# Two subplots, unpack the axes array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

# Three subplots sharing both x/y axes
f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing both axes')
ax2.scatter(x, y)
ax3.scatter(x, 2 * y ** 2 - 1, color='r')
# Fine-tune figure; make subplots close to each other and hide x ticks for
# all but bottom plot.
f.subplots_adjust(hspace=0)
plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)

# row and column sharing
f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')
ax1.plot(x, y)
ax1.set_title('Sharing x per column, y per row')
ax2.scatter(x, y)
ax3.scatter(x, 2 * y ** 2 - 1, color='r')
ax4.plot(x, 2 * y ** 2 - 1, color='r')

# Four axes, returned as a 2-d array
f, axarr = plt.subplots(2, 2)
axarr[0, 0].plot(x, y)
axarr[0, 0].set_title('Axis [0,0]')
axarr[0, 1].scatter(x, y)
axarr[0, 1].set_title('Axis [0,1]')
axarr[1, 0].plot(x, y ** 2)
axarr[1, 0].set_title('Axis [1,0]')
axarr[1, 1].scatter(x, y ** 2)
axarr[1, 1].set_title('Axis [1,1]')
# Fine-tune figure; hide x ticks for top plots and y ticks for right plots
plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)
plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)

# Four polar axes
f, axarr = plt.subplots(2, 2, subplot_kw=dict(projection='polar'))
axarr[0, 0].plot(x, y)
axarr[0, 0].set_title('Axis [0,0]')
axarr[0, 1].scatter(x, y)
axarr[0, 1].set_title('Axis [0,1]')
axarr[1, 0].plot(x, y ** 2)
axarr[1, 0].set_title('Axis [1,0]')
axarr[1, 1].scatter(x, y ** 2)
axarr[1, 1].set_title('Axis [1,1]')
# Fine-tune figure; make subplots farther from each other.
f.subplots_adjust(hspace=0.3)

plt.show()

```","Use [`.set_figwidth`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.set_figwidth) and [`.set_figheight`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.set_figheight) on the [`matplotlib.figure.Figure`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure) object returned by [`plt.subplots()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html), or set both with [`f.set_size_inches(w, h)`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.set_size_inches).

```
f.set_figheight(15)
f.set_figwidth(15)

```

**Note:** Unlike `set_size_inches()`, where the measurement unit is explicitly mentioned in the function's name, this is not the case for `set_figwidth()` and `set_figheight()`, which also use inches. This information is provided by the documentation of the function.

Alternatively, when using `.subplots()` to create a new figure, specify `figsize=`:

```
f, axs = plt.subplots(2, 2, figsize=(15, 15))

```

`.subplots` accepts `**fig_kw`, which are passed to [`pyplot.figure`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure), and is where `figsize` can be found.

Setting the figure's size may trigger the `ValueError` exception:

```
Image size of 240000x180000 pixels is too large. It must be less than 2^16 in each direction

```

This is a common problem for using the `set_fig*()` functions due to the assumptions that they work with pixels and not inches (obviously 240000\*180000 inches is too much)."
"I am getting error `Expecting value: line 1 column 1 (char 0)` when trying to decode JSON.

The URL I use for the API call works fine in the browser, but gives this error when done through a curl request. The following is the code I use for the curl request.

The error happens at `return simplejson.loads(response_json)`

```
response_json = self.web_fetch(url)
response_json = response_json.decode('utf-8')
return json.loads(response_json)


def web_fetch(self, url):
    buffer = StringIO()
    curl = pycurl.Curl()
    curl.setopt(curl.URL, url)
    curl.setopt(curl.TIMEOUT, self.timeout)
    curl.setopt(curl.WRITEFUNCTION, buffer.write)
    curl.perform()
    curl.close()
    response = buffer.getvalue().strip()
    return response

```

Traceback:

```
File ""/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/django/core/handlers/base.py"" in get_response
  111.                         response = callback(request, *callback_args, **callback_kwargs)
File ""/Users/nab/Desktop/pricestore/pricemodels/views.py"" in view_category
  620.     apicall=api.API().search_parts(category_id= str(categoryofpart.api_id), manufacturer = manufacturer, filter = filters, start=(catpage-1)*20, limit=20, sort_by='[[""mpn"",""asc""]]')
File ""/Users/nab/Desktop/pricestore/pricemodels/api.py"" in search_parts
  176.         return simplejson.loads(response_json)
File ""/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/simplejson/__init__.py"" in loads
  455.         return _default_decoder.decode(s)
File ""/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/simplejson/decoder.py"" in decode
  374.         obj, end = self.raw_decode(s)
File ""/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/simplejson/decoder.py"" in raw_decode
  393.         return self.scan_once(s, idx=_w(s, idx).end())

Exception Type: JSONDecodeError at /pricemodels/2/dir/
Exception Value: Expecting value: line 1 column 1 (char 0)

```","Your code produced an empty response body; you'd want to check for that or catch the exception raised. It is possible the server responded with a 204 No Content response, or a non-200-range status code was returned (404 Not Found, etc.). Check for this.

Note:

* There is no need to decode a response from UTF8 to Unicode, the `json.loads()` method can handle UTF8-encoded data natively.
* `pycurl` has a very archaic API. Unless you have a specific requirement for using it, there are better choices.

Either [`requests`](https://requests.readthedocs.io) or [`httpx`](https://www.python-httpx.org/) offer much friendlier APIs, including JSON support.

If you can, replace your call with the following `httpx` code:

```
import httpx

response = httpx.get(url)
response.raise_for_status()  # raises exception when not a 2xx response
if response.status_code != 204:
    return response.json()

```

Of course, this won't protect you from a URL that doesn't comply with HTTP standards; when using arbitrary URLs where this is a possibility, check if the server intended to give you JSON by checking the Content-Type header, and for good measure catch the exception:

```
if (
    response.status_code != 204 and
    response.headers[""content-type""].strip().startswith(""application/json"")
):
    try:
        return response.json()
    except ValueError:
        # decide how to handle a server that's misbehaving to this extent

```"
How do I get the current time in Python?,"Use [`datetime`](https://docs.python.org/3/library/datetime.html):

```
>>> import datetime
>>> now = datetime.datetime.now()
>>> now
datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)
>>> print(now)
2009-01-06 15:08:24.789150

```

For just the clock time without the date:

```
>>> now.time()
datetime.time(15, 8, 24, 78915)
>>> print(now.time())
15:08:24.789150

```

---

To save typing, you can import the `datetime` object from the [`datetime`](https://docs.python.org/3/library/datetime.html) module:

```
>>> from datetime import datetime

```

Then remove the prefix `datetime.` from all of the above."
"I downloaded pip and ran `python setup.py install` and everything worked just fine. The very next step in the tutorial is to run `pip install <lib you want>` but before it even tries to find anything online I get an error ""bash: pip: command not found"".

This is on Mac OS X. I'm assuming there's some kind of path setting that was not set correctly when I ran `setup.py`. How can I investigate further? What do I need to check to get a better idea of the exact cause of the problem?

**EDIT:** I also tried installing Python 2.7 for Mac in the hopes that the friendly install process would do any housekeeping like editing PATH and whatever else needs to happen for everything to work according to the tutorials, but this didn't work. After installing, running 'python' still ran Python 2.6 and PATH was not updated.","Why not just do `sudo easy_install pip` or if this is for python 2.6 `sudo easy_install-2.6 pip`?

This installs pip using the default python package installer system and saves you the hassle of manual set-up all at the same time.

This will allow you to then run the `pip` command for python package installation as it will be installed with the system python. I also recommend once you have pip using the [virtualenv](https://pypi.python.org/pypi/virtualenv) package and pattern. :)"
"I have a file called `tester.py`, located on `/project`.

`/project` has a subdirectory called `lib`, with a file called `BoxTime.py`:

```
/project/tester.py
/project/lib/BoxTime.py

```

I want to import `BoxTime` from `tester`. I have tried this:

```
import lib.BoxTime

```

Which resulted:

```
Traceback (most recent call last):
  File ""./tester.py"", line 3, in <module>
    import lib.BoxTime
ImportError: No module named lib.BoxTime

```

Any ideas how to import `BoxTime` from the subdirectory?

**EDIT**

The `__init__.py` was the problem, but don't forget to refer to `BoxTime` as `lib.BoxTime`, or use:

```
import lib.BoxTime as BT
...
BT.bt_function()

```","Take a look at the **[Packages documentation (Section 6.4)](https://docs.python.org/3/tutorial/modules.html#packages)**.

In short, you need to put a blank file named

```
__init__.py

```

in the `lib` directory."
"How do I count the number of `0`s and `1`s in the following array?

```
y = np.array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])

```

---

`y.count(0)` gives:

> `numpy.ndarray` object has no attribute `count`","Using [`numpy.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html):

```
import numpy
a = numpy.array([0, 3, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 1, 3, 4])
unique, counts = numpy.unique(a, return_counts=True)

>>> dict(zip(unique, counts))
{0: 7, 1: 4, 2: 1, 3: 2, 4: 1}

```

**Non-numpy method** using [`collections.Counter`](https://docs.python.org/2/library/collections.html#collections.Counter);

```
import collections, numpy
a = numpy.array([0, 3, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 1, 3, 4])
counter = collections.Counter(a)

>>> counter
Counter({0: 7, 1: 4, 3: 2, 2: 1, 4: 1})

```"
"The function `foo` below returns a string `'foo'`. How can I get the value `'foo'` which is returned from the thread's target?

```
from threading import Thread

def foo(bar):
    print('hello {}'.format(bar))
    return 'foo'
    
thread = Thread(target=foo, args=('world!',))
thread.start()
return_value = thread.join()

```

The ""one obvious way to do it"", shown above, doesn't work: `thread.join()` returned `None`.","One way I've seen is to pass a mutable object, such as a list or a dictionary, to the thread's constructor, along with a an index or other identifier of some sort. The thread can then store its results in its dedicated slot in that object. For example:

```
def foo(bar, result, index):
    print 'hello {0}'.format(bar)
    result[index] = ""foo""

from threading import Thread

threads = [None] * 10
results = [None] * 10

for i in range(len(threads)):
    threads[i] = Thread(target=foo, args=('world!', results, i))
    threads[i].start()

# do some other stuff

for i in range(len(threads)):
    threads[i].join()

print "" "".join(results)  # what sound does a metasyntactic locomotive make?

```

If you really want `join()` to return the return value of the called function, you can do this with a `Thread` subclass like the following:

```
from threading import Thread

def foo(bar):
    print 'hello {0}'.format(bar)
    return ""foo""

class ThreadWithReturnValue(Thread):
    def __init__(self, group=None, target=None, name=None,
                 args=(), kwargs={}, Verbose=None):
        Thread.__init__(self, group, target, name, args, kwargs, Verbose)
        self._return = None
    def run(self):
        if self._Thread__target is not None:
            self._return = self._Thread__target(*self._Thread__args,
                                                **self._Thread__kwargs)
    def join(self):
        Thread.join(self)
        return self._return

twrv = ThreadWithReturnValue(target=foo, args=('world!',))

twrv.start()
print twrv.join()   # prints foo

```

That gets a little hairy because of some name mangling, and it accesses ""private"" data structures that are specific to `Thread` implementation... but it works.

For Python 3:

```
class ThreadWithReturnValue(Thread):
    
    def __init__(self, group=None, target=None, name=None,
                 args=(), kwargs={}, Verbose=None):
        Thread.__init__(self, group, target, name, args, kwargs)
        self._return = None

    def run(self):
        if self._target is not None:
            self._return = self._target(*self._args,
                                                **self._kwargs)
    def join(self, *args):
        Thread.join(self, *args)
        return self._return

```"
"I have a Pandas Dataframe as below:

```
      itm Date                  Amount 
67    420 2012-09-30 00:00:00   65211
68    421 2012-09-09 00:00:00   29424
69    421 2012-09-16 00:00:00   29877
70    421 2012-09-23 00:00:00   30990
71    421 2012-09-30 00:00:00   61303
72    485 2012-09-09 00:00:00   71781
73    485 2012-09-16 00:00:00     NaN
74    485 2012-09-23 00:00:00   11072
75    485 2012-09-30 00:00:00  113702
76    489 2012-09-09 00:00:00   64731
77    489 2012-09-16 00:00:00     NaN

```

When I try to apply a function to the Amount column, I get the following error:

```
ValueError: cannot convert float NaN to integer

```

I have tried applying a function using `math.isnan`, pandas' `.replace` method, `.sparse` data attribute from pandas 0.9, if `NaN == NaN` statement in a function; I have also looked at [this Q/A](https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r); none of them works.

How do I do it?","[`DataFrame.fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) or [`Series.fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html) will do this for you.

Example:

```
In [7]: df
Out[7]: 
          0         1
0       NaN       NaN
1 -0.494375  0.570994
2       NaN       NaN
3  1.876360 -0.229738
4       NaN       NaN

In [8]: df.fillna(0)
Out[8]: 
          0         1
0  0.000000  0.000000
1 -0.494375  0.570994
2  0.000000  0.000000
3  1.876360 -0.229738
4  0.000000  0.000000

```

To fill the NaNs in only one column, select just that column.

```
In [12]: df[1] = df[1].fillna(0)

In [13]: df
Out[13]: 
          0         1
0       NaN  0.000000
1 -0.494375  0.570994
2       NaN  0.000000
3  1.876360 -0.229738
4       NaN  0.000000

```

Or you can use the built in column-specific functionality:

```
df = df.fillna({1: 0})

```"
"What's all this fuss about Python and CPython *(Jython,IronPython)*, I don't get it:

[python.org](http://www.python.org/) mentions that CPython is:

> *The ""traditional"" implementation of Python (nicknamed CPython)*

[yet another Stack Overflow question](https://stackoverflow.com/a/2324217/2425215) mentions that:

> *CPython is the default byte-code interpreter of Python, which is written in C.*

Honestly I don't get what both of those explanations practically mean but what I thought was that, *if I use CPython does that mean when I run a sample python code, it compiles it to C language and then executes it as if it were C code*

**So what exactly is CPython and how does it differ when compared with python and should I probably use CPython over Python and if so what are its advantages?**","So what is CPython?
-------------------

CPython is the *original* Python implementation. It is the implementation you download from Python.org. People call it CPython to distinguish it from other, later, Python implementations, and to distinguish the implementation of the language engine from the Python *programming language* itself.

The latter part is where your confusion comes from; you need to keep Python-the-language separate from whatever *runs* the Python code.

CPython *happens* to be implemented in C. That is just an implementation detail, really. CPython compiles your Python code into bytecode (transparently) and interprets that bytecode in a evaluation loop.

CPython is also the first to implement new features; Python-the-language development uses CPython as the base; other implementations follow.

What about Jython, etc.?
------------------------

[Jython](http://www.jython.org/), [IronPython](http://ironpython.net/) and [PyPy](https://pypy.org/) are the current ""other"" implementations of the Python programming language; these are implemented in Java, C# and RPython (a subset of Python), respectively. Jython compiles your Python code to *Java* bytecode, so your Python code can run on the JVM. IronPython lets you run Python on the [Microsoft CLR](https://learn.microsoft.com/en-us/dotnet/standard/clr). And PyPy, being implemented in (a subset of) Python, lets you run Python code faster than CPython, which rightly should blow your mind. :-)

Actually compiling to C
-----------------------

So CPython does **not** translate your Python code to C by itself. Instead, it runs an interpreter loop. There *is* a project that *does* translate Python-ish code to C, and that is called [Cython](http://cython.org/). Cython adds a few extensions to the Python language, and lets you compile your code to C extensions, code that plugs *into* the CPython interpreter."
"I know there is a method for a Python list to return the first index of something:

```
>>> xs = [1, 2, 3]
>>> xs.index(2)
1

```

Is there something like that for NumPy arrays?","Yes, given an array, `array`, and a value, `item` to search for, you can use [`np.where`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html) as:

```
itemindex = numpy.where(array == item)

```

The result is a tuple with first all the row indices, then all the column indices.

For example, if an array is two dimensions and it contained your item at two locations then

```
array[itemindex[0][0]][itemindex[1][0]]

```

would be equal to your item and so would be:

```
array[itemindex[0][1]][itemindex[1][1]]

```"
"How can I check if any of the strings in an array exists in another string?

For example:

```
a = ['a', 'b', 'c']
s = ""a123""
if a in s:
    print(""some of the strings found in s"")
else:
    print(""no strings found in s"")

```

How can I replace the `if a in s:` line to get the appropriate result?","You can use [**`any`**](http://docs.python.org/library/functions.html#any):

```
a_string = ""A string is more than its parts!""
matches = [""more"", ""wholesome"", ""milk""]

if any(x in a_string for x in matches):

```

Similarly to check if *all* the strings from the list are found, use [**`all`**](http://docs.python.org/library/functions.html#all) instead of `any`."
"I'm looking for the Python equivalent of

```
String str = ""many   fancy word \nhello    \thi"";
String whiteSpaceRegex = ""\\s"";
String[] words = str.split(whiteSpaceRegex);

[""many"", ""fancy"", ""word"", ""hello"", ""hi""]

```","The [`str.split()`](https://docs.python.org/3/library/stdtypes.html#str.split) method without an argument splits on whitespace:

```
>>> ""many   fancy word \nhello    \thi"".split()
['many', 'fancy', 'word', 'hello', 'hi']

```"
"How can I specify the type hint of a variable as a *function type*? There is no `typing.Function`, and I could not find anything in the relevant PEP, [PEP 483](https://www.python.org/dev/peps/pep-0483/).","As [@jonrsharpe](https://stackoverflow.com/questions/37835179/how-can-i-specify-the-function-type-in-my-type-hints/39624147#comment63131590_37835179) noted in a comment, this can be done with [`collections.abc.Callable`](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable):

```
from collections.abc import Callable

def my_function(func: Callable):

```

**Note:** `Callable` on its own is equivalent to `Callable[..., Any]`.
Such a `Callable` takes *any* number and type of arguments (`...`) and returns a value of *any* type (`Any`). If this is too unconstrained, one may also specify the types of the input argument list and return type.

For example, given:

```
def sum(a: int, b: int) -> int: return a+b

```

The corresponding annotation is:

```
Callable[[int, int], int]

```

That is, the parameters are sub-scripted in the outer subscription with the return type as the second element in the outer subscription. In general:

```
Callable[[ParamType1, ParamType2, ..., ParamTypeN], ReturnType]

```"
"I know that I can do:

```
try:
    # do something that may fail
except:
    # do this if ANYTHING goes wrong

```

I can also do this:

```
try:
    # do something that may fail
except IDontLikeYouException:
    # say please
except YouAreTooShortException:
    # stand on a ladder

```

But if I want to do the same thing inside two different exceptions, the best I can think of right now is to do this:

```
try:
    # do something that may fail
except IDontLikeYouException:
    # say please
except YouAreBeingMeanException:
    # say please

```

Is there a way that I can do something like this (since the action to take in both exceptions is to `say please`):

```
try:
    # do something that may fail
except IDontLikeYouException, YouAreBeingMeanException:
    # say please

```

Now this really won't work, as it matches the syntax for:

```
try:
    # do something that may fail
except Exception, e:
    # say please

```

So, my effort to catch the two distinct exceptions doesn't exactly come through.

Is there a way to do this?","From [Python Documentation](https://docs.python.org/3/tutorial/errors.html#handling-exceptions):

> An except clause may name multiple exceptions as a parenthesized tuple, for example

```
except (IDontLikeYouException, YouAreBeingMeanException) as e:
    pass

```

Or, for Python 2 only:

```
except (IDontLikeYouException, YouAreBeingMeanException), e:
    pass

```

Separating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using `as`."
"How do I find the full path of the currently running Python interpreter from within the currently executing Python script?

---

See [How do I check which version of Python is running my script?](https://stackoverflow.com/questions/1093322) if you are specifically interested in the *version* of Python for the currently running interpreter - for example, to bail out with an error message if your script doesn't support that Python version, or conditionally disable certain modules or code paths.","`sys.executable` contains full path of the currently running Python interpreter.

```
import sys

print(sys.executable)

```

which is now [documented here](https://docs.python.org/library/sys.html)"
"After you train a model in Tensorflow:

1. How do you save the trained model?
2. How do you later restore this saved model?","In (and after) **Tensorflow version 0.11**:

**Save the model:**

```
import tensorflow as tf

#Prepare to feed input, i.e. feed_dict and placeholders
w1 = tf.placeholder(""float"", name=""w1"")
w2 = tf.placeholder(""float"", name=""w2"")
b1= tf.Variable(2.0,name=""bias"")
feed_dict ={w1:4,w2:8}

#Define a test operation that we will restore
w3 = tf.add(w1,w2)
w4 = tf.multiply(w3,b1,name=""op_to_restore"")
sess = tf.Session()
sess.run(tf.global_variables_initializer())

#Create a saver object which will save all the variables
saver = tf.train.Saver()

#Run the operation by feeding input
print sess.run(w4,feed_dict)
#Prints 24 which is sum of (w1+w2)*b1 

#Now, save the graph
saver.save(sess, 'my_test_model',global_step=1000)

```

**Restore the model:**

```
import tensorflow as tf

sess=tf.Session()    
#First let's load meta graph and restore weights
saver = tf.train.import_meta_graph('my_test_model-1000.meta')
saver.restore(sess,tf.train.latest_checkpoint('./'))


# Access saved Variables directly
print(sess.run('bias:0'))
# This will print 2, which is the value of bias that we saved


# Now, let's access and create placeholders variables and
# create feed-dict to feed new data

graph = tf.get_default_graph()
w1 = graph.get_tensor_by_name(""w1:0"")
w2 = graph.get_tensor_by_name(""w2:0"")
feed_dict ={w1:13.0,w2:17.0}

#Now, access the op that you want to run. 
op_to_restore = graph.get_tensor_by_name(""op_to_restore:0"")

print sess.run(op_to_restore,feed_dict)
#This will print 60 which is calculated 

```

This and some more advanced use-cases have been explained very well here.

[A quick complete tutorial to save and restore Tensorflow models](http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/)"
"How do you access query parameters or the query string in Flask routes? It's not obvious from the Flask documentation.

The example route `/data` below illustrates the context that I would like to access that data. If someone requests something like `example.com/data?abc=123`, I would like access to the string `?abc=123` or to be able to retrieve the value of parameters like `abc`.

```
@app.route(""/data"")
def data():
    # query_string = ???
    return render_template(""data.html"")

```","```
from flask import request

@app.route('/data')
def data():
    # here we want to get the value of user (i.e. ?user=some-value)
    user = request.args.get('user')

```"
"Which types of objects fall into the domain of ""subscriptable""?","It basically means that the object implements the `__getitem__()` method. In other words, it describes objects that are ""containers"", meaning they contain other objects. This includes strings, lists, tuples, and dictionaries."
"So, I started learning to code in [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29) and later [Django](http://en.wikipedia.org/wiki/Django_%28web_framework%29). The first times it was hard looking at tracebacks and actually figure out what I did wrong and where the syntax error was. Some time has passed now and some way along the way, I guess I got a routine in debugging my Django code. As this was done early in my coding experience, I sat down and wondered if how I was doing this was ineffective and could be done faster. I usually manage to find and correct the bugs in my code, but I wonder if I should be doing it faster?

I usually just use the debug info Django gives when enabled. When things do end up as I thought it would, I break the code flow a lot with a syntax error, and look at the variables at that point in the flow to figure out, where the code does something other than what I wanted.

But can this be improved? Are there some good tools or better ways to debug your Django code?","There are a bunch of ways to do it, but the most straightforward is to simply
use the [Python debugger](https://docs.python.org/2/library/pdb.html). Just add following line in to a Django view function:

```
import pdb; pdb.set_trace()

```

or

```
breakpoint()  #from Python3.7

```

If you try to load that page in your browser, the browser will hang and you get a prompt to carry on debugging on actual executing code.

However there are other options (I am not recommending them):

```
* return HttpResponse({variable to inspect})

* print {variable to inspect}

* raise Exception({variable to inspect})

```

But the Python Debugger (pdb) is highly recommended for all types of Python code. If you are already into pdb, you'd also want to have a look at [**IPDB**](http://pypi.python.org/pypi/ipdb/) that uses [**ipython**](http://ipython.org/) for debugging.

Some more useful extension to pdb are

[**pdb++**](https://pypi.python.org/pypi/pdbpp/), suggested by [Antash](https://stackoverflow.com/users/5792269/antash).

[**pudb**](https://pypi.python.org/pypi/pudb), suggested by [PatDuJour](https://stackoverflow.com/users/5081188/patdujour).

[**Using the Python debugger in Django**](https://mike.tig.as/blog/2010/09/14/pdb/), suggested by [Seafangs](https://stackoverflow.com/users/884640/seafangs)."
"In the pyplot document for scatter plot:

```
matplotlib.pyplot.scatter(x, y, s=20, c='b', marker='o', cmap=None, norm=None,
                              vmin=None, vmax=None, alpha=None, linewidths=None,
                              faceted=True, verts=None, hold=None, **kwargs)

```

The marker size

> s:
> size in points^2. It is a scalar or an array of the same length as x and y.

What kind of unit is `points^2`? What does it mean? Does `s=100` mean `10 pixel x 10 pixel`?

Basically I'm trying to make scatter plots with different marker sizes, and I want to figure out what does the `s` number mean.","This can be a somewhat confusing way of defining the size but you are basically specifying the *area* of the marker. This means, to double the width (or height) of the marker you need to increase `s` by a factor of 4. [because A = W*H => (2W)*(2H)=4A]

There is a reason, however, that the size of markers is defined in this way. Because of the scaling of area as the square of width, doubling the width actually appears to increase the size by more than a factor 2 (in fact it increases it by a factor of 4). To see this consider the following two examples and the output they produce.

```
# doubling the width of markers
x = [0,2,4,6,8,10]
y = [0]*len(x)
s = [20*4**n for n in range(len(x))]
plt.scatter(x,y,s=s)
plt.show()

```

gives

![enter image description here](https://i.sstatic.net/m8xcU.png)

Notice how the size increases very quickly. If instead we have

```
# doubling the area of markers
x = [0,2,4,6,8,10]
y = [0]*len(x)
s = [20*2**n for n in range(len(x))]
plt.scatter(x,y,s=s)
plt.show()

```

gives

![enter image description here](https://i.sstatic.net/Znaw8.png)

Now the apparent size of the markers increases roughly linearly in an intuitive fashion.

As for the exact meaning of what a 'point' is, it is fairly arbitrary for plotting purposes, you can just scale all of your sizes by a constant until they look reasonable.

**Edit:** (In response to comment from @Emma)

It's probably confusing wording on my part. The question asked about doubling the width of a circle so in the first picture for each circle (as we move from left to right) it's width is double the previous one so for the area this is an exponential with base 4. Similarly the second example each circle has *area* double the last one which gives an exponential with base 2.

However it is the second example (where we are scaling area) that doubling area appears to make the circle twice as big to the eye. Thus if we want a circle to appear a factor of `n` bigger we would increase the area by a factor `n` not the radius so the apparent size scales linearly with the area.

**Edit** to visualize the comment by @TomaszGandor:

This is what it looks like for different functions of the marker size:

[![Exponential, Square, or Linear size](https://i.sstatic.net/3H1BQ.png)](https://i.sstatic.net/3H1BQ.png)

```
x = [0,2,4,6,8,10,12,14,16,18]
s_exp = [20*2**n for n in range(len(x))]
s_square = [20*n**2 for n in range(len(x))]
s_linear = [20*n for n in range(len(x))]
plt.scatter(x,[1]*len(x),s=s_exp, label='$s=2^n$', lw=1)
plt.scatter(x,[0]*len(x),s=s_square, label='$s=n^2$')
plt.scatter(x,[-1]*len(x),s=s_linear, label='$s=n$')
plt.ylim(-1.5,1.5)
plt.legend(loc='center left', bbox_to_anchor=(1.1, 0.5), labelspacing=3)
plt.show()

```"
"Suppose I have a Python function as defined below:

```
def foo(arg1,arg2):
    #do something with args
    a = arg1 + arg2
    return a

```

I can get the name of the function using `foo.func_name`. How can I programmatically get its source code, as I typed above?","If the function is from a source file available on the filesystem, then [`inspect.getsource(foo)`](https://docs.python.org/3/library/inspect.html#inspect.getsource) might be of help:

If `foo` is defined as:

```
def foo(arg1,arg2):         
    #do something with args 
    a = arg1 + arg2         
    return a  

```

Then:

```
import inspect
lines = inspect.getsource(foo)
print(lines)

```

Returns:

```
def foo(arg1,arg2):         
    #do something with args 
    a = arg1 + arg2         
    return a                

```

But I believe that if the function is compiled from a string, stream or imported from a compiled file, then you cannot retrieve its source code."
"I've been learning, working, and playing with Python for a year and a half now. As a biologist slowly making the turn to bio-informatics, this language has been at the very core of all the major contributions I have made in the lab. I more or less fell in love with the way Python permits me to express beautiful solutions and also with the semantics of the language that allows such a natural flow from thoughts to workable code.

What I would like to know is your answer to a kind of question I have seldom seen in this or other forums. This question seems central to me for anyone on the path to Python improvement but who wonders what his next steps should be.

Let me sum up what I do NOT want to ask first ;)

* I don't want to know how to QUICKLY learn Python
* Nor do I want to find out the best way to get acquainted with the language
* Finally, I don't want to know a 'one trick that does it all' approach.

What I do want to know your opinion about, is:

**What are the steps YOU would recommend to a Python journeyman, from apprenticeship to guru status (feel free to stop wherever your expertise dictates it), in order that one IMPROVES CONSTANTLY, becoming a better and better Python coder, one step at a time. Some of the people on SO almost seem worthy of worship for their Python prowess, please enlighten us :)**

The kind of answers I would enjoy (but feel free to surprise the readership :P ), is formatted more or less like this:

* Read this (eg: python tutorial), pay attention to that kind of details
* Code for so manytime/problems/lines of code
* Then, read this (eg: this or that book), but this time, pay attention to this
* Tackle a few real-life problems
* Then, proceed to reading Y.
* Be sure to grasp these concepts
* Code for X time
* Come back to such and such basics or move further to...
* (you get the point :)

I really care about knowing your opinion on what exactly one should pay attention to, at various stages, in order to progress CONSTANTLY (with due efforts, of course). If you come from a specific field of expertise, discuss the path you see as appropriate in this field.

EDIT: Thanks to your great input, I'm back on the Python improvement track! I really appreciate!","I thought the process of Python mastery went something like:

1. Discover [list comprehensions](http://en.wikipedia.org/wiki/List_comprehension#Python)
2. Discover [generators](http://en.wikipedia.org/wiki/Python_syntax_and_semantics#Generators)
3. Incorporate [map, reduce, filter, iter, range, xrange](http://docs.python.org/library/functions.html) often into your code
4. Discover [Decorators](http://wiki.python.org/moin/PythonDecorators)
5. Write recursive functions, a lot
6. Discover [itertools](http://docs.python.org/library/itertools.html) and [functools](http://docs.python.org/library/functools.html)
7. Read [Real World Haskell](https://rads.stackoverflow.com/amzn/click/com/0596514980) ([read free online](http://book.realworldhaskell.org/read/))
8. Rewrite all your old Python code with tons of higher order functions, recursion, and whatnot.
9. Annoy your cubicle mates every time they present you with a Python class. Claim it could be ""better"" implemented as a dictionary plus some functions. Embrace functional programming.
10. Rediscover the [Strategy](http://en.wikipedia.org/wiki/Strategy_pattern#Python) pattern and then [all those things](https://rads.stackoverflow.com/amzn/click/com/0596007124) from imperative code you tried so hard to forget after Haskell.
11. Find a balance."
"How do I check if a column exists in a Pandas DataFrame `df`?

```
   A   B    C
0  3  40  100
1  6  30  200

```

How would I check if the column `""A""` exists in the above DataFrame so that I can compute:

```
df['sum'] = df['A'] + df['C']

```

And if `""A""` doesn't exist:

```
df['sum'] = df['B'] + df['C']

```","This will work:

```
if 'A' in df:

```

But for clarity, I'd probably write it as:

```
if 'A' in df.columns:

```"
Is there an obvious way to do this that I'm missing? I'm just trying to make thumbnails.,"Define a maximum size.
Then, compute a resize ratio by taking `min(maxwidth/width, maxheight/height)`.

The proper size is `oldsize*ratio`.

There is of course also a library method to do this: the method `Image.thumbnail`.  
Below is an (edited) example from the [PIL documentation](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=thumbnail#PIL.Image.Image.thumbnail).

```
import os, sys
import Image

size = 128, 128

for infile in sys.argv[1:]:
    outfile = os.path.splitext(infile)[0] + "".thumbnail""
    if infile != outfile:
        try:
            im = Image.open(infile)
            im.thumbnail(size, Image.Resampling.LANCZOS)
            im.save(outfile, ""JPEG"")
        except IOError:
            print ""cannot create thumbnail for '%s'"" % infile

```"
How do I copy a file in Python?,"[`shutil`](http://docs.python.org/3/library/shutil.html) has many methods you can use. One of which is:

```
import shutil

shutil.copyfile(src, dst)

# 2nd option
shutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp

```

* Copy the contents of the file named `src` to a file named `dst`. Both `src` and `dst` need to be the entire filename of the files, including path.
* The destination location must be writable; otherwise, an `IOError` exception will be raised.
* If `dst` already exists, it will be replaced.
* Special files such as character or block devices and pipes cannot be copied with this function.
* With `copy`, `src` and `dst` are path names given as `str`s.

Another `shutil` method to look at is [`shutil.copy2()`](https://docs.python.org/3/library/shutil.html#shutil.copy2). It's similar but preserves more metadata (e.g. time stamps).

If you use `os.path` operations, use `copy` rather than `copyfile`. `copyfile` will only accept strings."
"How do I split a sentence and store each word in a list? e.g.

```
""these are words""   ‚ü∂   [""these"", ""are"", ""words""]

```

---

To split on other delimiters, see [Split a string by a delimiter in python](https://stackoverflow.com/questions/3475251).

To split into individual characters, see [How do I split a string into a list of characters?](https://stackoverflow.com/questions/4978787).","Given a string `sentence`, this stores each word in a list called `words`:

```
words = sentence.split()

```"
"In Python 2:

```
raw_input()

```

In Python 3, I get an error:

> NameError: name 'raw\_input' is not defined","Starting with Python 3, `raw_input()` was renamed to `input()`.

From [Whatâ€™s New In Python 3.0, Builtins section](https://docs.python.org/3/whatsnew/3.0.html#builtins) second item."
"I am using Python 3.2.1 and I can't import the `StringIO` module. I use
`io.StringIO` and it works, but I can't use it with `numpy`'s `genfromtxt()` like this:

```
x=""1 3\n 4.5 8""        
numpy.genfromtxt(io.StringIO(x))

```

I get the following error:

```
TypeError: Can't convert 'bytes' object to str implicitly  

```

and when I write `import StringIO` it says

```
ImportError: No module named 'StringIO'

```","> when i write import StringIO it says there is no such module.

From [Whatâ€™s New In Python 3.0](http://docs.python.org/3.0/whatsnew/3.0.html):

> The `StringIO` and `cStringIO` modules are gone. Instead, import the `io`
> module and use `io.StringIO` or `io.BytesIO` for text and data
> respectively.

.

---

A possibly useful method of fixing some Python 2 code to also work in Python 3 (caveat emptor):

```
try:
    from StringIO import StringIO ## for Python 2
except ImportError:
    from io import StringIO ## for Python 3

```

> *Note: This example may be tangential to the main issue of the question and is included only as something to consider when generically addressing the missing `StringIO` module.* For a more direct solution the message `TypeError: Can't convert 'bytes' object to str implicitly`, see [this answer](https://stackoverflow.com/a/11970414/86967)."
"I am trying to get a Python script to run on the linux server I'm connected to via ssh. The script uses mysqldb. I have all the other components I need, but when I try to install mySQLdb via setuptools like so:,

```
python setup.py install

```

I get the following error report related to the `mysql_config` command.

```
sh: mysql_config: command not found
Traceback (most recent call last):
  File ""setup.py"", line 15, in <module>
    metadata, options = get_config()
  File ""/usr/lib/python2.5/MySQL-python-1.2.3/setup_posix.py"", line 43, in get_config
    libs = mysql_config(""libs_r"")
  File ""/usr/lib/python2.5/MySQL-python-1.2.3/setup_posix.py"", line 24, in mysql_config
    raise EnvironmentError(""%s not found"" % (mysql_config.path,))
EnvironmentError: mysql_config not found

```

Has anyone else encountered this error and if so how did you resolve it/what can I do to successfully install mysqldb?","mySQLdb is a python interface for mysql, but it is not mysql itself. And apparently mySQLdb needs the command 'mysql\_config', so you need to install that first.

Can you confirm that you did or did not install mysql itself, by running ""mysql"" from the shell? That should give you a response other than ""mysql: command not found"".

Which linux distribution are you using? Mysql is pre-packaged for most linux distributions. For example, for debian / ubuntu, installing mysql is as easy as

```
sudo apt-get install mysql-server

```

mysql-config is in a different package, which can be installed from (again, assuming debian / ubuntu):

```
sudo apt-get install libmysqlclient-dev

```

if you are using mariadb, the drop in replacement for mysql, then run

```
sudo apt-get install libmariadbclient-dev

```

Reference:
<https://github.com/JudgeGirl/Judge-sender/issues/4#issuecomment-186542797>"
"I have a list of strings like this:

```
X = [""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i""]
Y = [ 0,   1,   1,   0,   1,   2,   2,   0,   1 ]

```

What is the shortest way of sorting X using values from Y to get the following output?

```
[""a"", ""d"", ""h"", ""b"", ""c"", ""e"", ""i"", ""f"", ""g""]

```

The order of the elements having the same ""key"" does not matter. I can resort to the use of `for` constructs but I am curious if there is a shorter way. Any suggestions?","**Shortest Code**

```
[x for _, x in sorted(zip(Y, X))]

```

**Example:**

```
X = [""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i""]
Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]

Z = [x for _,x in sorted(zip(Y,X))]
print(Z)  # [""a"", ""d"", ""h"", ""b"", ""c"", ""e"", ""i"", ""f"", ""g""]

```

---

**Generally Speaking**

```
[x for _, x in sorted(zip(Y, X), key=lambda pair: pair[0])]

```

**Explained:**

1. [`zip`](https://docs.python.org/3/library/functions.html#zip) the two `list`s.
2. create a new, sorted `list` based on the `zip` using [`sorted()`](https://www.programiz.com/python-programming/methods/built-in/sorted).
3. using a list comprehension *extract* the first elements of each pair from the sorted, zipped `list`.

*For more information on how to set\use the `key` parameter as well as the `sorted` function in general, take a look at [this](http://pythoncentral.io/how-to-sort-a-list-tuple-or-object-with-sorted-in-python/).*

---"
"I'm trying to map a list into hex, and then use the list elsewhere. In python 2.6, this was easy:

**A:** Python 2.6:

```
>>> map(chr, [66, 53, 0, 94])
['B', '5', '\x00', '^']

```

However, in Python 3.1, the above returns a map object.

**B:** Python 3.1:

```
>>> map(chr, [66, 53, 0, 94])
<map object at 0x00AF5570>

```

How do I retrieve the mapped list (as in **A** above) on Python 3.x?

Alternatively, is there a better way of doing this? My initial list object has around 45 items and id like to convert them to hex.","Do this:

```
list(map(chr,[66,53,0,94]))

```

In Python 3+, many processes that iterate over iterables return iterators themselves. In most cases, this ends up saving memory, and should make things go faster.

If all you're going to do is iterate over this list eventually, there's no need to even convert it to a list, because you can still iterate over the `map` object like so:

```
# Prints ""ABCD""
for ch in map(chr,[65,66,67,68]):
    print(ch)

```"
"Given a list `xs` and a value `item`, how can I check whether `xs` contains `item` (i.e., if any of the elements of `xs` is equal to `item`)? Is there something like `xs.contains(item)`?

---

For performance considerations, see [Fastest way to check if a value exists in a list](https://stackoverflow.com/questions/7571635).","Use:

```
if my_item in some_list:
    ...

```

Also, inverse operation:

```
if my_item not in some_list:
    ...

```

It works fine for lists, tuples, sets and dicts (check keys).

**Note** that this is an O(n) operation in lists and tuples, but an O(1) operation in sets and dicts."
What is the difference between abstract class and interface in Python?,"What you'll see sometimes is the following:

```
class Abstract1:
    """"""Some description that tells you it's abstract,
    often listing the methods you're expected to supply.""""""

    def aMethod(self):
        raise NotImplementedError(""Should have implemented this"")

```

Because Python doesn't have (and doesn't need) a formal Interface contract, the Java-style distinction between abstraction and interface doesn't exist. If someone goes through the effort to define a formal interface, it will also be an abstract class. The only differences would be in the stated intent in the docstring.

And the difference between abstract and interface is a hairsplitting thing when you have duck typing.

Java uses interfaces because it doesn't have multiple inheritance.

Because Python has multiple inheritance, you may also see something like this

```
class SomeAbstraction:
    pass  # lots of stuff - but missing something

class Mixin1:
    def something(self):
        pass  # one implementation

class Mixin2:
    def something(self):
        pass  # another

class Concrete1(SomeAbstraction, Mixin1):
    pass

class Concrete2(SomeAbstraction, Mixin2):
    pass

```

This uses a kind of abstract superclass with mixins to create concrete subclasses that are disjoint."
How do I have a Python script that can accept user input and how do I make it read in arguments if run from the command line?,"To read user input you can try [the `cmd` module](https://docs.python.org/dev/library/cmd.html) for easily creating a mini-command line interpreter (with help texts and autocompletion) and [`raw_input`](https://docs.python.org/2/library/functions.html#raw_input) ([`input`](https://docs.python.org/dev/library/functions.html#input) for Python 3+) for reading a line of text from the user.

```
text = raw_input(""prompt"")  # Python 2
text = input(""prompt"")  # Python 3

```

Command line inputs are in `sys.argv`. Try this in your script:

```
import sys
print (sys.argv)

```

There are two modules for parsing command line options: [~~`optparse`~~](https://docs.python.org/dev/library/optparse.html) (deprecated since Python 2.7, use [`argparse`](https://docs.python.org/dev/library/argparse.html) instead) and [`getopt`](https://docs.python.org/dev/library/getopt.html). If you just want to input files to your script, behold the power of [`fileinput`](https://docs.python.org/dev/library/fileinput.html).

The [Python library reference](https://docs.python.org/dev/library/) is your friend."
"I can't figure out how to rotate the text on the X Axis. Its a time stamp, so as the number of samples increase, they get closer and closer until they overlap. I'd like to rotate the text 90 degrees so as the samples get closer together, they aren't overlapping.

Below is what I have, it works fine with the exception that I can't figure out how to rotate the X axis text.

```
import sys

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import datetime

font = {'family' : 'normal',
        'weight' : 'bold',
        'size'   : 8}

matplotlib.rc('font', **font)

values = open('stats.csv', 'r').readlines()

time = [datetime.datetime.fromtimestamp(float(i.split(',')[0].strip())) for i in values[1:]]
delay = [float(i.split(',')[1].strip()) for i in values[1:]]

plt.plot(time, delay)
plt.grid(b='on')

plt.savefig('test.png')

```","This works for me:

```
plt.xticks(rotation=90)

```"
What is [`__init__.py`](https://docs.python.org/3/tutorial/modules.html#packages) for in a Python source directory?,"It used to be a required part of a package ([old, pre-3.3 ""regular package""](https://docs.python.org/3/reference/import.html#regular-packages), not [newer 3.3+ ""namespace package""](https://docs.python.org/3/reference/import.html#namespace-packages)).

[Here's the documentation.](https://docs.python.org/3/reference/import.html#regular-packages)

> Python defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an `__init__.py` file. When a regular package is imported, this `__init__.py` file is implicitly executed, and the objects it defines are bound to names in the packageâ€™s namespace. The `__init__.py` file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.

But just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without `__init__.py`."
"I would like to make a deep copy of a `dict` in python. Unfortunately the `.deepcopy()` method doesn't exist for the `dict`. How do I do that?

```
>>> my_dict = {'a': [1, 2, 3], 'b': [4, 5, 6]}
>>> my_copy = my_dict.deepcopy()
Traceback (most recent calll last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'dict' object has no attribute 'deepcopy'
>>> my_copy = my_dict.copy()
>>> my_dict['a'][2] = 7
>>> my_copy['a'][2]
7

```

The last line should be `3`.

I would like that modifications in `my_dict` don't impact the snapshot `my_copy`.

How do I do that? The solution should be compatible with Python 3.x.","How about:

```
import copy
d = { ... }
d2 = copy.deepcopy(d)

```

Python 2 or 3:

```
Python 3.2 (r32:88445, Feb 20 2011, 21:30:00) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import copy
>>> my_dict = {'a': [1, 2, 3], 'b': [4, 5, 6]}
>>> my_copy = copy.deepcopy(my_dict)
>>> my_dict['a'][2] = 7
>>> my_copy['a'][2]
3
>>>

```"
"Python is installed in a local directory.

My directory tree looks like this:

```
(local directory)/site-packages/toolkit/interface.py

```

My code is in here:

```
(local directory)/site-packages/toolkit/examples/mountain.py

```

To run the example, I write `python mountain.py`, and in the code I have:

```
from toolkit.interface import interface

```

And I get the error:

```
Traceback (most recent call last):
  File ""mountain.py"", line 28, in ?
    from toolkit.interface import interface
ImportError: No module named toolkit.interface

```

I have already checked `sys.path` and there I have the directory `/site-packages`. Also, I have the file `__init__.py.bin` in the toolkit folder to indicate to Python that this is a package. I also have a `__init__.py.bin` in the examples directory.

I do not know why Python cannot find the file when it is in `sys.path`. Any ideas? Can it be a permissions problem? Do I need some execution permission?","Based on your comments to orip's post, I guess this is what happened:

1. You edited `__init__.py` on windows.
2. The windows editor added something non-printing, perhaps a carriage-return (end-of-line in Windows is CR/LF; in unix it is LF only), or perhaps a CTRL-Z (windows end-of-file).
3. You used WinSCP to copy the file to your unix box.
4. WinSCP thought: ""This has something that's not basic text; I'll put a .bin extension to indicate binary data.""
5. The missing `__init__.py` (now called `__init__.py.bin`) means python doesn't understand toolkit as a package.
6. You create `__init__.py` in the appropriate directory and everything works... ?"
"How to convert a caught `Exception` (its description and stack trace) into a `str` for external use?

```
try:
    method_that_can_raise_an_exception(params)
except Exception as e:
    print(complete_exception_description(e))

```","See the `traceback` module, specifically the `format_exc()` function. [Here](https://docs.python.org/2.7/library/traceback.html#traceback.format_exc).

```
import traceback

try:
    raise ValueError
except ValueError:
    tb = traceback.format_exc()
else:
    tb = ""No error""
finally:
    print(tb)

```"
"Is there any benefit in using compile for regular expressions in Python?

```
h = re.compile('hello')
h.match('hello world')

```

vs

```
re.match('hello', 'hello world')

```","I've had a lot of experience running a compiled regex 1000s of times versus compiling on-the-fly, and have not noticed any perceivable difference. Obviously, this is anecdotal, and certainly not a great argument *against* compiling, but I've found the difference to be negligible.

EDIT:
After a quick glance at the actual Python 2.5 library code, I see that Python internally compiles AND CACHES regexes whenever you use them anyway (including calls to `re.match()`), so you're really only changing WHEN the regex gets compiled, and shouldn't be saving much time at all - only the time it takes to check the cache (a key lookup on an internal `dict` type).

From module re.py (comments are mine):

```
def match(pattern, string, flags=0):
    return _compile(pattern, flags).match(string)

def _compile(*key):

    # Does cache check at top of function
    cachekey = (type(key[0]),) + key
    p = _cache.get(cachekey)
    if p is not None: return p

    # ...
    # Does actual compilation on cache miss
    # ...

    # Caches compiled regex
    if len(_cache) >= _MAXCACHE:
        _cache.clear()
    _cache[cachekey] = p
    return p

```

I still often pre-compile regular expressions, but only to bind them to a nice, reusable name, not for any expected performance gain."
How can I print the version number of the current Python installation from my script?,"Try

```
import sys
print(sys.version)

```

This prints the full version information string. If you only want the python version number, then [Bastien LÃ©onard's solution](https://stackoverflow.com/questions/1252163/printing-python-version-in-output/1252175#1252175) is the best. You might want to examine the full string and see if you need it or portions of it."
"I came across the following header format for Python source files in a document about Python coding guidelines:

```
#!/usr/bin/env python

""""""Foobar.py: Description of what foobar does.""""""

__author__      = ""Barack Obama""
__copyright__   = ""Copyright 2009, Planet Earth""

```

Is this the standard format of headers in the Python world?
What other fields/information can I put in the header?
Python gurus share your guidelines for good Python source headers :-)","Its all metadata for the `Foobar` module.

The first one is the `docstring` of the module, that is already explained in [Peter's answer](https://stackoverflow.com/questions/1523427/python-what-is-the-common-header-format/1523435#1523435).

> [How do I organize my modules (source files)? (Archive)](http://web.archive.org/web/20111010053227/http://jaynes.colorado.edu/PythonGuidelines.html#module_formatting)
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
> **The first line of each file shoud be `#!/usr/bin/env python`.** This makes it possible to run the file as a script invoking the interpreter implicitly, e.g. in a CGI context.
>
> ***Next should be the docstring with a description.*** If the description is long, the first line should be a short summary that makes sense on its own, separated from the rest by a newline.
>
> **All code, including import statements, should follow the docstring.** Otherwise, the docstring will not be recognized by the interpreter, and you will not have access to it in interactive sessions (i.e. through `obj.__doc__`) or when generating documentation with automated tools.
>
> **Import built-in modules first, followed by third-party modules, followed by any changes to the path and your own modules.** Especially, additions to the path and names of your modules are likely to change rapidly: keeping them in one place makes them easier to find.
>
> **Next should be authorship information.** This information should follow this format:
>
> ```
> __author__ = ""Rob Knight, Gavin Huttley, and Peter Maxwell""
> __copyright__ = ""Copyright 2007, The Cogent Project""
> __credits__ = [""Rob Knight"", ""Peter Maxwell"", ""Gavin Huttley"",
>                     ""Matthew Wakefield""]
> __license__ = ""GPL""
> __version__ = ""1.0.1""
> __maintainer__ = ""Rob Knight""
> __email__ = ""rob@spot.colorado.edu""
> __status__ = ""Production""
>
> ```
>
> Status should typically be one of ""Prototype"", ""Development"", or ""Production"". `__maintainer__` should be the person who will fix bugs and make improvements if imported. `__credits__` differs from `__author__` in that `__credits__` includes people who reported bug fixes, made suggestions, etc. but did not actually write the code.

[Here](http://epydoc.sourceforge.net/manual-fields.html#module-metadata-variables) you have more information, listing `__author__`, `__authors__`, `__contact__`, `__copyright__`, `__license__`, `__deprecated__`, `__date__` and `__version__` as recognized metadata."
"`[]` = empty `list`

`()` = empty `tuple`

`{}` = empty `dict`

Is there a similar notation for an empty `set`?
Or do I have to write `set()`?","No, there's no literal syntax for the empty set. You have to write [`set()`](https://docs.python.org/3/library/stdtypes.html#set)."
"I want to apply my custom function (it uses an if-else ladder) to these six columns (`ERI_Hispanic`, `ERI_AmerInd_AKNatv`, `ERI_Asian`, `ERI_Black_Afr.Amer`, `ERI_HI_PacIsl`, `ERI_White`) in each row of my dataframe.

I've tried different methods from other questions but still can't seem to find the right answer for my problem. The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else. Even if they have a ""1"" in another ethnicity column they still are counted as Hispanic not two or more races. Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity(except for Hispanic).

It's almost like doing a for loop through each row and if each record meets a criterion they are added to one list and eliminated from the original.

From the dataframe below I need to calculate a new column based on the following spec in SQL:

**CRITERIA**

```
IF [ERI_Hispanic] = 1 THEN RETURN “Hispanic”
ELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN “Two or More”
ELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN “A/I AK Native”
ELSE IF [ERI_Asian] = 1 THEN RETURN “Asian”
ELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN “Black/AA”
ELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN “Haw/Pac Isl.”
ELSE IF [ERI_White] = 1 THEN RETURN “White”

```

Comment: If the ERI Flag for Hispanic is True (1), the employee is classified as “Hispanic”

Comment: If more than 1 non-Hispanic ERI Flag is true, return “Two or More”

**DATAFRAME**

```
     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined
0    MOST           JEFF        E       0               0           0               0               0               1           White
1    CRUISE         TOM         E       0               0           0               1               0               0           White
2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown
3    DICAP          LEO                 0               0           0               0               0               1           Unknown
4    BRANDO         MARLON      E       0               0           0               0               0               0           White
5    HANKS          TOM         0                       0           0               0               0               1           Unknown
6    DENIRO         ROBERT      E       0               1           0               0               0               1           White
7    PACINO         AL          E       0               0           0               0               0               1           White
8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White
9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White

```","OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:

```
def label_race(row):
   if row['eri_hispanic'] == 1:
      return 'Hispanic'
   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] > 1:
      return 'Two Or More'
   if row['eri_nat_amer'] == 1:
      return 'A/I AK Native'
   if row['eri_asian'] == 1:
      return 'Asian'
   if row['eri_afr_amer'] == 1:
      return 'Black/AA'
   if row['eri_hawaiian'] == 1:
      return 'Haw/Pac Isl.'
   if row['eri_white'] == 1:
      return 'White'
   return 'Other'

```

You may want to go over this, but it seems to do the trick - notice that the parameter going into the function is considered to be a Series object labelled ""row"".

Next, use the apply function in pandas to apply the function - e.g.

```
df.apply(label_race, axis=1)

```

Note the `axis=1` specifier, that means that the application is done at a row, rather than a column level. The results are here:

```
0           White
1        Hispanic
2           White
3           White
4           Other
5           White
6     Two Or More
7           White
8    Haw/Pac Isl.
9           White

```

If you're happy with those results, then run it again, saving the results into a new column in your original dataframe.

```
df['race_label'] = df.apply(label_race, axis=1)

```

The resultant dataframe looks like this (scroll to the right to see the new column):

```
      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label
0      MOST    JEFF      E             0          0             0              0             0          1       White         White
1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic
2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White
3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White
4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other
5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White
6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More
7    PACINO      AL      E             0          0             0              0             0          1       White         White
8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.
9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White

```"
"Is it possible to declare more than one variable using a `with` statement in Python?

Something like:

```
from __future__ import with_statement

with open(""out.txt"",""wt""), open(""in.txt"") as file_out, file_in:
    for line in file_in:
        file_out.write(line)

```

... or is cleaning up two resources at the same time the problem?","It is possible in [Python 3 since v3.1](http://docs.python.org/3.1/reference/compound_stmts.html#with) and [Python 2.7](http://docs.python.org/dev/whatsnew/2.7.html#other-language-changes). The new [`with` syntax](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement) supports multiple context managers:

```
with A() as a, B() as b, C() as c:
    doSomething(a,b,c)

```

Unlike the `contextlib.nested`, this guarantees that `a` and `b` will have their `__exit__()`'s called even if `C()` or it's `__enter__()` method raises an exception.

You can also use earlier variables in later definitions (h/t [Ahmad](https://stackoverflow.com/questions/893333/multiple-variables-in-a-with-statement#comment78254813_1073814) below):

```
with A() as a, B(a) as b, C(a, b) as c:
    doSomething(a, c)

```

As of Python 3.10, [you can use parentheses](https://docs.python.org/3/whatsnew/3.10.html#parenthesized-context-managers):

```
with (
    A() as a, 
    B(a) as b, 
    C(a, b) as c,
):
    doSomething(a, c)

```"
"What is the proper indentation for Python multiline strings within a function?

```
    def method():
        string = """"""line one
line two
line three""""""

```

or

```
    def method():
        string = """"""line one
        line two
        line three""""""

```

or something else?

It looks kind of weird to have the string hanging outside the function in the first example.","You probably want to line up with the `""""""`

```
def foo():
    string = """"""line one
             line two
             line three""""""

```

Since the newlines and spaces are included in the string itself, you will have to postprocess it. If you don't want to do that and you have a whole lot of text, you might want to store it separately in a text file. If a text file does not work well for your application and you don't want to postprocess, I'd probably go with

```
def foo():
    string = (""this is an ""
              ""implicitly joined ""
              ""string"")

```

If you want to postprocess a multiline string to trim out the parts you don't need, you should consider the [`textwrap`](http://docs.python.org/3/library/textwrap.html) module or the technique for postprocessing docstrings presented in [PEP 257](http://www.python.org/dev/peps/pep-0257/):

```
def trim(docstring):
    import sys
    if not docstring:
        return ''
    # Convert tabs to spaces (following the normal Python rules)
    # and split into a list of lines:
    lines = docstring.expandtabs().splitlines()
    # Determine minimum indentation (first line doesn't count):
    indent = sys.maxint
    for line in lines[1:]:
        stripped = line.lstrip()
        if stripped:
            indent = min(indent, len(line) - len(stripped))
    # Remove indentation (first line is special):
    trimmed = [lines[0].strip()]
    if indent < sys.maxint:
        for line in lines[1:]:
            trimmed.append(line[indent:].rstrip())
    # Strip off trailing and leading blank lines:
    while trimmed and not trimmed[-1]:
        trimmed.pop()
    while trimmed and not trimmed[0]:
        trimmed.pop(0)
    # Return a single string:
    return '\n'.join(trimmed)

```"
Is there a [ternary conditional operator](https://en.wikipedia.org/wiki/%3F:#Python) in Python?,"Yes, it was [added](https://mail.python.org/pipermail/python-dev/2005-September/056846.html ""[Python-Dev] Conditional Expression Resolution"") in version 2.5. The expression syntax is:

```
a if condition else b

```

First `condition` is evaluated, then exactly one of either `a` or `b` is evaluated and returned based on the [Boolean](https://en.wikipedia.org/wiki/Boolean_data_type ""Boolean data type"") value of `condition`. If `condition` evaluates to `True`, then `a` is evaluated and returned but `b` is ignored, or else when `b` is evaluated and returned but `a` is ignored.

This allows short-circuiting because when `condition` is true only `a` is evaluated and `b` is not evaluated at all, but when `condition` is false only `b` is evaluated and `a` is not evaluated at all.

For example:

```
>>> 'true' if True else 'false'
'true'
>>> 'true' if False else 'false'
'false'

```

Note that conditionals are an *expression*, not a *statement*. This means you can't use **statements** such as `pass`, or assignments with `=` (or ""augmented"" assignments like `+=`), within a conditional **expression**:

```
>>> pass if False else pass
  File ""<stdin>"", line 1
    pass if False else pass
         ^
SyntaxError: invalid syntax

>>> # Python parses this as `x = (1 if False else y) = 2`
>>> # The `(1 if False else x)` part is actually valid, but
>>> # it can't be on the left-hand side of `=`.
>>> x = 1 if False else y = 2
  File ""<stdin>"", line 1
SyntaxError: cannot assign to conditional expression

>>> # If we parenthesize it instead...
>>> (x = 1) if False else (y = 2)
  File ""<stdin>"", line 1
    (x = 1) if False else (y = 2)
       ^
SyntaxError: invalid syntax

```

(In 3.8 and above, the `:=` ""walrus"" operator allows simple assignment of values *as an expression*, which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.)

Similarly, because it is an expression, the `else` part is *mandatory*:

```
# Invalid syntax: we didn't specify what the value should be if the 
# condition isn't met. It doesn't matter if we can verify that
# ahead of time.
a if True

```

You can, however, use conditional expressions to assign a variable like so:

```
x = a if True else b

```

Or for example to return a value:

```
# Of course we should just use the standard library `max`;
# this is just for demonstration purposes.
def my_max(a, b):
    return a if a > b else b

```

Think of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will *do the same thing* with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to *do something different* depending on the condition, then use a normal `if` **statement** instead.

---

Keep in mind that it's frowned upon by some Pythonistas for several reasons:

* The order of the arguments is different from those of the classic `condition ? a : b` ternary operator from many other languages (such as [C](https://en.wikipedia.org/wiki/C_%28programming_language%29), [C++](https://en.wikipedia.org/wiki/C%2B%2B), [Go](https://en.wikipedia.org/wiki/Go_%28programming_language%29), [Perl](https://en.wikipedia.org/wiki/Perl), [Ruby](https://en.wikipedia.org/wiki/Ruby_%28programming_language%29), [Java](https://en.wikipedia.org/wiki/Java_%28programming_language%29), [JavaScript](https://en.wikipedia.org/wiki/JavaScript), etc.), which may lead to bugs when people unfamiliar with Python's ""surprising"" behaviour use it (they may reverse the argument order).
* Some find it ""unwieldy"", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).
* Stylistic reasons. (Although the 'inline `if`' can be *really* useful, and make your script more concise, it really does complicate your code)

If you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, `x = 4 if b > 8 else 9` is read aloud as `x will be 4 if b is greater than 8 otherwise 9`.

Official documentation:

* [Conditional expressions](https://docs.python.org/3/reference/expressions.html#conditional-expressions ""Conditional expressions"")
* [Is there an equivalent of C’s ”?:” ternary operator?](https://docs.python.org/3/faq/programming.html#is-there-an-equivalent-of-c-s-ternary-operator ""Is there an equivalent of C’s ”?:” ternary operator?"")"
"I captured the standard output of an external program into a `bytes` object:

```
>>> from subprocess import *
>>> stdout = Popen(['ls', '-l'], stdout=PIPE).communicate()[0]
>>> stdout
b'total 0\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\n'

```

I want to convert that to a normal Python string, so that I can print it like this:

```
>>> print(stdout)
-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1
-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2

```

How do I convert the `bytes` object to a `str` with Python 3?

---

See [Best way to convert string to bytes in Python 3?](https://stackoverflow.com/questions/7585435) for the other way around.","[Decode the `bytes` object](https://docs.python.org/3/library/stdtypes.html#bytes.decode) to produce a string:

```
>>> b""abcde"".decode(""utf-8"")
'abcde'

```

The above example *assumes* that the `bytes` object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!"
"The Situation
-------------

I’m trying to port an open-source library to Python 3. ([SymPy](http://sympy.org/), if anyone is wondering.)

So, I need to run `2to3` automatically when building for Python 3. To do that, I need to use `distribute`. Therefore, I need to port the current system, which (according to the doctest) is `distutils`.

The Problem
-----------

Unfortunately, I’m not sure what’s the difference between these modules—`distutils`, `distribute`, `setuptools`. The documentation is sketchy as best, as they all seem to be a fork of one another, intended to be compatible in most circumstances (but actually, not all)…and so on, and so forth.

The Question
------------

**Could someone explain the differences?** What am I supposed to use? What is the most modern solution? (As an aside, I’d also appreciate some guide on porting to `Distribute`, but that’s a tad beyond the scope of the question…)","As of May 2022, most of the other answers to this question are several years out-of-date. When you come across advice on Python packaging issues, remember to look at the date of publication, and don't trust out-of-date information.

The [Python Packaging User Guide](https://packaging.python.org/) is worth a read. Every page has a ""last updated"" date displayed, so you can check the recency of the manual, and it's quite comprehensive. The fact that it's hosted on a subdomain of python.org of the Python Software Foundation just adds credence to it. The [Project Summaries](https://packaging.python.org/en/latest/key_projects/) page is especially relevant here.

Summary of tools:
-----------------

Here's a summary of the Python packaging landscape:

### Supported tools:

* **`setuptools`** was developed to overcome Distutils' limitations, and is not included in the standard library. It introduced a command-line utility called `easy_install`. It also introduced the `setuptools` Python package that can be imported in your `setup.py` script, and the `pkg_resources` Python package that can be imported in your code to locate data files installed with a distribution. One of its gotchas is that it monkey-patches the `distutils` Python package. It should work well with `pip`. [It sees regular releases.](https://github.com/pypa/setuptools/releases)

  + [Official docs](https://setuptools.readthedocs.io/en/latest/) | [Pypi page](https://pypi.python.org/pypi/setuptools) | [GitHub repo](https://github.com/pypa/setuptools) | [`setuptools` section of Python Package User Guide](https://packaging.python.org/key_projects/#setuptools)
* **`scikit-build`** is an improved build system generator that internally uses CMake to build compiled Python extensions. Because scikit-build isn't based on distutils, it doesn't really have any of its limitations. When ninja-build is present, scikit-build can compile large projects over three times faster than the alternatives. It should work well with `pip`.

  + [Official docs](http://scikit-build.readthedocs.io/en/latest/) | [Pypi page](https://pypi.org/project/scikit-build/) | [GitHub repo](https://github.com/scikit-build/scikit-build) | [`scikit-build` section of Python Package User Guide](https://packaging.python.org/key_projects/#scikit-build)
* **`distlib`** is a library that provides functionality that is used by higher level tools like `pip`.

  + [Official Docs](http://pythonhosted.org/distlib/) | [Pypi page](https://pypi.org/project/distlib) | [Bitbucket repo](https://bitbucket.org/pypa/distlib) | [`distlib` section of Python Package User Guide](https://packaging.python.org/key_projects/#distlib)
* **`packaging`** is also a library that provides functionality used by higher level tools like `pip` and `setuptools`

  + [Official Docs](https://packaging.pypa.io/) | [Pypi page](https://pypi.org/project/packaging) | [GitHub repo](https://github.com/pypa/packaging) | [`packaging` section of Python Package User Guide](https://packaging.python.org/key_projects/#packaging)

### Deprecated/abandoned tools:

* **`distutils`** is still included in the standard library of Python, but is considered deprecated as of Python 3.10. It is useful for simple Python distributions, but lacks features. It introduces the `distutils` Python package that can be imported in your `setup.py` script.

  + [Official docs](https://docs.python.org/3/library/distutils.html) | [`distutils` section of Python Package User Guide](https://packaging.python.org/key_projects/#distutils)
* **`distribute`** was a fork of `setuptools`. It shared the same namespace, so if you had Distribute installed, `import setuptools` would actually import the package distributed with Distribute. ***Distribute was merged back into Setuptools 0.7***, so you don't need to use Distribute any more. In fact, the version on Pypi is just a compatibility layer that installs Setuptools.
* **`distutils2`** was an attempt to take the best of `distutils`, `setuptools` and `distribute` and become the standard tool included in Python's standard library. The idea was that `distutils2` would be distributed for old Python versions, and that `distutils2` would be renamed to `packaging` for Python 3.3, which would include it in its standard library. These plans did not go as intended, however, and currently, ***`distutils2` is an abandoned project***. The latest release was in March 2012, and its Pypi home page has finally been updated to reflect its death.

Others:
-------

There are other tools, if you are interested, read [Project Summaries](https://packaging.python.org/en/latest/key_projects/) in the Python Packaging User Guide. I won't list them all, to not repeat that page, and to keep the answer matching the question, which was only about `distribute`, `distutils`, `setuptools` and `distutils2`.

Recommendation:
---------------

If all of this is new to you, and you don't know where to start, **I would recommend learning `setuptools`**, along with `pip` and `virtualenv`, which all work very well together.

If you're looking into `virtualenv`, you might be interested in this question: [What is the difference between `venv`, `pyvenv`, `pyenv`, `virtualenv`, `virtualenvwrapper`, etc?](https://stackoverflow.com/q/41573587/247696). (Yes, I know, I groan with you.)"
"I am going over Sweigart's *Automate the Boring Stuff with Python* text. I'm using [IDLE](https://en.wikipedia.org/wiki/IDLE) and already installed the Selenium module and the Firefox browser.

Whenever I tried to run the webdriver function, I get this:

```
from selenium import webdriver
browser = webdriver.Firefox()

```

Exception:

```
Exception ignored in: <bound method Service.__del__ of <selenium.webdriver.firefox.service.Service object at 0x00000249C0DA1080>>
Traceback (most recent call last):
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 163, in __del__
    self.stop()
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 135, in stop
    if self.process is None:
AttributeError: 'Service' object has no attribute 'process'
Exception ignored in: <bound method Service.__del__ of <selenium.webdriver.firefox.service.Service object at 0x00000249C0E08128>>
Traceback (most recent call last):
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 163, in __del__
    self.stop()
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 135, in stop
    if self.process is None:
AttributeError: 'Service' object has no attribute 'process'
Traceback (most recent call last):
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 64, in start
    stdout=self.log_file, stderr=self.log_file)
  File ""C:\Python\Python35\lib\subprocess.py"", line 947, in __init__
    restore_signals, start_new_session)
  File ""C:\Python\Python35\lib\subprocess.py"", line 1224, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified

```

During handling of the above exception, another exception occurred:

```
Traceback (most recent call last):
  File ""<pyshell#11>"", line 1, in <module>
    browser = webdriver.Firefox()
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\firefox\webdriver.py"", line 135, in __init__
    self.service.start()
  File ""C:\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 71, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.

```

I think I need to set the path for `geckodriver`, but I am not sure how, so how would I do this?","> selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.

[First of all you will need to download latest executable geckodriver from here to run latest Firefox using Selenium](https://github.com/mozilla/geckodriver/releases)

Actually, the Selenium client bindings tries to locate the `geckodriver` executable from the system `PATH`. You will need to add the directory containing the executable to the system path.

* On Unix systems you can do the following to append it to your system’s search path, if you’re using a Bash-compatible shell:

  ```
  export PATH=$PATH:/path/to/directory/of/executable/downloaded/in/previous/step

  ```
* On Windows you will need to update the **Path system variable to add the full directory path to the executable geckodriver** [manually](https://www.google.co.in/amp/www.howtogeek.com/118594/how-to-edit-your-system-path-for-easy-command-line-access/amp/?client=ms-android-motorola) or [command line](https://www.windows-commandline.com/set-path-command-line/)\*\* (don't forget to restart your system after adding executable geckodriver into system PATH to take effect)\*\*. The principle is the same as on Unix.

Now you can run your code same as you're doing as below :-

```
from selenium import webdriver

browser = webdriver.Firefox()

```

> selenium.common.exceptions.WebDriverException: Message: Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line

The exception clearly states you have installed Firefox some other location while Selenium is trying to find Firefox and launch from the default location, but it couldn't find it. You need to provide explicitly Firefox installed binary location to launch Firefox as below :-

```
from selenium import webdriver
from selenium.webdriver.firefox.firefox_binary import FirefoxBinary

binary = FirefoxBinary('path/to/installed firefox binary')
browser = webdriver.Firefox(firefox_binary=binary)

```"
"I'm using the [subprocess module](http://docs.python.org/library/subprocess.html) to start a subprocess and connect to its output stream (standard output). I want to be able to execute non-blocking reads on its standard output. Is there a way to make .readline non-blocking or to check if there is data on the stream before I invoke `.readline`? I'd like this to be portable or at least work under Windows and Linux.

Here is how I do it for now (it's blocking on the `.readline` if no data is available):

```
p = subprocess.Popen('myprogram.exe', stdout = subprocess.PIPE)
output_str = p.stdout.readline()

```","[`fcntl`](https://stackoverflow.com/questions/375427/non-blocking-read-on-a-stream-in-python/4025909#4025909), [`select`](https://stackoverflow.com/questions/375427/non-blocking-read-on-a-stream-in-python/375511#375511), [`asyncproc`](https://stackoverflow.com/questions/375427/non-blocking-read-on-a-stream-in-python/437888#437888) won't help in this case.

A reliable way to read a stream without blocking regardless of operating system is to use [`Queue.get_nowait()`](https://docs.python.org/3/library/queue.html#queue.Queue.get_nowait):

```
import sys
from subprocess import PIPE, Popen
from threading  import Thread

try:
    from queue import Queue, Empty
except ImportError:
    from Queue import Queue, Empty  # python 2.x

ON_POSIX = 'posix' in sys.builtin_module_names

def enqueue_output(out, queue):
    for line in iter(out.readline, b''):
        queue.put(line)
    out.close()

p = Popen(['myprogram.exe'], stdout=PIPE, bufsize=1, close_fds=ON_POSIX)
q = Queue()
t = Thread(target=enqueue_output, args=(p.stdout, q))
t.daemon = True # thread dies with the program
t.start()

# ... do other things here

# read line without blocking
try:  line = q.get_nowait() # or q.get(timeout=.1)
except Empty:
    print('no output yet')
else: # got line
    # ... do something with line

```"
"[Requests](https://requests.readthedocs.io/) is a really nice library. I'd like to use it for downloading big files (>1GB).
The problem is it's not possible to keep whole file in memory; I need to read it in chunks. And this is a problem with the following code:

```
import requests

def DownloadFile(url)
    local_filename = url.split('/')[-1]
    r = requests.get(url)
    f = open(local_filename, 'wb')
    for chunk in r.iter_content(chunk_size=512 * 1024): 
        if chunk: # filter out keep-alive new chunks
            f.write(chunk)
    f.close()
    return 

```

For some reason it doesn't work this way; it still loads the response into memory before it is saved to a file.","With the following streaming code, the Python memory usage is restricted regardless of the size of the downloaded file:

```
def download_file(url):
    local_filename = url.split('/')[-1]
    # NOTE the stream=True parameter below
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192): 
                # If you have chunk encoded response uncomment if
                # and set chunk_size parameter to None.
                #if chunk: 
                f.write(chunk)
    return local_filename

```

Note that the number of bytes returned using `iter_content` is not exactly the `chunk_size`; it's expected to be a random number that is often far bigger, and is expected to be different in every iteration.

See [body-content-workflow](https://requests.readthedocs.io/en/latest/user/advanced/#body-content-workflow) and [Response.iter\_content](https://requests.readthedocs.io/en/latest/api/#requests.Response.iter_content) for further reference."
"I've been testing out Selenium with Chromedriver and I noticed that some pages can detect that you're using Selenium even though there's no automation at all. Even when I'm just browsing manually just using Chrome through Selenium and [Xephyr](https://en.wikipedia.org/wiki/Xephyr) I often get a page saying that suspicious activity was detected. I've checked my user agent, and my browser fingerprint, and they are all exactly identical to the normal Chrome browser.

When I browse to these sites in normal Chrome everything works fine, but the moment I use Selenium I'm detected.

In theory, chromedriver and Chrome should look literally exactly the same to any web server, but somehow they can detect it.

If you want some test code try out this:

```
from pyvirtualdisplay import Display
from selenium import webdriver

display = Display(visible=1, size=(1600, 902))
display.start()
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--disable-extensions')
chrome_options.add_argument('--profile-directory=Default')
chrome_options.add_argument(""--incognito"")
chrome_options.add_argument(""--disable-plugins-discovery"");
chrome_options.add_argument(""--start-maximized"")
driver = webdriver.Chrome(chrome_options=chrome_options)
driver.delete_all_cookies()
driver.set_window_size(800,800)
driver.set_window_position(0,0)
print 'arguments done'
driver.get('http://stubhub.com')

```

If you browse around stubhub you'll get redirected and 'blocked' within one or two requests. I've been investigating this and I can't figure out how they can tell that a user is using Selenium.

How do they do it?

I installed the Selenium IDE plugin in Firefox and I got banned when I went to stubhub.com in the normal Firefox browser with only the additional plugin.

When I use [Fiddler](https://learn.microsoft.com/en-us/windows/win32/win7appqual/fiddler-web-debugger-tool) to view the HTTP requests being sent back and forth I've noticed that the 'fake browser's' requests often have 'no-cache' in the response header.

Results like this *[Is there a way to detect that I'm in a Selenium Webdriver page from JavaScript?](https://stackoverflow.com/questions/3614472/is-there-a-way-to-detect-that-im-in-a-selenium-webdriver-page-from-javascript)* suggest that there should be no way to detect when you are using a webdriver. But this evidence suggests otherwise.

The site uploads a fingerprint to their servers, but I checked and the fingerprint of Selenium is identical to the fingerprint when using Chrome.

This is one of the fingerprint payloads that they send to their servers:

```
{""appName"":""Netscape"",""platform"":""Linuxx86_64"",""cookies"":1,""syslang"":""en-US"",""userlang"":""en-
US"",""cpu"":"""",""productSub"":""20030107"",""setTimeout"":1,""setInterval"":1,""plugins"":
{""0"":""ChromePDFViewer"",""1"":""ShockwaveFlash"",""2"":""WidevineContentDecryptionMo
dule"",""3"":""NativeClient"",""4"":""ChromePDFViewer""},""mimeTypes"":
{""0"":""application/pdf"",""1"":""ShockwaveFlashapplication/x-shockwave-
flash"",""2"":""FutureSplashPlayerapplication/futuresplash"",""3"":""WidevineContent
DecryptionModuleapplication/x-ppapi-widevine-
cdm"",""4"":""NativeClientExecutableapplication/x-
nacl"",""5"":""PortableNativeClientExecutableapplication/x-
pnacl"",""6"":""PortableDocumentFormatapplication/x-google-chrome-
pdf""},""screen"":{""width"":1600,""height"":900,""colorDepth"":24},""fonts"":
{""0"":""monospace"",""1"":""DejaVuSerif"",""2"":""Georgia"",""3"":""DejaVuSans"",""4"":""Trebu
chetMS"",""5"":""Verdana"",""6"":""AndaleMono"",""7"":""DejaVuSansMono"",""8"":""LiberationM
ono"",""9"":""NimbusMonoL"",""10"":""CourierNew"",""11"":""Courier""}}

```

It's identical in Selenium and in Chrome.

VPNs work for a single use, but they get detected after I load the first page. Clearly some JavaScript code is being run to detect Selenium.","Basically, the way the Selenium detection works, is that they test for predefined JavaScript variables which appear when running with Selenium. The bot detection scripts usually look anything containing word ""selenium"" / ""webdriver"" in any of the variables (on window object), and also document variables called `$cdc_` and `$wdc_`. Of course, all of this depends on which browser you are on. All the different browsers expose different things.

For me, I used Chrome, so, **all that I had to** do was to ensure that `$cdc_` didn't exist anymore as a document variable, and voil√† (download chromedriver source code, modify chromedriver and re-compile `$cdc_` under different name.)

This is the function I modified in chromedriver:

### File *call\_function.js*:

```
function getPageCache(opt_doc) {
  var doc = opt_doc || document;
  //var key = '$cdc_asdjflasutopfhvcZLmcfl_';
  var key = 'randomblabla_';
  if (!(key in doc))
    doc[key] = new Cache();
  return doc[key];
}

```

(Note the comment. All I did I turned `$cdc_` to `randomblabla_`.)

Here is pseudocode which demonstrates some of the techniques that bot networks might use:

```
runBotDetection = function () {
    var documentDetectionKeys = [
        ""__webdriver_evaluate"",
        ""__selenium_evaluate"",
        ""__webdriver_script_function"",
        ""__webdriver_script_func"",
        ""__webdriver_script_fn"",
        ""__fxdriver_evaluate"",
        ""__driver_unwrapped"",
        ""__webdriver_unwrapped"",
        ""__driver_evaluate"",
        ""__selenium_unwrapped"",
        ""__fxdriver_unwrapped"",
    ];

    var windowDetectionKeys = [
        ""_phantom"",
        ""__nightmare"",
        ""_selenium"",
        ""callPhantom"",
        ""callSelenium"",
        ""_Selenium_IDE_Recorder"",
    ];

    for (const windowDetectionKey in windowDetectionKeys) {
        const windowDetectionKeyValue = windowDetectionKeys[windowDetectionKey];
        if (window[windowDetectionKeyValue]) {
            return true;
        }
    };
    for (const documentDetectionKey in documentDetectionKeys) {
        const documentDetectionKeyValue = documentDetectionKeys[documentDetectionKey];
        if (window['document'][documentDetectionKeyValue]) {
            return true;
        }
    };

    for (const documentKey in window['document']) {
        if (documentKey.match(/\$[a-z]dc_/) && window['document'][documentKey]['cache_']) {
            return true;
        }
    }

    if (window['external'] && window['external'].toString() && (window['external'].toString()['indexOf']('Sequentum') != -1)) return true;

    if (window['document']['documentElement']['getAttribute']('selenium')) return true;
    if (window['document']['documentElement']['getAttribute']('webdriver')) return true;
    if (window['document']['documentElement']['getAttribute']('driver')) return true;

    return false;
};

```

According to [answer](https://stackoverflow.com/a/75776883/20443541), there are multiple methods to remove them. One of them is simply opening `chromedriver.exe` with a HEX-editor and removing all occurences of `$cdc_`"
"I am currently defining regular expressions in order to capture parameters in a URL, as described in the tutorial. How do I access parameters from the URL as part the `HttpRequest` object?

My `HttpRequest.GET` currently returns an empty `QueryDict` object.

I'd like to learn how to do this without a library, so I can get to know Django better.","Give URL:  
`domain/search/?q=haha`  
use:  
`request.GET.get('q', 'default')`.

`q` is the parameter, and `'default'` is the default value if `q` isn't found.

However, if you are instead just configuring your `URLconf`\*\*, then your captures from the `regex` are passed to the function as arguments (or named arguments).

Such as:

```
(r'^user/(?P<username>\w{0,50})/$', views.profile_page,),

```

Then in your `views.py` you would have

```
def profile_page(request, username):
    # Rest of the method

```"
"I have to run a Python script on a Windows server. How can I know which version of Python I have, and does it even really matter?

I was thinking of updating to the latest version of Python.","To check the version of one's Python's Software version, one should use the following code in command prompt:

```
python -V

```

Reference: <http://docs.python.org/using/cmdline.html#generic-options>"
How do I change the key of an entry in a Python dictionary?,"Easily done in 2 steps:

```
dictionary[new_key] = dictionary[old_key]
del dictionary[old_key]

```

Or in 1 step:

```
dictionary[new_key] = dictionary.pop(old_key)

```

which will raise `KeyError` if `dictionary[old_key]` is undefined. Note that this *will* delete `dictionary[old_key]`.

```
>>> dictionary = { 1: 'one', 2:'two', 3:'three' }
>>> dictionary['ONE'] = dictionary.pop(1)
>>> dictionary
{2: 'two', 3: 'three', 'ONE': 'one'}
>>> dictionary['ONE'] = dictionary.pop(1)
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
KeyError: 1

```"
"Is there a way to get **all** attributes/methods/fields/etc. of an object in Python?

`vars()` is *close* to what I want, but it doesn't work unless an object has a `__dict__`, which isn't always true (e.g. it's not true for a `list`, a `dict`, etc.).",Use the built-in function [`dir()`](http://docs.python.org/library/functions.html#dir).
"I have a conda environment named `old_name`, how can I change its name to `new_name` without breaking references?","New answer:
-----------

From [Conda 4.14](https://github.com/conda/conda/milestone/55) you will be able to use just:

```
conda rename -n old_name  new_name 

```

Although, under the hood, `conda rename` still uses [[1]](https://github.com/conda/conda/pull/11496/files#diff-2cf7ba1d1799c57376f2066177b166979c8f6854a2540c4dd5077f0b95031b93R241)[[2]](https://github.com/conda/conda/pull/11496/files#diff-66bae0eac87592b832fa7e85ad41263739a925b211d4b2f386385092e7df3ca4) undermentioned combination of `conda create` and `conda remove`.

Use the `-d` flag for dry-run (not destination, as of v22.11.0)

```
conda rename -n old_name -d new_name 

```

---

Old answer:
-----------

You can't.

One workaround is to [create](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) clone a new environment and then [remove](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#removing-an-environment) the original one.

First, remember to deactivate your current environment. You can do this with the commands:

* `deactivate` on Windows or
* `source deactivate` on macOS/Linux.

Then:

```
conda create --name new_name --clone old_name
conda remove --name old_name --all # or its alias: `conda env remove --name old_name`

```

Notice there are several drawbacks of this method:

1. It redownloads packages (you can use `--offline` flag to disable it)
2. Time consumed on copying environment's files
3. Temporary double disk usage

There is an open [issue](https://github.com/conda/conda/issues/3097) requesting this feature."
What is the difference between [`__str__`](https://docs.python.org/3/reference/datamodel.html#object.__str__) and [`__repr__`](https://docs.python.org/3/reference/datamodel.html#object.__repr__) in Python?,"[Alex Martelli summarized well](https://stackoverflow.com/a/1436756/3798217) but, surprisingly, was too succinct.

First, let me reiterate the main points in [Alex](https://stackoverflow.com/users/95810/alex-martelli)’s post:

* The default implementation is useless (it’s hard to think of one which wouldn’t be, but yeah)
* `__repr__` goal is to be unambiguous
* `__str__` goal is to be readable
* Container’s `__str__` uses contained objects’ `__repr__`

**Default implementation is useless**

This is mostly a surprise because Python’s defaults tend to be fairly useful. However, in this case, having a default for `__repr__` which would act like:

```
return ""%s(%r)"" % (self.__class__, self.__dict__)

```

Or in new f-string formatting:

```
return f""{self.__class__!s}({self.__dict__!r})""

```

would have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if `__repr__` is defined, and `__str__` is not, the object will behave as though `__str__=__repr__`.

This means, in simple terms: almost every object you implement should have a functional `__repr__` that’s usable for understanding the object. Implementing `__str__` is optional: do that if you need a “pretty print” functionality (for example, used by a report generator).

**The goal of `__repr__` is to be unambiguous**

Let me come right out and say it — I do not believe in debuggers. I don’t really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature — most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is a

```
log(INFO, ""I am in the weird function and a is"", a, ""and b is"", b, ""but I got a null C — using default"", default_c)

```

But you have to do the last step — make sure every object you implement has a useful repr, so code like that can just work. This is why the “eval” thing comes up: if you have enough information so `eval(repr(c))==c`, that means you know everything there is to know about `c`. If that’s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about `c` anyway. I usually use an eval-like format: `""MyClass(this=%r,that=%r)"" % (self.this,self.that)`. It does not mean that you can actually construct MyClass, or that those are the right constructor arguments — but it is a useful form to express “this is everything you need to know about this instance”.

Note: I used `%r` above, not `%s`. You always want to use `repr()` [or `%r` formatting character, equivalently] inside `__repr__` implementation, or you’re defeating the goal of repr. You want to be able to differentiate `MyClass(3)` and `MyClass(""3"")`.

**The goal of `__str__` is to be readable**

Specifically, it is not intended to be unambiguous — notice that `str(3)==str(""3"")`. Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be ""2010/4/12 15:35:22"", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class — as long is it supports readability, it is an improvement.

**Container’s `__str__` uses contained objects’ `__repr__`**

This seems surprising, doesn’t it? It is a little, but how readable would it be if it used their `__str__`?

```
[moshe is, 3, hello
world, this is a list, oh I don't know, containing just 4 elements]

```

Not very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you’re printing a list, just

```
print(""["" + "", "".join(lst) + ""]"")

```

(you can probably also figure out what to do about dictionaries).

**Summary**

Implement `__repr__` for any class you implement. This should be second nature. Implement `__str__` if you think it would be useful to have a string version which errs on the side of readability."
"How do I check if PyTorch is using the GPU? The `nvidia-smi` command can detect GPU activity, but I want to check it directly from inside a Python script.","These functions should help:

```
>>> import torch

>>> torch.cuda.is_available()
True

>>> torch.cuda.device_count()
1

>>> torch.cuda.current_device()
0

>>> torch.cuda.device(0)
<torch.cuda.device at 0x7efce0b03be0>

>>> torch.cuda.get_device_name(0)
'GeForce GTX 950M'

```

This tells us:

* CUDA is available and can be used by one device.
* `Device 0` refers to the GPU `GeForce GTX 950M`, and it is currently chosen by PyTorch."
"A simple looking, easy statement is throwing some errors in my face.

I have a JSON file called *strings.json* like this:

```
""strings"": [{""-name"": ""city"", ""#text"": ""City""}, {""-name"": ""phone"", ""#text"": ""Phone""}, ...,
            {""-name"": ""address"", ""#text"": ""Address""}]

```

I want to read the JSON file, just that for now. I have these statements which I found out, but it's not working:

```
import json
from pprint import pprint

with open('strings.json') as json_data:
    d = json.loads(json_data)
    json_data.close()
    pprint(d)

```

The error displayed on the console was this:

```
Traceback (most recent call last):
  File ""/home/.../android/values/manipulate_json.py"", line 5, in <module>
    d = json.loads(json_data)
  File ""/usr/lib/python2.7/json/__init__.py"", line 326, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.7/json/decoder.py"", line 366, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
TypeError: expected string or buffer
[Finished in 0.1s with exit code 1]

```

---

If I use `json.load` instead of `json.loads`, I get this error:

```
Traceback (most recent call last):
  File ""/home/.../android/values/manipulate_json.py"", line 5, in <module>
    d = json.load(json_data)
  File ""/usr/lib/python2.7/json/__init__.py"", line 278, in load
    **kw)
  File ""/usr/lib/python2.7/json/__init__.py"", line 326, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.7/json/decoder.py"", line 369, in decode
    raise ValueError(errmsg(""Extra data"", s, end, len(s)))
ValueError: Extra data: line 829 column 1 - line 829 column 2 (char 18476 - 18477)
[Finished in 0.1s with exit code 1]

```","The [`json.load()` method](http://docs.python.org/3/library/json.html#json.load) (without ""s"" in ""load"") can read a file directly:

```
import json

with open('strings.json') as f:
    d = json.load(f)
    print(d)

```

You were using the [`json.loads()` method](http://docs.python.org/3/library/json.html#json.loads), which is used for *string* arguments only.

---

The error you get with `json.loads` is a totally different problem. In that case, there is some invalid JSON content in that file. For that, I would recommend running the file through a [JSON validator](http://jsonlint.com/).

There are also solutions for fixing JSON like for example *[How do I automatically fix an invalid JSON string?](https://stackoverflow.com/questions/18514910/how-do-i-automatically-fix-an-invalid-json-string)*."
"I've tried to find a comprehensive guide on whether it is best to use `import module` or `from module import`. I've just started with Python and I'm trying to start off with best practices in mind.

Basically, I was hoping if anyone could share their experiences, what preferences other developers have and what's the best way to avoid any *gotchas* down the road?","The difference between `import module` and `from module import foo` is mainly subjective. Pick the one you like best and be consistent in your use of it. Here are some points to help you decide.

`import module`

* **Pros:**
  + Less maintenance of your `import` statements. Don't need to add any additional imports to start using another item from the module
* **Cons:**
  + Typing `module.foo` in your code can be tedious and redundant (tedium can be minimized by using `import module as mo` then typing `mo.foo`)

`from module import foo`

* **Pros:**
  + Less typing to use `foo`
  + More control over which items of a module can be accessed
* **Cons:**
  + To use a new item from the module you have to update your `import` statement
  + You lose context about `foo`. For example, it's less clear what `ceil()` does compared to `math.ceil()`

Either method is acceptable, but **don't** use `from module import *`.

For any reasonable large set of code, if you `import *` you will likely be cementing it into the module, unable to be removed. This is because it is difficult to determine what items used in the code are coming from 'module', making it easy to get to the point where you think you don't use the `import` any more but it's extremely difficult to be sure."
"I know the obvious answer is to use virtualenv and virtualenvwrapper, but for various reasons I can't/don't want to do that.

So how do I modify the command

```
pip install package_name

```

to make `pip` install the package somewhere other than the default `site-packages`?","The [--target](https://pip.pypa.io/en/latest/cli/pip_install/#cmdoption-t) switch is the thing you're looking for:

```
pip install --target d:\somewhere\other\than\the\default package_name

```

But you still need to add `d:\somewhere\other\than\the\default` to `PYTHONPATH` to actually use them from that location.

> **-t, --target <dir>**  
> Install packages into <dir>. By default this will not replace existing files/folders in <dir>.  
> Use --upgrade to replace existing packages in <dir> with new versions.

---

Upgrade pip if target switch is not available:

On Linux or OS X:

```
pip install -U pip

```

On Windows (this works around [an issue](https://github.com/pypa/pip/issues/1299)):

```
python -m pip install -U pip

```"
"When the user accesses this URL running on my flask app, I want the web service to be able to handle the parameters specified after the question mark:

```
http://10.1.1.1:5000/login?username=alex&password=pw1

#I just want to be able to manipulate the parameters
@app.route('/login', methods=['GET', 'POST'])
def login():
    username = request.form['username']
    print(username)
    password = request.form['password']
    print(password)

```","Use [`request.args`](http://flask.pocoo.org/docs/api/#flask.Request.args) to get parsed contents of query string:

```
from flask import request

@app.route(...)
def login():
    username = request.args.get('username')
    password = request.args.get('password')

```"
"I wrote a Python program that acts on a large input file to create a few million objects representing triangles. The algorithm is:

1. read an input file
2. process the file and create a list of triangles, represented by their vertices
3. output the vertices in the OFF format: a list of vertices followed by a list of triangles. The triangles are represented by indices into the list of vertices

The requirement of OFF that I print out the complete list of vertices before I print out the triangles means that I have to hold the list of triangles in memory before I write the output to file. In the meanwhile I'm getting memory errors because of the sizes of the lists.

What is the best way to tell Python that I no longer need some of the data, and it can be freed?","According to [Python Official Documentation](http://docs.python.org/library/gc.html), you can explicitly invoke the Garbage Collector to release unreferenced memory with `gc.collect()`. Example:

```
import gc

gc.collect()

```

You should do that after marking what you want to discard using `del`:

```
del my_array
del my_object
gc.collect()

```"
"Let's say I have two lists, `l1` and `l2`. I want to perform `l1 - l2`, which returns all elements of `l1` not in `l2`.

I can think of a naive loop approach to doing this, but that is going to be really inefficient. What is a pythonic and efficient way of doing this?

As an example, if I have `l1 = [1,2,6,8] and l2 = [2,3,5,8]`, `l1 - l2` should return `[1,6]`","Python has a language feature called [List Comprehensions](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) that is perfectly suited to making this sort of thing extremely easy. The following statement does exactly what you want and stores the result in `l3`:

```
l3 = [x for x in l1 if x not in l2]

```

`l3` will contain `[1, 6]`."
"How do I split a string every nth character?

```
'1234567890'   â†’   ['12', '34', '56', '78', '90']

```

---

For the same question with a list, see [How do I split a list into equally-sized chunks?](https://stackoverflow.com/q/312443).","```
>>> line = '1234567890'
>>> n = 2
>>> [line[i:i+n] for i in range(0, len(line), n)]
['12', '34', '56', '78', '90']

```"
"I'm getting a datetime string in a format like ""2009-05-28T16:15:00"" (this is ISO 8601, I believe). One hackish option seems to be to parse the string using `time.strptime` and passing the first six elements of the tuple into the datetime constructor, like:

```
datetime.datetime(*time.strptime(""2007-03-04T21:08:12"", ""%Y-%m-%dT%H:%M:%S"")[:6])

```

I haven't been able to find a ""cleaner"" way of doing this. Is there one?","I prefer using the [dateutil](https://pypi.org/project/python-dateutil/) library for timezone handling and generally solid date parsing. If you were to get an `ISO 8601` string like: `2010-05-08T23:41:54.000Z` you'd have a fun time parsing that with strptime, especially if you didn't know up front whether or not the timezone was included. `pyiso8601` has a couple of issues (check their tracker) that I ran into during my usage and it hasn't been updated in a few years. dateutil, by contrast, has been active and worked for me:

```
from dateutil import parser
yourdate = parser.parse(datestring)

```"
"Imagine this directory structure:

```
app/
   __init__.py
   sub1/
      __init__.py
      mod1.py
   sub2/
      __init__.py
      mod2.py

```

I'm coding `mod1`, and I need to import something from `mod2`. How should I do it?

I tried `from ..sub2 import mod2`, but I'm getting an ""Attempted relative import in non-package"".

I googled around, but I found only ""`sys.path` manipulation"" hacks. Isn't there a clean way?

---

All my `__init__.py`'s are currently empty

I'm trying to do this because sub2 contains classes that are shared across sub packages (`sub1`, `subX`, etc.).

The behaviour I'm looking for is the same as described in [PEP 366](http://www.python.org/dev/peps/pep-0366/) ([thanks John B](https://stackoverflow.com/questions/72852/how-to-do-relative-imports-in-python#comment8465_72852)).","The problem is that you're running the module as '\_\_main\_\_' by passing the mod1.py as an argument to the interpreter.

From [PEP 328](http://www.python.org/dev/peps/pep-0328/):

> Relative imports use a module's \_\_name\_\_ attribute to determine that module's position in the package hierarchy. If the module's name does not contain any package information (e.g. it is set to '\_\_main\_\_') then relative imports are resolved as if the module were a top level module, regardless of where the module is actually located on the file system.

In Python 2.6, they're adding the ability to reference modules relative to the main module. [PEP 366](http://python.org/dev/peps/pep-0366/) describes the change."
"How can I select rows from a DataFrame based on values in some column in Pandas?

In SQL, I would use:

```
SELECT *
FROM table
WHERE column_name = some_value

```","To select rows whose column value equals a scalar, `some_value`, use `==`:

```
df.loc[df['column_name'] == some_value]

```

To select rows whose column value is in an iterable, `some_values`, use `isin`:

```
df.loc[df['column_name'].isin(some_values)]

```

Combine multiple conditions with `&`:

```
df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]

```

Note the parentheses. Due to Python's [operator precedence rules](https://docs.python.org/3/reference/expressions.html#operator-precedence), `&` binds more tightly than `<=` and `>=`. Thus, the parentheses in the last example are necessary. Without the parentheses

```
df['column_name'] >= A & df['column_name'] <= B

```

is parsed as

```
df['column_name'] >= (A & df['column_name']) <= B

```

which results in a [Truth value of a Series is ambiguous error](https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o).

---

To select rows whose column value *does not equal* `some_value`, use `!=`:

```
df.loc[df['column_name'] != some_value]

```

The `isin` returns a boolean Series, so to select rows whose value is *not* in `some_values`, negate the boolean Series using `~`:

```
df = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place replacement

```

---

For example,

```
import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])

```

yields

```
     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14

```

---

If you have multiple values you want to include, put them in a
list (or more generally, any iterable) and use `isin`:

```
print(df.loc[df['B'].isin(['one','three'])])

```

yields

```
     A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14

```

---

Note, however, that if you wish to do this many times, it is more efficient to
make an index first, and then use `df.loc`:

```
df = df.set_index(['B'])
print(df.loc['one'])

```

yields

```
       A  C   D
B              
one  foo  0   0
one  bar  1   2
one  foo  6  12

```

or, to include multiple values from the index use `df.index.isin`:

```
df.loc[df.index.isin(['one','two'])]

```

yields

```
       A  C   D
B              
one  foo  0   0
one  bar  1   2
two  foo  2   4
two  foo  4   8
two  bar  5  10
one  foo  6  12

```"
"**How do I pivot the pandas dataframe `df` defined at bottom such that the `col` values become columns, `row` values become the index, and mean of `val0` becomes the values?** (in some cases this is called transforming from long-format to wide-format)

(See note at bottom: Why is this question not a duplicate? and why this is thematically one question and not too broad.)

### Subquestions

1. (How to avoid getting `ValueError: Index contains duplicate entries, cannot reshape`?)
2. **How do I pivot `df` defined at bottom, such that the `col` values become columns, `row` values become the index, and mean of `val0` are the values?**

   ```
   col   col0   col1   col2   col3  col4
   row
   row0  0.77  0.605    NaN  0.860  0.65
   row2  0.13    NaN  0.395  0.500  0.25
   row3   NaN  0.310    NaN  0.545   NaN
   row4   NaN  0.100  0.395  0.760  0.24

   ```

How do I pivot...

3. ... so that missing values are `0`?

   ```
   col   col0   col1   col2   col3  col4
   row
   row0  0.77  0.605  0.000  0.860  0.65
   row2  0.13  0.000  0.395  0.500  0.25
   row3  0.00  0.310  0.000  0.545  0.00
   row4  0.00  0.100  0.395  0.760  0.24

   ```
4. ... to do an aggregate function other than `mean`, like `sum`?

   ```
   col   col0  col1  col2  col3  col4
   row
   row0  0.77  1.21  0.00  0.86  0.65
   row2  0.13  0.00  0.79  0.50  0.50
   row3  0.00  0.31  0.00  1.09  0.00
   row4  0.00  0.10  0.79  1.52  0.24

   ```
5. ... to do more that one aggregation at a time?

   ```
          sum                          mean
   col   col0  col1  col2  col3  col4  col0   col1   col2   col3  col4
   row
   row0  0.77  1.21  0.00  0.86  0.65  0.77  0.605  0.000  0.860  0.65
   row2  0.13  0.00  0.79  0.50  0.50  0.13  0.000  0.395  0.500  0.25
   row3  0.00  0.31  0.00  1.09  0.00  0.00  0.310  0.000  0.545  0.00
   row4  0.00  0.10  0.79  1.52  0.24  0.00  0.100  0.395  0.760  0.24

   ```
6. ... to aggregate over multiple 'value' columns?

   ```
         val0                             val1
   col   col0   col1   col2   col3  col4  col0   col1  col2   col3  col4
   row
   row0  0.77  0.605  0.000  0.860  0.65  0.01  0.745  0.00  0.010  0.02
   row2  0.13  0.000  0.395  0.500  0.25  0.45  0.000  0.34  0.440  0.79
   row3  0.00  0.310  0.000  0.545  0.00  0.00  0.230  0.00  0.075  0.00
   row4  0.00  0.100  0.395  0.760  0.24  0.00  0.070  0.42  0.300  0.46

   ```
7. ... **to subdivide by multiple columns?** (item0,item1,item2..., col0,col1,col2...)

   ```
   item item0             item1                         item2
   col   col2  col3  col4  col0  col1  col2  col3  col4  col0   col1  col3  col4
   row
   row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.605  0.86  0.65
   row2  0.35  0.00  0.37  0.00  0.00  0.44  0.00  0.00  0.13  0.000  0.50  0.13
   row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.000  0.28  0.00
   row4  0.15  0.64  0.00  0.00  0.10  0.64  0.88  0.24  0.00  0.000  0.00  0.00

   ```
8. ... **to subdivide by multiple rows:** (key0,key1... row0,row1,row2...)

   ```
   item      item0             item1                         item2
   col        col2  col3  col4  col0  col1  col2  col3  col4  col0  col1  col3  col4
   key  row
   key0 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.86  0.00
        row2  0.00  0.00  0.37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50  0.00
        row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.00  0.00  0.00
        row4  0.15  0.64  0.00  0.00  0.00  0.00  0.00  0.24  0.00  0.00  0.00  0.00
   key1 row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.81  0.00  0.65
        row2  0.35  0.00  0.00  0.00  0.00  0.44  0.00  0.00  0.00  0.00  0.00  0.13
        row3  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.28  0.00
        row4  0.00  0.00  0.00  0.00  0.10  0.00  0.00  0.00  0.00  0.00  0.00  0.00
   key2 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.00  0.00
        row2  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.00  0.00
        row4  0.00  0.00  0.00  0.00  0.00  0.64  0.88  0.00  0.00  0.00  0.00  0.00

   ```
9. ... **to aggregate the frequency** in which the column and rows occur together, aka **""cross tabulation""**?

   ```
   col   col0  col1  col2  col3  col4
   row
   row0     1     2     0     1     1
   row2     1     0     2     1     2
   row3     0     1     0     2     0
   row4     0     1     2     2     1

   ```
10. ... to convert a DataFrame **from long-to-wide** by pivoting on ONLY two columns? Given:

    ```
    np.random.seed([3, 1415])
    df2 = pd.DataFrame({'A': list('aaaabbbc'), 'B': np.random.choice(15, 8)})
    df2
       A   B
    0  a   0
    1  a  11
    2  a   2
    3  a  11
    4  b  10
    5  b  10
    6  b  14
    7  c   7

    ```

    The expected should look something like

    ```
          a     b    c
    0   0.0  10.0  7.0
    1  11.0  10.0  NaN
    2   2.0  14.0  NaN
    3  11.0   NaN  NaN

    ```
11. ... **to flatten the multiple index to a single multi-index** after pivot?

    From:

    ```
       1  2
       1  1  2
    a  2  1  1
    b  2  1  0
    c  1  0  0

    ```

    To:

    ```
       1|1  2|1  2|2
    a    2    1    1
    b    2    1    0
    c    1    0    0

    ```

Setup
-----

Consider a dataframe df with columns 'key', 'row', 'item', 'col', and random float values 'val0', 'val1'. I conspicuously named the columns and relevant column values to correspond with how I want to pivot them.

```
import numpy as np
import pandas as pd
from numpy.core.defchararray import add

np.random.seed([3,1415])
n = 20

cols = np.array(['key', 'row', 'item', 'col'])
arr1 = (np.random.randint(5, size=(n, 4)) // [2, 1, 2, 1]).astype(str)

df = pd.DataFrame(
    add(cols, arr1), columns=cols
).join(
    pd.DataFrame(np.random.rand(n, 2).round(2)).add_prefix('val')
)
print(df)

```

```
     key   row   item   col  val0  val1
0   key0  row3  item1  col3  0.81  0.04
1   key1  row2  item1  col2  0.44  0.07
2   key1  row0  item1  col0  0.77  0.01
3   key0  row4  item0  col2  0.15  0.59
4   key1  row0  item2  col1  0.81  0.64
5   key1  row2  item2  col4  0.13  0.88
6   key2  row4  item1  col3  0.88  0.39
7   key1  row4  item1  col1  0.10  0.07
8   key1  row0  item2  col4  0.65  0.02
9   key1  row2  item0  col2  0.35  0.61
10  key2  row0  item2  col1  0.40  0.85
11  key2  row4  item1  col2  0.64  0.25
12  key0  row2  item2  col3  0.50  0.44
13  key0  row4  item1  col4  0.24  0.46
14  key1  row3  item2  col3  0.28  0.11
15  key0  row3  item1  col1  0.31  0.23
16  key0  row0  item2  col3  0.86  0.01
17  key0  row4  item0  col3  0.64  0.21
18  key2  row2  item2  col0  0.13  0.45
19  key0  row2  item0  col4  0.37  0.70

```

---

Why is this question not a duplicate? and more useful than the following autosuggestions:

1. [How to pivot a dataframe in Pandas?](https://stackoverflow.com/q/28337117/2336654) only covers the specific case of 'Country' to row-index, values of 'Indicator' for 'Year' to multiple columns and no aggregation of values.
2. [pandas pivot table to data frame](https://stackoverflow.com/q/42708193/2336654)
   asks how to pivot in pandas like in R, i.e. autogenerate an individual column for each value of `strength...`
3. [pandas pivoting a dataframe, duplicate rows](https://stackoverflow.com/q/11400181/2336654) asks about the syntax for [pivoting](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html#pandas.DataFrame.pivot) multiple columns, without needing to list them all.

None of the existing questions and answers are comprehensive, so this is an attempt at a [canonical question and answer](https://meta.stackoverflow.com/questions/291992/what-is-a-canonical-question-answer-and-what-is-their-purpose) that encompasses all aspects of pivoting.","Here is a list of idioms we can use to pivot

1. [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

   * A glorified version of `groupby` with more intuitive API. For many people, this is the preferred approach. And it is the intended approach by the developers.
   * Specify row level, column levels, values to be aggregated, and function(s) to perform aggregations.
2. [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) + [`pd.DataFrame.unstack`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html)

   * Good general approach for doing just about any type of pivot
   * You specify all columns that will constitute the pivoted row levels and column levels in one group by. You follow that by selecting the remaining columns you want to aggregate and the function(s) you want to perform the aggregation. Finally, you `unstack` the levels that you want to be in the column index.
3. [`pd.DataFrame.set_index`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) + [`pd.DataFrame.unstack`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html)

   * Convenient and intuitive for some (myself included). Cannot handle duplicate grouped keys.
   * Similar to the `groupby` paradigm, we specify all columns that will eventually be either row or column levels and set those to be the index. We then `unstack` the levels we want in the columns. If either the remaining index levels or column levels are not unique, this method will fail.
4. [`pd.DataFrame.pivot`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html)

   * Very similar to `set_index` in that it shares the duplicate key limitation. The API is very limited as well. It only takes scalar values for `index`, `columns`, `values`.
   * Similar to the `pivot_table` method in that we select rows, columns, and values on which to pivot. However, we cannot aggregate and if either rows or columns are not unique, this method will fail.
5. [`pd.crosstab`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

   * This a specialized version of `pivot_table` and in its purest form is the most intuitive way to perform several tasks.
6. [`pd.factorize`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html) + [`np.bincount`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.bincount.html)

   * This is a highly advanced technique that is very obscure but is very fast. It cannot be used in all circumstances, but when it can be used and you are comfortable using it, you will reap the performance rewards.
7. [`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) + [`pd.DataFrame.dot`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html)

   * I use this for cleverly performing cross tabulation.

See also:

* [Reshaping and pivot tables](https://pandas.pydata.org/docs/user_guide/reshaping.html) â€” pandas User Guide

---

### Question 1

> Why do I get `ValueError: Index contains duplicate entries, cannot reshape`

This occurs because pandas is attempting to reindex either a `columns` or `index` object with duplicate entries. There are varying methods to use that can perform a pivot. Some of them are not well suited to when there are duplicates of the keys on which it is being asked to pivot. For example: Consider `pd.DataFrame.pivot`. I know there are duplicate entries that share the `row` and `col` values:

```
df.duplicated(['row', 'col']).any()

True

```

So when I `pivot` using

```
df.pivot(index='row', columns='col', values='val0')

```

I get the error mentioned above. In fact, I get the same error when I try to perform the same task with:

```
df.set_index(['row', 'col'])['val0'].unstack()

```

---

Examples
--------

What I'm going to do for each subsequent question is to answer it using [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html). Then I'll provide alternatives to perform the same task.

### Questions 2 and 3

> How do I pivot `df` such that the `col` values are columns, `row` values are the index, and mean of `val0` are the values?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  ```
  df.pivot_table(
      values='val0', index='row', columns='col',
      aggfunc='mean')

  col   col0   col1   col2   col3  col4
  row                                  
  row0  0.77  0.605    NaN  0.860  0.65
  row2  0.13    NaN  0.395  0.500  0.25
  row3   NaN  0.310    NaN  0.545   NaN
  row4   NaN  0.100  0.395  0.760  0.24

  ```

  + `aggfunc='mean'` is the default and I didn't have to set it. I included it to be explicit.

> How do I make it so that missing values are 0?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  + `fill_value` is not set by default. I tend to set it appropriately. In this case I set it to `0`.

  ```
  df.pivot_table(
      values='val0', index='row', columns='col',
      fill_value=0, aggfunc='mean')

  col   col0   col1   col2   col3  col4
  row
  row0  0.77  0.605  0.000  0.860  0.65
  row2  0.13  0.000  0.395  0.500  0.25
  row3  0.00  0.310  0.000  0.545  0.00
  row4  0.00  0.100  0.395  0.760  0.24

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(['row', 'col'])['val0'].mean().unstack(fill_value=0)

  ```
* [`pd.crosstab`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

  ```
  pd.crosstab(
      index=df['row'], columns=df['col'],
      values=df['val0'], aggfunc='mean').fillna(0)

  ```

---

### Question 4

> Can I get something other than `mean`, like maybe `sum`?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  ```
  df.pivot_table(
      values='val0', index='row', columns='col',
      fill_value=0, aggfunc='sum')

  col   col0  col1  col2  col3  col4
  row
  row0  0.77  1.21  0.00  0.86  0.65
  row2  0.13  0.00  0.79  0.50  0.50
  row3  0.00  0.31  0.00  1.09  0.00
  row4  0.00  0.10  0.79  1.52  0.24

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(['row', 'col'])['val0'].sum().unstack(fill_value=0)

  ```
* [`pd.crosstab`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

  ```
  pd.crosstab(
      index=df['row'], columns=df['col'],
      values=df['val0'], aggfunc='sum').fillna(0)

  ```

---

### Question 5

> Can I do more that one aggregation at a time?

Notice that for `pivot_table` and `crosstab` I needed to pass list of callables. On the other hand, `groupby.agg` is able to take strings for a limited number of special functions. `groupby.agg` would also have taken the same callables we passed to the others, but it is often more efficient to leverage the string function names as there are efficiencies to be gained.

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  ```
  df.pivot_table(
      values='val0', index='row', columns='col',
      fill_value=0, aggfunc=[np.size, np.mean])

       size                      mean
  col  col0 col1 col2 col3 col4  col0   col1   col2   col3  col4
  row
  row0    1    2    0    1    1  0.77  0.605  0.000  0.860  0.65
  row2    1    0    2    1    2  0.13  0.000  0.395  0.500  0.25
  row3    0    1    0    2    0  0.00  0.310  0.000  0.545  0.00
  row4    0    1    2    2    1  0.00  0.100  0.395  0.760  0.24

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(['row', 'col'])['val0'].agg(['size', 'mean']).unstack(fill_value=0)

  ```
* [`pd.crosstab`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

  ```
  pd.crosstab(
      index=df['row'], columns=df['col'],
      values=df['val0'], aggfunc=[np.size, np.mean]).fillna(0, downcast='infer')

  ```

---

### Question 6

> Can I aggregate over multiple value columns?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html) we pass `values=['val0', 'val1']` but we could've left that off completely

  ```
  df.pivot_table(
      values=['val0', 'val1'], index='row', columns='col',
      fill_value=0, aggfunc='mean')

        val0                             val1
  col   col0   col1   col2   col3  col4  col0   col1  col2   col3  col4
  row
  row0  0.77  0.605  0.000  0.860  0.65  0.01  0.745  0.00  0.010  0.02
  row2  0.13  0.000  0.395  0.500  0.25  0.45  0.000  0.34  0.440  0.79
  row3  0.00  0.310  0.000  0.545  0.00  0.00  0.230  0.00  0.075  0.00
  row4  0.00  0.100  0.395  0.760  0.24  0.00  0.070  0.42  0.300  0.46

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(['row', 'col'])['val0', 'val1'].mean().unstack(fill_value=0)

  ```

---

### Question 7

> Can I subdivide by multiple columns?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  ```
  df.pivot_table(
      values='val0', index='row', columns=['item', 'col'],
      fill_value=0, aggfunc='mean')

  item item0             item1                         item2
  col   col2  col3  col4  col0  col1  col2  col3  col4  col0   col1  col3  col4
  row
  row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.605  0.86  0.65
  row2  0.35  0.00  0.37  0.00  0.00  0.44  0.00  0.00  0.13  0.000  0.50  0.13
  row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.000  0.28  0.00
  row4  0.15  0.64  0.00  0.00  0.10  0.64  0.88  0.24  0.00  0.000  0.00  0.00

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(
      ['row', 'item', 'col']
  )['val0'].mean().unstack(['item', 'col']).fillna(0).sort_index(1)

  ```

---

### Question 8

> Can I subdivide by multiple columns?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  ```
  df.pivot_table(
      values='val0', index=['key', 'row'], columns=['item', 'col'],
      fill_value=0, aggfunc='mean')

  item      item0             item1                         item2
  col        col2  col3  col4  col0  col1  col2  col3  col4  col0  col1  col3  col4
  key  row
  key0 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.86  0.00
       row2  0.00  0.00  0.37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50  0.00
       row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.00  0.00  0.00
       row4  0.15  0.64  0.00  0.00  0.00  0.00  0.00  0.24  0.00  0.00  0.00  0.00
  key1 row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.81  0.00  0.65
       row2  0.35  0.00  0.00  0.00  0.00  0.44  0.00  0.00  0.00  0.00  0.00  0.13
       row3  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.28  0.00
       row4  0.00  0.00  0.00  0.00  0.10  0.00  0.00  0.00  0.00  0.00  0.00  0.00
  key2 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.00  0.00
       row2  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.00  0.00
       row4  0.00  0.00  0.00  0.00  0.00  0.64  0.88  0.00  0.00  0.00  0.00  0.00

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(
      ['key', 'row', 'item', 'col']
  )['val0'].mean().unstack(['item', 'col']).fillna(0).sort_index(1)

  ```
* [`pd.DataFrame.set_index`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) because the set of keys are unique for both rows and columns

  ```
  df.set_index(
      ['key', 'row', 'item', 'col']
  )['val0'].unstack(['item', 'col']).fillna(0).sort_index(1)

  ```

---

### Question 9

> Can I aggregate the frequency in which the column and rows occur together, aka ""cross tabulation""?

* [`pd.DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  ```
  df.pivot_table(index='row', columns='col', fill_value=0, aggfunc='size')

  col   col0  col1  col2  col3  col4
  row
  row0     1     2     0     1     1
  row2     1     0     2     1     2
  row3     0     1     0     2     0
  row4     0     1     2     2     1

  ```
* [`pd.DataFrame.groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

  ```
  df.groupby(['row', 'col'])['val0'].size().unstack(fill_value=0)

  ```
* [`pd.crosstab`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

  ```
  pd.crosstab(df['row'], df['col'])

  ```
* [`pd.factorize`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html) + [`np.bincount`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.bincount.html)

  ```
  # get integer factorization `i` and unique values `r`
  # for column `'row'`
  i, r = pd.factorize(df['row'].values)
  # get integer factorization `j` and unique values `c`
  # for column `'col'`
  j, c = pd.factorize(df['col'].values)
  # `n` will be the number of rows
  # `m` will be the number of columns
  n, m = r.size, c.size
  # `i * m + j` is a clever way of counting the
  # factorization bins assuming a flat array of length
  # `n * m`.  Which is why we subsequently reshape as `(n, m)`
  b = np.bincount(i * m + j, minlength=n * m).reshape(n, m)
  # BTW, whenever I read this, I think 'Bean, Rice, and Cheese'
  pd.DataFrame(b, r, c)

        col3  col2  col0  col1  col4
  row3     2     0     0     1     0
  row2     1     2     1     0     2
  row0     1     0     1     2     1
  row4     2     2     0     1     1

  ```
* [`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)

  ```
  pd.get_dummies(df['row']).T.dot(pd.get_dummies(df['col']))

        col0  col1  col2  col3  col4
  row0     1     2     0     1     1
  row2     1     0     2     1     2
  row3     0     1     0     2     0
  row4     0     1     2     2     1

  ```

---

### Question 10

> How do I convert a DataFrame from long to wide by pivoting on ONLY two
> columns?

* [`DataFrame.pivot`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html)

  The first step is to assign a number to each row - this number will be the row index of that value in the pivoted result. This is done using [`GroupBy.cumcount`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html):

  ```
  df2.insert(0, 'count', df2.groupby('A').cumcount())
  df2

     count  A   B
  0      0  a   0
  1      1  a  11
  2      2  a   2
  3      3  a  11
  4      0  b  10
  5      1  b  10
  6      2  b  14
  7      0  c   7

  ```

  The second step is to use the newly created column as the index to call [`DataFrame.pivot`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html).

  ```
  df2.pivot(*df2)
  # df2.pivot(index='count', columns='A', values='B')

  A         a     b    c
  count
  0       0.0  10.0  7.0
  1      11.0  10.0  NaN
  2       2.0  14.0  NaN
  3      11.0   NaN  NaN

  ```
* [`DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html)

  Whereas [`DataFrame.pivot`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html) only accepts columns, [`DataFrame.pivot_table`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html) also accepts arrays, so the `GroupBy.cumcount` can be passed directly as the `index` without creating an explicit column.

  ```
  df2.pivot_table(index=df2.groupby('A').cumcount(), columns='A', values='B')

  A         a     b    c
  0       0.0  10.0  7.0
  1      11.0  10.0  NaN
  2       2.0  14.0  NaN
  3      11.0   NaN  NaN

  ```

---

### Question 11

> How do I flatten the multiple index to single index after `pivot`

If `columns` type `object` with string `join`

```
df.columns = df.columns.map('|'.join)

```

else `format`

```
df.columns = df.columns.map('{0[0]}|{0[1]}'.format)

```"
"I would like to get the first item from a list matching a condition. It's important that the resulting method not process the entire list, which could be quite large. For example, the following function is adequate:

```
def first(the_iterable, condition = lambda x: True):
    for i in the_iterable:
        if condition(i):
            return i

```

This function could be used something like this:

```
>>> first(range(10))
0
>>> first(range(10), lambda i: i > 3)
4

```

However, I can't think of a good built-in / one-liner to let me do this. I don't particularly want to copy this function around if I don't have to. Is there a built-in way to get the first item matching a condition?","Python 2.6+ and Python 3:
-------------------------

If you want `StopIteration` to be raised if no matching element is found:

```
next(x for x in the_iterable if x > 3)

```

If you want `default_value` (e.g. `None`) to be returned instead:

```
next((x for x in the_iterable if x > 3), default_value)

```

Note that you need an extra pair of parentheses around the generator expression in this case âˆ’ they are needed whenever the generator expression isn't the only argument.

I see most answers resolutely ignore the [`next`](https://docs.python.org/2/library/functions.html#next) built-in and so I assume that for some mysterious reason they're 100% focused on versions 2.5 and older -- without mentioning the Python-version issue (but then I don't see that mention in the answers that *do* mention the `next` built-in, which is why I thought it necessary to provide an answer myself -- at least the ""correct version"" issue gets on record this way;-).

Python <= 2.5
-------------

The [`.next()`](https://docs.python.org/2.5/ref/yieldexpr.html#l2h-407) method of iterators immediately raises `StopIteration` if the iterator immediately finishes -- i.e., for your use case, if no item in the iterable satisfies the condition. If you don't care (i.e., you know there **must** be at least one satisfactory item) then just use `.next()` (best on a genexp, line for the `next` built-in in Python 2.6 and better).

If you *do* care, wrapping things in a function as you had first indicated in your Q seems best, and while the function implementation you proposed is just fine, you could alternatively use `itertools`, a `for...: break` loop, or a genexp, or a `try/except StopIteration` as the function's body, as various answers suggested. There's not much added value in any of these alternatives so I'd go for the starkly-simple version you first proposed."
"My [Google-fu](https://english.stackexchange.com/questions/19967/what-does-google-fu-mean) has failed me.

In Python, are the following two tests for equality equivalent?

```
n = 5
# Test one.
if n == 5:
    print 'Yay!'

# Test two.
if n is 5:
    print 'Yay!'

```

Does this hold true for objects where you would be comparing instances (a `list` say)?

Okay, so this kind of answers my question:

```
L = []
L.append(1)
if L == [1]:
    print 'Yay!'
# Holds true, but...

if L is [1]:
    print 'Yay!'
# Doesn't.

```

So `==` tests value where `is` tests to see if they are the same object?","`is` will return `True` if two variables point to the same object (in memory), `==` if the objects referred to by the variables are equal.

```
>>> a = [1, 2, 3]
>>> b = a
>>> b is a 
True
>>> b == a
True

# Make a new copy of list `a` via the slice operator, 
# and assign it to variable `b`
>>> b = a[:] 
>>> b is a
False
>>> b == a
True

```

In your case, the second test only works because Python caches small integer objects, which is an implementation detail. For larger integers, this does not work:

```
>>> 1000 is 10**3
False
>>> 1000 == 10**3
True

```

The same holds true for string literals:

```
>>> ""a"" is ""a""
True
>>> ""aa"" is ""a"" * 2
True
>>> x = ""a""
>>> ""aa"" is x * 2
False
>>> ""aa"" is intern(x*2)
True

```

Please see [this question](https://stackoverflow.com/questions/26595/is-there-any-difference-between-foo-is-none-and-foo-none) as well."
"I am using the datetime Python module. I am looking to calculate the date 6 months from the current date. Could someone give me a little help doing this?

The reason I want to generate a date 6 months from the current date is to produce a *review date*. If the user enters data into the system it will have a review date of 6 months from the date they entered the data.","I found this solution to be good. (This uses the [python-dateutil extension](https://dateutil.readthedocs.org/en/latest/))

```
from datetime import date
from dateutil.relativedelta import relativedelta

six_months = date.today() + relativedelta(months=+6)

```

The advantage of this approach is that it takes care of issues with 28, 30, 31 days etc. This becomes very useful in handling business rules and scenarios (say invoice generation etc.)

```
$ date(2010,12,31)+relativedelta(months=+1)
  datetime.date(2011, 1, 31)

$ date(2010,12,31)+relativedelta(months=+2)
  datetime.date(2011, 2, 28)

```"
"I'm trying to use NetworkX to read a Shapefile and use the function `write_shp()` to generate the Shapefiles that will contain the nodes and edges, but when I try to run the code it gives me the following error:

```
Traceback (most recent call last):   File
""C:/Users/Felipe/PycharmProjects/untitled/asdf.py"", line 4, in
<module>
    nx.write_shp(redVial, ""shapefiles"")   File ""C:\Python34\lib\site-packages\networkx\readwrite\nx_shp.py"", line
192, in write_shp
    for key, data in e[2].iteritems(): AttributeError: 'dict' object has no attribute 'iteritems'

```

I'm using Python 3.4 and installed NetworkX via pip install.

Before this error it had already given me another one that said ""xrange does not exist"" or something like that, so I looked it up and just changed `xrange` to `range` in the nx\_shp.py file, which seemed to solve it.

From what I've read it could be related to the Python version (Python2 vs Python3).","As you are in python3 , use `dict.items()` instead of `dict.iteritems()`

`iteritems()` was removed in python3, so you can't use this method anymore.

Take a look at Python 3.0 Wiki [Built-in Changes](https://wiki.python.org/moin/Python3.0#Built-In_Changes) section, where it is stated:

> Removed `dict.iteritems()`, `dict.iterkeys()`, and `dict.itervalues()`.
>
> Instead: use `dict.items()`, `dict.keys()`, and `dict.values()`
> respectively."
How do I declare an array in [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29)?,"```
variable = []

```

Now `variable` refers to an empty list\*.

Of course this is an assignment, not a declaration. There's no way to say in Python ""this variable should never refer to anything other than a list"", since Python is dynamically typed.

---

\*The default built-in Python type is called a *list*, not an array. It is an ordered container of arbitrary length that can hold a heterogenous collection of objects (their types do not matter and can be freely mixed). This should not be confused with the [`array` module](https://docs.python.org/2/library/array.html), which offers a type closer to the C `array` type; the contents must be homogenous (all of the same type), but the length is still dynamic."
"How do I learn where the source file for a given Python module is installed? Is the method different on Windows than on Linux?

I'm trying to look for the source of the `datetime` module in particular, but I'm interested in a more general answer as well.","For a pure python module you can find the source by looking at `themodule.__file__`.
The datetime module, however, is written in C, and therefore `datetime.__file__` points to a .so file (there is no `datetime.__file__` on Windows), and therefore, you can't see the source.

If you download a python source tarball and extract it, the modules' code can be found in the **Modules** subdirectory.

For example, if you want to find the datetime code for python 2.6, you can look at

```
Python-2.6/Modules/datetimemodule.c

```

You can also find the latest version of this file on github on the web at
<https://github.com/python/cpython/blob/main/Modules/_datetimemodule.c>"
"I'm trying to understand what `conftest.py` files are meant to be used for.

In my (currently small) test suite I have one `conftest.py` file at the project root. I use it to define the fixtures that I inject into my tests.

I have two questions:

1. Is this the correct use of `conftest.py`? Does it have other uses?
2. Can I have more than one `conftest.py` file? When would I want to do that? Examples will be appreciated.

More generally, how would you define the purpose and correct use of `conftest.py` file(s) in a pytest test suite?","> Is this the correct use of conftest.py?

Yes it is. Fixtures are a potential and common use of `conftest.py`. The
fixtures that you will define will be shared among all tests in your test suite. However, defining fixtures in the root `conftest.py` might be useless and it would slow down testing if such fixtures are not used by all tests.

> Does it have other uses?

Yes it does.

* **Fixtures**: Define fixtures for static data used by tests. This data can be accessed by all tests in the suite unless specified otherwise. This could be data as well as helpers of modules which will be passed to all tests.
* **External plugin loading**: `conftest.py` is used to import external plugins or modules. By defining the following global variable, pytest will load the module and make it available for its test. Plugins are generally files defined in your project or other modules which might be needed in your tests. You can also load a set of predefined plugins as explained [here](https://pytest.org/en/latest/plugins.html#requiring-loading-plugins-in-a-test-module-or-conftest-file).

  `pytest_plugins = ""someapp.someplugin""`
* **Hooks**: You can specify hooks such as setup and teardown methods and much more to improve your tests. For a set of available hooks, read [Hooks link](https://docs.pytest.org/en/6.2.x/reference.html#hooks). Example:

  ```
    def pytest_runtest_setup(item):
         """""" called before ``pytest_runtest_call(item). """"""
         #do some stuff`

  ```
* **Test root path**: This is a bit of a hidden feature. By defining `conftest.py` in your root path, you will have `pytest` recognizing your application modules without specifying `PYTHONPATH`. In the background, py.test modifies your `sys.path` by including all submodules which are found from the root path.

> Can I have more than one conftest.py file?

Yes you can and it is strongly recommended if your test structure is somewhat complex. `conftest.py` files have directory scope. Therefore, creating targeted fixtures and helpers is good practice.

> When would I want to do that? Examples will be appreciated.

Several cases could fit:

Creating a set of tools or **hooks** for a particular group of tests.

**root/mod/conftest.py**

```
def pytest_runtest_setup(item):
    print(""I am mod"")
    #do some stuff


test root/mod2/test.py will NOT produce ""I am mod""

```

Loading a set of **fixtures** for some tests but not for others.

**root/mod/conftest.py**

```
@pytest.fixture()
def fixture():
    return ""some stuff""

```

**root/mod2/conftest.py**

```
@pytest.fixture()
def fixture():
    return ""some other stuff""

```

**root/mod2/test.py**

```
def test(fixture):
    print(fixture)

```

Will print ""some other stuff"".

**Overriding** hooks inherited from the root `conftest.py`.

**root/mod/conftest.py**

```
def pytest_runtest_setup(item):
    print(""I am mod"")
    #do some stuff

```

**root/conftest.py**

```
def pytest_runtest_setup(item):
    print(""I am root"")
    #do some stuff

```

By running any test inside `root/mod`, only ""I am mod"" is printed.

You can read more about `conftest.py` [here](http://pytest.readthedocs.org/en/latest/plugins.html).

**EDIT:**

> What if I need plain-old helper functions to be called from a number
> of tests in different modules - will they be available to me if I put
> them in a conftest.py? Or should I simply put them in a helpers.py
> module and import and use it in my test modules?

You can use `conftest.py` to define your helpers. However, you should follow common practice. Helpers can be used as fixtures at least in `pytest`. For example in my tests I have a mock redis helper which I inject into my tests this way.

**root/helper/redis/redis.py**

```
@pytest.fixture
def mock_redis():
    return MockRedis()

```

**root/tests/stuff/conftest.py**

```
pytest_plugin=""helper.redis.redis""

```

**root/tests/stuff/test.py**

```
def test(mock_redis):
    print(mock_redis.get('stuff'))

```

This will be a test module that you can freely import in your tests. **NOTE** that you could potentially name `redis.py` as `conftest.py` if your module `redis` contains more tests. However, that practice is discouraged because of ambiguity.

If you want to use `conftest.py`, you can simply put that helper in your root `conftest.py` and inject it when needed.

**root/tests/conftest.py**

```
@pytest.fixture
def mock_redis():
    return MockRedis()

```

**root/tests/stuff/test.py**

```
def test(mock_redis):
    print(mock_redis.get(stuff))

```

Another thing you can do is to write an installable plugin. In that case your helper can be written anywhere but it needs to define an entry point to be installed in your and other potential test frameworks. See [this](https://pytest.org/en/latest/writing_plugins.html).

If you don't want to use fixtures, you could of course define a simple helper and just use the plain old import wherever it is needed.

**root/tests/helper/redis.py**

```
class MockRedis():
    # stuff

```

**root/tests/stuff/test.py**

```
from helper.redis import MockRedis

def test():
    print(MockRedis().get(stuff))

```

However, here you might have problems with the path since the module is not in a child folder of the test. You should be able to overcome this (not tested) by adding an `__init__.py` to your helper

**root/tests/helper/**init**.py**

```
from .redis import MockRedis

```

Or simply adding the helper module to your `PYTHONPATH`."
"Consider these three functions:

```
def my_func1():
  print ""Hello World""
  return None

def my_func2():
  print ""Hello World""
  return

def my_func3():
  print ""Hello World""

```

They all appear to return `None`. Are there any differences between how the returned value of these functions behave? Are there any reasons to prefer one versus the other?

---

See also: [is it possible to not return anything from a function in python?](https://stackoverflow.com/questions/10067013)","On the actual behavior, there is no difference. They all return `None` and that's it. However, there is a time and place for all of these.
The following instructions are basically how the different methods should be used (or at least how I was taught they should be used), but they are not absolute rules so you can mix them up if you feel necessary to.

Using `return None`
-------------------

This tells that the function is indeed meant to return a value for later use, and in this case it returns `None`. This value `None` can then be used elsewhere. `return None` is never used if there are no other possible return values from the function.

In the following example, we return `person`'s `mother` if the `person` given is a human. If it's not a human, we return `None` since the `person` doesn't have a `mother` (let's suppose it's not an animal or something).

```
def get_mother(person):
    if is_human(person):
        return person.mother
    else:
        return None

```

Using `return`
--------------

This is used for the same reason as `break` in loops. The return value doesn't matter and you only want to exit the whole function. It's extremely useful in some places, even though you don't need it that often.

We've got 15 `prisoners` and we know one of them has a knife. We loop through each `prisoner` one by one to check if they have a knife. If we hit the person with a knife, we can just exit the function because we know there's only one knife and no reason the check rest of the `prisoners`. If we don't find the `prisoner` with a knife, we raise an alert. This could be done in many different ways and using `return` is probably not even the best way, but it's just an example to show how to use `return` for exiting a function.

```
def find_prisoner_with_knife(prisoners):
    for prisoner in prisoners:
        if ""knife"" in prisoner.items:
            prisoner.move_to_inquisition()
            return # no need to check rest of the prisoners nor raise an alert
    raise_alert()

```

Note: You should never do `var = find_prisoner_with_knife()`, since the return value is not meant to be caught.

Using no `return` at all
------------------------

This will also return `None`, but that value is not meant to be used or caught. It simply means that the function ended successfully. It's basically the same as `return` in `void` functions in languages such as C++ or Java.

In the following example, we set person's mother's name and then the function exits after completing successfully.

```
def set_mother(person, mother):
    if is_human(person):
        person.mother = mother

```

Note: You should never do `var = set_mother(my_person, my_mother)`, since the return value is not meant to be caught."
"Why does the following behave unexpectedly in Python?

```
>>> a = 256
>>> b = 256
>>> a is b
True           # This is an expected result
>>> a = 257
>>> b = 257
>>> a is b
False          # What happened here? Why is this False?
>>> 257 is 257
True           # Yet the literal numbers compare properly

```

I am using Python 2.5.2. Trying some different versions of Python, it appears that Python 2.3.3 shows the above behaviour between 99 and 100.

Based on the above, I can hypothesize that Python is internally implemented such that ""small"" integers are stored in a different way than larger integers and the `is` operator can tell the difference. Why the leaky abstraction? What is a better way of comparing two arbitrary objects to see whether they are the same when I don't know in advance whether they are numbers or not?","Take a look at this:

```
>>> a = 256
>>> b = 256
>>> id(a) == id(b)
True
>>> a = 257
>>> b = 257
>>> id(a) == id(b)
False

```

Here's what I found in the documentation for [""Plain Integer Objects""](https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong):

> The current implementation keeps an array of integer objects for all integers between `-5` and `256`. When you create an int in that range you actually just get back a reference to the existing object.

So, integers 256 are [identical](https://docs.python.org/3/library/functions.html#id), but 257 are not. This is a CPython implementation detail, and not guaranteed for other Python implementations."
How do I add a new key to an existing dictionary? It doesn't have an `.add()` method.,"You create a new key/value pair on a dictionary by assigning a value to that key

```
d = {'key': 'value'}
print(d)  # {'key': 'value'}

d['mynewkey'] = 'mynewvalue'

print(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}

```

If the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten."
"I am quite new to Python and I am now struggling with formatting my data nicely for printed output.

I have one list that is used for two headings, and a matrix that should be the contents of the table. Like so:

```
teams_list = [""Man Utd"", ""Man City"", ""T Hotspur""]
data = np.array([[1, 2, 1],
                 [0, 1, 0],
                 [2, 4, 2]])

```

Note that the heading names are not necessarily the same lengths. The data entries are all integers, though.

Now, I want to represent this in a table format, something like this:

|  | Man Utd | Man City | T Hotspur |
| --- | --- | --- | --- |
| Man Utd | 1 | 0 | 0 |
| Man City | 1 | 1 | 0 |
| T Hotspur | 0 | 1 | 2 |

I have a hunch that there must be a data structure for this, but I cannot find it. I have tried using a dictionary and formatting the printing, I have tried for-loops with indentation and I have tried printing as strings.

I am sure there must be a very simple way to do this, but I am probably missing it due to lack of experience.","There are some light and useful python packages for this purpose:

**1. tabulate**: <https://pypi.python.org/pypi/tabulate>

```
from tabulate import tabulate
print(tabulate([['Alice', 24], ['Bob', 19]], headers=['Name', 'Age']))

```

```
Name      Age
------  -----
Alice      24
Bob        19

```

tabulate has many options to specify headers and table format.

```
print(tabulate([['Alice', 24], ['Bob', 19]], headers=['Name', 'Age'], tablefmt='orgtbl'))

```

```
| Name   |   Age |
|--------+-------|
| Alice  |    24 |
| Bob    |    19 |

```

**2. PrettyTable**: <https://pypi.python.org/pypi/PrettyTable>

```
from prettytable import PrettyTable
t = PrettyTable(['Name', 'Age'])
t.add_row(['Alice', 24])
t.add_row(['Bob', 19])
print(t)

```

```
+-------+-----+
|  Name | Age |
+-------+-----+
| Alice |  24 |
|  Bob  |  19 |
+-------+-----+

```

PrettyTable has options to read data from csv, html, sql database. Also you are able to select subset of data, sort table and change table styles.

**3. texttable**: <https://pypi.python.org/pypi/texttable>

```
from texttable import Texttable
t = Texttable()
t.add_rows([['Name', 'Age'], ['Alice', 24], ['Bob', 19]])
print(t.draw())

```

```
+-------+-----+
| Name  | Age |
+=======+=====+
| Alice | 24  |
+-------+-----+
| Bob   | 19  |
+-------+-----+

```

with texttable you can control horizontal/vertical align, border style and data types.

**4. termtables**: <https://github.com/nschloe/termtables>

```
import termtables as tt

string = tt.to_string(
    [[""Alice"", 24], [""Bob"", 19]],
    header=[""Name"", ""Age""],
    style=tt.styles.ascii_thin_double,
    # alignment=""ll"",
    # padding=(0, 1),
)
print(string)

```

```
+-------+-----+
| Name  | Age |
+=======+=====+
| Alice | 24  |
+-------+-----+
| Bob   | 19  |
+-------+-----+

```

with texttable you can control horizontal/vertical align, border style and data types.

Other options:

* [terminaltables](https://github.com/Robpol86/terminaltables) Easily draw tables in terminal/console applications from a list of lists of strings. Supports multi-line rows.
* [asciitable](https://pypi.python.org/pypi/asciitable/0.8.0) Asciitable can read and write a wide range of ASCII table formats via built-in Extension Reader Classes."
"I noticed a Python script I was writing was acting squirrelly, and traced it to an infinite loop, where the loop condition was `while line is not ''`. Running through it in the debugger, it turned out that line was in fact `''`. When I changed it to `!=''` rather than `is not ''`, it worked fine.

Also, is it generally considered better to just use '==' by default, even when comparing int or Boolean values? I've always liked to use 'is' because I find it more aesthetically pleasing and pythonic (which is how I fell into this trap...), but I wonder if it's intended to just be reserved for when you care about finding two objects with the same id.","> For all built-in Python objects (like
> strings, lists, dicts, functions,
> etc.), if x is y, then x==y is also
> True.

Not always. NaN is a counterexample. But *usually*, identity (`is`) implies equality (`==`). The converse is not true: Two distinct objects can have the same value.

> Also, is it generally considered better to just use '==' by default, even
> when comparing int or Boolean values?

You use `==` when comparing values and `is` when comparing identities.

When comparing ints (or immutable types in general), you pretty much always want the former. There's an optimization that allows small integers to be compared with `is`, but don't rely on it.

For boolean values, you shouldn't be doing comparisons at all. Instead of:

```
if x == True:
    # do something

```

write:

```
if x:
    # do something

```

For comparing against `None`, `is None` is preferred over `== None`.

> I've always liked to use 'is' because
> I find it more aesthetically pleasing
> and pythonic (which is how I fell into
> this trap...), but I wonder if it's
> intended to just be reserved for when
> you care about finding two objects
> with the same id.

Yes, that's exactly what it's for."
"1. Is there a performance or code maintenance issue with using `assert` as part of the standard code instead of using it just for debugging purposes?

   Is

   ```
   assert x >= 0, 'x is less than zero'

   ```

   better or worse than

   ```
   if x < 0:
       raise Exception('x is less than zero')

   ```
2. Also, is there any way to set a business rule like `if x < 0 raise error` that is always checked without the `try/except/finally` so, if at anytime throughout the code `x` is less than 0 an error is raised, like if you set `assert x < 0` at the start of a function, anywhere within the function where `x` becomes less then 0 an exception is raised?","Asserts should be used to test conditions that *should never happen*. The purpose is to crash early in the case of a corrupt program state.

Exceptions should be used for errors that can conceivably happen, and **you should almost always create your own Exception classes**.

---

For example, if you're writing a function to read from a configuration file into a `dict`, improper formatting in the file should raise a `ConfigurationSyntaxError`, while you can `assert` that you're not about to return `None`.

---

In your example, if `x` is a value set via a user interface or from an external source, an exception is best.

If `x` is only set by your own code in the same program, go with an assertion."
"```
class Package:
    def __init__(self):
        self.files = []

    # ...

    def __del__(self):
        for file in self.files:
            os.unlink(file)

```

`__del__(self)` above fails with an AttributeError exception. I understand [Python doesn't guarantee](http://docs.python.org/reference/datamodel.html#customization) the existence of ""global variables"" (member data in this context?) when `__del__()` is invoked. If that is the case and this is the reason for the exception, how do I make sure the object destructs properly?","I'd recommend using Python's `with` statement for managing resources that need to be cleaned up. The problem with using an explicit `close()` statement is that you have to worry about people forgetting to call it at all or forgetting to place it in a `finally` block to prevent a resource leak when an exception occurs.

To use the `with` statement, create a class with the following methods:

```
def __enter__(self)
def __exit__(self, exc_type, exc_value, traceback)

```

In your example above, you'd use

```
class Package:
    def __init__(self):
        self.files = []

    def __enter__(self):
        return self

    # ...

    def __exit__(self, exc_type, exc_value, traceback):
        for file in self.files:
            os.unlink(file)

```

Then, when someone wanted to use your class, they'd do the following:

```
with Package() as package_obj:
    # use package_obj

```

The variable package\_obj will be an instance of type Package (it's the value returned by the `__enter__` method). Its `__exit__` method will automatically be called, regardless of whether or not an exception occurs.

You could even take this approach a step further. In the example above, someone could still instantiate Package using its constructor without using the `with` clause. You don't want that to happen. You can fix this by creating a PackageResource class that defines the `__enter__` and `__exit__` methods. Then, the Package class would be defined strictly inside the `__enter__` method and returned. That way, the caller never could instantiate the Package class without using a `with` statement:

```
class PackageResource:
    def __enter__(self):
        class Package:
            ...
        self.package_obj = Package()
        return self.package_obj

    def __exit__(self, exc_type, exc_value, traceback):
        self.package_obj.cleanup()

```

You'd use this as follows:

```
with PackageResource() as package_obj:
    # use package_obj

```"
"This ""underscoring"" seems to occur a lot, and I was wondering if this was a requirement in the Python language, or merely a matter of convention?

Also, could someone name and explain which functions tend to have the underscores, and why (`__init__`, for instance)?","From the [Python PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/):

> ### [Descriptive: Naming Styles](https://www.python.org/dev/peps/pep-0008/#descriptive-naming-styles)
>
> The following special forms using leading or trailing underscores are
> recognized (these can generally be combined with any case convention):
>
> * `_single_leading_underscore`: weak ""internal use"" indicator. E.g. `from M import *` does not import objects whose name starts with an underscore.
> * `single_trailing_underscore_`: used by convention to avoid conflicts with Python keyword, e.g.
>
>   `Tkinter.Toplevel(master, class_='ClassName')`
> * `__double_leading_underscore`: when naming a class attribute, invokes name mangling (inside class FooBar, `__boo` becomes `_FooBar__boo`; see below).
> * `__double_leading_and_trailing_underscore__`: ""magic"" objects or attributes that live in user-controlled namespaces. E.g. `__init__`,
>   `__import__` or `__file__`. Never invent such names; only use them as documented.

Note that names with double leading and trailing underscores are essentially reserved for Python itself: ""Never invent such names; only use them as documented""."
"What is the proper way to use `**kwargs` in Python when it comes to default values?

`kwargs` returns a dictionary, but what is the best way to set default values, or is there one? Should I just access it as a dictionary? Use `get` function?

```
class ExampleClass:
    def __init__(self, **kwargs):
        self.val = kwargs['val']
        self.val2 = kwargs.get('val2')

```

People do it different ways in code that I've seen and it's hard to know what to use.","You can pass a default value to `get()` for keys that are not in the dictionary:

```
self.val2 = kwargs.get('val2',""default value"")

```

However, if you plan on using a particular argument with a particular default value, why not use named arguments in the first place?

```
def __init__(self, val2=""default value"", **kwargs):

```"
"I've seen some code samples and tutorials that use

```
def main():
    # my code here

if __name__ == ""__main__"":
    main()

```

But why? Is there any reason not do define your functions at the top of the file, then just write code under it? ie

```
def my_function()
    # my code here

def my_function_two()
    # my code here

# some code
# call function
# print(something)

```

I just wonder if there is any rhyme to the main?","Without the main sentinel, the code would be executed even if the script were imported as a module."
"I am writing a project in Django and I see that 80% of the code is in the file `models.py`. This code is confusing and, after a certain time, I cease to understand what is really happening.

Here is what bothers me:

1. I find it ugly that my model level (which was supposed to be
   responsible only for the work with data from a database) is also
   sending email, walking on API to other services, etc.
2. Also, I find it unacceptable to place business logic in the view, because
   this way it becomes difficult to control. For example, in my
   application there are at least three ways to create new
   instances of `User`, but technically it should create them uniformly.
3. I do not always notice when the methods and
   properties of my models become non-deterministic and when they develop
   side effects.

Here is a simple example. At first, the `User` model was like this:

```
class User(db.Models):

    def get_present_name(self):
        return self.name or 'Anonymous'

    def activate(self):
        self.status = 'activated'
        self.save()

```

Over time, it turned into this:

```
class User(db.Models):

    def get_present_name(self): 
        # property became non-deterministic in terms of database
        # data is taken from another service by api
        return remote_api.request_user_name(self.uid) or 'Anonymous' 

    def activate(self):
        # method now has a side effect (send message to user)
        self.status = 'activated'
        self.save()
        send_mail('Your account is activated!', 'â€¦', [self.email])

```

What I want is to separate entities in my code:

1. **Database level entities, i.e. database level logic: What kind of data does my application store?**
2. **application level entities, i.e. business level logic: What does my application do?**

What are the good practices to implement such an approach that can be applied in Django?","It seems like you are asking about the difference between the *data model* and the *domain model* – the latter is where you can find the business logic and entities as perceived by your end user, the former is where you actually store your data.

Furthermore, I've interpreted the 3rd part of your question as: how to notice failure to keep these models separate.

These are two very different concepts and it's always hard to keep them separate. However, there are some common patterns and tools that can be used for this purpose.

About the Domain Model
----------------------

The first thing you need to recognize is that your domain model is not really about data; it is about *actions* and *questions* such as ""activate this user"", ""deactivate this user"", ""which users are currently activated?"", and ""what is this user's name?"". In classical terms: it's about *queries* and *commands*.

Thinking in Commands
--------------------

Let's start by looking at the commands in your example: ""activate this user"" and ""deactivate this user"". The nice thing about commands is that they can easily be expressed by small given-when-then scenario's:

> **given** an inactive user   
> **when** the admin activates this user   
> **then** the user becomes active   
> **and** a confirmation e-mail is sent to the user   
> **and** an entry is added to the system log  
> (etc. etc.)

Such scenario's are useful to see how different parts of your infrastructure can be affected by a single command – in this case your database (some kind of 'active' flag), your mail server, your system log, etc.

Such scenario's also really help you in setting up a Test Driven Development environment.

And finally, thinking in commands really helps you create a task-oriented application. Your users will appreciate this :-)

Expressing Commands
-------------------

Django provides two easy ways of expressing commands; they are both valid options and it is not unusual to mix the two approaches.

### The service layer

The *service module* has already been [described by @Hedde](https://stackoverflow.com/a/12579490/383793). Here you define a separate module and each command is represented as a function.

**services.py**

```
def activate_user(user_id):
    user = User.objects.get(pk=user_id)

    # set active flag
    user.active = True
    user.save()

    # mail user
    send_mail(...)

    # etc etc

```

### Using forms

The other way is to use a Django Form for each command. I prefer this approach, because it combines multiple closely related aspects:

* execution of the command (what does it do?)
* validation of the command parameters (can it do this?)
* presentation of the command (how can I do this?)

**forms.py**

```
class ActivateUserForm(forms.Form):

    user_id = IntegerField(widget = UsernameSelectWidget, verbose_name=""Select a user to activate"")
    # the username select widget is not a standard Django widget, I just made it up

    def clean_user_id(self):
        user_id = self.cleaned_data['user_id']
        if User.objects.get(pk=user_id).active:
            raise ValidationError(""This user cannot be activated"")
        # you can also check authorizations etc. 
        return user_id

    def execute(self):
        """"""
        This is not a standard method in the forms API; it is intended to replace the 
        'extract-data-from-form-in-view-and-do-stuff' pattern by a more testable pattern. 
        """"""
        user_id = self.cleaned_data['user_id']

        user = User.objects.get(pk=user_id)

        # set active flag
        user.active = True
        user.save()

        # mail user
        send_mail(...)

        # etc etc

```

Thinking in Queries
-------------------

You example did not contain any queries, so I took the liberty of making up a few useful queries. I prefer to use the term ""question"", but queries is the classical terminology. Interesting queries are: ""What is the name of this user?"", ""Can this user log in?"", ""Show me a list of deactivated users"", and ""What is the geographical distribution of deactivated users?""

Before embarking on answering these queries, you should always ask yourself this question, is this:

* a *presentational* query just for my templates, and/or
* a *business logic* query tied to executing my commands, and/or
* a *reporting* query.

Presentational queries are merely made to improve the user interface. The answers to business logic queries directly affect the execution of your commands. Reporting queries are merely for analytical purposes and have looser time constraints. These categories are not mutually exclusive.

The other question is: ""do I have complete control over the answers?"" For example, when querying the user's name (in this context) we do not have any control over the outcome, because we rely on an external API.

Making Queries
--------------

The most basic query in Django is the use of the Manager object:

```
User.objects.filter(active=True)

```

Of course, this only works if the data is actually represented in your data model. This is not always the case. In those cases, you can consider the options below.

### Custom tags and filters

The first alternative is useful for queries that are merely presentational: custom tags and template filters.

**template.html**

```
<h1>Welcome, {{ user|friendly_name }}</h1>

```

**template\_tags.py**

```
@register.filter
def friendly_name(user):
    return remote_api.get_cached_name(user.id)

```

### Query methods

If your query is not merely presentational, you could add queries to your **services.py** (if you are using that), or introduce a **queries.py** module:

**queries.py**

```
def inactive_users():
    return User.objects.filter(active=False)


def users_called_publysher():
    for user in User.objects.all():
        if remote_api.get_cached_name(user.id) == ""publysher"":
            yield user 

```

### Proxy models

Proxy models are very useful in the context of business logic and reporting. You basically define an enhanced subset of your model. You can override a Manager’s base QuerySet by overriding the [`Manager.get_queryset()`](https://docs.djangoproject.com/en/2.0/topics/db/managers/#modifying-a-manager-s-initial-queryset) method.

**models.py**

```
class InactiveUserManager(models.Manager):
    def get_queryset(self):
        query_set = super(InactiveUserManager, self).get_queryset()
        return query_set.filter(active=False)

class InactiveUser(User):
    """"""
    >>> for user in InactiveUser.objects.all():
    …        assert user.active is False 
    """"""

    objects = InactiveUserManager()
    class Meta:
        proxy = True

```

### Query models

For queries that are inherently complex, but are executed quite often, there is the possibility of query models. A query model is a form of denormalization where relevant data for a single query is stored in a separate model. The trick of course is to keep the denormalized model in sync with the primary model. Query models can only be used if changes are entirely under your control.

**models.py**

```
class InactiveUserDistribution(models.Model):
    country = CharField(max_length=200)
    inactive_user_count = IntegerField(default=0)

```

The first option is to update these models in your commands. This is very useful if these models are only changed by one or two commands.

**forms.py**

```
class ActivateUserForm(forms.Form):
    # see above
   
    def execute(self):
        # see above
        query_model = InactiveUserDistribution.objects.get_or_create(country=user.country)
        query_model.inactive_user_count -= 1
        query_model.save()

```

A better option would be to use custom signals. These signals are of course emitted by your commands. Signals have the advantage that you can keep multiple query models in sync with your original model. Furthermore, signal processing can be offloaded to background tasks, using Celery or similar frameworks.

**signals.py**

```
user_activated = Signal(providing_args = ['user'])
user_deactivated = Signal(providing_args = ['user'])

```

**forms.py**

```
class ActivateUserForm(forms.Form):
    # see above
   
    def execute(self):
        # see above
        user_activated.send_robust(sender=self, user=user)

```

**models.py**

```
class InactiveUserDistribution(models.Model):
    # see above

@receiver(user_activated)
def on_user_activated(sender, **kwargs):
        user = kwargs['user']
        query_model = InactiveUserDistribution.objects.get_or_create(country=user.country)
        query_model.inactive_user_count -= 1
        query_model.save()
    

```

Keeping it clean
----------------

When using this approach, it becomes ridiculously easy to determine if your code stays clean. Just follow these guidelines:

* Does my model contain methods that do more than managing database state? You should extract a command.
* Does my model contain properties that do not map to database fields? You should extract a query.
* Does my model reference infrastructure that is not my database (such as mail)? You should extract a command.

The same goes for views (because views often suffer from the same problem).

* Does my view actively manage database models? You should extract a command.

Some References
---------------

[Django documentation: proxy models](https://docs.djangoproject.com/en/dev/topics/db/models/#proxy-models)

[Django documentation: signals](https://docs.djangoproject.com/en/dev/topics/signals/)

[Architecture: Domain Driven Design](https://rads.stackoverflow.com/amzn/click/com/0321125215)"
"I want to create a list of dates, starting with today, and going back an arbitrary number of days, say, in my example 100 days. Is there a better way to do it than this?

```
import datetime

a = datetime.datetime.today()
numdays = 100
dateList = []
for x in range (0, numdays):
    dateList.append(a - datetime.timedelta(days = x))
print dateList

```","Marginally better...

```
base = datetime.datetime.today()
date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]

```"
"I have a list of 2-item tuples and I'd like to convert them to 2 lists where the first contains the first item in each tuple and the second list holds the second item.

**For example:**

```
original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]
# and I want to become...
result = (['a', 'b', 'c', 'd'], [1, 2, 3, 4])

```

Is there a builtin function that does that?

---

**See also:** [Transpose list of lists](https://stackoverflow.com/questions/6473679) if the specific choice of lists vs. tuples in the result matters. Most answers here assume it doesn't.","In 2.x, [`zip`](http://docs.python.org/library/functions.html#zip) is its own inverse! Provided you [use the special \* operator](https://stackoverflow.com/questions/36901).

```
>>> zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)])
[('a', 'b', 'c', 'd'), (1, 2, 3, 4)]

```

This is equivalent to calling `zip` with each element of the list as a separate argument:

```
zip(('a', 1), ('b', 2), ('c', 3), ('d', 4))

```

except the arguments are passed to `zip` directly (after being converted to a tuple), so there's no need to worry about the number of arguments getting too big.

In 3.x, `zip` [returns a lazy iterator](https://stackoverflow.com/questions/27431390), but this is trivially converted:

```
>>> list(zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)]))
[('a', 'b', 'c', 'd'), (1, 2, 3, 4)]

```"
"I'm looking for a `string.contains` or `string.indexof` method in Python.

I want to do:

```
if not somestring.contains(""blah""):
   continue

```","Use the [`in` operator](https://docs.python.org/reference/expressions.html#membership-test-details):

```
if ""blah"" not in somestring: 
    continue

```

Note: This is case-sensitive."
How do I get a list of all files (and directories) in a given directory in Python?,"This is a way to traverse every file and directory in a directory tree:

```
import os

for dirname, dirnames, filenames in os.walk('.'):
    # print path to all subdirectories first.
    for subdirname in dirnames:
        print(os.path.join(dirname, subdirname))

    # print path to all filenames.
    for filename in filenames:
        print(os.path.join(dirname, filename))

    # Advanced usage:
    # editing the 'dirnames' list will stop os.walk() from recursing into there.
    if '.git' in dirnames:
        # don't go into any .git directories.
        dirnames.remove('.git')

```"
"I have a dictionary which looks like this: `di = {1: ""A"", 2: ""B""}`

I would like to apply it to the `col1` column of a dataframe similar to:

```
     col1   col2
0       w      a
1       1      2
2       2    NaN

```

to get:

```
     col1   col2
0       w      a
1       A      2
2       B    NaN

```

How can I best do this?","You can use [`.replace`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html). For example:

```
>>> df = pd.DataFrame({'col2': {0: 'a', 1: 2, 2: np.nan}, 'col1': {0: 'w', 1: 1, 2: 2}})
>>> di = {1: ""A"", 2: ""B""}
>>> df
  col1 col2
0    w    a
1    1    2
2    2  NaN
>>> df.replace({""col1"": di})
  col1 col2
0    w    a
1    A    2
2    B  NaN

```

or directly on the [`Series`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html), i.e. `df[""col1""].replace(di, inplace=True)`."
"I have a Python dictionary:

```
{u'2012-07-01': 391,
 u'2012-07-02': 392,
 u'2012-07-03': 392,
 u'2012-07-04': 392,
 u'2012-07-05': 392,
 u'2012-07-06': 392}

```

I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns; the expected result looks like:

```
     Date         DateValue
0    2012-07-01    391
1    2012-07-02    392
2    2012-07-03    392
.    2012-07-04    392
.    ...           ...

```

Is there a direct way to do this?","The error here, is since calling the DataFrame constructor with scalar values (where it expects values to be a list/dict/... i.e. have multiple columns):

```
pd.DataFrame(d)
ValueError: If using all scalar values, you must must pass an index

```

You could take the items from the dictionary (i.e. the key-value pairs):

```
In [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3
Out[11]:
            0    1
0  2012-07-01  391
1  2012-07-02  392
2  2012-07-03  392
3  2012-07-04  392
4  2012-07-05  392
5  2012-07-06  392

In [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])
Out[12]:
         Date  DateValue
0  2012-07-01        391
1  2012-07-02        392
2  2012-07-03        392
3  2012-07-04        392
4  2012-07-05        392
5  2012-07-06        392

```

But I think it makes more sense to pass the Series constructor:

```
In [20]: s = pd.Series(d, name='DateValue')

In [21]: s
Out[21]:
2012-07-01    391
2012-07-02    392
2012-07-03    392
2012-07-04    392
2012-07-05    392
2012-07-06    392
Name: DateValue, dtype: int64

In [22]: s.index.name = 'Date'

In [23]: s.reset_index()
Out[23]:
         Date  DateValue
0  2012-07-01        391
1  2012-07-02        392
2  2012-07-03        392
3  2012-07-04        392
4  2012-07-05        392
5  2012-07-06        392

```"
"I have a Python module installed on my system and I'd like to be able to see what functions/classes/methods are available in it.

I want to call the `help` function on each one. In Ruby I can do something like `ClassName.methods` to get a list of all the methods available on that class. Is there something similar in Python?

e.g. something like:

```
from somemodule import foo
print(foo.methods)  # or whatever is the correct method to call

```",You can use `dir(module)` to see all available methods/attributes. Also check out PyDocs.
"What would be the quickest way to construct a Python binding to a C or C++ library?

(I am using Windows if this matters.)","[ctypes](https://docs.python.org/3/library/ctypes.html) module is part of the standard library, and therefore is more stable and widely available than [swig](http://www.swig.org/), which always tended to give me [problems](http://groups.google.com/group/comp.lang.python/browse_thread/thread/d94badd9847fe43a?pli=1).

With ctypes, you need to satisfy any compile time dependency on python, and your binding will work on any python that has ctypes, not just the one it was compiled against.

Suppose you have a simple C++ example class you want to talk to in a file called foo.cpp:

```
#include <iostream>

class Foo{
    public:
        void bar(){
            std::cout << ""Hello"" << std::endl;
        }
};

```

Since ctypes can only talk to C functions, you need to provide those declaring them as extern ""C""

```
extern ""C"" {
    Foo* Foo_new(){ return new Foo(); }
    void Foo_bar(Foo* foo){ foo->bar(); }
}

```

Next you have to compile this to a shared library

```
g++ -c -fPIC foo.cpp -o foo.o
g++ -shared -Wl,-soname,libfoo.so -o libfoo.so  foo.o

```

And finally you have to write your python wrapper (e.g. in fooWrapper.py)

```
from ctypes import cdll
lib = cdll.LoadLibrary('./libfoo.so')

class Foo(object):
    def __init__(self):
        self.obj = lib.Foo_new()

    def bar(self):
        lib.Foo_bar(self.obj)

```

Once you have that you can call it like

```
f = Foo()
f.bar() #and you will see ""Hello"" on the screen

```"
[`index()`](https://docs.python.org/3/tutorial/datastructures.html) will give the first occurrence of an item in a list. Is there a neat trick which returns all indices in a list for an element?,"You can use a list comprehension with [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate):

```
indices = [i for i, x in enumerate(my_list) if x == ""whatever""]

```

The iterator `enumerate(my_list)` yields pairs `(index, item)` for each item in the list. Using `i, x` as loop variable target unpacks these pairs into the index `i` and the list item `x`. We filter down to all `x` that match our criterion, and select the indices `i` of these elements."
How do I tell the time difference in minutes between two `datetime` objects?,"```
>>> import datetime
>>> first_time = datetime.datetime.now()
>>> later_time = datetime.datetime.now()
>>> difference = later_time - first_time
datetime.timedelta(0, 8, 562000)
>>> seconds_in_day = 24 * 60 * 60
>>> divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds

```

Subtracting the later time from the first time `difference = later_time - first_time` creates a datetime object that only holds the difference.
In the example above it is 0 minutes, 8 seconds and 562000 microseconds."
"It seems there are already quite some questions here about relative import in python 3, but after going through many of them I still didn't find the answer for my issue.
so here is the question.

I have a package shown below

```
package/
   __init__.py
   A/
      __init__.py
      foo.py
   test_A/
      __init__.py
      test.py

```

and I have a single line in test.py:

```
from ..A import foo

```

now, I am in the folder of `package`, and I run

```
python -m test_A.test

```

I got message

```
""ValueError: attempted relative import beyond top-level package""

```

but if I am in the parent folder of `package`, e.g., I run:

```
cd ..
python -m package.test_A.test

```

everything is fine.

**Now my question is:**
when I am in the folder of `package`, and I run the module inside the test\_A sub-package as `test_A.test`, based on my understanding, `..A` goes up only one level, which is still within the `package` folder, why it gives message saying `beyond top-level package`. What is exactly the reason that causes this error message?","EDIT: I revisited this, 7 years later, and understand better. I'm leaving the original answer here, but I have written a more general/ comprehensive answer [here](https://stackoverflow.com/a/79209936/4082104). In particular, I now know the answer to the last question I posed in my original answer.

Other more coherent answers:

* [Sibling package imports](https://stackoverflow.com/questions/6323860/sibling-package-imports)
* [Relative imports for the billionth time](https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time/14132912#14132912)

---

**Why doesn't it work?** It's because python doesn't record where a package was loaded from. So when you do `python -m test_A.test`, it basically just discards the knowledge that `test_A.test` is actually stored in `package` (i.e. `package` is not considered a package). Attempting `from ..A import foo` is trying to access information it doesn't have any more (i.e. sibling directories of a loaded location). It's conceptually similar to allowing `from ..os import path` in a file in `math`. This would be bad because you want the packages to be distinct. If they need to use something from another package, then they should refer to them globally with `from os import path` and let python work out where that is with `$PATH` and `$PYTHONPATH`.

When you use `python -m package.test_A.test`, then using `from ..A import foo` resolves just fine because it kept track of what's in `package` and you're just accessing a child directory of a loaded location.

**Why doesn't python consider the current working directory to be a package?** *NO CLUE*, but gosh it would be useful."
"I need to print some stuff only when a boolean variable is set to `True`. So, after looking at [this](https://stackoverflow.com/questions/1319214/shorter-more-pythonic-way-of-writing-an-if-statements), I tried with a simple example:

```
>>> a = 100
>>> b = True
>>> print a if b
  File ""<stdin>"", line 1
    print a if b
             ^
SyntaxError: invalid syntax  

```

Same thing if I write `print a if b==True`.

What am I missing here?","Python does **not** have a trailing `if` **statement**.

There are two kinds of `if` in Python:

1. `if` statement:

   ```
   if condition: statement
   if condition:
       block

   ```
2. `if` **expression** (introduced in Python 2.5)

   ```
   expression_if_true if condition else expression_if_false

   ```

And note, that both `print a` and `b = a` are statements. Only the `a` part is an expression. So if you write

```
print a if b else 0

```

it means

```
print (a if b else 0)

```

and similarly when you write

```
x = a if b else 0

```

it means

```
x = (a if b else 0)

```

Now what would it print/assign if there was no `else` clause? The print/assignment is **still there**.

And note, that if you don't want it to be there, you can always write the regular `if` statement on a single line, though it's less readable and there is really no reason to avoid the two-line variant."
I am trying to understand when to define `__getattr__` or `__getattribute__`. The [python documentation](http://docs.python.org/reference/datamodel.html?highlight=__getattribute__#object.__getattribute__) mentions `__getattribute__` applies to new-style classes. What are new-style classes?,"A key difference between `__getattr__` and `__getattribute__` is that `__getattr__` is only invoked if the attribute wasn't found the usual ways. It's good for implementing a fallback for missing attributes, and is probably the one of two you want.

`__getattribute__` is invoked before looking at the actual attributes on the object, and so can be tricky to implement correctly. You can end up in infinite recursions very easily.

New-style classes derive from `object`, old-style classes are those in Python 2.x with no explicit base class. But the distinction between old-style and new-style classes is not the important one when choosing between `__getattr__` and `__getattribute__`.

You almost certainly want `__getattr__`."
How can I delete a file or folder in Python?,"Use one of these methods:

* [`pathlib.Path.unlink()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.unlink) removes a file or symbolic link.
* [`pathlib.Path.rmdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.rmdir) removes an empty directory.
* [`shutil.rmtree()`](http://docs.python.org/library/shutil.html#shutil.rmtree) deletes a directory and all its contents.

---

On Python 3.3 and below, you can use these methods instead of the [`pathlib`](https://docs.python.org/3/library/pathlib.html) ones:

* [`os.remove()`](http://docs.python.org/library/os.html#os.remove) removes a file.
* [`os.unlink()`](https://docs.python.org/3/library/os.html#os.unlink) removes a symbolic link.
* [`os.rmdir()`](http://docs.python.org/library/os.html#os.rmdir) removes an empty directory."
"I want to plot a graph with one logarithmic axis using matplotlib.

Sample program:

```
import matplotlib.pyplot as plt
a = [pow(10, i) for i in range(10)]  # exponential
fig = plt.figure()
ax = fig.add_subplot(2, 1, 1)

line, = ax.plot(a, color='blue', lw=2)
plt.show()

```","You can use the [`Axes.set_yscale`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_yscale.html) method. That allows you to change the scale after the `Axes` object is created. That would also allow you to build a control to let the user pick the scale if you needed to.

The relevant line to add is:

```
ax.set_yscale('log')

```

You can use `'linear'` to switch back to a linear scale. Here's what your code would look like:

```
import pylab
import matplotlib.pyplot as plt
a = [pow(10, i) for i in range(10)]
fig = plt.figure()
ax = fig.add_subplot(2, 1, 1)

line, = ax.plot(a, color='blue', lw=2)

ax.set_yscale('log')

pylab.show()

```

[![result chart](https://i.sstatic.net/CmQwl.png)](https://i.sstatic.net/CmQwl.png)"
"I'm running into a weird error when trying to install Django on my computer.

This is the sequence that I typed into my command line:

```
C:\Python34> python get-pip.py
Requirement already up-to-date: pip in c:\python34\lib\site-packages
Cleaning up...

C:\Python34> pip install Django
'pip' is not recognized as an internal or external command,
operable program or batch file.

C:\Python34> lib\site-packages\pip install Django
'lib\site-packages\pip' is not recognized as an internal or external command,
operable program or batch file.

```

What could be causing this?

This is what I get when I type in `echo %PATH%`:

```
C:\Python34>echo %PATH%
C:\Program Files\ImageMagick-6.8.8-Q16;C:\Program Files (x86)\Intel\iCLS Client\
;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\S
ystem32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\
Windows Live\Shared;C:\Program Files (x86)\Intel\OpenCL SDK\2.0\bin\x86;C:\Progr
am Files (x86)\Intel\OpenCL SDK\2.0\bin\x64;C:\Program Files\Intel\Intel(R) Mana
gement Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine C
omponents\IPT;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components
\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\P
rogram Files (x86)\nodejs\;C:\Program Files (x86)\Heroku\bin;C:\Program Files (x
86)\git\cmd;C:\RailsInstaller\Ruby2.0.0\bin;C:\RailsInstaller\Git\cmd;C:\RailsIn
staller\Ruby1.9.3\bin;C:\Users\Javi\AppData\Roaming\npm

```","**You need to add the path of your pip installation to your PATH system variable**. By default, pip is installed to `C:\Python34\Scripts\pip` (pip now comes bundled with new versions of python), so the path ""C:\Python34\Scripts"" needs to be added to your PATH variable.

To check if it is already in your PATH variable, type `echo %PATH%` at the CMD prompt

**To add the path of your pip installation to your PATH variable**, you can use the Control Panel or the `setx` command. For example:

```
setx PATH ""%PATH%;C:\Python34\Scripts""

```

---

**Note**:
According to the [official documentation](https://technet.microsoft.com/en-us/library/cc755104%28v=ws.11%29.aspx), ""[v]ariables set with setx variables are available in future command windows only, not in the current command window"". In particular, *you will need to start a new cmd.exe instance* after entering the above command in order to utilize the new environment variable.

Thanks to Scott Bartell for pointing this out."
"Python has `string.find()` and `string.rfind()` to get the index of a substring in a string.

I'm wondering whether there is something like `string.find_all()` which can return all found indexes (not only the first from the beginning or the first from the end).

For example:

```
string = ""test test test test""

print string.find('test') # 0
print string.rfind('test') # 15

#this is the goal
print string.find_all('test') # [0,5,10,15]

```

---

For *counting* the occurrences, see [Count number of occurrences of a substring in a string](https://stackoverflow.com/questions/8899905).","There is no simple built-in string function that does what you're looking for, but you could use the more powerful [regular expressions](http://www.regular-expressions.info/):

```
import re
[m.start() for m in re.finditer('test', 'test test test test')]
#[0, 5, 10, 15]

```

If you want to find overlapping matches, [lookahead](http://www.regular-expressions.info/lookaround.html) will do that:

```
[m.start() for m in re.finditer('(?=tt)', 'ttt')]
#[0, 1]

```

If you want a reverse find-all without overlaps, you can combine positive and negative lookahead into an expression like this:

```
search = 'tt'
[m.start() for m in re.finditer('(?=%s)(?!.{1,%d}%s)' % (search, len(search)-1, search), 'ttt')]
#[1]

```

[`re.finditer`](http://docs.python.org/library/re.html#re.finditer) returns a [generator](http://wiki.python.org/moin/Generators), so you could change the `[]` in the above to `()` to get a generator instead of a list which will be more efficient if you're only iterating through the results once."
"I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.

My code so far:

```
for each_line in fileinput.input(input_file):
    do_something(each_line)

    for each_line_again in fileinput.input(input_file):
        do_something(each_line_again)

```

Executing this code gives an error message: `device active`.

Any suggestions?

The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line.

Nov. 2022 Edit: A related question that was asked 8 months after this question has many useful answers and comments. To get a deeper understanding of python logic, do also read this related question [How should I read a file line-by-line in Python?](https://stackoverflow.com/questions/11555468/how-should-i-read-a-file-line-by-line-in-python)","The correct, fully Pythonic way to read a file is the following:

```
with open(...) as f:
    for line in f:
        # Do something with 'line'

```

The `with` statement handles opening and closing the file, including if an exception is raised in the inner block. The `for line in f` treats the file object `f` as an iterable, which automatically uses buffered I/O and memory management so you don't have to worry about large files.

> There should be one -- and preferably only one -- obvious way to do it."
"I saw this in someone's code. What does it mean?

```
    def __enter__(self):
        return self

    def __exit__(self, type, value, tb):
        self.stream.close()

```

Here is the complete code.

```
from __future__ import with_statement#for python2.5 

class a(object):
    def __enter__(self):
        print 'sss'
        return 'sss111'
    def __exit__(self ,type, value, traceback):
        print 'ok'
        return False

with a() as s:
    print s
    
    
print s

```","Using these magic methods (`__enter__`, `__exit__`) allows you to implement objects which can be used easily with the `with` statement.

The idea is that it makes it easy to build code which needs some 'cleandown' code executed (think of it as a `try-finally` block). [Some more explanation here](https://web.archive.org/web/20160427130542/http://effbot.org/zone/python-with-statement.htm).

A useful example could be a database connection object (which then automagically closes the connection once the corresponding 'with'-statement goes out of scope):

```
class DatabaseConnection(object):

    def __enter__(self):
        # make a database connection and return it
        ...
        return self.dbconn

    def __exit__(self, exc_type, exc_val, exc_tb):
        # make sure the dbconnection gets closed
        self.dbconn.close()
        ...

```

As explained above, use this object with the `with` statement (you may need to do `from __future__ import with_statement` at the top of the file if you're on Python 2.5).

```
with DatabaseConnection() as mydbconn:
    # do stuff

```

[PEP343 -- The 'with' statement'](https://peps.python.org/pep-0343/) has a nice writeup as well."
"How can I pretty print a dictionary with depth of ~4 in Python? I tried pretty printing with `pprint()`, but it did not work:

```
import pprint 
pp = pprint.PrettyPrinter(indent=4)
pp.pprint(mydict)

```

I simply want an indentation (`""\t""`) for each nesting, so that I get something like this:

```
key1
    value1
    value2
    key2
       value1
       value2

```

etc.

How can I do this?","My first thought was that the JSON serializer is probably pretty good at nested dictionaries, so I'd cheat and use that:

```
>>> import json
>>> print(json.dumps({'a':2, 'b':{'x':3, 'y':{'t1': 4, 't2':5}}},
...                  sort_keys=True, indent=4))
{
    ""a"": 2,
    ""b"": {
        ""x"": 3,
        ""y"": {
            ""t1"": 4,
            ""t2"": 5
        }
    }
}

```"
"Sometimes I come across code such as this:

```
import matplotlib.pyplot as plt
x = [1, 2, 3, 4, 5]
y = [1, 4, 9, 16, 25]
fig = plt.figure()
fig.add_subplot(111)
plt.scatter(x, y)
plt.show()

```

Which produces:

![Example plot produced by the included code](https://i.sstatic.net/yCOG3.png)

I've been reading the documentation like crazy but I can't find an explanation for the `111`. sometimes I see a `212`.

What does the argument of `fig.add_subplot()` mean?","I think this would be best explained by the following picture:

![enter image description here](https://i.sstatic.net/AEGXG.png)

To initialize the above, one would type:

```
import matplotlib.pyplot as plt
fig = plt.figure()
fig.add_subplot(221)   #top left
fig.add_subplot(222)   #top right
fig.add_subplot(223)   #bottom left
fig.add_subplot(224)   #bottom right 
plt.show()

```"
"`pytest` will not print to the console when I use `print`. The [documentation](http://pytest.org/latest/capture.html) seems to say that it should work by default.

I am using `pytest my_tests.py` to run this test:

```
import myapplication as tum

class TestBlogger:
    @classmethod
    def setup_class(self):
        self.user = ""alice""
        self.b = tum.Blogger(self.user)
        print ""This should be printed, but it won't be!""

    def test_inherit(self):
        assert issubclass(tum.Blogger, tum.Site)
        links = self.b.get_links(posts)
        print len(links)   # This won't print either.

```

Nothing gets printed to my standard output console (just the normal progress and how many many tests passed/failed).

And the script that I'm testing contains print:

```
class Blogger(Site):
    get_links(self, posts):
        print len(posts)   # It won't get printed in the test.

```

In `unittest` module, everything gets printed by default, which is exactly what I need. However, I wish to use `pytest` for other reasons.

**Does anyone know how to make the print statements get shown?**","Use the `-s` option:

```
pytest -s

```

Detailed answer
---------------

From [the docs](https://docs.pytest.org/en/latest/capture.html):

> During test execution any output sent to **stdout** and **stderr** is captured. If a test or a setup method fails its according captured output will usually be shown along with the failure traceback.

`pytest` has the option `--capture=method` in which `method` is per-test capturing method, and could be one of the following: `fd`, `sys` or `no`. `pytest` also has the option `-s` which is a shortcut for `--capture=no`, and this is the option that will allow you to see your print statements in the console.

```
pytest --capture=no     # show print statements in console
pytest -s               # equivalent to previous command

```

### Setting capturing methods or disabling capturing

There are two ways in which `pytest` can perform capturing:

1. **file descriptor (FD) level capturing** (default): All writes going to the operating system file descriptors 1 and 2 will be captured.
2. **sys level capturing**: Only writes to Python files sys.stdout and sys.stderr will be captured. No capturing of writes to filedescriptors is performed.

```
pytest -s            # disable all capturing
pytest --capture=sys # replace sys.stdout/stderr with in-mem files
pytest --capture=fd  # also point filedescriptors 1 and 2 to temp file

```"
How do I reimport a module? I want to reimport a module after making changes to its .py file.,"For **Python 3.4+**:

```
import importlib
importlib.reload(nameOfModule)

```

For **Python < 3.4**:

```
reload(my.module)

```

From the [Python docs](http://docs.python.org/2.7/library/functions.html#reload)

> Reload a previously imported module. The argument must be a module object, so it must have been successfully imported before. This is useful if you have edited the module source file using an external editor and want to try out the new version without leaving the Python interpreter.

Don't forget the caveats of using this method:

* When a module is reloaded, its dictionary (containing the module’s global variables) is retained. Redefinitions of names will override the old definitions, so this is generally not a problem, but if the new version of a module does not define a name that was defined by the old version, the old definition is not removed.
* If a module imports objects from another module using `from ... import ...`, calling `reload()` for the other module does not redefine the objects imported from it — one way around this is to re-execute the `from` statement, another is to use `import` and qualified names (`module.*name*`) instead.
* If a module instantiates instances of a class, reloading the module that defines the class does not affect the method definitions of the instances — they continue to use the old class definition. The same is true for derived classes."
"If `mydict` is not empty, I access an arbitrary element as:

```
mydict[list(mydict.keys())[0]]

```

Is there any better way to do this?","On Python 3, non-destructively and iteratively:

```
next(iter(mydict.values()))

```

On Python 2, non-destructively and iteratively:

```
mydict.itervalues().next()

```

If you want it to work in both Python 2 and 3, you can use the `six` package:

```
six.next(six.itervalues(mydict))

```

though at this point it is quite cryptic and I'd rather prefer your code.

If you want to remove any item, do:

```
key, value = mydict.popitem()

```

Note that ""first"" may not be an appropriate term here because `dict` is not an ordered type in Python < 3.6. Python 3.6+ `dicts` are ordered."
"Anyone tinkering with Python long enough has been bitten (or torn to pieces) by the following issue:

```
def foo(a=[]):
    a.append(5)
    return a

```

Python novices would expect this function called with no parameter to always return a list with only one element: `[5]`. The result is instead very different, and very astonishing (for a novice):

```
>>> foo()
[5]
>>> foo()
[5, 5]
>>> foo()
[5, 5, 5]
>>> foo()
[5, 5, 5, 5]
>>> foo()

```

A manager of mine once had his first encounter with this feature, and called it ""a dramatic design flaw"" of the language. I replied that the behavior had an underlying explanation, and it is indeed very puzzling and unexpected if you don't understand the internals. However, I was not able to answer (to myself) the following question: what is the reason for binding the default argument at function definition, and not at function execution? I doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs?)

**Edit**:

[Baczek made an interesting example](https://stackoverflow.com/a/1137164/7487335). Together with most of your comments and [Utaal's in particular](https://stackoverflow.com/a/1134623/7487335), I elaborated further:

```
def a():
    print(""a executed"")
    return []

           
def b(x=a()):
    x.append(5)
    print(x)

a executed
>>> b()
[5]
>>> b()
[5, 5]

```

To me, it seems that the design decision was relative to where to put the scope of parameters: inside the function, or ""together"" with it?

Doing the binding inside the function would mean that `x` is effectively bound to the specified default when the function is called, not defined, something that would present a deep flaw: the `def` line would be ""hybrid"" in the sense that part of the binding (of the function object) would happen at definition, and part (assignment of default parameters) at function invocation time.

The actual behavior is more consistent: everything of that line gets evaluated when that line is executed, meaning at function definition.","Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are [*first-class objects*](https://en.wikipedia.org/wiki/First-class_function), and not only a piece of code.

As soon as you think of it this way, then it completely makes sense: a function is an *object* being evaluated on its definition; default parameters are kind of *""member data""* and therefore their state may change from one call to the other - exactly as in any other object.

In any case, the Effbot ([Fredrik Lundh](https://lwn.net/Articles/878325/)) has a very nice explanation of the reasons for this behavior in [Default Parameter Values in Python](https://web.archive.org/web/20200221224620id_/http://effbot.org/zone/default-values.htm). I found it very clear, and I really suggest reading it for a better knowledge of how function objects work."
"I have scripts calling other script files but I need to get the filepath of the file that is currently running within the process.

For example, let's say I have three files. Using [execfile](http://docs.python.org/library/functions.html#execfile):

* `script_1.py` calls `script_2.py`.
* In turn, `script_2.py` calls `script_3.py`.

How can I get the file name and path of **`script_3.py`**, *from code within `script_3.py`*, without having to pass that information as arguments from `script_2.py`?

(Executing `os.getcwd()` returns the original starting script's filepath not the current file's.)","```
__file__

```

as others have said. You may also want to use [os.path.realpath](https://docs.python.org/3/library/os.path.html#os.path.realpath) to eliminate symlinks:

```
import os

os.path.realpath(__file__)

```"
"I have a Python pandas DataFrame `rpt`:

```
rpt
<class 'pandas.core.frame.DataFrame'>
MultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')
Data columns:
STK_ID                    47518  non-null values
STK_Name                  47518  non-null values
RPT_Date                  47518  non-null values
sales                     47518  non-null values

```

I can filter the rows whose stock id is `'600809'` like this: `rpt[rpt['STK_ID'] == '600809']`

```
<class 'pandas.core.frame.DataFrame'>
MultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')
Data columns:
STK_ID                    25  non-null values
STK_Name                  25  non-null values
RPT_Date                  25  non-null values
sales                     25  non-null values

```

and I want to get all the rows of some stocks together, such as `['600809','600141','600329']`. That means I want a syntax like this:

```
stk_list = ['600809','600141','600329']

rst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas 

```

Since pandas not accept above command, how to achieve the target?","Use the `isin` method:

`rpt[rpt['STK_ID'].isin(stk_list)]`"
"```
df = pd.read_csv('somefile.csv')

```

...gives an error:

> .../site-packages/pandas/io/parsers.py:1130:
> DtypeWarning: Columns (4,5,7,16) have mixed types. Specify dtype
> option on import or set low\_memory=False.

Why is the `dtype` option related to `low_memory`, and why might `low_memory=False` help?","The deprecated low\_memory option
=================================

The `low_memory` option is not properly deprecated, but it should be, since it does not actually do anything differently[[source](https://github.com/pydata/pandas/issues/5888)]

The reason you get this `low_memory` warning is because guessing dtypes for each column is very memory demanding. Pandas tries to determine what dtype to set by analyzing the data in each column.

Dtype Guessing (very bad)
=========================

Pandas can only determine what dtype a column should have once the whole file is read. This means nothing can really be parsed before the whole file is read unless you risk having to change the dtype of that column when you read the last value.

Consider the example of one file which has a column called user\_id.
It contains 10 million rows where the user\_id is always numbers.
Since pandas cannot know it is only numbers, it will probably keep it as the original strings until it has read the whole file.

Specifying dtypes (should always be done)
=========================================

adding

```
dtype={'user_id': int}

```

to the [`pd.read_csv()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call will make pandas know when it starts reading the file, that this is only integers.

Also worth noting is that if the last line in the file would have `""foobar""` written in the `user_id` column, the loading would crash if the above dtype was specified.

### Example of broken data that breaks when dtypes are defined

```
import pandas as pd
try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO


csvdata = """"""user_id,username
1,Alice
3,Bob
foobar,Caesar""""""
sio = StringIO(csvdata)
pd.read_csv(sio, dtype={""user_id"": int, ""username"": ""string""})

ValueError: invalid literal for long() with base 10: 'foobar'

```

dtypes are typically a numpy thing, read more about them here:
<http://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html>

What dtypes exists?
===================

We have access to numpy dtypes: float, int, bool, timedelta64[ns] and datetime64[ns]. Note that the numpy date/time dtypes are *not* time zone aware.

Pandas extends this set of dtypes with its own:

`'datetime64[ns, <tz>]'` Which is a time zone aware timestamp.

'category' which is essentially an enum (strings represented by integer keys to save

'period[]' Not to be confused with a timedelta, these objects are actually anchored to specific time periods

'Sparse', 'Sparse[int]', 'Sparse[float]' is for sparse data or 'Data that has a lot of holes in it' Instead of saving the NaN or None in the dataframe it omits the objects, saving space.

'Interval' is a topic of its own but its main use is for indexing. [See more here](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced-intervalindex)

'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64' are all pandas specific integers that are nullable, unlike the numpy variant.

'string' is a specific dtype for working with string data and gives access to the `.str` attribute on the series.

'boolean' is like the numpy 'bool' but it also supports missing data.

Read the complete reference here:

[Pandas dtype reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html)

Gotchas, caveats, notes
=======================

Setting `dtype=object` will silence the above warning, but will not make it more memory efficient, only process efficient if anything.

Setting `dtype=unicode` will not do anything, since to numpy, a `unicode` is represented as `object`.

### Usage of converters

@sparrow correctly points out the usage of converters to avoid pandas blowing up when encountering `'foobar'` in a column specified as `int`. I would like to add that converters are really heavy and inefficient to use in pandas and should be used as a last resort. This is because the read\_csv process is a single process.

CSV files can be processed line by line and thus can be processed by multiple converters in parallel more efficiently by simply cutting the file into segments and running multiple processes, something that pandas does not support. But this is a different story."
"I have the following code:

```
url = 'abcdc.com'
print(url.strip('.com'))

```

I expected: `abcdc`

I got: `abcd`

Now I do

```
url.rsplit('.com', 1)

```

Is there a better way?

---

See [How do the .strip/.rstrip/.lstrip string methods work in Python?](https://stackoverflow.com/questions/40990694) for a specific explanation of what the first attempt is doing.","[`strip`](https://docs.python.org/library/stdtypes.html#str.strip) doesn't mean ""remove this substring"". `x.strip(y)` treats `y` as a set of characters and strips any characters in that set from both ends of `x`.

On **Python 3.9 and newer** you can use the [`removeprefix`](https://docs.python.org/library/stdtypes.html#str.removeprefix) and [`removesuffix`](https://docs.python.org/library/stdtypes.html#str.removesuffix) methods to remove an entire substring from either side of the string:

```
url = 'abcdc.com'
url.removesuffix('.com')    # Returns 'abcdc'
url.removeprefix('abcdc.')  # Returns 'com'

```

The relevant Python Enhancement Proposal is [PEP-616](https://www.python.org/dev/peps/pep-0616/).

On **Python 3.8 and older** you can use [`endswith`](https://docs.python.org/library/stdtypes.html#str.endswith) and slicing:

```
url = 'abcdc.com'
if url.endswith('.com'):
    url = url[:-4]

```

Or a [regular expression](https://docs.python.org/library/re.html):

```
import re
url = 'abcdc.com'
url = re.sub('\.com$', '', url)

```"
"Is it possible to read binary MATLAB .mat files in Python?

I've seen that SciPy has alleged support for reading .mat files, but I'm unsuccessful with it. I installed SciPy version 0.7.0, and I can't find the `loadmat()` method.","An import is required, `import scipy.io`...

```
import scipy.io
mat = scipy.io.loadmat('file.mat')

```"
"Given a dataframe, I want to groupby the first column and get second column as lists in rows, so that a dataframe like:

```
a b
A 1
A 2
B 5
B 5
B 4
C 6

```

becomes

```
A [1,2]
B [5,5,4]
C [6]

```

How do I do this?","You can do this using `groupby` to group on the column of interest and then `apply` `list` to every group:

```
In [1]: df = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})
        df

Out[1]: 
   a  b
0  A  1
1  A  2
2  B  5
3  B  5
4  B  4
5  C  6

In [2]: df.groupby('a')['b'].apply(list)
Out[2]: 
a
A       [1, 2]
B    [5, 5, 4]
C          [6]
Name: b, dtype: object

In [3]: df1 = df.groupby('a')['b'].apply(list).reset_index(name='new')
        df1
Out[3]: 
   a        new
0  A     [1, 2]
1  B  [5, 5, 4]
2  C        [6]

```"
"Is there any benefit to use one over the other? In Python 2, they both seem to return the same results:

```
>>> 6/3
2
>>> 6//3
2

```","In Python 3.x, `5 / 2` will return `2.5` and `5 // 2` will return `2`. The former is floating point division, and the latter is ***floor division***, sometimes also called ***integer division***.

In Python 2.2 or later in the 2.x line, there is no difference for integers unless you perform a `from __future__ import division`, which causes Python 2.x to adopt the 3.x behavior.

Regardless of the future import, `5.0 // 2` will return `2.0` since that's the floor division result of the operation.

You can find a detailed description at *[PEP 238: Changing the Division Operator](https://docs.python.org/whatsnew/2.2.html#pep-238-changing-the-division-operator)*."
"I need to generate a whole bunch of vertically-stacked plots in matplotlib. The result will be saved using `savefig` and viewed on a webpage, so I don't care how tall the final image is, as long as the subplots are spaced so they don't overlap.

No matter how big I allow the figure to be, the subplots always seem to overlap.

My code currently looks like

```
import matplotlib.pyplot as plt
import my_other_module

titles, x_lists, y_lists = my_other_module.get_data()

fig = plt.figure(figsize=(10,60))
for i, y_list in enumerate(y_lists):
    plt.subplot(len(titles), 1, i)
    plt.xlabel(""Some X label"")
    plt.ylabel(""Some Y label"")
    plt.title(titles[i])
    plt.plot(x_lists[i],y_list)
fig.savefig('out.png', dpi=100)

```","Please review [matplotlib: Tight Layout guide](https://matplotlib.org/stable/users/explain/axes/tight_layout_guide.html) and try using [`matplotlib.pyplot.tight_layout`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html), or [`matplotlib.figure.Figure.tight_layout`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.tight_layout)

As a quick example:

```
import matplotlib.pyplot as plt

fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))
fig.tight_layout() # Or equivalently,  ""plt.tight_layout()""

plt.show()

```

---

Without Tight Layout

[![enter image description here](https://i.sstatic.net/U7agc.png)](https://i.sstatic.net/U7agc.png)

---

With Tight Layout

[![enter image description here](https://i.sstatic.net/G4NNT.png)](https://i.sstatic.net/G4NNT.png)"
How do I add values to an existing `set`?,"```
your_set.update(your_sequence_of_values)

```

e.g, `your_set.update([1, 2, 3, 4])`. Or, if you *have* to produce the values in a loop for some other reason,

```
for value in ...:
    your_set.add(value)

```

But, of course, doing it in bulk with a single [`.update`](https://docs.python.org/3/library/stdtypes.html#frozenset.update) call is faster and handier, when otherwise feasible."
"I'm trying to convert a server side Ajax response script into a Django HttpResponse, but apparently it's not working.

This is the server-side script:

```
/* RECEIVE VALUE */
$validateValue=$_POST['validateValue'];
$validateId=$_POST['validateId'];
$validateError=$_POST['validateError'];

/* RETURN VALUE */
$arrayToJs = array();
$arrayToJs[0] = $validateId;
$arrayToJs[1] = $validateError;

if($validateValue ==""Testuser""){  // Validate??
    $arrayToJs[2] = ""true"";       // RETURN TRUE
    echo '{""jsonValidateReturn"":'.json_encode($arrayToJs).'}';  // RETURN ARRAY WITH success
}
else{
    for($x=0;$x<1000000;$x++){
        if($x == 990000){
            $arrayToJs[2] = ""false"";
            echo '{""jsonValidateReturn"":'.json_encode($arrayToJs).'}';   // RETURNS ARRAY WITH ERROR.
        }
    }
}

```

And this is the converted code

```
def validate_user(request):
    if request.method == 'POST':
        vld_value = request.POST.get('validateValue')
        vld_id = request.POST.get('validateId')
        vld_error = request.POST.get('validateError')

        array_to_js = [vld_id, vld_error, False]

        if vld_value == ""TestUser"":
            array_to_js[2] = True
            x = simplejson.dumps(array_to_js)
            return HttpResponse(x)
        else:
            array_to_js[2] = False
            x = simplejson.dumps(array_to_js)
            error = 'Error'
            return render_to_response('index.html',{'error':error},context_instance=RequestContext(request))
    return render_to_response('index.html',context_instance=RequestContext(request))

```

I'm using simplejson to encode the Python list (so it will return a JSON array). I couldn't figure out the problem yet. But I think that I did something wrong about the 'echo'.","I usually use a dictionary, not a list to return JSON content.

```
import json

from django.http import HttpResponse

response_data = {}
response_data['result'] = 'error'
response_data['message'] = 'Some error message'

```

Pre-Django 1.7 you'd return it like this:

```
return HttpResponse(json.dumps(response_data), content_type=""application/json"")

```

For Django 1.7+, use [`JsonResponse`](https://docs.djangoproject.com/en/stable/ref/request-response/#jsonresponse-objects) as shown in [this SO answer](https://stackoverflow.com/a/24411716/7376) like so :

```
from django.http import JsonResponse
return JsonResponse({'foo':'bar'})

```"
"What do `*args` and `**kwargs` mean in these function definitions?

```
def foo(x, y, *args):
    pass

def bar(x, y, **kwargs):
    pass

```

---

See [What do \*\* (double star/asterisk) and \* (star/asterisk) mean in a function call?](https://stackoverflow.com/questions/2921847) for the complementary question about arguments.","The `*args` and `**kwargs` are common idioms to allow an arbitrary number of arguments to functions, as described in the section [more on defining functions](http://docs.python.org/3/tutorial/controlflow.html#more-on-defining-functions) in the Python tutorial.

The `*args` will give you all positional arguments [as a tuple](https://docs.python.org/3/tutorial/controlflow.html#arbitrary-argument-lists):

```
def foo(*args):
    for a in args:
        print(a)        

foo(1)
# 1

foo(1, 2, 3)
# 1
# 2
# 3

```

The `**kwargs` will give you all
keyword arguments as a dictionary:

```
def bar(**kwargs):
    for a in kwargs:
        print(a, kwargs[a])  

bar(name='one', age=27)
# name one
# age 27

```

Both idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:

```
def foo(kind, *args, bar=None, **kwargs):
    print(kind, args, bar, kwargs)

foo(123, 'a', 'b', apple='red')
# 123 ('a', 'b') None {'apple': 'red'}

```

It is also possible to use this the other way around:

```
def foo(a, b, c):
    print(a, b, c)

obj = {'b':10, 'c':'lee'}

foo(100, **obj)
# 100 10 lee

```

Another usage of the `*l` idiom is to **unpack argument lists** when calling a function.

```
def foo(bar, lee):
    print(bar, lee)

baz = [1, 2]

foo(*baz)
# 1 2

```

In Python 3 it is possible to use `*l` on the left side of an assignment ([Extended Iterable Unpacking](http://www.python.org/dev/peps/pep-3132/)), though it gives a list instead of a tuple in this context:

```
first, *rest = [1, 2, 3, 4]
# first = 1
# rest = [2, 3, 4]

```

Also Python 3 adds a new semantic (refer [PEP 3102](https://www.python.org/dev/peps/pep-3102/)):

```
def func(arg1, arg2, arg3, *, kwarg1, kwarg2):
    pass

```

Such function accepts only 3 positional arguments, and everything after `*` can only be passed as keyword arguments.

### Note:

A Python `dict`, semantically used for keyword argument passing, is arbitrarily ordered. However, in Python 3.6+, keyword arguments are guaranteed to remember insertion order.
""The order of elements in `**kwargs` now corresponds to the order in which keyword arguments were passed to the function."" - [Whatâ€™s New In Python 3.6](https://docs.python.org/3/whatsnew/3.6.html).
In fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, and this becomes standard in Python 3.7."
"I'm trying to scrape a website, but it gives me an error.

I'm using the following code:

```
import urllib.request
from bs4 import BeautifulSoup

get = urllib.request.urlopen(""https://www.website.com/"")
html = get.read()

soup = BeautifulSoup(html)

```

And I'm getting the following error:

```
File ""C:\Python34\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 70924-70950: character maps to <undefined>

```

What can I do to fix this?","I was getting the same `UnicodeEncodeError` when saving scraped web content to a file. To fix it I replaced this code:

```
with open(fname, ""w"") as f:
    f.write(html)

```

with this:

```
with open(fname, ""w"", encoding=""utf-8"") as f:
    f.write(html)

```

If you need to support Python 2, then use this:

```
import io
with io.open(fname, ""w"", encoding=""utf-8"") as f:
    f.write(html)

```

If you want to use a different encoding than UTF-8, specify whatever your actual encoding is for `encoding`."
"I am writing scripts in Python2.6 with use of [pyVmomi](https://github.com/vmware/pyvmomi) and while using one of the connection methods:

```
service_instance = connect.SmartConnect(host=args.ip,
                                        user=args.user,
                                        pwd=args.password)

```

I get the following warning:

```
/usr/lib/python2.6/site-packages/requests/packages/urllib3/connectionpool.py:734: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)

```

What's interesting is that I do not have urllib3 installed with pip (but it's there in */usr/lib/python2.6/site-packages/requests/packages/urllib3/*).

I have tried as suggested [here](https://urllib3.readthedocs.org/en/latest/security.html)

```
import urllib3
...
urllib3.disable_warnings()

```

but that didn't change anything.","You can disable any Python warnings via the `PYTHONWARNINGS` environment variable. In this case, you want:

```
export PYTHONWARNINGS=""ignore:Unverified HTTPS request""

```

To disable using Python code (`requests >= 2.16.0`):

```
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

```

For `requests < 2.16.0`, see original answer below.

**Original answer**

The reason doing `urllib3.disable_warnings()` didn't work for you is because it looks like you're using a separate instance of urllib3 vendored inside of requests.

I gather this based on the path here: `/usr/lib/python2.6/site-packages/requests/packages/urllib3/connectionpool.py`

To disable warnings in requests' vendored urllib3, you'll need to import that specific instance of the module:

```
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

```"
"I have this code:

```
def hello():
    return 'Hi :)'

```

How would I run this directly from the command line?

---

**See also:** [What does if \_\_name\_\_ == ""\_\_main\_\_"": do?](https://stackoverflow.com/questions/419163/) to explain the standard idiom for getting the code started;  
[Why doesn't the main() function run when I start a Python script? Where does the script start running (what is its entry point)?](https://stackoverflow.com/questions/17257631/) for why things like this are necessary","With the `-c` *(command)* argument (assuming your file is named `foo.py`):

```
$ python -c 'import foo; print foo.hello()'

```

Alternatively, if you don't care about namespace pollution:

```
$ python -c 'from foo import *; print hello()'

```

And the middle ground:

```
$ python -c 'from foo import hello; print hello()'

```"
"I have a dataframe `df` imported from an Excel document like this:

```
cluster load_date   budget  actual  fixed_price
A   1/1/2014    1000    4000    Y
A   2/1/2014    12000   10000   Y
A   3/1/2014    36000   2000    Y
B   4/1/2014    15000   10000   N
B   4/1/2014    12000   11500   N
B   4/1/2014    90000   11000   N
C   7/1/2014    22000   18000   N
C   8/1/2014    30000   28960   N
C   9/1/2014    53000   51200   N

```

I want to be able to return the contents of column 1 `df['cluster']` as a list, so I can run a for-loop over it, and create an Excel worksheet for every cluster.

Is it also possible to return the contents of a whole column or row to a list? e.g.

```
list = [], list[column1] or list[df.ix(row1)]

```","Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call `x.tolist()` on to turn them into a Python list. Alternatively you cast it with `list(x)`.

```
import pandas as pd

data_dict = {'one': pd.Series([1, 2, 3], index=['a', 'b', 'c']),
             'two': pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(data_dict)

print(f""DataFrame:\n{df}\n"")
print(f""column types:\n{df.dtypes}"")

col_one_list = df['one'].tolist()

col_one_arr = df['one'].to_numpy()

print(f""\ncol_one_list:\n{col_one_list}\ntype:{type(col_one_list)}"")
print(f""\ncol_one_arr:\n{col_one_arr}\ntype:{type(col_one_arr)}"")

```

Output:

```
DataFrame:
   one  two
a  1.0    1
b  2.0    2
c  3.0    3
d  NaN    4

column types:
one    float64
two      int64
dtype: object

col_one_list:
[1.0, 2.0, 3.0, nan]
type:<class 'list'>

col_one_arr:
[ 1.  2.  3. nan]
type:<class 'numpy.ndarray'>

```"
"How can I *iterate* over a string in Python (get each character from the string, one at a time, each time through a loop)?","As Johannes pointed out,

```
for c in ""string"":
    #do something with c

```

You can iterate pretty much anything in python using the `for loop` construct,

for example, `open(""file.txt"")` returns a file object (and opens the file), iterating over it iterates over lines in that file

```
with open(filename) as f:
    for line in f:
        # do something with line

```

If that seems like magic, well it kinda is, but the idea behind it is really simple.

There's a simple iterator protocol that can be applied to any kind of object to make the `for` loop work on it.

Simply implement an iterator that defines a `next()` method, and implement an `__iter__` method on a class to make it iterable. (the `__iter__` of course, should return an iterator object, that is, an object that defines `next()`)

[See official documentation](http://docs.python.org/library/stdtypes.html#iterator-types)"
"I am curious as to why `df[2]` is not supported, while `df.ix[2]` and `df[2:3]` both work.

```
In [26]: df.ix[2]
Out[26]: 
A    1.027680
B    1.514210
C   -1.466963
D   -0.162339
Name: 2000-01-03 00:00:00

In [27]: df[2:3]
Out[27]: 
                  A        B         C         D
2000-01-03  1.02768  1.51421 -1.466963 -0.162339

```

I would expect `df[2]` to work the same way as `df[2:3]` to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?","echoing @HYRY, see the new docs in 0.11

<http://pandas.pydata.org/pandas-docs/stable/indexing.html>

Here we have new operators, `.iloc` to explicity support only integer indexing, and `.loc` to explicity support only label indexing

e.g. imagine this scenario

```
In [1]: df = pd.DataFrame(np.random.rand(5,2),index=range(0,10,2),columns=list('AB'))

In [2]: df
Out[2]: 
          A         B
0  1.068932 -0.794307
2 -0.470056  1.192211
4 -0.284561  0.756029
6  1.037563 -0.267820
8 -0.538478 -0.800654

In [5]: df.iloc[[2]]
Out[5]: 
          A         B
4 -0.284561  0.756029

In [6]: df.loc[[2]]
Out[6]: 
          A         B
2 -0.470056  1.192211

```

`[]` slices the rows (by label location) only"
"How do I create a list of numbers between two values? For example, a list between 11 and 16:

```
[11, 12, 13, 14, 15, 16]

```","Use [`range`](http://docs.python.org/2.7/library/functions.html#range). In Python 2, it returns a list directly:

```
>>> range(11, 17)
[11, 12, 13, 14, 15, 16]

```

In Python 3, [`range`](https://docs.python.org/3/library/stdtypes.html#typesseq-range) is an iterator. To convert it to a list:

```
>>> list(range(11, 17))
[11, 12, 13, 14, 15, 16]

```

**Note**: The second number in `range(start, stop)` is exclusive. So, `stop = 16+1 = 17`.

---

To increment by steps of `0.5`, consider using [numpy's](https://pypi.python.org/pypi/numpy) [`arange()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html) and [`.tolist()`](https://numpy.org/devdocs/reference/generated/numpy.ndarray.tolist.html):

```
>>> import numpy as np
>>> np.arange(11, 17, 0.5).tolist()

[11.0, 11.5, 12.0, 12.5, 13.0, 13.5,
 14.0, 14.5, 15.0, 15.5, 16.0, 16.5]

```

See: [How do I use a decimal step value for range()?](https://stackoverflow.com/questions/477486/how-do-i-use-a-decimal-step-value-for-range)"
"I've been looking at dynamic evaluation of Python code, and come across the `eval()` and `compile()` functions, and the `exec` statement.

Can someone please explain the difference between `eval` and `exec`, and how the different modes of `compile()` fit in?","The short answer, or TL;DR
==========================

Basically, [`eval`](https://docs.python.org/3/library/functions.html#eval) is used to **eval**uate a single dynamically generated Python expression, and [`exec`](https://docs.python.org/3/library/functions.html#exec) is used to **exec**ute dynamically generated Python code only for its side effects.

`eval` and `exec` have these two differences:

1. `eval` accepts only a **single expression**, `exec` can take a code block that has Python statements: loops, `try: except:`, `class` and function/method `def`initions and so on.

   An expression in Python is whatever you can have as the value in a variable assignment:

   ```
   a_variable = (anything you can put within these parentheses is an expression)

   ```
2. `eval` **returns the value** of the given expression, whereas `exec` ignores the return value from its code, and always returns `None` (in Python 2 it is a statement and cannot be used as an expression, so it really does not return anything).

In versions 1.0 - 2.7, `exec` was a statement, because CPython needed to produce a different kind of code object for functions that used `exec` for its side effects inside the function.

In Python 3, `exec` is a function; its use has no effect on the compiled bytecode of the function where it is used.

---

Thus basically:

```
>>> a = 5
>>> eval('37 + a')   # it is an expression
42
>>> exec('37 + a')   # it is an expression statement; value is ignored (None is returned)
>>> exec('a = 47')   # modify a global variable as a side effect
>>> a
47
>>> eval('a = 47')  # you cannot evaluate a statement
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 1
    a = 47
      ^
SyntaxError: invalid syntax

```

---

The `compile` in `'exec'` mode compiles any number of statements into a bytecode that implicitly always returns `None`, whereas in `'eval'` mode it compiles a *single* expression into bytecode that *returns* the value of that expression.

```
>>> eval(compile('42', '<string>', 'exec'))  # code returns None
>>> eval(compile('42', '<string>', 'eval'))  # code returns 42
42
>>> exec(compile('42', '<string>', 'eval'))  # code returns 42,
>>>                                          # but ignored by exec

```

In the `'eval'` mode (and thus with the `eval` function if a string is passed in), the `compile` raises an exception if the source code contains statements or anything else beyond a single expression:

```
>>> compile('for i in range(3): print(i)', '<string>', 'eval')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 1
    for i in range(3): print(i)
      ^
SyntaxError: invalid syntax

```

---

Actually the statement *""eval accepts only a single expression""* applies only when a string (which contains Python *source code*) is passed to `eval`. Then it is internally compiled to bytecode using [`compile(source, '<string>', 'eval')`](https://docs.python.org/3/library/functions.html#compile) This is where the difference really comes from.

If a `code` object (which contains Python *bytecode*) is passed to `exec` or `eval`, *they behave identically*, excepting for the fact that `exec` ignores the return value, still returning `None` always. So it is possible use `eval` to execute something that has statements, if you just `compile`d it into bytecode before instead of passing it as a string:

```
>>> eval(compile('if 1: print(""Hello"")', '<string>', 'exec'))
Hello
>>>

```

works without problems, even though the compiled code contains statements. It still returns `None`, because that is the return value of the code object returned from `compile`.

In the `'eval'` mode (and thus with the `eval` function if a string is passed in), the `compile` raises an exception if the source code contains statements or anything else beyond a single expression:

```
>>> compile('for i in range(3): print(i)', '<string>'. 'eval')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 1
    for i in range(3): print(i)
      ^
SyntaxError: invalid syntax

```

The longer answer, a.k.a the gory details
=========================================

`exec` and `eval`
-----------------

The [`exec`](https://docs.python.org/3/library/functions.html#exec) function (which was [a statement in Python 2](https://docs.python.org/2/reference/simple_stmts.html#exec)) is used for executing a dynamically created statement or program:

```
>>> program = '''
for i in range(3):
    print(""Python is cool"")
'''
>>> exec(program)
Python is cool
Python is cool
Python is cool
>>> 

```

The [`eval`](https://docs.python.org/3/library/functions.html#eval) function does the same for a [single expression](https://docs.python.org/3/reference/expressions.html), *and* returns the value of the expression:

```
>>> a = 2
>>> my_calculation = '42 * a'
>>> result = eval(my_calculation)
>>> result
84

```

`exec` and `eval` both accept the program/expression to be run either as a `str`, `unicode` or `bytes` object containing source code, or as a *`code` object* which contains Python bytecode.

If a `str`/`unicode`/`bytes` containing source code was passed to `exec`, it behaves equivalently to:

```
exec(compile(source, '<string>', 'exec'))

```

and `eval` similarly behaves equivalent to:

```
eval(compile(source, '<string>', 'eval'))

```

---

Since all expressions can be used as statements in Python (these are called the `Expr` nodes in the Python [abstract grammar](https://docs.python.org/3/library/ast.html#abstract-grammar); the opposite is not true), you can always use `exec` if you do not need the return value. That is to say, you can use either `eval('my_func(42)')` or `exec('my_func(42)')`, the difference being that `eval` returns the value returned by `my_func`, and `exec` discards it:

```
>>> def my_func(arg):
...     print(""Called with %d"" % arg)
...     return arg * 2
... 
>>> exec('my_func(42)')
Called with 42
>>> eval('my_func(42)')
Called with 42
84
>>> 

```

Of the 2, only `exec` accepts source code that contains statements, like `def`, `for`, `while`, `import`, or `class`, the assignment statement (a.k.a `a = 42`), or entire programs:

```
>>> exec('for i in range(3): print(i)')
0
1
2
>>> eval('for i in range(3): print(i)')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 1
    for i in range(3): print(i)
      ^
SyntaxError: invalid syntax

```

---

Both `exec` and `eval` accept 2 additional positional arguments - `globals` and `locals` - which are the global and local variable scopes that the code sees. These default to the `globals()` and `locals()` within the scope that called `exec` or `eval`, but any dictionary can be used for `globals` and any `mapping` for `locals` (including `dict` of course). These can be used not only to restrict/modify the variables that the code sees, but are often also used for capturing the variables that the `exec`uted code creates:

```
>>> g = dict()
>>> l = dict()
>>> exec('global a; a, b = 123, 42', g, l)
>>> g['a']
123
>>> l
{'b': 42}

```

(If you display the value of the entire `g`, it would be much longer, because `exec` and `eval` add the built-ins module as `__builtins__` to the globals automatically if it is missing).

In Python 2, the official syntax for the `exec` statement is actually `exec code in globals, locals`, as in

```
>>> exec 'global a; a, b = 123, 42' in g, l

```

However the alternate syntax `exec(code, globals, locals)` has always been accepted too (see below).

`compile`
---------

The [`compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1)`](https://docs.python.org/3/library/functions.html#compile) built-in can be used to speed up repeated invocations of the same code with `exec` or `eval` by compiling the source into a `code` object beforehand. The `mode` parameter controls the kind of code fragment the `compile` function accepts and the kind of bytecode it produces. The choices are `'eval'`, `'exec'` and `'single'`:

* `'eval'` mode expects a single expression, and will produce bytecode that when run will return the value of **that expression**:

  ```
  >>> dis.dis(compile('a + b', '<string>', 'eval'))
    1           0 LOAD_NAME                0 (a)
                3 LOAD_NAME                1 (b)
                6 BINARY_ADD
                7 RETURN_VALUE

  ```
* `'exec'` accepts any kinds of python constructs from single expressions to whole modules of code, and executes them as if they were module top-level statements. The code object returns `None`:

  ```
  >>> dis.dis(compile('a + b', '<string>', 'exec'))
    1           0 LOAD_NAME                0 (a)
                3 LOAD_NAME                1 (b)
                6 BINARY_ADD
                7 POP_TOP                             <- discard result
                8 LOAD_CONST               0 (None)   <- load None on stack
               11 RETURN_VALUE                        <- return top of stack

  ```
* `'single'` is a limited form of `'exec'` which accepts a source code containing a **single** statement (or multiple statements separated by `;`) if the last statement is an expression statement, the resulting bytecode also *prints the `repr` of the value of that expression to the standard output(!)*.

  An `if`-`elif`-`else` chain, a loop with `else`, and `try` with its `except`, `else` and `finally` blocks is considered a single statement.

  A source fragment containing 2 top-level statements is an error for the `'single'`, except in Python 2 there is *a bug* that sometimes allows multiple toplevel statements in the code; only the first is compiled; the rest are ignored:

  In Python 2.7.8:

  ```
  >>> exec(compile('a = 5\na = 6', '<string>', 'single'))
  >>> a
  5

  ```

  And in Python 3.4.2:

  ```
  >>> exec(compile('a = 5\na = 6', '<string>', 'single'))
  Traceback (most recent call last):
    File ""<stdin>"", line 1, in <module>
    File ""<string>"", line 1
      a = 5
          ^
  SyntaxError: multiple statements found while compiling a single statement

  ```

  This is very useful for making interactive Python shells. However, the value of the expression is *not returned*, even if you `eval` the resulting code.

Thus greatest distinction of `exec` and `eval` actually comes from the `compile` function and its modes.

---

In addition to compiling source code to bytecode, `compile` supports compiling [*abstract syntax trees*](https://docs.python.org/3/library/ast.html#abstract-grammar) (parse trees of Python code) into `code` objects; and source code into abstract syntax trees (the `ast.parse` is written in Python and just calls `compile(source, filename, mode, PyCF_ONLY_AST)`); these are used for example for modifying source code on the fly, and also for dynamic code creation, as it is often easier to handle the code as a tree of nodes instead of lines of text in complex cases.

---

While `eval` only allows you to evaluate a string that contains a single expression, you can `eval` a whole statement, or even a whole module that has been `compile`d into bytecode; that is, with Python 2, `print` is a statement, and cannot be `eval`led directly:

```
>>> eval('for i in range(3): print(""Python is cool"")')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 1
    for i in range(3): print(""Python is cool"")
      ^
SyntaxError: invalid syntax

```

`compile` it with `'exec'` mode into a `code` object and you can **`eval` it**; the `eval` function will return `None`.

```
>>> code = compile('for i in range(3): print(""Python is cool"")',
                   'foo.py', 'exec')
>>> eval(code)
Python is cool
Python is cool
Python is cool

```

If one looks into [`eval`](https://hg.python.org/cpython/file/ec6ed10d611e/Python/bltinmodule.c#l805) and [`exec`](https://hg.python.org/cpython/file/ec6ed10d611e/Python/bltinmodule.c#l882) source code in CPython 3, this is very evident; they both call `PyEval_EvalCode` with same arguments, the only difference being that [`exec` explicitly returns `None`](https://hg.python.org/cpython/file/ec6ed10d611e/Python/bltinmodule.c#l903).

Syntax differences of `exec` between Python 2 and Python 3
----------------------------------------------------------

One of the major differences in Python **2** is that `exec` is a statement and `eval` is a built-in function (both are built-in functions in Python 3).
It is a well-known fact that the official syntax of `exec` in Python 2 is `exec code [in globals[, locals]]`.

Unlike majority of the Python 2-to-3 [porting](http://python3porting.com/differences.html#exec) [guides](http://python-future.org/reference.html?highlight=exec#future.utils.exec_) [seem](http://docs.pythonsprints.com/python3_porting/py-porting.html#exec-statement) [to suggest](http://nedbatchelder.com/blog/200910/running_the_same_code_on_python_2x_and_3x.html), the `exec` statement in CPython 2 can be also used with syntax that *looks* **exactly** like the `exec` function invocation in Python 3. The reason is that Python 0.9.9 had the `exec(code, globals, locals)` built-in function! And that built-in function was replaced with `exec` statement [somewhere before Python 1.0 release](https://hg.python.org/cpython/file/fccd415e2eb8/Python/ceval.c).

Since it was desirable to not break backwards compatibility with Python 0.9.9, [Guido van Rossum added a compatibility hack in 1993](https://hg.python.org/cpython/file/fccd415e2eb8/Python/ceval.c#l2521): if the `code` was a tuple of length 2 or 3, and `globals` and `locals` were not passed into the `exec` statement otherwise, the `code` would be interpreted as if the 2nd and 3rd element of the tuple were the `globals` and `locals` respectively. The compatibility hack was not mentioned even in [Python 1.4 documentation (the earliest available version online)](https://docs.python.org/release/1.4/ref/ref6.html#HDR10); and thus was not known to many writers of the porting guides and tools, until it was [documented](https://docs.python.org/2/reference/simple_stmts.html#the-exec-statement) again [in November 2012](https://mail.python.org/pipermail/python-dev/2012-November/122651.html):

> The first expression may also be a tuple of length 2 or 3. In this case, the optional parts must be omitted. The form `exec(expr, globals)` is equivalent to `exec expr in globals`, while the form `exec(expr, globals, locals)` is equivalent to `exec expr in globals, locals`. The tuple form of `exec` provides compatibility with Python 3, where `exec` is a function rather than a statement.

Yes, in CPython 2.7 that it is handily referred to as being a forward-compatibility option (why confuse people over that there is a backward compatibility option at all),
when it actually had been there for *backward-compatibility for two decades*.

Thus while `exec` is a statement in Python 1 and Python 2, and a built-in function in Python 3 and Python 0.9.9,

```
>>> exec(""print(a)"", globals(), {'a': 42})
42

```

has had identical behaviour in possibly every widely released Python version ever; and works in Jython 2.5.2, PyPy 2.3.1 (Python 2.7.6) and IronPython 2.6.1 too (kudos to them following the undocumented behaviour of CPython closely).

What you cannot do in Pythons 1.0 - 2.7 with its compatibility hack, is to store the return value of `exec` into a variable:

```
Python 2.7.11+ (default, Apr 17 2016, 14:00:29) 
[GCC 5.3.1 20160413] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> a = exec('print(42)')
  File ""<stdin>"", line 1
    a = exec('print(42)')
           ^
SyntaxError: invalid syntax

```

(which wouldn't be useful in Python 3 either, as `exec` always returns `None`), or pass a reference to `exec`:

```
>>> call_later(exec, 'print(42)', delay=1000)
  File ""<stdin>"", line 1
    call_later(exec, 'print(42)', delay=1000)
                  ^
SyntaxError: invalid syntax

```

Which a pattern that someone might actually have used, though unlikely;

Or use it in a list comprehension:

```
>>> [exec(i) for i in ['print(42)', 'print(foo)']
  File ""<stdin>"", line 1
    [exec(i) for i in ['print(42)', 'print(foo)']
        ^
SyntaxError: invalid syntax

```

which is abuse of list comprehensions (use a `for` loop instead!)."
"I've been working on a web app using Django, and I'm curious if there is a way to schedule a job to run periodically.

Basically I just want to run through the database and make some calculations/updates on an automatic, regular basis, but I can't seem to find any documentation on doing this.

Does anyone know how to set this up?

To clarify: I know I can set up a `cron` job to do this, but I'm curious if there is some feature in Django that provides this functionality. I'd like people to be able to deploy this app themselves without having to do much config (preferably zero).

I've considered triggering these actions ""retroactively"" by simply checking if a job should have been run since the last time a request was sent to the site, but I'm hoping for something a bit cleaner.","One solution that I have employed is to do this:

1) Create a [custom management command](http://docs.djangoproject.com/en/dev/howto/custom-management-commands/#howto-custom-management-commands), e.g.

```
python manage.py my_cool_command

```

2) Use `cron` (on Linux) or `at` (on Windows) to run my command at the required times.

This is a simple solution that doesn't require installing a heavy AMQP stack. However there are nice advantages to using something like Celery, mentioned in the other answers. In particular, with Celery it is nice to not have to spread your application logic out into crontab files. However the cron solution works quite nicely for a small to medium sized application and where you don't want a lot of external dependencies.

EDIT:

In later version of windows the `at` command is deprecated for Windows 8, Server 2012 and above. You can use `schtasks.exe` for same use.

\*\*\*\* UPDATE \*\*\*\*
This the new [link](https://docs.djangoproject.com/en/2.2/howto/custom-management-commands/#howto-custom-management-commands) of django doc for writing the custom management command"
"I have a try...except block in my code and When an exception is throw. I really just want to continue with the code because in that case, everything is still able to run just fine. The problem is if you leave the except: block empty or with a #do nothing, it gives you a syntax error. I can't use continue because its not in a loop. Is there a keyword i can use that tells the code to just keep going?","```
except Exception:
    pass

```

[Python docs for the pass statement](https://docs.python.org/3.7/tutorial/controlflow.html#pass-statements)"
How can I list all files of a directory in Python and add them to a `list`?,"**[`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir ""os.listdir"")** returns everything inside a directory -- including both **files** and **directories**.

[**`os.path`**](https://docs.python.org/3/library/os.path.html#module-os.path ""os.path"")'s `isfile()` can be used to only list files:

```
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]

```

Alternatively, **[`os.walk()`](https://docs.python.org/3/library/os.html#os.walk ""os.walk"")** **yields two lists** for each directory it visits -- one for **files** and one for **dirs**. If you only want the top directory you can break the first time it yields:

```
from os import walk

f = []
for (dirpath, dirnames, filenames) in walk(mypath):
    f.extend(filenames)
    break

```

or, shorter:

```
from os import walk

filenames = next(walk(mypath), (None, None, []))[2]  # [] if no file

```"
"I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like `[1,5,6,10,11]` and I would like to reset it to `[0,1,2,3,4]`. How can I do it?

---

The following seems to work:

```
df = df.reset_index()
del df['index']

```

The following does not work:

```
df = df.reindex()

```","[`DataFrame.reset_index`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) is what you're looking for. If you don't want it saved as a column, then do:

```
df = df.reset_index(drop=True)

```

If you don't want to reassign:

```
df.reset_index(drop=True, inplace=True)

```"
"I recently came across this syntax, I am unaware of the difference.

I would appreciate it if someone could tell me the difference.","The answer is explained [here](http://jaredgrubb.blogspot.com/2009/04/python-is-none-vs-none.html).

To quote:

> A class is free to implement
> comparison any way it chooses, and it
> can choose to make comparison against
> None mean something (which actually
> makes sense; if someone told you to
> implement the None object from
> scratch, how else would you get it to
> compare True against itself?).

Practically-speaking, there is not much difference since custom comparison operators are rare. But you should use `is None` as a general rule."
"How do I use a progress bar when my script is doing some task that is likely to take time?

For example, a function which takes some time to complete and returns `True` when done. How can I display a progress bar during the time the function is being executed?

Note that I need this to be in real time, so I can't figure out what to do about it. Do I need a `thread` for this? I have no idea.

Right now I am not printing anything while the function is being executed, however a progress bar would be nice. Also I am more interested in how this can be done from a code point of view.","With [tqdm](https://github.com/tqdm/tqdm) (`conda install tqdm` or `pip install tqdm`) you can add a progress meter to your loops in a second:

```
from time import sleep
from tqdm import tqdm
for i in tqdm(range(10)):
    sleep(3)

 60%|██████    | 6/10 [00:18<00:12,  0.33 it/s]

```

Also, there is a [notebook version](https://github.com/tqdm/tqdm/#ipython-jupyter-integration):

```
from tqdm.notebook import tqdm
for i in tqdm(range(100)):
    sleep(3)

```

You can use `tqdm.auto` instead of `tqdm.notebook` to work in both a terminal and notebooks.

[`tqdm.contrib`](https://tqdm.github.io/docs/contrib) contains some helper functions to do things like `enumerate`, `map`, and `zip`. There are concurrent maps in [`tqdm.contrib.concurrent`](https://tqdm.github.io/docs/contrib.concurrent).

You can even get progress sent to your phone after disconnecting from a jupyter notebook using [`tqdm.contrib.telegram`](https://tqdm.github.io/docs/contrib.telegram) or [`tqdm.contrib.discord`](https://tqdm.github.io/docs/contrib.discord).

![GIF showing an example of the output of using tqdm.contrib.telegram to display progress bar in Telegram mobile app](https://raw.githubusercontent.com/tqdm/img/src/screenshot-telegram.gif)"
"I'm adding UTC time strings to Bitbucket API responses that currently only contain Amsterdam (!) time strings. For consistency with the UTC time strings returned elsewhere, the desired format is `2011-11-03 11:07:04` (followed by `+00:00`, but that's not germane).

What's the best way to create such a string (*without* a microsecond component) from a `datetime` instance *with* a microsecond component?

```
>>> import datetime
>>> print unicode(datetime.datetime.now())
2011-11-03 11:13:39.278026

```

I'll add the best option that's occurred to me as a possible answer, but there may well be a more elegant solution.

**Edit:** I should mention that I'm not *actually* printing the current time â€“ I used `datetime.now` to provide a quick example. So the solution should not assume that any `datetime` instances it receives will include microsecond components.","If you want to format a `datetime` object in a specific format that is different from the standard format, it's best to explicitly specify that format:

```
>>> import datetime
>>> datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
'2011-11-03 18:21:26'

```

See [the documentation of `datetime.strftime()`](https://docs.python.org/3/library/datetime.html?highlight=strftime#strftime-and-strptime-behavior) for an explanation of the `%` directives.

Starting from Python 3.6, the `isoformat()` method is flexible enough to also produce this format:

```
datetime.datetime.now().isoformat(sep="" "", timespec=""seconds"")

```"
"I need a function which takes in a `list` and outputs `True` if all elements in the input list evaluate as equal to each other using the standard equality operator and `False` otherwise.

I feel it would be best to iterate through the list comparing adjacent elements and then `AND` all the resulting Boolean values. But I'm not sure what's the most Pythonic way to do that.","Use [`itertools.groupby`](https://docs.python.org/3/library/itertools.html#itertools.groupby) (see [the `itertools` recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes)):

```
from itertools import groupby

def all_equal(iterable):
    g = groupby(iterable)
    return next(g, True) and not next(g, False)

```

or without `groupby`:

```
def all_equal(iterator):
    iterator = iter(iterator)
    try:
        first = next(iterator)
    except StopIteration:
        return True
    return all(first == x for x in iterator)

```

---

There are a number of alternative one-liners you might consider:

1. Converting the input to a set and checking that it only has one or zero (in case the input is empty) items

   ```
   def all_equal2(iterator):
       return len(set(iterator)) <= 1

   ```
2. Comparing against the input list without the first item

   ```
   def all_equal3(lst):
       return lst[:-1] == lst[1:]

   ```
3. [Counting how many times the first item appears in the list](http://stackoverflow.com/q/3844948/)

   ```
   def all_equal_ivo(lst):
       return not lst or lst.count(lst[0]) == len(lst)

   ```
4. [Comparing against a list of the first element repeated](https://stackoverflow.com/q/3844931/)

   ```
   def all_equal_6502(lst):
       return not lst or [lst[0]]*len(lst) == lst

   ```

But they have some downsides, namely:

1. `all_equal` and `all_equal2` can use any iterators, but the others must take a sequence input, typically concrete containers like a list or tuple.
2. `all_equal` and `all_equal3` stop as soon as a difference is found (what is called ""[short circuit](https://en.wikipedia.org/wiki/Short-circuit_evaluation)""), whereas all the alternatives require iterating over the entire list, even if you can tell that the answer is `False` just by looking at the first two elements.
3. In `all_equal2` the content must be [hashable](https://docs.python.org/3/glossary.html#term-hashable). A list of lists will raise a `TypeError` for example.
4. `all_equal2` (in the worst case) and `all_equal_6502` create a copy of the list, meaning you need to use double the memory.

On Python 3.9, using [`perfplot`](https://github.com/nschloe/perfplot), we get these timings (lower `Runtime [s]` is better):

[![for a list with a difference in the first two elements, groupby is fastest](https://i.sstatic.net/P44QA.png)](https://i.sstatic.net/P44QA.png)[![for a list with no differences, count(l[0]) is fastest](https://i.sstatic.net/jLwdT.png)](https://i.sstatic.net/jLwdT.png)"
I have a pandas dataframe with multiple columns. I want to change the values of the only the first column without affecting the other columns. How can I do that using [`apply()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) in pandas?,"Given a sample dataframe `df` as:

```
   a  b
0  1  2
1  2  3
2  3  4
3  4  5

```

what you want is:

```
df['a'] = df['a'].apply(lambda x: x + 1)

```

that returns:

```
   a  b
0  2  2
1  3  3
2  4  4
3  5  5

```"
"Here's my code:

```
import urllib2.request

response = urllib2.urlopen(""http://www.google.com"")
html = response.read()
print(html)

```

Any help?","As stated in the [`urllib2` documentation](https://docs.python.org/2/library/urllib2.html):

> The `urllib2` module has been split across several modules in Python 3 named `urllib.request` and `urllib.error`. The `2to3` tool will automatically adapt imports when converting your sources to Python 3.

So you should instead be saying

```
from urllib.request import urlopen
html = urlopen(""http://www.google.com/"").read()
print(html)

```

Your current, now-edited code sample is incorrect because you are saying `urllib.urlopen(""http://www.google.com/"")` instead of just `urlopen(""http://www.google.com/"")`."
"I am not sure why we need `finally` in `try...except...finally` statements. In my opinion, this code block

```
try:
    run_code1()
except TypeError:
    run_code2()
other_code()

```

is the same with this one using `finally`:

```
try:
    run_code1()
except TypeError:
    run_code2()
finally:
    other_code()

```

Am I missing something?","It makes a difference if you return early:

```
try:
    run_code1()
except TypeError:
    run_code2()
    return None   # The finally block is run before the method returns
finally:
    other_code()

```

Compare to this:

```
try:
    run_code1()
except TypeError:
    run_code2()
    return None   

other_code()  # This doesn't get run if there's an exception.

```

Other situations that can cause differences:

* If an exception is thrown inside the except block.
* If an exception is thrown in `run_code1()` but it's not a `TypeError`.
* Other control flow statements such as `continue` and `break` statements."
"I have a dataframe that look like this:

```
          a         b         c         d
0  0.418762  0.042369  0.869203  0.972314
1  0.991058  0.510228  0.594784  0.534366
2  0.407472  0.259811  0.396664  0.894202
3  0.726168  0.139531  0.324932  0.906575

```

How I can get all columns except `b`?","When the columns are not a MultiIndex, `df.columns` is just an array of column names so you can do:

```
df.loc[:, df.columns != 'b']

          a         c         d
0  0.561196  0.013768  0.772827
1  0.882641  0.615396  0.075381
2  0.368824  0.651378  0.397203
3  0.788730  0.568099  0.869127

```"
"What are ""iterable"", ""iterator"", and ""iteration"" in Python? How are they defined?

---

See also: [How to build a basic iterator?](https://stackoverflow.com/questions/19151)","**Iteration** is a general term for taking each item of something, one after another. Any time you use a loop, explicit or implicit, to go over a group of items, that is iteration.

In Python, **iterable** and **iterator** have specific meanings.

An **iterable** is an object that has an `__iter__` method which returns an **iterator**, or which defines a `__getitem__` method that can take sequential indexes starting from zero (and raises an `IndexError` when the indexes are no longer valid). So an **iterable** is an object that you can get an **iterator** from.

An **iterator** is an object with a `next` (Python 2) or `__next__` (Python 3) method.

Whenever you use a `for` loop, or `map`, or a list comprehension, etc. in Python, the `next` method is called automatically to get each item from the **iterator**, thus going through the process of **iteration**.

A good place to start learning would be the [iterators section of the tutorial](https://docs.python.org/3/tutorial/classes.html#iterators) and the [iterator types section of the standard types page](https://docs.python.org/dev/library/stdtypes.html#iterator-types). After you understand the basics, try the [iterators section of the Functional Programming HOWTO](https://docs.python.org/dev/howto/functional.html#iterators)."
How can I get the value of an environment variable in Python?,"Environment variables are accessed through [`os.environ`](https://docs.python.org/library/os.html#os.environ):

```
import os
print(os.environ['HOME'])

```

To see a list of all environment variables:

```
print(os.environ)

```

---

If a key is not present, attempting to access it will raise a `KeyError`. To avoid this:

```
# Returns `None` if the key doesn't exist
print(os.environ.get('KEY_THAT_MIGHT_EXIST'))

# Returns `default_value` if the key doesn't exist
print(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))

# Returns `default_value` if the key doesn't exist
print(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))

```"
"Currently I am working on a python project that contains sub modules and uses numpy/scipy. Ipython is used as interactive console. Unfortunately I am not very happy with workflow that I am using right now, I would appreciate some advice.

In IPython, the framework is loaded by a simple `import` command. However, it is often necessary to change code in one of the submodules of the framework. At this point a model is already loaded and I use IPython to interact with it.

Now, the framework contains many modules that depend on each other, i.e. when the framework is initially loaded the main module is importing and configuring the submodules. The changes to the code are only executed if the module is reloaded using `reload(main_mod.sub_mod)`. This is cumbersome as I need to reload all changed modules individually using the full path. It would be very convenient if `reload(main_module)` would also reload all sub modules, but without reloading numpy/scipy..","IPython comes with some [automatic reloading](http://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) magic:

```
%load_ext autoreload
%autoreload 2

```

It will reload all changed modules every time before executing a new line. The way this works is slightly different than `dreload`. Some caveats apply, type `%autoreload?` to see what can go wrong.

---

If you want to always enable this settings, modify your IPython configuration file `~/.ipython/profile_default/ipython_config.py`[1] and appending:

```
c.InteractiveShellApp.extensions = ['autoreload']     
c.InteractiveShellApp.exec_lines = ['%autoreload 2']

```

*Credit to @Kos via a comment below.*

[1]
If you don't have the file `~/.ipython/profile_default/ipython_config.py`, you need to call `ipython profile create` first. Or the file may be located at `$IPYTHONDIR`."
"I wrote a simple console app to upload and download files from an FTP server using the ftplib.

I would like the app to show some visualization of its download/upload progress for the user; each time a data chunk is downloaded, I would like it to provide a progress update, even if it's just a numeric representation like a percentage.

Importantly, I want to avoid erasing all the text that's been printed to the console in previous lines (i.e. I don't want to ""clear"" the entire terminal while printing the updated progress).

This seems a fairly common task â€“ how can I go about making a progress bar or similar visualization that outputs to my console while preserving prior program output?","Python 3
========

A Simple, Customizable Progress Bar
-----------------------------------

Here's an aggregate of many of the answers below that I use regularly (no imports required).

**Note:** All code in this answer was created for Python 3; see end of answer to use this code with Python 2.

```
# Print iterations progress
def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = ""\r""):
    """"""
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        length      - Optional  : character length of bar (Int)
        fill        - Optional  : bar fill character (Str)
        printEnd    - Optional  : end character (e.g. ""\r"", ""\r\n"") (Str)
    """"""
    percent = (""{0:."" + str(decimals) + ""f}"").format(100 * (iteration / float(total)))
    filledLength = int(length * iteration // total)
    bar = fill * filledLength + '-' * (length - filledLength)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)
    # Print New Line on Complete
    if iteration == total: 
        print()

```

### Sample Usage

```
import time

# A List of Items
items = list(range(0, 57))
l = len(items)

# Initial call to print 0% progress
printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)
for i, item in enumerate(items):
    # Do stuff...
    time.sleep(0.1)
    # Update Progress Bar
    printProgressBar(i + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 50)

```

### Sample Output

```
Progress: |█████████████████████████████████████████████-----| 90.0% Complete

```

### Update

There was discussion in the comments regarding an option that allows the progress bar to adjust dynamically to the terminal window width. While I don't recommend this, here's a [gist](https://gist.github.com/greenstick/b23e475d2bfdc3a82e34eaa1f6781ee4) that implements this feature (and notes the caveats).

Single-Call Version of The Above
--------------------------------

A comment below referenced a nice [answer](https://stackoverflow.com/a/34482761/2206251) posted in response to a similar question. I liked the ease of use it demonstrated and wrote a similar one, but opted to leave out the import of the `sys` module while adding in some of the features of the original `printProgressBar` function above.

Some benefits of this approach over the original function above include the elimination of an initial call to the function to print the progress bar at 0% and the use of `enumerate` becoming optional (i.e. it is no longer explicitly required to make the function work).

```
def progressBar(iterable, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = ""\r""):
    """"""
    Call in a loop to create terminal progress bar
    @params:
        iterable    - Required  : iterable object (Iterable)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        length      - Optional  : character length of bar (Int)
        fill        - Optional  : bar fill character (Str)
        printEnd    - Optional  : end character (e.g. ""\r"", ""\r\n"") (Str)
    """"""
    total = len(iterable)
    # Progress Bar Printing Function
    def printProgressBar (iteration):
        percent = (""{0:."" + str(decimals) + ""f}"").format(100 * (iteration / float(total)))
        filledLength = int(length * iteration // total)
        bar = fill * filledLength + '-' * (length - filledLength)
        print(f'\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)
    # Initial Call
    printProgressBar(0)
    # Update Progress Bar
    for i, item in enumerate(iterable):
        yield item
        printProgressBar(i + 1)
    # Print New Line on Complete
    print()

```

### Sample Usage

```
import time

# A List of Items
items = list(range(0, 57))

# A Nicer, Single-Call Usage
for item in progressBar(items, prefix = 'Progress:', suffix = 'Complete', length = 50):
    # Do stuff...
    time.sleep(0.1)

```

### Sample Output

```
Progress: |█████████████████████████████████████████████-----| 90.0% Complete

```

Python 2
========

To use the above functions in Python 2, set the encoding to UTF-8 at the top of your script:

```
# -*- coding: utf-8 -*-

```

And replace the Python 3 string formatting in this line:

```
print(f'\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)

```

With Python 2 string formatting:

```
print('\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)

```"
"There is a string, for example. `EXAMPLE`.

How can I remove the middle character, i.e., `M` from it? I don't need the code. I want to know:

* Do strings in Python end in any special character?
* Which is a better way - shifting everything right to left starting from the middle character OR creation of a new string and not copying the middle character?","In Python, strings are immutable, so you have to create a new string. You have a few options of how to create the new string. If you want to remove the 'M' wherever it appears:

```
newstr = oldstr.replace(""M"", """")

```

If you want to remove the central character:

```
midlen = len(oldstr) // 2
newstr = oldstr[:midlen] + oldstr[midlen+1:]

```

You asked if strings end with a special character. No, you are thinking like a C programmer. In Python, strings are [stored with their length](https://en.wikipedia.org/wiki/String_(computer_science)#Length-prefixed), so any byte value, including `\0`, can appear in a string."
"I have two existing dictionaries, and I wish to 'append' one of them to the other. By that I mean that the key,values of the other dictionary should be made into the first dictionary. For example:

```
orig = {
   'A': 1,
   'B': 2,
   'C': 3,
}

extra = {
   'D': 4,
   'E': 5,
}

dest = # Something here involving orig and extra

print dest
{
   'A': 1,
   'B': 2,
   'C': 3,
   'D': 4,
   'E': 5
}

```

I think this all can be achieved through a `for` loop (maybe?), but is there some method of dictionaries or any other module that saves this job for me? The actual dictionaries I'm using are really big...","You can do

```
orig.update(extra)

```

or, if you don't want `orig` to be modified, make a copy first:

```
dest = dict(orig)  # or orig.copy()
dest.update(extra)

```

Note that if extra and orig have overlapping keys, the final value will be taken from extra. For example,

```
>>> d1 = {1: 1, 2: 2}
>>> d2 = {2: 'ha!', 3: 3}
>>> d1.update(d2)
>>> d1
{1: 1, 2: 'ha!', 3: 3}

```"
"I am trying to install a Python package from a private GitHub repository. For a public repository, I can issue the following command which works fine:

```
pip install git+git://github.com/django/django.git

```

However, if I try this for a private repository:

```
pip install git+git://github.com/echweb/echweb-utils.git

```

I get the following output:

```
Downloading/unpacking git+git://github.com/echweb/echweb-utils.git
Cloning Git repository git://github.com/echweb/echweb-utils.git to /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build
Complete output from command /usr/local/bin/git clone git://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build:
fatal: The remote end hung up unexpectedly

Cloning into /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build...

----------------------------------------
Command /usr/local/bin/git clone git://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build failed with error code 128

```

I guess this is because I am trying to access a private repository without providing any authentication. I therefore tried to use Git + `ssh` hoping that pip would use my SSH public key to authenticate:

```
pip install git+ssh://github.com/echweb/echweb-utils.git

```

This gives the following output:

```
Downloading/unpacking git+ssh://github.com/echweb/echweb-utils.git
Cloning Git repository ssh://github.com/echweb/echweb-utils.git to /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build
Complete output from command /usr/local/bin/git clone ssh://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build:
Cloning into /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build...

Permission denied (publickey).

fatal: The remote end hung up unexpectedly

----------------------------------------
Command /usr/local/bin/git clone ssh://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build failed with error code 128

```

Is what I am trying to achieve even possible? If so, how can I do it?","You can use the `git+ssh` URI scheme, but you *must* set a username. Notice the **`git@`** part in the URI:

```
pip install git+ssh://git@github.com/echweb/echweb-utils.git

```

Also read about [deploy keys](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys).

PS: In my installation, the ""git+ssh"" URI scheme works only with ""editable"" requirements:

```
pip install -e URI#egg=EggName

```

**Remember**: Change the `:` character that `git remote -v` prints to a `/` character before using the remote's address in the `pip` command:

```
$ git remote -v
origin  git@github.com:echweb/echweb-utils.git (fetch)
#                     ^ change this to a '/' character

```

If you forget, you will get this error:

```
ssh: Could not resolve hostname github.com:echweb:
         nodename nor servname provided, or not known

```"
"```
r = {'is_claimed': 'True', 'rating': 3.5}
r = json.dumps(r)
file.write(str(r['rating']))

```

I am not able to access my data in the JSON. What am I doing wrong?

```
TypeError: string indices must be integers, not str

```","`json.dumps()` converts a dictionary to `str` object, not a `json(dict)` object! So you have to load your `str` into a `dict` to use it by using [**`json.loads()`**](https://docs.python.org/2/library/json.html#json.loads) method

See `json.dumps()` as a save method and `json.loads()` as a retrieve method.

This is the code sample which might help you understand it more:

```
import json

r = {'is_claimed': 'True', 'rating': 3.5}
r = json.dumps(r)
loaded_r = json.loads(r)
loaded_r['rating'] #Output 3.5
type(r) #Output str
type(loaded_r) #Output dict

```"
"Could someone tell me how to get the parent directory of a path in Python in a cross platform way. E.g.

```
C:\Program Files ---> C:\

```

and

```
C:\ ---> C:\

```

If the directory doesn't have a parent directory, it returns the directory itself. The question might seem simple but I couldn't dig it up through Google.","**Python 3.4**
==============

Use the [`pathlib`](https://docs.python.org/3/library/pathlib.html) module.

```
from pathlib import Path
path = Path(""/here/your/path/file.txt"")
print(path.parent.absolute())

```

**Old answer**
==============

Try this:

```
import os
print os.path.abspath(os.path.join(yourpath, os.pardir))

```

where `yourpath` is the path you want the parent for."
"Some time ago, I saw a Mono application with colored output, presumably because of its log system (because all the messages were standardized).

Now, Python has the `logging` module, which lets you specify a lot of options to customize output. So, I'm imagining something similar would be possible with Python, but I canâ€™t find out how to do this anywhere.

Is there any way to make the Python `logging` module output in color?

What I want (for instance) errors in red, debug messages in blue or yellow, and so on.

Of course this would probably require a compatible terminal (most modern terminals are); but I could fallback to the original `logging` output if color isn't supported.

Any ideas how I can get colored output with the logging module?","### A Python 3 solution, with no additional packages required

***Note to the community:** please do not edit the answer. I know its not the most optimal way in term of coding, but the easiest to understand and most readable way to get the essence of the process*

### 1. Define a class

```
import logging

class CustomFormatter(logging.Formatter):

    grey = ""\x1b[38;20m""
    yellow = ""\x1b[33;20m""
    red = ""\x1b[31;20m""
    bold_red = ""\x1b[31;1m""
    reset = ""\x1b[0m""
    format = ""%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)""

    FORMATS = {
        logging.DEBUG: grey + format + reset,
        logging.INFO: grey + format + reset,
        logging.WARNING: yellow + format + reset,
        logging.ERROR: red + format + reset,
        logging.CRITICAL: bold_red + format + reset
    }

    def format(self, record):
        log_fmt = self.FORMATS.get(record.levelno)
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)

```

### 2. Instantiate logger:

```
# create logger with 'spam_application'
logger = logging.getLogger(""My_app"")
logger.setLevel(logging.DEBUG)

# create console handler with a higher log level
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)

ch.setFormatter(CustomFormatter())

logger.addHandler(ch)

```

### 3. And use:

```
logger.debug(""debug message"")
logger.info(""info message"")
logger.warning(""warning message"")
logger.error(""error message"")
logger.critical(""critical message"")

```

### Result:

[![enter image description here](https://i.sstatic.net/klCcc.png)](https://i.sstatic.net/klCcc.png)

### The full color scheme:

[![enter image description here](https://i.sstatic.net/iZU5S.png)](https://i.sstatic.net/iZU5S.png)

#### For Windows:

This solution works on Mac OS, IDE terminals. Looks like the Windows command prompt doesn't have colors at all by default. Here are instructions on how to enable them, which I haven't try <https://www.howtogeek.com/322432/how-to-customize-your-command-prompts-color-scheme-with-microsofts-colortool/>"
"What is the easiest way in Python to replace a character in a string?

For example:

```
text = ""abcdefg"";
text[1] = ""Z"";
           ^

```","Don't modify strings.

Work with them as lists; turn them into strings only when needed.

```
>>> s = list(""Hello zorld"")
>>> s
['H', 'e', 'l', 'l', 'o', ' ', 'z', 'o', 'r', 'l', 'd']
>>> s[6] = 'W'
>>> s
['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd']
>>> """".join(s)
'Hello World'

```

Python strings are immutable (i.e. they can't be modified). There are [a lot](https://web.archive.org/web/20201031092707/http://effbot.org/pyfaq/why-are-python-strings-immutable.htm) of reasons for this. Use lists until you have no choice, only then turn them into strings."
"From time to time in Python, I see the block:

```
try:
   try_this(whatever)
except SomeException as exception:
   #Handle exception
else:
   return something

```

**What is the reason for the try-except-else to exist?**

I do not like that kind of programming, as it is using exceptions to perform flow control. However, if it is included in the language, there must be a good reason for it, isn't it?

**It is my understanding that exceptions are not errors**, and that they should only be used for exceptional conditions (e.g. I try to write a file into disk and there is no more space, or maybe I do not have permission), and not for flow control.

Normally I handle exceptions as:

```
something = some_default_value
try:
    something = try_this(whatever)
except SomeException as exception:
    #Handle exception
finally:
    return something

```

Or if I really do not want to return anything if an exception happens, then:

```
try:
    something = try_this(whatever)
    return something
except SomeException as exception:
    #Handle exception

```","> ""I do not know if it is out of ignorance, but I do not like that
> kind of programming, as it is using exceptions to perform flow control.""

In the Python world, using exceptions for flow control is common and normal.

Even the Python core developers use exceptions for flow-control and that style is heavily baked into the language (i.e. the iterator protocol uses [*StopIteration*](http://docs.python.org/2.7/library/exceptions.html#exceptions.StopIteration) to signal loop termination).

In addition, the try-except-style is used to prevent the race-conditions inherent in some of the [""look-before-you-leap""](http://docs.python.org/2.7/glossary.html#term-lbyl) constructs. For example, testing [*os.path.exists*](http://docs.python.org/2.7/library/os.path.html#os.path.exists) results in information that may be out-of-date by the time you use it. Likewise, [*Queue.full*](http://docs.python.org/2.7/library/queue.html#Queue.Queue.full) returns information that may be stale. The [try-except-else style](http://docs.python.org/2.7/glossary.html#term-eafp) will produce more reliable code in these cases.

> ""It my understanding that exceptions are not errors, they should only
> be used for exceptional conditions""

In some other languages, that rule reflects their cultural norms as reflected in their libraries. The ""rule"" is also based in-part on performance considerations for those languages.

The Python cultural norm is somewhat different. In many cases, you *must* use exceptions for control-flow. Also, the use of exceptions in Python does not slow the surrounding code and calling code as it does in some compiled languages (i.e. [CPython](http://en.wikipedia.org/wiki/CPython) already implements code for exception checking at every step, regardless of whether you actually use exceptions or not).

In other words, your understanding that ""exceptions are for the exceptional"" is a rule that makes sense in some other languages, but not for Python.

> ""However, if it is included in the language itself, there must be a
> good reason for it, isn't it?""

Besides helping to avoid race-conditions, exceptions are also very useful for pulling error-handling outside loops. This is a necessary optimization in interpreted languages which do not tend to have automatic [loop invariant code motion](http://en.wikipedia.org/wiki/Loop-invariant_code_motion).

Also, exceptions can simplify code quite a bit in common situations where the ability to handle an issue is far removed from where the issue arose. For example, it is common to have top level user-interface code calling code for business logic which in turn calls low-level routines. Situations arising in the low-level routines (such as duplicate records for unique keys in database accesses) can only be handled in top-level code (such as asking the user for a new key that doesn't conflict with existing keys). The use of exceptions for this kind of control-flow allows the mid-level routines to completely ignore the issue and be nicely decoupled from that aspect of flow-control.

There is a [nice blog post on the indispensibility of exceptions here](http://uberpython.wordpress.com/2012/09/23/why-im-not-leaving-python-for-go/).

Also, see this StackÂ Overflow answer: [Are exceptions really for exceptional errors?](https://stackoverflow.com/questions/180937/are-exceptions-really-for-exceptional-errors)

> ""What is the reason for the try-except-else to exist?""

The else-clause itself is interesting. It runs when there is no exception but before the finally-clause. That is its primary purpose.

Without the else-clause, the only option to run additional code before finalization would be the clumsy practice of adding the code to the try-clause. That is clumsy because it risks
raising exceptions in code that wasn't intended to be protected by the try-block.

The use-case of running additional unprotected code prior to finalization doesn't arise very often. So, don't expect to see many examples in published code. It is somewhat rare.

Another use-case for the else-clause is to perform actions that must occur when no exception occurs and that do not occur when exceptions are handled. For example:

```
recip = float('Inf')
try:
    recip = 1 / f(x)
except ZeroDivisionError:
    logging.info('Infinite result')
else:
    logging.info('Finite result')

```

Another example occurs in unittest runners:

```
try:
    tests_run += 1
    run_testcase(case)
except Exception:
    tests_failed += 1
    logging.exception('Failing test case: %r', case)
    print('F', end='')
else:
    logging.info('Successful test case: %r', case)
    print('.', end='')

```

Lastly, the most common use of an else-clause in a try-block is for a bit of beautification (aligning the exceptional outcomes and non-exceptional outcomes at the same level of indentation). This use is always optional and isn't strictly necessary."
What are [metaclasses](https://docs.python.org/3/reference/datamodel.html#metaclasses)? What are they used for?,"Classes as objects
==================

Prior to delving into metaclasses, a solid grasp of Python classes is beneficial. Python holds a particularly distinctive concept of classes, a notion it adopts from the Smalltalk language.

In most languages, classes are just pieces of code that describe how to produce an object. That is somewhat true in Python too:

```
>>> class ObjectCreator(object):
...     pass

>>> my_object = ObjectCreator()
>>> print(my_object)
    <__main__.ObjectCreator object at 0x8974f2c>

```

But classes are more than that in Python. **Classes are objects too.**

Yes, objects.

When a Python script runs, every line of code is executed from top to bottom. When the Python interpreter encounters the `class` keyword, Python creates an **object** out of the ""description"" of the class that follows. Thus, the following instruction

```
>>> class ObjectCreator(object):
...     pass

```

...creates an *object* with the name `ObjectCreator`!

This object (the class) is itself capable of creating objects (called *instances*).

But still, it's an object. Therefore, like all objects:

* you can assign it to a variable1

  ```
  JustAnotherVariable = ObjectCreator

  ```
* you can attach attributes to it

  ```
  ObjectCreator.class_attribute = 'foo'

  ```
* you can pass it as a function parameter

  ```
  print(ObjectCreator)

  ```

1 Note that merely assigning it to another variable doesn't change the class's `__name__`, i.e.,

```
>>> print(JustAnotherVariable)
    <class '__main__.ObjectCreator'>

>>> print(JustAnotherVariable())
    <__main__.ObjectCreator object at 0x8997b4c>

```

Creating classes dynamically
============================

Since classes are objects, you can create them on the fly, like any object.

First, you can create a class in a function using `class`:

```
>>> def choose_class(name):
...     if name == 'foo':
...         class Foo(object):
...             pass
...         return Foo # return the class, not an instance
...     else:
...         class Bar(object):
...             pass
...         return Bar

>>> MyClass = choose_class('foo')

>>> print(MyClass) # the function returns a class, not an instance
    <class '__main__.Foo'>

>>> print(MyClass()) # you can create an object from this class
    <__main__.Foo object at 0x89c6d4c>

```

But it's not so dynamic, since you still have to write the whole class yourself.

Since classes are objects, they must be generated by something.

When you use the `class` keyword, Python creates this object automatically. But as
with most things in Python, it gives you a way to do it manually.

Remember the function `type`? The good old function that lets you know what
type an object is:

```
>>> print(type(1))
    <class 'int'>

>>> print(type(""1""))
    <class 'str'>

>>> print(type(ObjectCreator))
    <class 'type'>

>>> print(type(ObjectCreator()))
    <class '__main__.ObjectCreator'>

```

Well, [`type`](http://docs.python.org/2/library/functions.html#type) also has a completely different ability: it can create classes on the fly. `type` can take the description of a class as parameters,
and return a class.

(I know, it's silly that the same function can have two completely different uses according to the parameters you pass to it. It's an issue due to backward
compatibility in Python)

`type` works this way:

```
type(name, bases, attrs)

```

Where:

* **`name`**: name of the class
* **`bases`**: tuple of the parent class (for inheritance, can be empty)
* **`attrs`**: dictionary containing attributes names and values

e.g.:

```
>>> class MyShinyClass(object):
...     pass

```

can be created manually this way:

```
>>> MyShinyClass = type('MyShinyClass', (), {}) # returns a class object
>>> print(MyShinyClass)
    <class '__main__.MyShinyClass'>

>>> print(MyShinyClass()) # create an instance with the class
    <__main__.MyShinyClass object at 0x8997cec>

```

You'll notice that we use `MyShinyClass` as the name of the class
and as the variable to hold the class reference. They can be different,
but there is no reason to complicate things.

`type` accepts a dictionary to define the attributes of the class. So:

```
>>> class Foo(object):
...     bar = True

```

Can be translated to:

```
>>> Foo = type('Foo', (), {'bar':True})

```

And used as a normal class:

```
>>> print(Foo)
    <class '__main__.Foo'>

>>> print(Foo.bar)
    True

>>> f = Foo()
>>> print(f)
    <__main__.Foo object at 0x8a9b84c>

>>> print(f.bar)
    True

```

And of course, you can inherit from it, so:

```
>>> class FooChild(Foo):
...     pass

```

would be:

```
>>> FooChild = type('FooChild', (Foo,), {})
>>> print(FooChild)
    <class '__main__.FooChild'>

>>> print(FooChild.bar) # bar is inherited from Foo
    True

```

Eventually, you'll want to add methods to your class. Just define a function
with the proper signature and assign it as an attribute.

```
>>> def echo_bar(self):
...     print(self.bar)

>>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar})

>>> hasattr(Foo, 'echo_bar')
    False

>>> hasattr(FooChild, 'echo_bar')
    True

>>> my_foo = FooChild()
>>> my_foo.echo_bar()
    True

```

And you can add even more methods after you dynamically create the class, just like adding methods to a normally created class object.

```
>>> def echo_bar_more(self):
...     print('yet another method')

>>> FooChild.echo_bar_more = echo_bar_more
>>> hasattr(FooChild, 'echo_bar_more')
    True

```

You see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically.

This is what Python does when you use the keyword `class`, and it does so by using a metaclass.

What are metaclasses (finally)
==============================

Metaclasses are the 'stuff' that creates classes.

You define classes in order to create objects, right?

But we learned that Python classes are objects.

Well, metaclasses are what create these objects. They are the classes' classes,
you can picture them this way:

```
MyClass = MetaClass()
my_object = MyClass()

```

You've seen that `type` lets you do something like this:

```
MyClass = type('MyClass', (), {})

```

It's because the function `type` is in fact a metaclass. `type` is the
metaclass Python uses to create all classes behind the scenes.

Now you wonder ""why the heck is it written in lowercase, and not `Type`?""

Well, I guess it's a matter of consistency with `str`, the class that creates
strings objects, and `int` the class that creates integer objects. `type` is
just the class that creates class objects.

You see that by checking the `__class__` attribute.

Everything, and I mean everything, is an object in Python. That includes integers,
strings, functions and classes. All of them are objects. And all of them have
been created from a class:

```
>>> age = 35
>>> age.__class__
    <type 'int'>

>>> name = 'bob'
>>> name.__class__
    <type 'str'>

>>> def foo(): pass
>>> foo.__class__
    <type 'function'>

>>> class Bar(object): pass
>>> b = Bar()
>>> b.__class__
    <class '__main__.Bar'>

```

Now, what is the `__class__` of any `__class__` ?

```
>>> age.__class__.__class__
    <type 'type'>

>>> name.__class__.__class__
    <type 'type'>

>>> foo.__class__.__class__
    <type 'type'>

>>> b.__class__.__class__
    <type 'type'>

```

So, a metaclass is just the stuff that creates class objects.

You can call it a 'class factory' if you wish.

`type` is the built-in metaclass Python uses, but of course, you can create your
own metaclass.

The [`__metaclass__`](http://docs.python.org/2/reference/datamodel.html?highlight=__metaclass__#__metaclass__) attribute
========================================================================================================================

In Python 2, you can add a `__metaclass__` attribute when you write a class (see next section for the Python 3 syntax):

```
class Foo(object):
    __metaclass__ = something...
    [...]

```

If you do so, Python will use the metaclass to create the class `Foo`.

Careful, it's tricky.

You write `class Foo(object)` first, but the class object `Foo` is not created
in memory yet.

Python will look for `__metaclass__` in the class definition. If it finds it,
it will use it to create the class object `Foo`. If it doesn't, it will use
`type` to create the class.

Read that several times.

When you do:

```
class Foo(Bar):
    pass

```

Python does the following:

Is there a `__metaclass__` attribute in `Foo`?

If yes, create in-memory a class object (I said a class object, stay with me here), with the name `Foo` by using what is in `__metaclass__`.

If Python can't find `__metaclass__`, it will look for a `__metaclass__` at the MODULE level, and try to do the same (but only for classes that don't inherit anything, basically old-style classes).

Then if it can't find any `__metaclass__` at all, it will use the `Bar`'s (the first parent) own metaclass (which might be the default `type`) to create the class object.

Be careful here that the `__metaclass__` attribute will not be inherited, the metaclass of the parent (`Bar.__class__`) will be. If `Bar` used a `__metaclass__` attribute that created `Bar` with `type()` (and not `type.__new__()`), the subclasses will not inherit that behavior.

Now the big question is, what can you put in `__metaclass__`?

The answer is something that can create a class.

And what can create a class? `type`, or anything that subclasses or uses it.

Metaclasses in Python 3
=======================

The syntax to set the metaclass has been changed in Python 3:

```
class Foo(object, metaclass=something):
    ...

```

i.e. the `__metaclass__` attribute is no longer used, in favor of a keyword argument in the list of base classes.

The behavior of metaclasses however stays [largely the same](https://www.python.org/dev/peps/pep-3115/).

One thing added to metaclasses in Python 3 is that you can also pass attributes as keyword-arguments into a metaclass, like so:

```
class Foo(object, metaclass=something, kwarg1=value1, kwarg2=value2):
    ...

```

Read the section below for how Python handles this.

Custom metaclasses
==================

The main purpose of a metaclass is to change the class automatically,
when it's created.

You usually do this for APIs, where you want to create classes matching the
current context.

Imagine a stupid example, where you decide that all classes in your module
should have their attributes written in uppercase. There are several ways to
do this, but one way is to set `__metaclass__` at the module level.

This way, all classes of this module will be created using this metaclass,
and we just have to tell the metaclass to turn all attributes to uppercase.

Luckily, `__metaclass__` can actually be any callable, it doesn't need to be a
formal class (I know, something with 'class' in its name doesn't need to be
a class, go figure... but it's helpful).

So we will start with a simple example, by using a function.

```
# the metaclass will automatically get passed the same argument
# that you usually pass to `type`
def upper_attr(future_class_name, future_class_parents, future_class_attrs):
    """"""
      Return a class object, with the list of its attribute turned
      into uppercase.
    """"""
    # pick up any attribute that doesn't start with '__' and uppercase it
    uppercase_attrs = {
        attr if attr.startswith(""__"") else attr.upper(): v
        for attr, v in future_class_attrs.items()
    }

    # let `type` do the class creation
    return type(future_class_name, future_class_parents, uppercase_attrs)

__metaclass__ = upper_attr # this will affect all classes in the module

class Foo(): # global __metaclass__ won't work with ""object"" though
    # but we can define __metaclass__ here instead to affect only this class
    # and this will work with ""object"" children
    bar = 'bip'

```

Let's check:

```
>>> hasattr(Foo, 'bar')
    False

>>> hasattr(Foo, 'BAR')
    True

>>> Foo.BAR
    'bip'

```

Now, let's do exactly the same, but using a real class for a metaclass:

```
# remember that `type` is actually a class like `str` and `int`
# so you can inherit from it
class UpperAttrMetaclass(type):
    # __new__ is the method called before __init__
    # it's the method that creates the object and returns it
    # while __init__ just initializes the object passed as parameter
    # you rarely use __new__, except when you want to control how the object
    # is created.
    # here the created object is the class, and we want to customize it
    # so we override __new__
    # you can do some stuff in __init__ too if you wish
    # some advanced use involves overriding __call__ as well, but we won't
    # see this
    def __new__(
        upperattr_metaclass,
        future_class_name,
        future_class_parents,
        future_class_attrs
    ):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in future_class_attrs.items()
        }
        return type(future_class_name, future_class_parents, uppercase_attrs)

```

Let's rewrite the above, but with shorter and more realistic variable names now that we know what they mean:

```
class UpperAttrMetaclass(type):
    def __new__(cls, clsname, bases, attrs):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in attrs.items()
        }
        return type(clsname, bases, uppercase_attrs)

```

You may have noticed the extra argument `cls`. There is
nothing special about it: `__new__` always receives the class it's defined in, as the first parameter. Just like you have `self` for ordinary methods which receive the instance as the first parameter, or the defining class for class methods.

But this is not proper OOP. We are calling `type` directly and we aren't overriding or calling the parent's `__new__`. Let's do that instead:

```
class UpperAttrMetaclass(type):
    def __new__(cls, clsname, bases, attrs):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in attrs.items()
        }
        return type.__new__(cls, clsname, bases, uppercase_attrs)

```

We can make it even cleaner by using `super`, which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type):

```
class UpperAttrMetaclass(type):
    def __new__(cls, clsname, bases, attrs):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in attrs.items()
        }

        # Python 2 requires passing arguments to super:
        return super(UpperAttrMetaclass, cls).__new__(
            cls, clsname, bases, uppercase_attrs)

        # Python 3 can use no-arg super() which infers them:
        return super().__new__(cls, clsname, bases, uppercase_attrs)

```

Oh, and in Python 3 if you do this call with keyword arguments, like this:

```
class Foo(object, metaclass=MyMetaclass, kwarg1=value1):
    ...

```

It translates to this in the metaclass to use it:

```
class MyMetaclass(type):
    def __new__(cls, clsname, bases, dct, kwargs1=default):
        ...

```

That's it. There is really nothing more about metaclasses.

The reason behind the complexity of the code using metaclasses is not because
of metaclasses, it's because you usually use metaclasses to do twisted stuff
relying on introspection, manipulating inheritance, vars such as `__dict__`, etc.

Indeed, metaclasses are especially useful to do black magic, and therefore
complicated stuff. But by themselves, they are simple:

* intercept a class creation
* modify the class
* return the modified class

Why would you use metaclasses classes instead of functions?
===========================================================

Since `__metaclass__` can accept any callable, why would you use a class
since it's obviously more complicated?

There are several reasons to do so:

* The intention is clear. When you read `UpperAttrMetaclass(type)`, you know
  what's going to follow
* You can use OOP. Metaclass can inherit from metaclass, override parent methods. Metaclasses can even use metaclasses.
* Subclasses of a class will be instances of its metaclass if you specified a metaclass-class, but not with a metaclass-function.
* You can structure your code better. You never use metaclasses for something as trivial as the above example. It's usually for something complicated. Having the ability to make several methods and group them in one class is very useful to make the code easier to read.
* You can hook on `__new__`, `__init__` and `__call__`. Which will allow you to do different stuff, Even if usually you can do it all in `__new__`,
  some people are just more comfortable using `__init__`.
* These are called metaclasses, damn it! It must mean something!

Why would you use metaclasses?
==============================

Now the big question. Why would you use some obscure error-prone feature?

Well, usually you don't:

> Metaclasses are deeper magic that
> 99% of users should never worry about it.
> If you wonder whether you need them,
> you don't (the people who actually
> need them know with certainty that
> they need them, and don't need an
> explanation about why).

*Python Guru Tim Peters*

The main use case for a metaclass is creating an API. A typical example of this is the Django ORM. It allows you to define something like this:

```
class Person(models.Model):
    name = models.CharField(max_length=30)
    age = models.IntegerField()

```

But if you do this:

```
person = Person(name='bob', age='35')
print(person.age)

```

It won't return an `IntegerField` object. It will return an `int`, and can even take it directly from the database.

This is possible because `models.Model` defines `__metaclass__` and
it uses some magic that will turn the `Person` you just defined with simple statements
into a complex hook to a database field.

Django makes something complex look simple by exposing a simple API
and using metaclasses, recreating code from this API to do the real job
behind the scenes.

The last word
=============

First, you know that classes are objects that can create instances.

Well, in fact, classes are themselves instances. Of metaclasses.

```
>>> class Foo(object): pass
>>> id(Foo)
    142630324

```

Everything is an object in Python, and they are all either instance of classes
or instances of metaclasses.

Except for `type`.

`type` is actually its own metaclass. This is not something you could
reproduce in pure Python, and is done by cheating a little bit at the implementation
level.

Secondly, metaclasses are complicated. You may not want to use them for
very simple class alterations. You can change classes by using two different techniques:

* [monkey patching](http://en.wikipedia.org/wiki/Monkey_patch)
* class decorators

99% of the time you need class alteration, you are better off using these.

But 98% of the time, you don't need class alteration at all."
"I have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.

I can sort on the keys, but how can I sort based on the values?

Note: I have read Stack Overflow question here *[How do I sort a list of dictionaries by a value of the dictionary?](https://stackoverflow.com/questions/72899)* and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a simpler solution to sort either in ascending or descending order.","### Python 3.7+ or CPython 3.6

Dicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but [it's an implementation detail](https://stackoverflow.com/q/39980323/4518341).

```
>>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}
>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}
{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}

```

or

```
>>> dict(sorted(x.items(), key=lambda item: item[1]))
{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}

```

### Older Python

It is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a listâ€”probably a list of tuples.

For instance,

```
import operator
x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}
sorted_x = sorted(x.items(), key=operator.itemgetter(1))

```

`sorted_x` will be a list of tuples sorted by the second element in each tuple. `dict(sorted_x) == x`.

And for those wishing to sort on keys instead of values:

```
import operator
x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}
sorted_x = sorted(x.items(), key=operator.itemgetter(0))

```

In Python3 since [unpacking is not allowed](https://stackoverflow.com/a/15712231/4293057) we can use

```
x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}
sorted_x = sorted(x.items(), key=lambda kv: kv[1])

```

If you want the output as a dict, you can use [`collections.OrderedDict`](https://docs.python.org/3/library/collections.html#collections.OrderedDict):

```
import collections

sorted_dict = collections.OrderedDict(sorted_x)

```"
What's the difference between Django `OneToOneField` and `ForeignKey`?,"Differences between `OneToOneField(SomeModel)` and `ForeignKey(SomeModel, unique=True)` as stated in [*The Definitive Guide to Django*](https://web.archive.org/web/20190119091044/http://djangobook.com/model-definition-reference/):

> **OneToOneField**
>
> A one-to-one relationship. Conceptually, this is similar to a `ForeignKey` with `unique=True`, but the ""reverse"" side of the relation will directly return a single object.

In contrast to the `OneToOneField` ""reverse"" relation, a `ForeignKey` ""reverse"" relation returns a `QuerySet`.

Example
-------

For example, if we have the following two models (full model code below):

1. `Car` model uses `OneToOneField(Engine)`
2. `Car2` model uses `ForeignKey(Engine2, unique=True)`

From within `python manage.py shell` execute the following:

### `OneToOneField` Example

```
>>> from testapp.models import Car, Engine
>>> c = Car.objects.get(name='Audi')
>>> e = Engine.objects.get(name='Diesel')
>>> e.car
<Car: Audi>

```

### `ForeignKey` with `unique=True` Example

```
>>> from testapp.models import Car2, Engine2
>>> c2 = Car2.objects.get(name='Mazda')
>>> e2 = Engine2.objects.get(name='Wankel')
>>> e2.car2_set.all()
[<Car2: Mazda>]

```

Model Code
----------

```
from django.db import models

class Engine(models.Model):
    name = models.CharField(max_length=25)

    def __unicode__(self):
        return self.name

class Car(models.Model):
    name = models.CharField(max_length=25)
    engine = models.OneToOneField(Engine)

    def __unicode__(self):
        return self.name

class Engine2(models.Model):
    name = models.CharField(max_length=25)

    def __unicode__(self):
        return self.name

class Car2(models.Model):
    name = models.CharField(max_length=25)
    engine = models.ForeignKey(Engine2, unique=True, on_delete=models.CASCADE)

    def __unicode__(self):
        return self.name

```"
"If you're writing a library, or an app, where do the unit test files go?

It's nice to separate the test files from the main app code, but it's awkward to put them into a ""tests"" subdirectory inside of the app root directory, because it makes it harder to import the modules that you'll be testing.

Is there a best practice here?","For a file `module.py`, the unit test should normally be called `test_module.py`, following Pythonic naming conventions.

There are several commonly accepted places to put `test_module.py`:

1. In the same directory as `module.py`.
2. In `../tests/test_module.py` (at the same level as the code directory).
3. In `tests/test_module.py` (one level under the code directory).

I prefer #1 for its simplicity of finding the tests and importing them. Whatever build system you're using can easily be configured to run files starting with `test_`. Actually, the [default `unittest` pattern used for test discovery is `test*.py`](https://docs.python.org/library/unittest.html#test-discovery)."
"Is there a direct way to import the contents of a CSV file into a record array, just like how R's `read.table()`, `read.delim()`, and `read.csv()` import data into R dataframes?

Or should I use [`csv.reader()`](https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python) and then apply `numpy.core.records.fromrecords()`?","Use [`numpy.genfromtxt()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html) by setting the `delimiter` kwarg to a comma:

```
from numpy import genfromtxt
my_data = genfromtxt('my_file.csv', delimiter=',')

```"
"In C# there's a [null-coalescing operator](http://msdn.microsoft.com/en-us/library/ms173224.aspx) (written as `??`) that allows for easy (short) null checking during assignment:

```
string s = null;
var other = s ?? ""some default value"";

```

Is there a python equivalent?

I know that I can do:

```
s = None
other = s if s else ""some default value""

```

But is there an even shorter way (where I don't need to repeat `s`)?","```
other = s or ""some default value""

```

Ok, it must be clarified how the `or` operator works. It is a boolean operator, so it works in a boolean context. If the values are not boolean, they are converted to boolean for the purposes of the operator.

Note that the `or` operator does not return only `True` or `False`. Instead, it returns the first operand if the first operand evaluates to true, and it returns the second operand if the first operand evaluates to false.

In this case, the expression `x or y` returns `x` if it is `True` or evaluates to true when converted to boolean. Otherwise, it returns `y`. For most cases, this will serve for the very same purpose of Câ™¯'s null-coalescing operator, but keep in mind:

```
42    or ""something""    # returns 42
0     or ""something""    # returns ""something""
None  or ""something""    # returns ""something""
False or ""something""    # returns ""something""
""""    or ""something""    # returns ""something""

```

If you use your variable `s` to hold something that is either a reference to the instance of a class or `None` (as long as your class does not define members `__nonzero__()` and `__len__()`), it is secure to use the same semantics as the null-coalescing operator.

In fact, it may even be useful to have this side-effect of Python. Since you know what values evaluates to false, you can use this to trigger the default value without using `None` specifically (an error object, for example).

In some languages this behavior is referred to as the [Elvis operator](https://en.wikipedia.org/wiki/Elvis_operator)."
"I have a data frame with a hierarchical index in axis 1 (columns) (from a `groupby.agg` operation):

```
     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       
                                     sum   sum   sum    sum   amax   amin
0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94

```

I want to flatten it, so that it looks like this (names aren't critical - I could rename):

```
     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   
0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94

```

How do I do this? (I've tried a lot, to no avail.)

Per a suggestion, here is the head in dict form

```
{('USAF', ''): {0: '702730',
  1: '702730',
  2: '702730',
  3: '702730',
  4: '702730'},
 ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},
 ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},
 ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},
 ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},
 ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},
 ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},
 ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},
 ('tempf', 'amax'): {0: 30.920000000000002,
  1: 32.0,
  2: 23.0,
  3: 10.039999999999999,
  4: 19.939999999999998},
 ('tempf', 'amin'): {0: 24.98,
  1: 24.98,
  2: 6.9799999999999969,
  3: 3.9199999999999982,
  4: 10.940000000000001},
 ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}

```","I think the easiest way to do this would be to set the columns to the top level:

```
df.columns = df.columns.get_level_values(0)

```

*Note: if the to level has a name you can also access it by this, rather than 0.*

.

If you want to combine/[`join`](http://docs.python.org/2/library/stdtypes.html#str.join) your MultiIndex into one Index *(assuming you have just string entries in your columns)* you could:

```
df.columns = [' '.join(col).strip() for col in df.columns.values]

```

*Note: we must [`strip`](http://docs.python.org/2/library/stdtypes.html#str.strip) the whitespace for when there is no second index.*

```
In [11]: [' '.join(col).strip() for col in df.columns.values]
Out[11]: 
['USAF',
 'WBAN',
 'day',
 'month',
 's_CD sum',
 's_CL sum',
 's_CNT sum',
 's_PC sum',
 'tempf amax',
 'tempf amin',
 'year']

```"
"Suppose I have a function like:

```
def myfun(a, b, c):
    return (a * 2, b + c, c + b)

```

Given a tuple `some_tuple = (1, ""foo"", ""bar"")`, how would I use `some_tuple` to call `myfun`? This should output the result `(2, ""foobar"", ""barfoo"")`.

I know could define `myfun` so that it accepts the tuple directly, but I want to call the existing `myfun`.

---

See also: [What do \*\* (double star/asterisk) and \* (star/asterisk) mean in a function call?](https://stackoverflow.com/questions/2921847) - the corresponding question for people who encounter the syntax and are confused by it.",`myfun(*some_tuple)` does exactly what you request. The `*` operator simply unpacks the tuple (or any iterable) and passes them as the positional arguments to the function. Read more about [unpacking arguments](https://docs.python.org/3/tutorial/controlflow.html#tut-unpacking-arguments).
"In Django doc:

> [`select_related()`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#select-related) ""follows"" foreign-key relationships, selecting additional related-object data when it executes its query.
>
> [`prefetch_related()`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#prefetch-related) does a separate lookup for each relationship, and does the ""joining"" in Python.

What does it mean by ""doing the joining in python""? Can someone illustrate with an example?

My understanding is that for foreign key relationship, use `select_related`; and for M2M relationship, use `prefetch_related`. Is this correct?","Your understanding is **mostly** correct:

* `select_related`: when the object that you're going to be selecting is a single object, so `OneToOneField` or a `ForeignKey`
* `prefetch_related`: when you're going to get a ""set"" of things, so `ManyToManyField`s as you stated or reverse `ForeignKey`s.

Just to clarify what I mean by reverse `ForeignKey`s, here's an example:

```
class ModelA(models.Model):
    pass

class ModelB(models.Model):
    a = ForeignKey(ModelA)

# Forward ForeignKey relationship
ModelB.objects.select_related('a').all()

# Reverse ForeignKey relationship
ModelA.objects.prefetch_related('modelb_set').all() 

```

The difference is that:

* `select_related` does an SQL join and therefore gets the results back as part of the table from the SQL server
* `prefetch_related` on the other hand executes another query and therefore reduces the redundant columns in the original object (`ModelA` in the above example)

You may use `prefetch_related` for anything that you can use `select_related` for.

The tradeoffs are that `prefetch_related` has to create and send a list of IDs to select back to the server, this can take a while. I'm not sure if there's a nice way of doing this in a transaction, but my understanding is that Django always just sends a list and says SELECT ... WHERE pk IN (...,...,...) basically. In this case if the prefetched data is sparse (let's say U.S. State objects linked to people's addresses) this can be very good, however if it's closer to one-to-one, this can waste a lot of communications. If in doubt, try both and see which performs better.

Everything discussed above is basically about the communications with the database. On the Python side however `prefetch_related` has the extra benefit that a single object is used to represent each object in the database. With `select_related` duplicate objects will be created in Python for each ""parent"" object. Since objects in Python have a decent bit of memory overhead this can also be a consideration."
"I would like to increase the width of the ipython notebook in my browser. I have a high-resolution screen, and I would like to expand the cell width/size to make use of this extra space.","If you don't want to change your default settings, and you only want to change the width of the current notebook you're working on, you can enter the following into a cell:

```
from IPython.display import display, HTML
display(HTML(""<style>.container { width:100% !important; }</style>""))

```"
"How do you know whether a variable has been set at a particular place in the code at runtime? This is not always obvious because (1) the variable could be conditionally set, and (2) the variable could be conditionally deleted. I'm looking for something like `defined()` in Perl or `isset()` in PHP or `defined?` in Ruby.

```
if condition:
    a = 42

# is ""a"" defined here?

if other_condition:
    del a

# is ""a"" defined here?

```","```
try:
    thevariable
except NameError:
    print(""well, it WASN'T defined after all!"")
else:
    print(""sure, it was defined."")

```"
"What are the advantages of [NumPy](http://en.wikipedia.org/wiki/NumPy) over regular Python lists?

I have approximately 100 financial markets series, and I am going to create a cube array of 100x100x100 = 1 million cells. I will be regressing (3-variable) each x with each y and z, to fill the array with standard errors.

I have heard that for ""large matrices"" I should use NumPy as opposed to Python lists, for performance and scalability reasons. Thing is, I know Python lists and they seem to work for me.

What will the benefits be if I move to NumPy?

What if I had 1000 series (that is, 1 billion floating point cells in the cube)?","NumPy's arrays are more compact than Python lists -- a list of lists as you describe, in Python, would take at least 20 MB or so, while a NumPy 3D array with single-precision floats in the cells would fit in 4 MB. Access in reading and writing items is also faster with NumPy.

Maybe you don't care that much for just a million cells, but you definitely would for a billion cells -- neither approach would fit in a 32-bit architecture, but with 64-bit builds NumPy would get away with 4 GB or so, Python alone would need at least about 12 GB (lots of pointers which double in size) -- a much costlier piece of hardware!

The difference is mostly due to ""indirectness"" -- a Python list is an array of pointers to Python objects, at least 4 bytes per pointer plus 16 bytes for even the smallest Python object (4 for type pointer, 4 for reference count, 4 for value -- and the memory allocators rounds up to 16). A NumPy array is an array of uniform values -- single-precision numbers takes 4 bytes each, double-precision ones, 8 bytes. Less flexible, but you pay substantially for the flexibility of standard Python lists!"
"While using `new_list = my_list`, any modifications to `new_list` changes `my_list` every time. Why is this, and how can I clone or copy the list to prevent it? For example:

```
>>> my_list = [1, 2, 3]
>>> new_list = my_list
>>> new_list.append(4)
>>> my_list
[1, 2, 3, 4]

```","`new_list = my_list` doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both `new_list` and `my_list` refer to the same list after the assignment.

To actually copy the list, you have several options:

* You can use the built-in [`list.copy()`](https://docs.python.org/library/stdtypes.html#mutable-sequence-types) method (available since Python 3.3):

  ```
  new_list = old_list.copy()

  ```
* You can slice it:

  ```
  new_list = old_list[:]

  ```

  [Alex Martelli](https://en.wikipedia.org/wiki/Alex_Martelli)'s opinion (at least [back in 2007](https://www.youtube.com/watch?v=g7V89K8QfgQ)) about this is, that *it is a weird syntax and it does not make sense to use it ever*. ;) (In his opinion, the next one is more readable).
* You can use the built-in [`list()`](https://docs.python.org/library/stdtypes.html#list) constructor:

  ```
  new_list = list(old_list)

  ```
* You can use generic [`copy.copy()`](https://docs.python.org/library/copy.html#copy.copy):

  ```
  import copy
  new_list = copy.copy(old_list)

  ```

  This is a little slower than `list()` because it has to find out the datatype of `old_list` first.
* If you need to copy the elements of the list as well, use generic [`copy.deepcopy()`](https://docs.python.org/library/copy.html#copy.deepcopy):

  ```
  import copy
  new_list = copy.deepcopy(old_list)

  ```

  Obviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).

**Example:**

```
import copy

class Foo(object):
    def __init__(self, val):
         self.val = val

    def __repr__(self):
        return f'Foo({self.val!r})'

foo = Foo(1)

a = ['foo', foo]
b = a.copy()
c = a[:]
d = list(a)
e = copy.copy(a)
f = copy.deepcopy(a)

# edit orignal list and instance 
a.append('baz')
foo.val = 5

print(f'original: {a}\nlist.copy(): {b}\nslice: {c}\nlist(): {d}\ncopy: {e}\ndeepcopy: {f}')

```

Result:

```
original: ['foo', Foo(5), 'baz']
list.copy(): ['foo', Foo(5)]
slice: ['foo', Foo(5)]
list(): ['foo', Foo(5)]
copy: ['foo', Foo(5)]
deepcopy: ['foo', Foo(1)]

```"
"How do I check if a string matches the following pattern?

Uppercase letter, number(s), uppercase letter, number(s)...

Example:

* These would match:

  ```
  A1B2
  B10L1
  C1N200J1

  ```
* These wouldn't ('^' points to problem)

  ```
  a1B2
  ^
  A10B
     ^
  AB400
  ^

  ```","```
import re
pattern = re.compile(""^([A-Z][0-9]+)+$"")
pattern.match(string)

```"
"Yes, I know this subject has been covered before:

* [Python idiom to chain (flatten) an infinite iterable of finite iterables?](https://stackoverflow.com/questions/120886)
* [Flattening a shallow list in Python](https://stackoverflow.com/questions/406121)
* [Comprehension for flattening a sequence of sequences?](https://stackoverflow.com/questions/457215)
* [How do I make a flat list out of a list of lists?](https://stackoverflow.com/questions/952914)

but as far as I know, all solutions, except for one, fail on a list like `[[[1, 2, 3], [4, 5]], 6]`, where the desired output is `[1, 2, 3, 4, 5, 6]` (or perhaps even better, an iterator).

The only solution I saw that works for an arbitrary nesting is found [in this question](https://stackoverflow.com/questions/406121):

```
def flatten(x):
    result = []
    for el in x:
        if hasattr(el, ""__iter__"") and not isinstance(el, basestring):
            result.extend(flatten(el))
        else:
            result.append(el)
    return result

```

Is this the best approach? Did I overlook something? Any problems?","Using generator functions can make your example easier to read and improve performance.

### Python 2

Using the [`Iterable` ABC](https://docs.python.org/2/library/collections.html#collections-abstract-base-classes) added in 2.6:

```
from collections import Iterable

def flatten(xs):
    for x in xs:
        if isinstance(x, Iterable) and not isinstance(x, basestring):
            for item in flatten(x):
                yield item
        else:
            yield x

```

### Python 3

In Python 3, `basestring` is no more, but the tuple `(str, bytes)` gives the same effect. Also, the [`yield from`](https://docs.python.org/3/whatsnew/3.3.html#pep-380) operator returns an item from a generator one at a time.

```
from collections.abc import Iterable

def flatten(xs):
    for x in xs:
        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):
            yield from flatten(x)
        else:
            yield x

```"
"I have the following code to do this, but how can I do it better? Right now I think it's better than nested loops, but it starts to get Perl-one-linerish when you have a generator in a list comprehension.

```
day_count = (end_date - start_date).days + 1
for single_date in [d for d in (start_date + timedelta(n) for n in range(day_count)) if d <= end_date]:
    print strftime(""%Y-%m-%d"", single_date.timetuple())

```

Notes
-----

* I'm not actually using this to print. That's just for demo purposes.
* The `start_date` and `end_date` variables are `datetime.date` objects because I don't need the timestamps. (They're going to be used to generate a report).

Sample Output
-------------

For a start date of `2009-05-30` and an end date of `2009-06-09`:

```
2009-05-30
2009-05-31
2009-06-01
2009-06-02
2009-06-03
2009-06-04
2009-06-05
2009-06-06
2009-06-07
2009-06-08
2009-06-09

```","Why are there two nested iterations? For me it produces the same list of data with only one iteration:

```
for single_date in (start_date + timedelta(n) for n in range(day_count)):
    print ...

```

And no list gets stored, only one generator is iterated over. Also the ""if"" in the generator seems to be unnecessary.

After all, a linear sequence should only require one iterator, not two.

Update after discussion with John Machin:
-----------------------------------------

Maybe the most elegant solution is using a generator function to completely hide/abstract the iteration over the range of dates:

```
from datetime import date, timedelta

def daterange(start_date: date, end_date: date):
    days = int((end_date - start_date).days)
    for n in range(days):
        yield start_date + timedelta(n)

start_date = date(2013, 1, 1)
end_date = date(2015, 6, 2)
for single_date in daterange(start_date, end_date):
    print(single_date.strftime(""%Y-%m-%d""))

```

NB: For consistency with the built-in `range()` function this iteration stops **before** reaching the `end_date`. So for inclusive iteration use the next day, as you would with `range()`."
"I'm trying to plot a figure without tickmarks or numbers on either of the axes (I use axes in the traditional sense, not the matplotlib nomenclature!). An issue I have come across is where matplotlib adjusts the x(y)ticklabels by subtracting a value N, then adds N at the end of the axis.

This may be vague, but the following simplified example highlights the issue, with '6.18' being the offending value of N:

```
import matplotlib.pyplot as plt
import random
prefix = 6.18

rx = [prefix+(0.001*random.random()) for i in arange(100)]
ry = [prefix+(0.001*random.random()) for i in arange(100)]
plt.plot(rx,ry,'ko')

frame1 = plt.gca()
for xlabel_i in frame1.axes.get_xticklabels():
    xlabel_i.set_visible(False)
    xlabel_i.set_fontsize(0.0)
for xlabel_i in frame1.axes.get_yticklabels():
    xlabel_i.set_fontsize(0.0)
    xlabel_i.set_visible(False)
for tick in frame1.axes.get_xticklines():
    tick.set_visible(False)
for tick in frame1.axes.get_yticklines():
    tick.set_visible(False)

plt.show()

```

The three things I would like to know are:

1. How to turn off this behaviour in the first place (although in most cases it is useful, it is not always!) I have looked through `matplotlib.axis.XAxis` and cannot find anything appropriate
2. How can I make N disappear (i.e. `X.set_visible(False)`)
3. Is there a better way to do the above anyway? My final plot would be 4x4 subplots in a figure, if that is relevant.","Instead of hiding each element, you can hide the whole axis:

```
frame1.axes.get_xaxis().set_visible(False)
frame1.axes.get_yaxis().set_visible(False)

```

Or, you can set the ticks to an empty list:

```
frame1.axes.get_xaxis().set_ticks([])
frame1.axes.get_yaxis().set_ticks([])

```

In this second option, you can still use `plt.xlabel()` and `plt.ylabel()` to add labels to the axes."
"I have a directory structure

```
├── simulate.py
├── src
│   ├── networkAlgorithm.py
│   ├── ...

```

And I can access the network module with `sys.path.insert()`.

```
import sys
import os.path
sys.path.insert(0, ""./src"")
from networkAlgorithm import *

```

However, pycharm complains that it cannot access the module. How can I teach pycham to resolve the reference?

![enter image description here](https://i.sstatic.net/VwqcZ.png)","Manually adding it as you have done *is* indeed one way of doing this, but there is a simpler method, and that is by simply telling pycharm that you want to add the `src` folder as a source root, and then adding the sources root to your python path.

This way, you don't have to hard code things into your interpreter's settings:

* Add `src` as a source content root:

![enter image description here](https://i.sstatic.net/9WunC.png)

* Then make sure to add add sources to your `PYTHONPATH` under:

  ```
  Preferences ~ Build, Execution, Deployment ~ Console ~ Python Console

  ```

![enter image description here](https://i.sstatic.net/rk59O.png)

* Now imports will be resolved:

![enter image description here](https://i.sstatic.net/4Z6RD.png)

This way, you can add whatever you want as a source root, and things will simply work. If you unmarked it as a source root however, you *will* get an error:

![enter image description here](https://i.sstatic.net/vMII4.png)

After all this don't forget to restart. In PyCharm menu select: File --> Invalidate Caches / Restart"
"I want to build a dictionary in Python. However, all the examples that I see are instantiating a dictionary from a list, etc . ..

How do I create a new empty dictionary in Python?","Call `dict` with no parameters

```
new_dict = dict()

```

or simply write

```
new_dict = {}

```"
"In Python, there are two similarly-named functions, `exit()` and `sys.exit()`. What's the difference and when should I use one over the other?","[`exit`](http://docs.python.org/library/constants.html#exit) is a helper for the interactive shell - [`sys.exit`](http://docs.python.org/library/sys.html#sys.exit) is intended for use in programs.

> The [`site`](https://docs.python.org/library/site.html#module-site) module (which is imported automatically during startup, except if the [`-S`](https://docs.python.org/using/cmdline.html#cmdoption-S) command-line option is given) adds several constants to the built-in namespace *(e.g. `exit`)*. **They are useful for the interactive interpreter shell and should not be used in programs**.

---

Technically, they do mostly the same: raising [`SystemExit`](https://docs.python.org/library/exceptions.html#SystemExit). `sys.exit` does so in [*sysmodule.c*](https://github.com/python/cpython/blob/06fe77a84bd29d51506ab2ff703ae585a6121af2/Python/sysmodule.c#L337-L346):

```
static PyObject *
sys_exit(PyObject *self, PyObject *args)
{
    PyObject *exit_code = 0;
    if (!PyArg_UnpackTuple(args, ""exit"", 0, 1, &exit_code))
        return NULL;
    /* Raise SystemExit so callers may catch it or clean up. */
    PyErr_SetObject(PyExc_SystemExit, exit_code);
   return NULL;
}

```

While `exit` is defined in [site.py](https://github.com/python/cpython/blob/06fe77a84bd29d51506ab2ff703ae585a6121af2/Lib/site.py#L365-L366) and [\_sitebuiltins.py](https://github.com/python/cpython/blob/06fe77a84bd29d51506ab2ff703ae585a6121af2/Lib/_sitebuiltins.py#L13-L26), respectively.

```
class Quitter(object):
    def __init__(self, name):
        self.name = name
    def __repr__(self):
        return 'Use %s() or %s to exit' % (self.name, eof)
    def __call__(self, code=None):
        # Shells like IDLE catch the SystemExit, but listen when their
        # stdin wrapper is closed.
        try:
            sys.stdin.close()
        except:
            pass
        raise SystemExit(code)
__builtin__.quit = Quitter('quit')
__builtin__.exit = Quitter('exit')

```

---

Note that there is a third exit option, namely [os.\_exit](http://docs.python.org/library/os.html#os._exit), which exits without calling cleanup handlers, flushing stdio buffers, etc. (and which should normally only be used in the child process after a `fork()`)."
"In Python, what happens when two modules attempt to `import` each other? More generally, what happens if multiple modules attempt to `import` in a cycle?

---

See also [What can I do about ""ImportError: Cannot import name X"" or ""AttributeError: ... (most likely due to a circular import)""?](https://stackoverflow.com/questions/9252543/) for the common problem that may result, and advice on how to rewrite code to avoid such imports. See [Why do circular imports seemingly work further up in the call stack but then raise an ImportError further down?](https://stackoverflow.com/questions/22187279) for technical details on **why and how** the problem occurs.","If you do `import foo` (inside `bar.py`) and `import bar` (inside `foo.py`), it will work fine. By the time anything actually runs, both modules will be fully loaded and will have references to each other.

The problem is when instead you do `from foo import abc` (inside `bar.py`) and `from bar import xyz` (inside `foo.py`). Because now each module requires the other module to already be imported (so that the name we are importing exists) before it can be imported.

Examples of working circular imports in Python 2 and Python 3
-------------------------------------------------------------

The article [When are Python circular imports fatal?](https://gist.github.com/Mark24Code/2073470277437f2241033c2003f98358) gives four examples when circular imports are, for the reason explained above, nonfatal.

### Top of module; no from; Python 2 only

```
# lib/foo.py                         # lib/bar.py
import bar                           import foo

def abc():                           def xyz():
    print(bar.xyz.__name__)              print(foo.abc.__name__)

```

### Top of module; from ok; relative ok; Python 3 only

```
# lib/foo.py                         # lib/bar.py
from . import bar                    from . import foo

def abc():                           def xyz():
    print(bar.xyz.__name__)              print(abc.__name__)

```

### Top of module; no from; no relative

```
# lib/foo.py                         # lib/bar.py
import lib.bar                       import lib.foo

def abc():                           def xyz():
    print(lib.bar.xyz.__name__)          print(lib.foo.abc.__name__)

```

### Bottom of module; import attribute, not module; from okay

```
# lib/foo.py                         # lib/bar.py
def abc():                           def xyz():
    print(xyz.__name__)                  print(abc.__name__)


from .bar import xyz                 from .foo import abc

```

### Top of function; from okay

```
# lib/foo.py                         # lib/bar.py
def abc():                           def xyz():
    from . import bar                    from . import foo
    print(bar.xyz.__name__)              print(foo.abc.__name__)

```

Additional examples
-------------------

The article cited above does not discuss star imports."
"I know that some other languages, [such as PHP](https://www.php.net/manual/en/language.variables.variable.php), support a concept of ""variable variable names"" - that is, the contents of a string can be used as part of a variable name.

I heard that this is a bad idea in general, but I think it would solve some problems I have in my Python code.

Is it possible to do something like this in Python? What can go wrong?

---

If you are just trying to *look up an existing* variable by its name, see [How can I select a variable by (string) name?](https://stackoverflow.com/questions/47496415). However, first consider whether you can reorganize the code to avoid that need, following the advice in this question.","You can use [dictionaries](https://docs.python.org/tutorial/datastructures.html#dictionaries) to accomplish this. Dictionaries are stores of keys and values.

```
>>> dct = {'x': 1, 'y': 2, 'z': 3}
>>> dct
{'x': 1, 'y': 2, 'z': 3}
>>> dct[""y""]
2

```

You can use variable key names to achieve the effect of variable variables without the security risk.

```
>>> x = ""spam""
>>> z = {x: ""eggs""}
>>> z[""spam""]
'eggs'

```

For cases where you're thinking of doing something like

```
var1 = 'foo'
var2 = 'bar'
var3 = 'baz'
...

```

a [list](https://docs.python.org/tutorial/introduction.html#lists) may be more appropriate than a dict. A list represents an ordered sequence of objects, with integer indices:

```
lst = ['foo', 'bar', 'baz']
print(lst[1])           # prints bar, because indices start at 0
lst.append('potatoes')  # lst is now ['foo', 'bar', 'baz', 'potatoes']

```

For ordered sequences, lists are more convenient than dicts with integer keys, because lists support iteration in index order, [slicing](https://stackoverflow.com/questions/509211/understanding-pythons-slice-notation), `append`, and other operations that would require awkward key management with a dict."
"I completed my first proper project in Python and now my task is to write tests for it.

Since this is the first time I did a project, this is the first time I would be writing tests for it.

The question is, *how* do I start? I have absolutely no idea. Can anyone point me to some documentation/ tutorial/ link/ book that I can use to start with writing tests (and I guess unit testing in particular)

Any advice will be welcomed on this topic.","If you're brand new to using unittests, the simplest approach to learn is often the best. On that basis along I recommend using [`py.test`](https://docs.pytest.org/en/latest/) rather than [the default `unittest` module](https://docs.python.org/3/library/unittest.html).

Consider these two examples, which do the same thing:

Example 1 (unittest):

```
import unittest

class LearningCase(unittest.TestCase):
    def test_starting_out(self):
        self.assertEqual(1, 1)

def main():
    unittest.main()

if __name__ == ""__main__"":
    main()

```

Example 2 (pytest):

```
def test_starting_out():
    assert 1 == 1

```

Assuming that both files are named `test_unittesting.py`, how do we run the tests?

Example 1 (unittest):

```
cd /path/to/dir/
python test_unittesting.py

```

Example 2 (pytest):

```
cd /path/to/dir/
py.test

```"
"I wrote this class for testing:

```
class PassByReference:
    def __init__(self):
        self.variable = 'Original'
        self.change(self.variable)
        print(self.variable)

    def change(self, var):
        var = 'Changed'

```

When I tried creating an instance, the output was `Original`. So it seems like parameters in Python are passed by value. Is that correct? How can I modify the code to get the effect of pass-by-reference, so that the output is `Changed`?

---

Sometimes people are surprised that code like `x = 1`, where `x` is a parameter name, doesn't impact on the caller's argument, but code like `x[0] = 1` does. This happens because *item assignment* and *slice assignment* are ways to **mutate** an existing object, rather than reassign a variable, despite the `=` syntax. See [Why can a function modify some arguments as perceived by the caller, but not others?](https://stackoverflow.com/questions/575196/) for details.

See also [What's the difference between passing by reference vs. passing by value?](https://stackoverflow.com/questions/373419/) for important, language-agnostic terminology discussion.","Arguments are [passed by assignment](http://docs.python.org/3/faq/programming.html#how-do-i-write-a-function-with-output-parameters-call-by-reference). The rationale behind this is twofold:

1. the parameter passed in is actually a *reference* to an object (but the reference is passed by value)
2. some data types are mutable, but others aren't

So:

* If you pass a *mutable* object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object.
* If you pass an *immutable* object to a method, you still can't rebind the outer reference, and you can't even mutate the object.

To make it even more clear, let's have some examples.

List - a mutable type
---------------------

**Let's try to modify the list that was passed to a method:**

```
def try_to_change_list_contents(the_list):
    print('got', the_list)
    the_list.append('four')
    print('changed to', the_list)

outer_list = ['one', 'two', 'three']

print('before, outer_list =', outer_list)
try_to_change_list_contents(outer_list)
print('after, outer_list =', outer_list)

```

Output:

```
before, outer_list = ['one', 'two', 'three']
got ['one', 'two', 'three']
changed to ['one', 'two', 'three', 'four']
after, outer_list = ['one', 'two', 'three', 'four']

```

Since the parameter passed in is a reference to `outer_list`, not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.

**Now let's see what happens when we try to change the reference that was passed in as a parameter:**

```
def try_to_change_list_reference(the_list):
    print('got', the_list)
    the_list = ['and', 'we', 'can', 'not', 'lie']
    print('set to', the_list)

outer_list = ['we', 'like', 'proper', 'English']

print('before, outer_list =', outer_list)
try_to_change_list_reference(outer_list)
print('after, outer_list =', outer_list)

```

Output:

```
before, outer_list = ['we', 'like', 'proper', 'English']
got ['we', 'like', 'proper', 'English']
set to ['and', 'we', 'can', 'not', 'lie']
after, outer_list = ['we', 'like', 'proper', 'English']

```

Since the `the_list` parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The `the_list` was a copy of the `outer_list` reference, and we had `the_list` point to a new list, but there was no way to change where `outer_list` pointed.

String - an immutable type
--------------------------

**It's immutable, so there's nothing we can do to change the contents of the string**

**Now, let's try to change the reference**

```
def try_to_change_string_reference(the_string):
    print('got', the_string)
    the_string = 'In a kingdom by the sea'
    print('set to', the_string)

outer_string = 'It was many and many a year ago'

print('before, outer_string =', outer_string)
try_to_change_string_reference(outer_string)
print('after, outer_string =', outer_string)

```

Output:

```
before, outer_string = It was many and many a year ago
got It was many and many a year ago
set to In a kingdom by the sea
after, outer_string = It was many and many a year ago

```

Again, since the `the_string` parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The `the_string` was a copy of the `outer_string` reference, and we had `the_string` point to a new string, but there was no way to change where `outer_string` pointed.

I hope this clears things up a little.

**EDIT:** It's been noted that this doesn't answer the question that @David originally asked, ""Is there something I can do to pass the variable by actual reference?"". Let's work on that.

How do we get around this?
--------------------------

As @Andrea's answer shows, you could return the new value. This doesn't change the way things are passed in, but does let you get the information you want back out:

```
def return_a_whole_new_string(the_string):
    new_string = something_to_do_with_the_old_string(the_string)
    return new_string

# then you could call it like
my_string = return_a_whole_new_string(my_string)

```

If you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:

```
def use_a_wrapper_to_simulate_pass_by_reference(stuff_to_change):
    new_string = something_to_do_with_the_old_string(stuff_to_change[0])
    stuff_to_change[0] = new_string

# then you could call it like
wrapper = [my_string]
use_a_wrapper_to_simulate_pass_by_reference(wrapper)

do_something_with(wrapper[0])

```

Although this seems a little cumbersome."
"I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.

Thanks!","Scikit Learn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is a good one. It will split both numpy arrays and dataframes.

```
from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2)

```"
"Is it possible to install packages using pip from the local filesystem?

I have run `python setup.py sdist` for my package, which has created the appropriate tar.gz file. This file is stored on my system at `/srv/pkg/mypackage/mypackage-0.1.0.tar.gz`.

Now in a virtual environment I would like to install packages either coming from pypi or from the specific local location `/srv/pkg`.

Is this possible?

**PS**
I know that I can specify `pip install /srv/pkg/mypackage/mypackage-0.1.0.tar.gz`. That will work, but I am talking about using the `/srv/pkg` location as another place for pip to search if I typed `pip install mypackage`.","What about::

```
pip install --help
...
  -e, --editable <path/url>   Install a project in editable mode (i.e. setuptools
                              ""develop mode"") from a local project path or a VCS url.

```

eg, `pip install -e /srv/pkg`

where /srv/pkg is the top-level directory where 'setup.py' can be found."
"In Python `remove()` will remove the first occurrence of value in a list.

How to remove *all* occurrences of a value from a list?

This is what I have in mind:

```
>>> remove_values_from_list([1, 2, 3, 4, 2, 2, 3], 2)
[1, 3, 4, 3]

```","Functional approach:

**Python 3.x**

```
>>> x = [1,2,3,2,2,2,3,4]
>>> list(filter((2).__ne__, x))
[1, 3, 3, 4]

```

or

```
>>> x = [1,2,3,2,2,2,3,4]
>>> list(filter(lambda a: a != 2, x))
[1, 3, 3, 4]

```

or

```
>>> [i for i in x if i != 2]

```

**Python 2.x**

```
>>> x = [1,2,3,2,2,2,3,4]
>>> filter(lambda a: a != 2, x)
[1, 3, 3, 4]

```"
"When writing custom classes it is often important to allow equivalence by means of the `==` and `!=` operators. In Python, this is made possible by implementing the `__eq__` and `__ne__` special methods, respectively. The easiest way I've found to do this is the following method:

```
class Foo:
    def __init__(self, item):
        self.item = item

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return self.__dict__ == other.__dict__
        else:
            return False

    def __ne__(self, other):
        return not self.__eq__(other)

```

Do you know of more elegant means of doing this? Do you know of any particular disadvantages to using the above method of comparing `__dict__`s?

**Note**: A bit of clarification--when `__eq__` and `__ne__` are undefined, you'll find this behavior:

```
>>> a = Foo(1)
>>> b = Foo(1)
>>> a is b
False
>>> a == b
False

```

That is, `a == b` evaluates to `False` because it really runs `a is b`, a test of identity (i.e., ""Is `a` the same object as `b`?"").

When `__eq__` and `__ne__` are defined, you'll find this behavior (which is the one we're after):

```
>>> a = Foo(1)
>>> b = Foo(1)
>>> a is b
False
>>> a == b
True

```","Consider this simple problem:

```
class Number:

    def __init__(self, number):
        self.number = number


n1 = Number(1)
n2 = Number(1)

n1 == n2 # False -- oops

```

So, Python by default uses the object identifiers for comparison operations:

```
id(n1) # 140400634555856
id(n2) # 140400634555920

```

Overriding the `__eq__` function seems to solve the problem:

```
def __eq__(self, other):
    """"""Overrides the default implementation""""""
    if isinstance(other, Number):
        return self.number == other.number
    return False


n1 == n2 # True
n1 != n2 # True in Python 2 -- oops, False in Python 3

```

In *Python 2*, always remember to override the `__ne__` function as well, as the [documentation](https://docs.python.org/2.7/reference/datamodel.html#object.__ne__) states:

> There are no implied relationships among the comparison operators. The
> truth of `x==y` does not imply that `x!=y` is false. Accordingly, when
> defining `__eq__()`, one should also define `__ne__()` so that the
> operators will behave as expected.

```
def __ne__(self, other):
    """"""Overrides the default implementation (unnecessary in Python 3)""""""
    return not self.__eq__(other)


n1 == n2 # True
n1 != n2 # False

```

In *Python 3*, this is no longer necessary, as the [documentation](https://docs.python.org/3/reference/datamodel.html#object.__ne__) states:

> By default, `__ne__()` delegates to `__eq__()` and inverts the result
> unless it is `NotImplemented`. There are no other implied
> relationships among the comparison operators, for example, the truth
> of `(x<y or x==y)` does not imply `x<=y`.

But that does not solve all our problems. Let’s add a subclass:

```
class SubNumber(Number):
    pass


n3 = SubNumber(1)

n1 == n3 # False for classic-style classes -- oops, True for new-style classes
n3 == n1 # True
n1 != n3 # True for classic-style classes -- oops, False for new-style classes
n3 != n1 # False

```

**Note:** Python 2 has two kinds of classes:

* *[classic-style](https://docs.python.org/2/reference/datamodel.html#new-style-and-classic-classes)* (or *old-style*) classes, that do *not* inherit from `object` and that are declared as `class A:`, `class A():` or `class A(B):` where `B` is a classic-style class;
* *[new-style](https://docs.python.org/2/reference/datamodel.html#new-style-and-classic-classes)* classes, that do inherit from `object` and that are declared as `class A(object)` or `class A(B):` where `B` is a new-style class. Python 3 has only new-style classes that are declared as `class A:`, `class A(object):` or `class A(B):`.

For classic-style classes, a comparison operation always calls the method of the first operand, while for new-style classes, it always calls the method of the subclass operand, [regardless of the order of the operands](https://stackoverflow.com/a/12984987/78234).

So here, if `Number` is a classic-style class:

* `n1 == n3` calls `n1.__eq__`;
* `n3 == n1` calls `n3.__eq__`;
* `n1 != n3` calls `n1.__ne__`;
* `n3 != n1` calls `n3.__ne__`.

And if `Number` is a new-style class:

* both `n1 == n3` and `n3 == n1` call `n3.__eq__`;
* both `n1 != n3` and `n3 != n1` call `n3.__ne__`.

To fix the non-commutativity issue of the `==` and `!=` operators for Python 2 classic-style classes, the `__eq__` and `__ne__` methods should return the `NotImplemented` value when an operand type is not supported. The [documentation](https://docs.python.org/2.7/reference/datamodel.html#the-standard-type-hierarchy) defines the `NotImplemented` value as:

> Numeric methods and rich comparison methods may return this value if
> they do not implement the operation for the operands provided. (The
> interpreter will then try the reflected operation, or some other
> fallback, depending on the operator.) Its truth value is true.

In this case the operator delegates the comparison operation to the *reflected method* of the *other* operand. The [documentation](https://docs.python.org/2.7/reference/datamodel.html#object.__lt__) defines reflected methods as:

> There are no swapped-argument versions of these methods (to be used
> when the left argument does not support the operation but the right
> argument does); rather, `__lt__()` and `__gt__()` are each other’s
> reflection, `__le__()` and `__ge__()` are each other’s reflection, and
> `__eq__()` and `__ne__()` are their own reflection.

The result looks like this:

```
def __eq__(self, other):
    """"""Overrides the default implementation""""""
    if isinstance(other, Number):
        return self.number == other.number
    return NotImplemented

def __ne__(self, other):
    """"""Overrides the default implementation (unnecessary in Python 3)""""""
    x = self.__eq__(other)
    if x is NotImplemented:
        return NotImplemented
    return not x

```

Returning the `NotImplemented` value instead of `False` is the right thing to do even for new-style classes if *commutativity* of the `==` and `!=` operators is desired when the operands are of unrelated types (no inheritance).

Are we there yet? Not quite. How many unique numbers do we have?

```
len(set([n1, n2, n3])) # 3 -- oops

```

Sets use the hashes of objects, and by default Python returns the hash of the identifier of the object. Let’s try to override it:

```
def __hash__(self):
    """"""Overrides the default implementation""""""
    return hash(tuple(sorted(self.__dict__.items())))

len(set([n1, n2, n3])) # 1

```

The end result looks like this (I added some assertions at the end for validation):

```
class Number:

    def __init__(self, number):
        self.number = number

    def __eq__(self, other):
        """"""Overrides the default implementation""""""
        if isinstance(other, Number):
            return self.number == other.number
        return NotImplemented

    def __ne__(self, other):
        """"""Overrides the default implementation (unnecessary in Python 3)""""""
        x = self.__eq__(other)
        if x is not NotImplemented:
            return not x
        return NotImplemented

    def __hash__(self):
        """"""Overrides the default implementation""""""
        return hash(tuple(sorted(self.__dict__.items())))


class SubNumber(Number):
    pass


n1 = Number(1)
n2 = Number(1)
n3 = SubNumber(1)
n4 = SubNumber(4)

assert n1 == n2
assert n2 == n1
assert not n1 != n2
assert not n2 != n1

assert n1 == n3
assert n3 == n1
assert not n1 != n3
assert not n3 != n1

assert not n1 == n4
assert not n4 == n1
assert n1 != n4
assert n4 != n1

assert len(set([n1, n2, n3, ])) == 1
assert len(set([n1, n2, n3, n4])) == 2

```"
"Is there a mechanism to comment out large blocks of Python code?

Right now, the only ways I can see of commenting out code are to either start every line with a `#`, or to enclose the code in triple quotes: `""""""`.

The problem with these is that inserting `#` before every line is cumbersome and `""""""` makes the string I want to use as a comment show up in generated documentation.

After reading all comments, the answer seems to be ""No"".","Python does not have such a mechanism. Prepend a `#` to each line to block comment. For more information see [PEP 8](http://www.python.org/dev/peps/pep-0008/). Most Python IDEs support a mechanism to do the block-commenting-with-hash-signs automatically for you. For example, in IDLE on my machine, it's `Alt`+`3` and `Alt`+`4`.

**Don't use triple-quotes;** as you discovered, this is for documentation strings not block comments, although it has a similar effect. If you're just commenting things out temporarily, this is fine as a temporary measure."
"Given a DataFrame with a column ""BoolCol"", we want to find the indexes of the DataFrame in which the values for ""BoolCol"" == True

I currently have the iterating way to do it, which works perfectly:

```
for i in range(100,3000):
    if df.iloc[i]['BoolCol']== True:
         print i,df.iloc[i]['BoolCol']

```

But this is not the correct pandas way to do it. After some research, I am currently using this code:

```
df[df['BoolCol'] == True].index.tolist()

```

This one gives me a list of indexes, but they don't match, when I check them by doing:

```
df.iloc[i]['BoolCol']

```

The result is actually False!!

Which would be the correct pandas way to do this?","`df.iloc[i]` returns the `ith` row of `df`. `i` does not refer to the index label, `i` is a 0-based index.

In contrast, **the attribute `index` returns actual index labels**, not numeric row-indices:

```
df.index[df['BoolCol'] == True].tolist()

```

or equivalently,

```
df.index[df['BoolCol']].tolist()

```

You can see the difference quite clearly by playing with a DataFrame with
a non-default index that does not equal to the row's numerical position:

```
df = pd.DataFrame({'BoolCol': [True, False, False, True, True]},
       index=[10,20,30,40,50])

In [53]: df
Out[53]: 
   BoolCol
10    True
20   False
30   False
40    True
50    True

[5 rows x 1 columns]

In [54]: df.index[df['BoolCol']].tolist()
Out[54]: [10, 40, 50]

```

---

**If you want to use the index**,

```
In [56]: idx = df.index[df['BoolCol']]

In [57]: idx
Out[57]: Int64Index([10, 40, 50], dtype='int64')

```

**then you can select the rows using `loc` instead of `iloc`**:

```
In [58]: df.loc[idx]
Out[58]: 
   BoolCol
10    True
40    True
50    True

[3 rows x 1 columns]

```

---

Note that **`loc` can also accept boolean arrays**:

```
In [55]: df.loc[df['BoolCol']]
Out[55]: 
   BoolCol
10    True
40    True
50    True

[3 rows x 1 columns]

```

---

**If you have a boolean array, `mask`, and need ordinal index values, you can compute them using `np.flatnonzero`**:

```
In [110]: np.flatnonzero(df['BoolCol'])
Out[112]: array([0, 3, 4])

```

Use `df.iloc` to select rows by ordinal index:

```
In [113]: df.iloc[np.flatnonzero(df['BoolCol'])]
Out[113]: 
   BoolCol
10    True
40    True
50    True

```"
"I have a parameter file of the form:

```
parameter-name parameter-value

```

Where the parameters may be in any order but there is only one parameter per line. I want to replace one parameter's `parameter-value` with a new value.

I am using a line replace function [posted previously](https://stackoverflow.com/questions/39086/search-and-replace-a-line-in-a-file-in-python) to replace the line which uses Python's [`string.replace(pattern, sub)`](http://www.tutorialspoint.com/python/string_replace.htm%20String.Replace). The regular expression that I'm using works for instance in vim but doesn't appear to work in `string.replace()`.

Here is the regular expression that I'm using:

```
line.replace(""^.*interfaceOpDataFile.*$/i"", ""interfaceOpDataFile %s"" % (fileIn))

```

Where `""interfaceOpDataFile""` is the parameter name that I'm replacing (/i for case-insensitive) and the new parameter value is the contents of the `fileIn` variable.

Is there a way to get Python to recognize this regular expression or else is there another way to accomplish this task?","`str.replace()` [v2](https://docs.python.org/2/library/stdtypes.html#str.replace)|[v3](https://docs.python.org/3/library/stdtypes.html#str.replace) does not recognize regular expressions.

To perform a substitution using a regular expression, use `re.sub()` [v2](https://docs.python.org/2/library/re.html#re.sub)|[v3](https://docs.python.org/3/library/re.html#re.sub).

For example:

```
import re

line = re.sub(
           r""(?i)^.*interfaceOpDataFile.*$"", 
           ""interfaceOpDataFile %s"" % fileIn, 
           line
       )

```

In a loop, it would be better to compile the regular expression first:

```
import re

regex = re.compile(r""^.*interfaceOpDataFile.*$"", re.IGNORECASE)
for line in some_file:
    line = regex.sub(""interfaceOpDataFile %s"" % fileIn, line)
    # do something with the updated line

```"
"In Python, I can compile a regular expression to be case-insensitive using `re.compile`:

```
>>> s = 'TeSt'
>>> casesensitive = re.compile('test')
>>> ignorecase = re.compile('test', re.IGNORECASE)
>>> 
>>> print casesensitive.match(s)
None
>>> print ignorecase.match(s)
<_sre.SRE_Match object at 0x02F0B608>

```

Is there a way to do the same, but without using `re.compile`. I can't find anything like Perl's `i` suffix (e.g. `m/test/i`) in the documentation.","Pass `re.IGNORECASE` to the `flags` param of [`search`](https://docs.python.org/library/re.html#re.search), [`match`](https://docs.python.org/library/re.html#re.match), or [`sub`](https://docs.python.org/library/re.html#re.sub):

```
re.search('test', 'TeSt', re.IGNORECASE)
re.match('test', 'TeSt', re.IGNORECASE)
re.sub('test', 'xxxx', 'Testing', flags=re.IGNORECASE)

```"
"I have a list of tuples that looks something like this:

```
[('abc', 121),('abc', 231),('abc', 148), ('abc',221)]

```

I want to sort this list in ascending order by the integer value inside the tuples. Is it possible?","Try using the `key` keyword argument of `sorted()`, which sorts in increasing order by default:

```
sorted(
    [('abc', 121), ('abc', 231), ('abc', 148), ('abc', 221)], 
    key=lambda x: x[1]
)

```

`key` should be a function that identifies how to retrieve the comparable element from your data structure. In your case, it is the second element of the tuple, so we access `[1]`.

For optimization, see jamylak's response using `operator.itemgetter(1)`, which is essentially a faster version of `lambda x: x[1]`."
"Is there a way to grab a list of attributes that exist on instances of a class?

```
class new_class():
    def __init__(self, number):
        self.multi = int(number) * 2
        self.str = str(number)

a = new_class(2)
print(', '.join(a.SOMETHING))

```

The desired result is that ""multi, str"" will be output. I want this to see the current attributes from various parts of a script.","```
>>> class new_class():
...   def __init__(self, number):
...     self.multi = int(number) * 2
...     self.str = str(number)
... 
>>> a = new_class(2)
>>> a.__dict__
{'multi': 4, 'str': '2'}
>>> a.__dict__.keys()
dict_keys(['multi', 'str'])

```

You may also find [pprint](http://docs.python.org/py3k/library/pprint.html) helpful."
How do I raise an exception in Python so that it can later be caught via an `except` block?,"> How do I manually throw/raise an exception in Python?
> -----------------------------------------------------

[Use the most specific Exception constructor that semantically fits your issue](https://docs.python.org/3/library/exceptions.html#exception-hierarchy).

Be specific in your message, e.g.:

```
raise ValueError('A very specific bad thing happened.')

```

Don't raise generic exceptions
------------------------------

Avoid raising a generic `Exception`. To catch it, you'll have to catch all other more specific exceptions that subclass it.

### Problem 1: Hiding bugs

```
raise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.

```

For example:

```
def demo_bad_catch():
    try:
        raise ValueError('Represents a hidden bug, do not catch this')
        raise Exception('This is the exception you expect to handle')
    except Exception as error:
        print('Caught this error: ' + repr(error))

>>> demo_bad_catch()
Caught this error: ValueError('Represents a hidden bug, do not catch this',)

```

### Problem 2: Won't catch

And more specific catches won't catch the general exception:

```
def demo_no_catch():
    try:
        raise Exception('general exceptions not caught by specific handling')
    except ValueError as e:
        print('we will not catch exception: Exception')
 

>>> demo_no_catch()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in demo_no_catch
Exception: general exceptions not caught by specific handling

```

Best Practices: `raise` statement
---------------------------------

[Instead, use the most specific Exception constructor that semantically fits your issue](https://docs.python.org/3/library/exceptions.html#exception-hierarchy).

```
raise ValueError('A very specific bad thing happened')

```

which also handily allows an arbitrary number of arguments to be passed to the constructor:

```
raise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz') 

```

These arguments are accessed by the `args` attribute on the `Exception` object. For example:

```
try:
    some_code_that_may_raise_our_value_error()
except ValueError as err:
    print(err.args)

```

prints

```
('message', 'foo', 'bar', 'baz')    

```

In Python 2.5, an actual `message` attribute was added to `BaseException` in favor of encouraging users to subclass Exceptions and stop using `args`, but [the introduction of `message` and the original deprecation of args has been retracted](http://www.python.org/dev/peps/pep-0352/#retracted-ideas).

Best Practices: `except` clause
-------------------------------

When inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:

```
logger = logging.getLogger(__name__)

try:
    do_something_in_app_that_breaks_easily()
except AppError as error:
    logger.error(error)
    raise                 # just this!
    # raise AppError      # Don't do this, you'll lose the stack trace!

```

### Don't modify your errors... but if you insist.

You can preserve the stacktrace (and error value) with `sys.exc_info()`, but **this is way more error prone** and **has compatibility problems between Python 2 and 3**, prefer to use a bare `raise` to re-raise.

To explain - the `sys.exc_info()` returns the type, value, and traceback.

```
type, value, traceback = sys.exc_info()

```

This is the syntax in Python 2 - note this is not compatible with Python 3:

```
raise AppError, error, sys.exc_info()[2] # avoid this.
# Equivalently, as error *is* the second object:
raise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]

```

If you want to, you can modify what happens with your new raise - e.g. setting new `args` for the instance:

```
def error():
    raise ValueError('oops!')

def catch_error_modify_message():
    try:
        error()
    except ValueError:
        error_type, error_instance, traceback = sys.exc_info()
        error_instance.args = (error_instance.args[0] + ' <modification>',)
        raise error_type, error_instance, traceback

```

And we have preserved the whole traceback while modifying the args. Note that this is **not a best practice** and it is **invalid syntax** in Python 3 (making keeping compatibility much harder to work around).

```
>>> catch_error_modify_message()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in catch_error_modify_message
  File ""<stdin>"", line 2, in error
ValueError: oops! <modification>

```

In [Python 3](https://docs.python.org/3/reference/simple_stmts.html#the-raise-statement):

```
raise error.with_traceback(sys.exc_info()[2])

```

Again: avoid manually manipulating tracebacks. It's [less efficient](https://docs.python.org/2/reference/simple_stmts.html#the-raise-statement) and more error prone. And if you're using threading and `sys.exc_info` you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)

### Python 3, Exception chaining

In Python 3, you can chain Exceptions, which preserve tracebacks:

```
raise RuntimeError('specific message') from error

```

Be aware:

* this *does* allow changing the error type raised, and
* this is *not* compatible with Python 2.

### Deprecated Methods:

These can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, **but not the one intended!**

[Valid in Python 2, but not in Python 3](http://www.python.org/dev/peps/pep-3109/) is the following:

```
raise ValueError, 'message' # Don't do this, it's deprecated!

```

Only [valid in much older versions of Python](https://docs.python.org/2/whatsnew/2.5.html#pep-352-exceptions-as-new-style-classes) (2.4 and lower), you may still see people raising strings:

```
raise 'message' # really really wrong. don't do this.

```

In all modern versions, this will actually raise a `TypeError`, because you're not raising a `BaseException` type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.

Example Usage
-------------

I raise Exceptions to warn consumers of my API if they're using it incorrectly:

```
def api_func(foo):
    '''foo should be either 'baz' or 'bar'. returns something very useful.'''
    if foo not in _ALLOWED_ARGS:
        raise ValueError('{foo} wrong, use ""baz"" or ""bar""'.format(foo=repr(foo)))

```

Create your own error types when apropos
----------------------------------------

> **""I want to make an error on purpose, so that it would go into the except""**

You can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:

```
class MyAppLookupError(LookupError):
    '''raise this when there's a lookup error for my app'''

```

and usage:

```
if important_key not in resource_dict and not ok_to_be_missing:
    raise MyAppLookupError('resource is missing, and that is not ok.')

```"
"When I use the `re.search()` function to find matches in a block of text, the program exits once it finds the first match in the block of text.

How do I do this repeatedly where the program doesn't stop until ALL matches have been found? Is there a separate function to do this?","Use `re.findall` or `re.finditer` instead.

[`re.findall(pattern, string)`](http://docs.python.org/library/re.html#re.findall) returns a list of matching strings.

[`re.finditer(pattern, string)`](http://docs.python.org/library/re.html#re.finditer) returns an iterator over [`MatchObject`](https://docs.python.org/3/library/re.html#match-objects) objects.

**Example:**

```
re.findall( r'all (.*?) are', 'all cats are smarter than dogs, all dogs are dumber than cats')
# Output: ['cats', 'dogs']

[x.group() for x in re.finditer( r'all (.*?) are', 'all cats are smarter than dogs, all dogs are dumber than cats')]
# Output: ['all cats are', 'all dogs are']

```"
There seem to be many ways to define [singletons](http://en.wikipedia.org/wiki/Singleton_pattern) in Python. Is there a consensus opinion on StackÂ Overflow?,"I don't really see the need, as a module with functions (and not a class) would serve well as a singleton. All its variables would be bound to the module, which could not be instantiated repeatedly anyway.

If you do wish to use a class, there is no way of creating private classes or private constructors in Python, so you can't protect against multiple instantiations, other than just via convention in use of your API. I would still just put methods in a module, and consider the module as the singleton."
Is it possible to determine if the current script is running inside a virtualenv environment?,"The reliable and documented way is to compare [`sys.prefix`](https://docs.python.org/3/library/sys.html#sys.prefix) and [`sys.base_prefix`](https://docs.python.org/3/library/sys.html#sys.base_prefix). If they're equal, you're not in a virtual environment, otherwise you are. Inside a venv, `sys.prefix` points to the directory of the virtual environment, and `sys.base_prefix` to the Python interpreter used to create the environment.

This is documented under [*How venvs work*](https://docs.python.org/3/library/venv.html#how-venvs-work):

> It is sufficient to check `sys.prefix != sys.base_prefix` to determine if the current interpreter is running from a virtual environment.

This works for Python stdlib [`venv`](https://docs.python.org/3/library/venv.html) and for [`virtualenv`](https://pypi.org/project/virtualenv/) (since version 20):

```
def in_venv():
    return sys.prefix != sys.base_prefix

```

Older versions of `virtualenv` used `sys.real_prefix` instead of `sys.base_prefix`, and `sys.real_prefix` did not exist outside a virtual environment. In Python 3.3 and earlier `sys.base_prefix` did not ever exist. So a check that also handles some legacy cases could look like this:

```
import sys


def get_base_prefix_compat():
    """"""Get base/real prefix, or sys.prefix if there is none.""""""
    return (
        getattr(sys, ""base_prefix"", None)
        or getattr(sys, ""real_prefix"", None)
        or sys.prefix
    )


def in_virtualenv():
    return sys.prefix != get_base_prefix_compat()

```

*Using the `VIRTUAL_ENV` environment variable is not reliable*. It is set by the virtualenv `activate` shell script, but a virtualenv can be used without activation by directly running an executable from the virtualenv's `bin/` (or `Scripts`) directory, in which case `$VIRTUAL_ENV` will not be set. Or a non-virtualenv Python binary can be executed directly while a virtualenv is activated in the shell, in which case `$VIRTUAL_ENV` may be set in a Python process that is not actually running in that virtualenv."
"[PEP 8](http://www.python.org/dev/peps/pep-0008/) states:

> Imports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.

However if the class/method/function that I am importing is only used in rare cases, surely it is more efficient to do the import when it is needed?

Isn't this:

```
class SomeClass(object):

    def not_often_called(self)
        from datetime import datetime
        self.datetime = datetime.now()

```

more efficient than this?

```
from datetime import datetime

class SomeClass(object):

    def not_often_called(self)
        self.datetime = datetime.now()

```","Module importing is quite fast, but not instant. This means that:

* Putting the imports at the top of the module is fine, because it's a trivial cost that's only paid once.
* Putting the imports within a function will cause calls to that function to take longer.

So if you care about efficiency, put the imports at the top. Only move them into a function if your profiling shows that would help (you **did** profile to see where best to improve performance, right??)

---

The best reasons I've seen to perform lazy imports are:

* Optional library support. If your code has multiple paths that use different libraries, don't break if an optional library is not installed.
* In the `__init__.py` of a plugin, which might be imported but not actually used. Examples are Bazaar plugins, which use `bzrlib`'s lazy-loading framework."
"I would like to know how to check whether a string starts with ""hello"" in Python.

In Bash I usually do:

```
if [[ ""$string"" =~ ^hello ]]; then
 do something here
fi

```

How do I achieve the same in Python?","```
aString = ""hello world""
aString.startswith(""hello"")

```

More info about [`startswith`](https://docs.python.org/3/library/stdtypes.html?highlight=startswith#str.startswith)."
"Say I want to make a file:

```
filename = ""/foo/bar/baz.txt""

with open(filename, ""w"") as f:
    f.write(""FOOBAR"")

```

This gives an `IOError`, since `/foo/bar` does not exist.

What is the most pythonic way to generate those directories automatically? Is it necessary for me explicitly call `os.path.exists` and `os.mkdir` on every single one (i.e., /foo, then /foo/bar)?","In Python 3.2+, using the APIs requested by the OP, you can [elegantly](https://docs.python.org/3/library/os.html#os.makedirs) do the following:

```

import os

filename = ""/foo/bar/baz.txt""
os.makedirs(os.path.dirname(filename), exist_ok=True)
with open(filename, ""w"") as f:
    f.write(""FOOBAR"")


```

---

With the Pathlib module (introduced in Python 3.4), there is an alternate syntax (thanks David258):

```
from pathlib import Path
output_file = Path(""/foo/bar/baz.txt"")
output_file.parent.mkdir(exist_ok=True, parents=True)
output_file.write_text(""FOOBAR"")

```

---

In older python, there is a less elegant way:

The [`os.makedirs`](http://docs.python.org/library/os.html#os.makedirs) function does this. Try the following:

```
import os
import errno

filename = ""/foo/bar/baz.txt""
if not os.path.exists(os.path.dirname(filename)):
    try:
        os.makedirs(os.path.dirname(filename))
    except OSError as exc: # Guard against race condition
        if exc.errno != errno.EEXIST:
            raise

with open(filename, ""w"") as f:
    f.write(""FOOBAR"")


```

The reason to add the `try-except` block is to handle the case when the directory was created between the `os.path.exists` and the `os.makedirs` calls, so that to protect us from race conditions.

---"
"I want to find the position (or index) of the last occurrence of a certain substring in given input string `str`.

For example, suppose the input string is `str = 'hello'` and the substring is `target = 'l'`, then it should output 3.

How can I do this?","Use [`.rfind()`](http://docs.python.org/py3k/library/stdtypes.html#str.rfind):

```
>>> s = 'hello'
>>> s.rfind('l')
3

```

Also don't use `str` as variable name or you'll shadow the built-in [`str()`](http://docs.python.org/py3k/library/functions.html#str)."
"I'm trying to read a [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) file into Python ([Spyder](https://en.wikipedia.org/wiki/Spyder_%28software%29)), but I keep getting an error. My code:

```
import csv

data = open(""C:\Users\miche\Documents\school\jaar2\MIK\2.6\vektis_agb_zorgverlener"")
data = csv.reader(data)
print(data)

```

I get the following error:

> SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes
> in position 2-3: truncated \UXXXXXXXX escape

I have tried to replace the `\` with `\\` or with `/` and I've tried to put an `r` before `""C..`, but all these things didn't work.","This error occurs, because you are using a normal string as a path. You can use one of the three following solutions to fix your problem:

1: Just put `r` before your normal string. It converts a normal string to a raw string:

```
pandas.read_csv(r""C:\Users\DeePak\Desktop\myac.csv"")

```

2:

```
pandas.read_csv(""C:/Users/DeePak/Desktop/myac.csv"")

```

3:

```
pandas.read_csv(""C:\\Users\\DeePak\\Desktop\\myac.csv"")

```"
"While reading up the documentation for `dict.copy()`, it says that it makes a shallow copy of the dictionary. Same goes for the book I am following (Beazley's Python Reference), which says:

> The m.copy() method makes a shallow
> copy of the items contained in a
> mapping object and places them in a
> new mapping object.

Consider this:

```
>>> original = dict(a=1, b=2)
>>> new = original.copy()
>>> new.update({'c': 3})
>>> original
{'a': 1, 'b': 2}
>>> new
{'a': 1, 'c': 3, 'b': 2}

```

So I assumed this would update the value of `original` (and add 'c': 3) also since I was doing a shallow copy. Like if you do it for a list:

```
>>> original = [1, 2, 3]
>>> new = original
>>> new.append(4)
>>> new, original
([1, 2, 3, 4], [1, 2, 3, 4])

```

This works as expected.

Since both are shallow copies, why is that the `dict.copy()` doesn't work as I expect it to? Or my understanding of shallow vs deep copying is flawed?","By ""shallow copying"" it means the *content* of the dictionary is not copied by value, but just creating a new reference.

```
>>> a = {1: [1,2,3]}
>>> b = a.copy()
>>> a, b
({1: [1, 2, 3]}, {1: [1, 2, 3]})
>>> a[1].append(4)
>>> a, b
({1: [1, 2, 3, 4]}, {1: [1, 2, 3, 4]})

```

In contrast, a deep copy will copy all contents by value.

```
>>> import copy
>>> c = copy.deepcopy(a)
>>> a, c
({1: [1, 2, 3, 4]}, {1: [1, 2, 3, 4]})
>>> a[1].append(5)
>>> a, c
({1: [1, 2, 3, 4, 5]}, {1: [1, 2, 3, 4]})

```

So:

1. `b = a`: Reference assignment, Make `a` and `b` points to the same object.

   ![Illustration of 'a = b': 'a' and 'b' both point to '{1: L}', 'L' points to '[1, 2, 3]'.](https://i.sstatic.net/4AQC6.png)
2. `b = a.copy()`: Shallow copying, `a` and `b` will become two isolated objects, but their contents still share the same reference

   ![Illustration of 'b = a.copy()': 'a' points to '{1: L}', 'b' points to '{1: M}', 'L' and 'M' both point to '[1, 2, 3]'.](https://i.sstatic.net/Vtk4m.png)
3. `b = copy.deepcopy(a)`: Deep copying, `a` and `b`'s structure and content become completely isolated.

   ![Illustration of 'b = copy.deepcopy(a)': 'a' points to '{1: L}', 'L' points to '[1, 2, 3]'; 'b' points to '{1: M}', 'M' points to a different instance of '[1, 2, 3]'.](https://i.sstatic.net/BO4qO.png)"
"In Python, when should you use lists and when tuples?

Sometimes you don't have a choice, for example if you have

```
""hello %s you are %s years old"" % x

```

then x must be a tuple.

But if I am the one who designs the API and gets to choose the data types, then what are the guidelines?","[Tuples](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences) are fixed size in nature whereas [lists](https://docs.python.org/3/tutorial/introduction.html#lists) are dynamic.  
In other words, a `tuple` is **immutable** whereas a `list` is **mutable**.

1. You can't add elements to a tuple. Tuples have no append or extend method.
2. You can't remove elements from a tuple. Tuples have no remove or pop method.
3. You can find elements in a tuple, since this doesn’t change the tuple.
4. You can also use the `in` operator to check if an element exists in the tuple.

---

* **Tuples are faster than lists.** If you're defining a constant set of values and all you're ever going to do with it is iterate through it, use a tuple instead of a list.
* It makes your code safer if you “write-protect” data that does not need to be changed. Using a tuple instead of a list is like having an implied assert statement that this data is constant, and that special thought (and a specific function) is required to override that.
* Some tuples can be used as dictionary keys (specifically, tuples that contain immutable values like strings, numbers, and other tuples). Lists can never be used as dictionary keys, because lists are mutable.

Source: [Dive into Python 3](https://diveintopython3.net/native-datatypes.html#tuples)"
How do I output colored text to the terminal in Python?,"This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here's some Python code from the [Blender build scripts](https://svn.blender.org/svnroot/bf-blender/trunk/blender/build_files/scons/tools/bcolors.py):

```
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

```

To use code like this, you can do something like:

```
print(bcolors.WARNING + ""Warning: No active frommets remain. Continue?"" + bcolors.ENDC)

```

Or, with Python 3.6+:

```
print(f""{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}"")

```

This will work on unixes including OS X, Linux and Windows (provided you use [ANSICON](https://github.com/adoxa/ansicon), or in Windows 10 provided you enable [VT100 emulation](https://msdn.microsoft.com/en-us/library/mt638032)). There are ANSI codes for setting the color, moving the cursor, and more.

If you are going to get complicated with this (and it sounds like you are if you are writing a game), you should look into the ""[curses](https://en.wikipedia.org/wiki/Curses_%28programming_library%29)"" module, which handles a lot of the complicated parts of this for you. The [Python Curses HowTO](http://docs.python.org/howto/curses.html ""Python Curses howto"") is a good introduction.

If you are not using extended ASCII (i.e., not on a PC), you are stuck with the ASCII characters below 127, and '#' or '@' is probably your best bet for a block. If you can ensure your terminal is using a IBM [extended ASCII character set](http://telecom.tbi.net/asc-ibm.html), you have many more options. Characters 176, 177, 178 and 219 are the ""block characters"".

Some modern text-based programs, such as ""Dwarf Fortress"", emulate text mode in a graphical mode, and use images of the classic PC font. You can find some of these bitmaps that you can use on the [Dwarf Fortress Wiki](http://dwarffortresswiki.org/DF2014:Tilesets) see ([user-made tilesets](http://dwarffortresswiki.org/Tileset_repository)).

The [Text Mode Demo Contest](http://en.wikipedia.org/wiki/TMDC ""text mode demo contest"") has more resources for doing graphics in text mode."
"As we all know, there's list comprehension, like

```
[i for i in [1, 2, 3, 4]]

```

and there is dictionary comprehension, like

```
{i:j for i, j in {1: 'a', 2: 'b'}.items()}

```

but

```
(i for i in (1, 2, 3))

```

will end up in a generator, not a `tuple` comprehension. Why is that?

My guess is that a `tuple` is immutable, but this does not seem to be the answer.","You can use a generator expression:

```
tuple(i for i in (1, 2, 3))

```

but parentheses were already taken for â€¦ generator expressions."
"What's the best way to extend the User model (bundled with Django's authentication app) with custom fields? I would also possibly like to use the email as the username (for authentication purposes).

I've already seen a [few](http://scottbarnham.com/blog/2008/08/21/extending-the-django-user-model-with-inheritance/) [ways](http://www.b-list.org/weblog/2006/jun/06/django-tips-extending-user-model/) to do it, but can't decide on which one is the best.","The least painful and indeed Django-recommended way of doing this is through a `OneToOneField(User)` property.

> [Extending the existing User model](https://docs.djangoproject.com/en/dev/topics/auth/customizing/#extending-the-existing-user-model)
> -------------------------------------------------------------------------------------------------------------------------------------
>
> …
>
> If you wish to store information related to `User`, you can use a [one-to-one relationship](https://docs.djangoproject.com/en/dev/ref/models/fields/#ref-onetoone) to a model containing the fields for additional information. This one-to-one model is often called a profile model, as it might store non-auth related information about a site user.

That said, extending `django.contrib.auth.models.User` and supplanting it also works...

> [Substituting a custom User model](https://docs.djangoproject.com/en/dev/topics/auth/customizing/#substituting-a-custom-user-model)
> -----------------------------------------------------------------------------------------------------------------------------------
>
> Some kinds of projects may have authentication requirements for which Django’s built-in `User` model is not always appropriate. For instance, on some sites it makes more sense to use an email address as your identification token instead of a username.
>
> *[Ed: **Two warnings and a notification follow**, mentioning that this is **pretty drastic**.]*

I would definitely stay away from changing the actual User class in your Django source tree and/or copying and altering the auth module."
"It's well known that comparing floats for equality is a little fiddly due to rounding and precision issues. For examples on this, see the blog post *[Comparing Floating Point Numbers, 2012Â Edition](https://web.archive.org/web/20240316073323/https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)* by Bruce Dawson.

How do I deal with this in Python?

Is a standard library function for this available somewhere?","Python 3.5 adds the [`math.isclose` and `cmath.isclose` functions](https://docs.python.org/3/whatsnew/3.5.html#pep-485-a-function-for-testing-approximate-equality) as described in [PEP 485](http://www.python.org/dev/peps/pep-0485).

If you're using an earlier version of Python, the equivalent function is given in the [documentation](https://www.python.org/dev/peps/pep-0485/#proposed-implementation).

```
def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):
    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)

```

`rel_tol` is a relative tolerance, it is multiplied by the greater of the magnitudes of the two arguments; as the values get larger, so does the allowed difference between them while still considering them equal.

`abs_tol` is an absolute tolerance that is applied as-is in all cases. If the difference is less than either of those tolerances, the values are considered equal."
What is the best way (or are the various ways) to pretty print XML in Python?,"```
import xml.dom.minidom

dom = xml.dom.minidom.parse(xml_fname) # or xml.dom.minidom.parseString(xml_string)
pretty_xml_as_string = dom.toprettyxml()

```"
"I'm calling a function in Python which I know may stall and force me to restart the script.

How do I call the function or what do I wrap it in so that if it takes longer than 5 seconds the script cancels it and does something else?","You may use the [signal](http://docs.python.org/library/signal.html) package if you are running on UNIX:

```
In [1]: import signal

# Register an handler for the timeout
In [2]: def handler(signum, frame):
   ...:     print(""Forever is over!"")
   ...:     raise Exception(""end of time"")
   ...: 

# This function *may* run for an indetermined time...
In [3]: def loop_forever():
   ...:     import time
   ...:     while 1:
   ...:         print(""sec"")
   ...:         time.sleep(1)
   ...:         
   ...:         

# Register the signal function handler
In [4]: signal.signal(signal.SIGALRM, handler)
Out[4]: 0

# Define a timeout for your function
In [5]: signal.alarm(10)
Out[5]: 0

In [6]: try:
   ...:     loop_forever()
   ...: except Exception, exc:
   ...:     print(exc)
   ....: 
sec
sec
sec
sec
sec
sec
sec
sec
Forever is over!
end of time

# Cancel the timer if the function returned before timeout
# (ok, mine won't but yours maybe will :)
In [7]: signal.alarm(0)
Out[7]: 0

```

10 seconds after the call `signal.alarm(10)`, the handler is called. This raises an exception that you can intercept from the regular Python code.

This module doesn't play well with threads (but then, who does?)

**Note that** since we raise an exception when timeout happens, it may end up caught and ignored inside the function, for example of one such function:

```
def loop_forever():
    while 1:
        print('sec')
        try:
            time.sleep(10)
        except:
            continue

```"
"This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.

So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).

For example, let's say I want to pull the 1.2 value in `Btime` as a variable.

Whats the right way to do this?

```
>>> df_test
    ATime   X   Y   Z   Btime  C   D   E
0    1.2  2  15   2    1.2  12  25  12
1    1.4  3  12   1    1.3  13  22  11
2    1.5  1  10   6    1.4  11  20  16
3    1.6  2   9  10    1.7  12  29  12
4    1.9  1   1   9    1.9  11  21  19
5    2.0  0   0   0    2.0   8  10  11
6    2.4  0   0   0    2.4  10  12  15

```","To select the `ith` row, [use `iloc`](http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing-loc-iloc-and-ix):

```
In [31]: df_test.iloc[0]
Out[31]: 
ATime     1.2
X         2.0
Y        15.0
Z         2.0
Btime     1.2
C        12.0
D        25.0
E        12.0
Name: 0, dtype: float64

```

To select the ith value in the `Btime` column you could use:

```
In [30]: df_test['Btime'].iloc[0]
Out[30]: 1.2

```

---

There is a difference between `df_test['Btime'].iloc[0]` (recommended) and `df_test.iloc[0]['Btime']`:
------------------------------------------------------------------------------------------------------

DataFrames store data in column-based blocks (where each block has a single
dtype). If you select by column first, a *view* can be returned (which is
quicker than returning a copy) and the original dtype is preserved. In contrast,
if you select by row first, and if the DataFrame has columns of different
dtypes, then Pandas *copies* the data into a new Series of object dtype. So
selecting columns is a bit faster than selecting rows. Thus, although
`df_test.iloc[0]['Btime']` works, `df_test['Btime'].iloc[0]` is a little bit
more efficient.

There is a big difference between the two when it comes to assignment.
`df_test['Btime'].iloc[0] = x` affects `df_test`, but `df_test.iloc[0]['Btime']`
may not. See below for an explanation of why. Because a subtle difference in
the order of indexing makes a big difference in behavior, it is better to use single indexing assignment:

```
df.iloc[0, df.columns.get_loc('Btime')] = x

```

---

`df.iloc[0, df.columns.get_loc('Btime')] = x` (recommended):
------------------------------------------------------------

The **[recommended way](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-view-versus-copy)** to assign new values to a
DataFrame is to [avoid chained indexing](https://www.dataquest.io/blog/settingwithcopywarning/), and instead use the method [shown by
andrew](https://stackoverflow.com/a/32103253/190597),

```
df.loc[df.index[n], 'Btime'] = x

```

or

```
df.iloc[n, df.columns.get_loc('Btime')] = x

```

The latter method is a bit faster, because `df.loc` has to convert the row and column labels to
positional indices, so there is a little less conversion necessary if you use
`df.iloc` instead.

---

`df['Btime'].iloc[0] = x` works, but is not recommended:
--------------------------------------------------------

Although this works, it is taking advantage of the way DataFrames are *currently* implemented. There is no guarantee that Pandas has to work this way in the future. In particular, it is taking advantage of the fact that (currently) `df['Btime']` always returns a
view (not a copy) so `df['Btime'].iloc[n] = x` can be used to *assign* a new value
at the nth location of the `Btime` column of `df`.

Since Pandas makes no explicit guarantees about when indexers return a view versus a copy, assignments that use chained indexing generally always raise a `SettingWithCopyWarning` even though in this case the assignment succeeds in modifying `df`:

```
In [22]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])
In [24]: df['bar'] = 100
In [25]: df['bar'].iloc[0] = 99
/home/unutbu/data/binky/bin/ipython:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self._setitem_with_indexer(indexer, value)

In [26]: df
Out[26]: 
  foo  bar
0   A   99  <-- assignment succeeded
2   B  100
1   C  100

```

---

`df.iloc[0]['Btime'] = x` does not work:
----------------------------------------

In contrast, assignment with `df.iloc[0]['bar'] = 123` does not work because `df.iloc[0]` is returning a copy:

```
In [66]: df.iloc[0]['bar'] = 123
/home/unutbu/data/binky/bin/ipython:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy

In [67]: df
Out[67]: 
  foo  bar
0   A   99  <-- assignment failed
2   B  100
1   C  100

```

---

**Warning**: I had previously suggested `df_test.ix[i, 'Btime']`. But this is not guaranteed to give you the `ith` value since `ix` tries to index by *label* before trying to index by *position*. So if the DataFrame has an integer index which is not in sorted order starting at 0, then using `ix[i]` will return the row *labeled* `i` rather than the `ith` row. For example,

```
In [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])

In [2]: df
Out[2]: 
  foo
0   A
2   B
1   C

In [4]: df.ix[1, 'foo']
Out[4]: 'C'

```"
"For example I have two dicts:

```
Dict A: {'a': 1, 'b': 2, 'c': 3}
Dict B: {'b': 3, 'c': 4, 'd': 5}

```

I need a pythonic way of 'combining' two dicts such that the result is:

```
{'a': 1, 'b': 5, 'c': 7, 'd': 5}

```

That is to say: if a key appears in both dicts, add their values, if it appears in only one dict, keep its value.","Use [`collections.Counter`](http://docs.python.org/library/collections.html#collections.Counter):

```
>>> from collections import Counter
>>> A = Counter({'a':1, 'b':2, 'c':3})
>>> B = Counter({'b':3, 'c':4, 'd':5})
>>> A + B
Counter({'c': 7, 'b': 5, 'd': 5, 'a': 1})

```

Counters are basically a subclass of `dict`, so you can still do everything else with them you'd normally do with that type, such as iterate over their keys and values."
"Given a `datetime.time` value in Python, is there a standard way to add an integer number of seconds to it, so that `11:34:59` + 3 = `11:35:02`, for example?

These obvious ideas don't work:

```
>>> datetime.time(11, 34, 59) + 3
TypeError: unsupported operand type(s) for +: 'datetime.time' and 'int'
>>> datetime.time(11, 34, 59) + datetime.timedelta(0, 3)
TypeError: unsupported operand type(s) for +: 'datetime.time' and 'datetime.timedelta'
>>> datetime.time(11, 34, 59) + datetime.time(0, 0, 3)
TypeError: unsupported operand type(s) for +: 'datetime.time' and 'datetime.time'

```

In the end I have written functions like this:

```
def add_secs_to_time(timeval, secs_to_add):
    secs = timeval.hour * 3600 + timeval.minute * 60 + timeval.second
    secs += secs_to_add
    return datetime.time(secs // 3600, (secs % 3600) // 60, secs % 60)

```

I can't help thinking that I'm missing an easier way to do this though.

### Related

* [python time + timedelta equivalent](https://stackoverflow.com/questions/656297/python-time-timedelta-equivalent)","You can use full `datetime` variables with `timedelta`, and by providing a dummy date then using `time` to just get the time value.

For example:

```
import datetime
a = datetime.datetime(100,1,1,11,34,59)
b = a + datetime.timedelta(0,3) # days, seconds, then other fields.
print(a.time())
print(b.time())

```

results in the two values, three seconds apart:

```
11:34:59
11:35:02

```

You could also opt for the more readable

```
b = a + datetime.timedelta(seconds=3)

```

if you're so inclined.

---

If you're after a function that can do this, you can look into using `addSecs` below:

```
import datetime

def addSecs(tm, secs):
    fulldate = datetime.datetime(100, 1, 1, tm.hour, tm.minute, tm.second)
    fulldate = fulldate + datetime.timedelta(seconds=secs)
    return fulldate.time()

a = datetime.datetime.now().time()
b = addSecs(a, 300)
print(a)
print(b)

```

This outputs:

```
 09:11:55.775695
 09:16:55

```"
"How can I tell what mode the shell is in, from within the shell?

I've tried looking at the [platform](http://docs.python.org/library/platform.html) module, but it seems only to tell you about ""the bit architecture and the linkage format used for the executable"". My binary is compiled as 64bit (I'm running on OS X 10.6), so it seems to always report 64bit even though I'm using the methods [described here](https://developer.apple.com/mac/library/documentation/Darwin/Reference/ManPages/man1/python.1.html) to force 32bit mode.","One way is to look at `sys.maxsize` as documented [here](http://docs.python.org/library/platform.html#cross-platform):

```
$ python-32 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize > 2**32)'
('7fffffff', False)
$ python-64 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize > 2**32)'
('7fffffffffffffff', True)

```

On Windows, run the same commands formatted as follows:

```
python -c ""import sys;print(\""%x\"" % sys.maxsize, sys.maxsize > 2**32)""

```

`sys.maxsize` was introduced in Python 2.6. If you need a test for older systems, this slightly more complicated test should work on all Python 2 and 3 releases:

```
$ python-32 -c 'import struct;print( 8 * struct.calcsize(""P""))'
32
$ python-64 -c 'import struct;print( 8 * struct.calcsize(""P""))'
64

```

BTW, you might be tempted to use `platform.architecture()` for this. Unfortunately, its results are not always reliable, [particularly in the case of OS X universal binaries](http://docs.python.org/library/platform.html#platform.architecture).

```
$ arch -x86_64 /usr/bin/python2.6 -c 'import sys,platform; print platform.architecture()[0], sys.maxsize > 2**32'
64bit True
$ arch -i386 /usr/bin/python2.6 -c 'import sys,platform; print platform.architecture()[0], sys.maxsize > 2**32'
64bit False

```"
"I'm trying out Python's type annotations with abstract base classes to write some interfaces. Is there a way to annotate the possible types of `*args` and `**kwargs`?

For example, how would one express that the sensible arguments to a function are either an `int` or two `int`s? `type(args)` gives `Tuple` so my guess was to annotate the type as `Union[Tuple[int, int], Tuple[int]]`, but this doesn't work.

```
from typing import Union, Tuple

def foo(*args: Union[Tuple[int, int], Tuple[int]]):
    try:
        i, j = args
        return i + j
    except ValueError:
        assert len(args) == 1
        i = args[0]
        return i

# ok
print(foo((1,)))
print(foo((1, 2)))
# mypy does not like this
print(foo(1))
print(foo(1, 2))

```

Error messages from mypy:

```
t.py: note: In function ""foo"":
t.py:6: error: Unsupported operand types for + (""tuple"" and ""Union[Tuple[int, int], Tuple[int]]"")
t.py: note: At top level:
t.py:12: error: Argument 1 to ""foo"" has incompatible type ""int""; expected ""Union[Tuple[int, int], Tuple[int]]""
t.py:14: error: Argument 1 to ""foo"" has incompatible type ""int""; expected ""Union[Tuple[int, int], Tuple[int]]""
t.py:15: error: Argument 1 to ""foo"" has incompatible type ""int""; expected ""Union[Tuple[int, int], Tuple[int]]""
t.py:15: error: Argument 2 to ""foo"" has incompatible type ""int""; expected ""Union[Tuple[int, int], Tuple[int]]""

```

It makes sense that mypy doesn't like this for the function call because it expects there to be a `tuple` in the call itself. The addition after unpacking also gives a typing error that I don't understand.

How does one annotate the sensible types for `*args` and `**kwargs`?","For variable positional arguments (`*args`) and variable keyword arguments (`**kw`) you only need to specify the expected value for *one* such argument.

From the [*Arbitrary argument lists and default argument values* section](https://www.python.org/dev/peps/pep-0484/#arbitrary-argument-lists-and-default-argument-values) of the *Type Hints* PEP:

> Arbitrary argument lists can as well be type annotated, so that the definition:
>
> ```
> def foo(*args: str, **kwds: int): ...
>
> ```
>
> is acceptable and it means that, e.g., all of the following represent function calls with valid types of arguments:
>
> ```
> foo('a', 'b', 'c')
> foo(x=1, y=2)
> foo('', z=0)
>
> ```

So you'd want to specify your method like this:

```
def foo(*args: int):

```

However, if your function can only accept either one or two integer values, you should not use `*args` at all, use one explicit positional argument and a second keyword argument:

```
def foo(first: int, second: Optional[int] = None):

```

Now your function is actually limited to one or two arguments, and both must be integers if specified. `*args` *always* means 0 or more, and can't be limited by type hints to a more specific range."
"Why is `super()` used?

Is there a difference between using `Base.__init__` and `super().__init__`?

```
class Base(object):
    def __init__(self):
        print ""Base created""
        
class ChildA(Base):
    def __init__(self):
        Base.__init__(self)
        
class ChildB(Base):
    def __init__(self):
        super(ChildB, self).__init__()
        
ChildA() 
ChildB()

```","`super()` lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of [fun stuff](http://www.artima.com/weblogs/viewpost.jsp?thread=236275) can happen. See the [standard docs on super](https://docs.python.org/2/library/functions.html#super) if you haven't already.

Note that [the syntax changed in Python 3.0](https://docs.python.org/3/library/functions.html#super): you can just say `super().__init__()` instead of `super(ChildB, self).__init__()` which IMO is quite a bit nicer. The standard docs also refer to a [guide to using `super()`](https://rhettinger.wordpress.com/2011/05/26/super-considered-super/) which is quite explanatory."
"How can you produce the following list with `range()` in Python?

```
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]

```","Use `reversed()` function (efficient since `range` implements [`__reversed__`](https://docs.python.org/3/reference/datamodel.html#object.__reversed__)):

```
reversed(range(10))

```

It's much more meaningful.

**Update:** `list` cast

If you want it to be a `list` (as @btk pointed out):

```
list(reversed(range(10)))

```

**Update:** `range`-only solution

If you want to use only `range` to achieve the same result, you can use all its parameters. `range(start, stop, step)`

For example, to generate a list `[3, 2, 1, 0]`, you can use the following:

```
range(3, -1, -1)

```

It may be less intuitive, but it works the same with less text. [This answer by @Wolf](https://stackoverflow.com/a/44519681) indicates this approach is slightly faster than `reversed`."
"`touch` is a Unix utility that sets the modification and access times of files to the current time of day. If the file doesn't exist, it is created with default permissions.

How would you implement it as a Python function? Try to be cross platform and complete.

(Current Google results for ""python touch file"" are not that great, but point to [os.utime](http://docs.python.org/library/os.html#os.utime).)","Looks like this is new as of Python 3.4 - [`pathlib`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.touch).

```
from pathlib import Path

Path('path/to/file.txt').touch()

```

This will create a `file.txt` at the path.

--

> Path.touch(mode=0o777, exist\_ok=True)
>
> Create a file at this given path. If mode is given, it is combined with the processâ€™ umask value to determine the file mode and access flags. If the file already exists, the function succeeds if exist\_ok is true (and its modification time is updated to the current time), otherwise FileExistsError is raised."
"When should you use generator expressions and when should you use list comprehensions in Python?

```
# Generator expression
(x*2 for x in range(256))

# List comprehension
[x*2 for x in range(256)]

```","[John's answer](https://stackoverflow.com/a/47792/4518341) is good (that list comprehensions are better when you want to iterate over something multiple times). However, it's also worth noting that you should use a list if you want to use any of the list methods. For example, the following code won't work:

```
def gen():
    return (something for something in get_some_stuff())

print gen()[:2]     # generators don't support indexing or slicing
print [5,6] + gen() # generators can't be added to lists

```

Basically, use a generator expression if all you're doing is iterating once. If you want to store and use the generated results, then you're probably better off with a list comprehension.

Since performance is the most common reason to choose one over the other, my advice is to not worry about it and just pick one; if you find that your program is running too slowly, then and only then should you go back and worry about tuning your code."
"How would I compare two dates to see which is later, using Python?

For example, I want to check if the current date is past the last date in this list I am creating, of holiday dates, so that it will send an email automatically, telling the admin to update the holiday.txt file.","Use the `datetime` method and the operator `<` and its kin.

```
>>> from datetime import datetime, timedelta
>>> past = datetime.now() - timedelta(days=1)
>>> present = datetime.now()
>>> past < present
True
>>> datetime(3000, 1, 1) < present
False
>>> present - datetime(2000, 4, 4)
datetime.timedelta(4242, 75703, 762105)

```"
"How can I get the Cartesian product (every possible combination of values) from a group of lists?

For example, given

```
somelists = [
   [1, 2, 3],
   ['a', 'b'],
   [4, 5]
]

```

How do I get this?

```
[(1, 'a', 4), (1, 'a', 5), (1, 'b', 4), (1, 'b', 5), (2, 'a', 4), (2, 'a', 5), ...]

```

---

One common application for this technique is to avoid deeply nested loops. See [Avoiding nested for loops](https://stackoverflow.com/questions/11174745) for a more specific duplicate. Similarly, this technique might be used to ""explode"" a dictionary with list values; see [Combine Python Dictionary Permutations into List of Dictionaries](https://stackoverflow.com/questions/15211568) .

If you want a Cartesian product of *the same* list with itself multiple times, `itertools.product` can handle that elegantly. See [Operation on every pair of element in a list](https://stackoverflow.com/questions/942543) or [How can I get ""permutations with repetitions"" from a list (Cartesian product of a list with itself)?](https://stackoverflow.com/questions/3099987).

Many people who already know about `itertools.product` struggle with the fact that it expects separate arguments for each input sequence, rather than e.g. a list of lists. The accepted answer shows how to handle this with `*`. However, the use of `*` here to unpack arguments is **fundamentally not different** from any other time it's used in a function call. Please see [Expanding tuples into arguments](https://stackoverflow.com/questions/1993727) for this topic (and use that instead to close duplicate questions, as appropriate).","Use [`itertools.product`](https://docs.python.org/3/library/itertools.html#itertools.product), which has been available since Python 2.6.

```
import itertools

somelists = [
   [1, 2, 3],
   ['a', 'b'],
   [4, 5]
]
for element in itertools.product(*somelists):
    print(element)

```

This is the same as:

```
for element in itertools.product([1, 2, 3], ['a', 'b'], [4, 5]):
    print(element)

```"
"I'm working on a simple script that involves CAS, jspring security check, redirection, etc. I would like to use Kenneth Reitz's python requests because it's a great piece of work! However, CAS requires getting validated via SSL so I have to get past that step first. I don't know what Python requests is wanting? Where is this SSL certificate supposed to reside?

```
Traceback (most recent call last):
  File ""./test.py"", line 24, in <module>
  response = requests.get(url1, headers=headers)
  File ""build/bdist.linux-x86_64/egg/requests/api.py"", line 52, in get
  File ""build/bdist.linux-x86_64/egg/requests/api.py"", line 40, in request
  File ""build/bdist.linux-x86_64/egg/requests/sessions.py"", line 209, in request 
  File ""build/bdist.linux-x86_64/egg/requests/models.py"", line 624, in send
  File ""build/bdist.linux-x86_64/egg/requests/models.py"", line 300, in _build_response
  File ""build/bdist.linux-x86_64/egg/requests/models.py"", line 611, in send
requests.exceptions.SSLError: [Errno 1] _ssl.c:503: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed

```","The problem you are having is caused by an untrusted SSL certificate.

Like @dirk mentioned in a previous comment, the *quickest* fix is setting `verify=False`:

```
requests.get('https://example.com', verify=False)

```

Please note that this will cause the certificate not to be verified. **This will expose your application to security risks, such as man-in-the-middle attacks.**

Of course, apply judgment. As mentioned in the comments, this *may* be acceptable for quick/throwaway applications/scripts, *but really should not go to production software*.

If just skipping the certificate check is not acceptable in your particular context, consider the following options, your best option is to set the `verify` parameter to a string that is the path of the `.pem` file of the certificate (which you should obtain by some sort of secure means).

So, as of version 2.0, the `verify` parameter accepts the following values, with their respective semantics:

* `True`: causes the certificate to validated against the library's own trusted certificate authorities (Note: you can see which Root Certificates Requests uses via the Certifi library, a trust database of RCs extracted from Requests: [Certifi - Trust Database for Humans](http://certifiio.readthedocs.org/en/latest/ ""Certifi - Trust Database for Humans"")).
* `False`: bypasses certificate validation *completely*.
* Path to a CA\_BUNDLE file for Requests to use to validate the certificates.

Source: [Requests - SSL Cert Verification](https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification)

Also take a look at the `cert` parameter on the same link."
"I have the following string: `""aaaabbbb""`

How can I get the last four characters and store them in a string using Python?","Like this:

```
>>> mystr = ""abcdefghijkl""
>>> mystr[-4:]
'ijkl'

```

This [slices](https://docs.python.org/3/reference/expressions.html#slicings) the string's last 4 characters. The -4 starts the range from the string's end. A modified expression with `[:-4]` removes the same 4 characters from the end of the string:

```
>>> mystr[:-4]
'abcdefgh'

```

For more information on slicing see [this Stack Overflow answer](https://stackoverflow.com/a/509295/8117067)."
"What is the `__main__.py` file for, what sort of code should I put into it, and when should I have one?","Often, a Python program is run by naming a .py file on the command line:

```
$ python my_program.py

```

You can also create a directory or zipfile full of code, and include a `__main__.py`. Then you can simply name the directory or zipfile on the command line, and it executes the `__main__.py` automatically:

```
$ python my_program_dir
$ python my_program.zip
# Or, if the program is accessible as a module
$ python -m my_program

```

You'll have to decide for yourself whether your application could benefit from being executed like this.

---

Note that a `__main__` *module* usually doesn't come from a `__main__.py` file. It can, but it usually doesn't. When you run a script like `python my_program.py`, the script will run as the `__main__` module instead of the `my_program` module. This also happens for modules run as `python -m my_module`, or in several other ways.

If you saw the name `__main__` in an error message, that doesn't necessarily mean you should be looking for a `__main__.py` file."
"Let's say I have a string `'gfgfdAAA1234ZZZuijjk'` and I want to extract just the `'1234'` part.

I only know what will be the few characters directly before `AAA`, and after `ZZZ` the part I am interested in `1234`.

With `sed` it is possible to do something like this with a string:

```
echo ""$STRING"" | sed -e ""s|.*AAA\(.*\)ZZZ.*|\1|""

```

And this will give me `1234` as a result.

How to do the same thing in Python?","Using regular expressions - [documentation](http://docs.python.org/2/library/re.html) for further reference

```
import re

text = 'gfgfdAAA1234ZZZuijjk'

m = re.search('AAA(.+?)ZZZ', text)
if m:
    found = m.group(1)

# found: 1234

```

or:

```
import re

text = 'gfgfdAAA1234ZZZuijjk'

try:
    found = re.search('AAA(.+?)ZZZ', text).group(1)
except AttributeError:
    # AAA, ZZZ not found in the original string
    found = '' # apply your error handling

# found: 1234

```"
"I want to take two lists and find the values that appear in both.

```
a = [1, 2, 3, 4, 5]
b = [9, 8, 7, 6, 5]

returnMatches(a, b)

```

would return `[5]`, for instance.","Not the most efficient one, but by far the most obvious way to do it is:

```
>>> a = [1, 2, 3, 4, 5]
>>> b = [9, 8, 7, 6, 5]
>>> set(a) & set(b)
{5}

```

if order is significant you can do it with list comprehensions like this:

```
>>> [i for i, j in zip(a, b) if i == j]
[5]

```

(only works for equal-sized lists, which order-significance implies)."
How do I put a time delay in a Python script?,"This delays for 2.5 seconds:

```
import time

time.sleep(2.5)

```

---

Here is another example where something is run approximately once a minute:

```
import time

while True:
    print(""This prints once a minute."")
    time.sleep(60) # Delay for 1 minute (60 seconds).

```"
"If I have a class ...

```
class MyClass:

    def method(arg):
        print(arg)

```

... which I use to create an object ...

```
my_object = MyClass()

```

... on which I call `method(""foo"")` like so ...

```
>>> my_object.method(""foo"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: method() takes exactly 1 positional argument (2 given)

```

... why does Python tell me I gave it two arguments, when I only gave one?","In Python, this:

```
my_object.method(""foo"")

```

... is [syntactic sugar](http://en.wikipedia.org/wiki/Syntactic_sugar), which the interpreter translates behind the scenes into:

```
MyClass.method(my_object, ""foo"")

```

... which, as you can see, does indeed have two arguments - it's just that the first one is implicit, from the point of view of the caller.

This is because most methods do some work with the object they're called on, so there needs to be some way for that object to be referred to inside the method. By convention, this first argument is called `self` inside the method definition:

```
class MyNewClass:

    def method(self, arg):
        print(self)
        print(arg)

```

If you call `method(""foo"")` on an instance of `MyNewClass`, it works as expected:

```
>>> my_new_object = MyNewClass()
>>> my_new_object.method(""foo"")
<__main__.MyNewClass object at 0x29045d0>
foo

```

Occasionally (but not often), you really *don't* care about the object that your method is bound to, and in that circumstance, you can [decorate](https://docs.python.org/3/glossary.html#term-decorator) the method with the builtin [`staticmethod()`](https://docs.python.org/3/library/functions.html#staticmethod) function to say so:

```
class MyOtherClass:

    @staticmethod
    def method(arg):
        print(arg)

```

... in which case you don't need to add a `self` argument to the method definition, and it still works:

```
>>> my_other_object = MyOtherClass()
>>> my_other_object.method(""foo"")
foo

```"
Why are there no `++` and `--` operators in Python?,"It's not because it doesn't make sense; it makes perfect sense to define ""x++"" as ""x += 1, evaluating to the previous binding of x"".

If you want to know the original reason, you'll have to either wade through old Python mailing lists or ask somebody who was there (eg. Guido), but it's easy enough to justify after the fact:

Simple increment and decrement aren't needed as much as in other languages. You don't write things like `for(int i = 0; i < 10; ++i)` in Python very often; instead you do things like `for i in range(0, 10)`.

Since it's not needed nearly as often, there's much less reason to give it its own special syntax; when you do need to increment, `+=` is usually just fine.

It's not a decision of whether it makes sense, or whether it can be done--it does, and it can. It's a question of whether the benefit is worth adding to the core syntax of the language. Remember, this is *four* operators--postinc, postdec, preinc, predec, and each of these would need to have its own class overloads; they all need to be specified, and tested; it would add opcodes to the language (implying a larger, and therefore slower, VM engine); every class that supports a logical increment would need to implement them (on top of `+=` and `-=`).

This is all redundant with `+=` and `-=`, so it would become a net loss."
"Is it possible to create a dictionary comprehension in Python (for the keys)?

Without list comprehensions, you can use something like this:

```
l = []
for n in range(1, 11):
    l.append(n)

```

We can shorten this to a list comprehension: `l = [n for n in range(1, 11)]`.

However, say I want to set a dictionary's keys to the same value.
I can do:

```
d = {}
for n in range(1, 11):
     d[n] = True # same value for each

```

I've tried this:

```
d = {}
d[i for i in range(1, 11)] = True

```

However, I get a `SyntaxError` on the `for`.

In addition (I don't need this part, but just wondering), can you set a dictionary's keys to a bunch of different values, like this:

```
d = {}
for n in range(1, 11):
    d[n] = n

```

Is this possible with a dictionary comprehension?

```
d = {}
d[i for i in range(1, 11)] = [x for x in range(1, 11)]

```

This also raises a `SyntaxError` on the `for`.","There are [dictionary comprehensions in Python 2.7+](https://www.python.org/dev/peps/pep-0274/), but they don't work quite the way you're trying. Like a list comprehension, they create a *new* dictionary; you can't use them to add keys to an existing dictionary. Also, you have to specify the keys and values, although of course you can specify a dummy value if you like.

```
>>> d = {n: n**2 for n in range(5)}
>>> print d
{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}

```

If you want to set them all to True:

```
>>> d = {n: True for n in range(5)}
>>> print d
{0: True, 1: True, 2: True, 3: True, 4: True}

```

What you seem to be asking for is a way to set multiple keys at once on an existing dictionary. There's no direct shortcut for that. You can either loop like you already showed, or you could use a dictionary comprehension to create a new dict with the new values, and then do `oldDict.update(newDict)` to merge the new values into the old dict."
"How do I add an optional flag to my command line args?

eg. so I can write

```
python myprog.py 

```

or

```
python myprog.py -w

```

I tried

```
parser.add_argument('-w')

```

But I just get an error message saying

```
Usage [-w W]
error: argument -w: expected one argument

```

which I take it means that it wants an argument value for the -w option. What's the way of just accepting a flag?

I'm finding <http://docs.python.org/library/argparse.html> rather opaque on this question.","As you have it, the argument `w` is expecting a value after `-w` on the command line. If you are just looking to flip a switch by setting a variable `True` or `False`, have a look [here](http://docs.python.org/dev/library/argparse.html#action) (specifically store\_true and store\_false)

```
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('-w', action='store_true')

```

where `action='store_true'` implies `default=False`.

Conversely, you could have`action='store_false'`, which implies `default=True`."
"I am using Python version 2.5.4 and install MySQL version 5.0 and Django. Django is working fine with Python, but not MySQL. I am using it in Windows Vista.","You need to use one of the following commands. Which one depends on what OS and software you have and use.

1. **easy\_install mysql-python** (mix os)
2. **pip install mysql-python** (mix os/ python 2)
3. **pip install mysqlclient** (mix os/ python 3)
4. **apt-get install python-mysqldb** (Linux Ubuntu, ...)
5. **cd /usr/ports/databases/py-MySQLdb && make install clean** (FreeBSD)
6. **yum install MySQL-python** (Linux Fedora, CentOS ...)

For Windows, see this answer: [Install mysql-python (Windows)](https://stackoverflow.com/q/21440230/4646678)"
"I see a lot on converting a date string to an `datetime` object in Python, but I want to go the other way.  
I've got

```
datetime.datetime(2012, 2, 23, 0, 0)

```

and I would like to convert it to string like `'2/23/2012'`.","You can use [strftime](http://docs.python.org/2/library/time.html#time.strftime) to help you format your date.

E.g.,

```
import datetime
t = datetime.datetime(2012, 2, 23, 0, 0)
t.strftime('%m/%d/%Y')

```

will yield:

```
'02/23/2012'

```

More information about formatting see [here](http://docs.python.org/library/datetime.html#strftime-strptime-behavior)"
"How do I find out which directories are listed in my systemâ€™s `PYTHONPATH` variable, from within a Python script (or the interactive shell)?","You would probably also want this:

```
import sys
print(sys.path)

```

Or as a one liner from the terminal:

```
python -c ""import sys; print('\n'.join(sys.path))""

```

---

**Caveat**: If you have multiple versions of Python installed you should use a corresponding command `python2` or `python3`."
"I would like to create views or dataframes from an existing dataframe based on column selections.

For example, I would like to create a dataframe `df2` from a dataframe `df1` that holds all columns from it except two of them. I tried doing the following, but it didn't work:

```
import numpy as np
import pandas as pd

# Create a dataframe with columns A,B,C and D
df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))

# Try to create a second dataframe df2 from df with all columns except 'B' and D
my_cols = set(df.columns)
my_cols.remove('B').remove('D')

# This returns an error (""unhashable type: set"")
df2 = df[my_cols]

```

What am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and **exclusions** of arbitrary sets of columns from a dataframe?","You can either Drop the columns you do not need OR Select the ones you need

```
# Using DataFrame.drop
df.drop(df.columns[[1, 2]], axis=1, inplace=True)

# drop by Name
df1 = df1.drop(['B', 'C'], axis=1)

# Select the ones you want
df1 = df[['a','d']]

```"
"Suppose I have a dictionary of lists:

```
d = {'a': [1], 'b': [1, 2], 'c': [], 'd':[]}

```

Now I want to remove key-value pairs where the values are empty lists. I tried this code:

```
for i in d:
    if not d[i]:
        d.pop(i)

```

but this gives an error:

```
RuntimeError: dictionary changed size during iteration

```

I understand that entries can't be added or removed from a dictionary while iterating through it. How can I work around this limitation in order to solve the problem?

---

See [Modifying a Python dict while iterating over it](https://stackoverflow.com/questions/6777485/) for citations that this can cause problems, and why.","In Python 3.x and 2.x you can use use [`list`](https://docs.python.org/3/library/stdtypes.html#list) to force a copy of the keys to be made:

```
for i in list(d):

```

In Python 2.x calling [`.keys`](https://docs.python.org/3/library/stdtypes.html#dict.keys) made a copy of the keys that you could iterate over while modifying the `dict`:

```
for i in d.keys():

```

but on Python 3.x, `.keys` returns a [view object](https://docs.python.org/3/library/stdtypes.html#dict-views) instead, so it won't fix your error."
"This is probably a trivial question, but how do I parallelize the following loop in python?

```
# setup output lists
output1 = list()
output2 = list()
output3 = list()

for j in range(0, 10):
    # calc individual parameter value
    parameter = j * offset
    # call the calculation
    out1, out2, out3 = calc_stuff(parameter = parameter)

    # put results into correct output list
    output1.append(out1)
    output2.append(out2)
    output3.append(out3)

```

I know how to start single threads in Python but I don't know how to ""collect"" the results.

Multiple processes would be fine too - whatever is easiest for this case. I'm using currently Linux but the code should run on Windows and Mac as-well.

What's the easiest way to parallelize this code?","The CPython implementation currently has a *global interpreter lock* (GIL) that prevents threads of the same interpreter from concurrently executing Python code. This means CPython threads are useful for concurrent I/O-bound workloads, but usually not for CPU-bound workloads. The naming `calc_stuff()` indicates that your workload is CPU-bound, so you want to use multiple processes here (which is often the better solution for CPU-bound workloads anyway, regardless of the GIL).

There are two easy ways of creating a process pool into the Python standard library. The first one is the `multiprocessing` module, which can be used like this:

```
pool = multiprocessing.Pool(4)
out1, out2, out3 = zip(*pool.map(calc_stuff, range(0, 10 * offset, offset)))

```

Note that this won't work in the interactive interpreter due to the way `multiprocessing` is implemented.

The second way to create a process pool is [`concurrent.futures.ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor):

```
with concurrent.futures.ProcessPoolExecutor() as pool:
    out1, out2, out3 = zip(*pool.map(calc_stuff, range(0, 10 * offset, offset)))

```

This uses the `multiprocessing` module under the hood, so it behaves identically to the first version."
How do I change the size of figure drawn with Matplotlib?,"[`figure`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html) tells you the call signature:

```
from matplotlib.pyplot import figure

figure(figsize=(8, 6), dpi=80)

```

`figure(figsize=(1,1))` would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument."
"```
class A:
    def __init__(self):
        print(""world"")

class B(A):
    def __init__(self):
       print(""hello"")

B()  # output: hello

```

In all other languages I've worked with the super constructor is invoked implicitly. How does one invoke it in Python? I would expect `super(self)` but this doesn't work.","In line with the other answers, there are multiple ways to call super class methods (including the constructor), however in Python 3 the process has been simplified:

### Python 3

```
class A(object):
    def __init__(self):
        print(""world"")

class B(A):
    def __init__(self):
        print(""hello"")
        super().__init__()

```

### Python 2

In Python 2, you have to call the slightly more verbose version `super(<containing classname>, self)`, which is equivalent to `super()`as per [the docs](https://docs.python.org/3.5/library/functions.html#super).

```
class A(object):
    def __init__(self):
        print ""world""

class B(A):
    def __init__(self):
        print ""hello""
        super(B, self).__init__()

```"
"What exactly are the Python scoping rules?

If I have some code:

```
code1
class Foo:
   code2
   def spam.....
      code3
      for code4..:
       code5
       x()

```

Where is `x` found? Some possible choices include the list below:

1. In the enclosing source file
2. In the class namespace
3. In the function definition
4. In the for loop index variable
5. Inside the for loop

Also there is the context during execution, when the function `spam` is passed somewhere else. And maybe [lambda functions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) pass a bit differently?

There must be a simple reference or algorithm somewhere. It's a confusing world for intermediate Python programmers.","Actually, a concise rule for Python Scope resolution, from [Learning Python, 3rd. Ed.](https://rads.stackoverflow.com/amzn/click/com/0596513984). (These rules are specific to variable names, not attributes. If you reference it without a period, these rules apply.)

**LEGB Rule**

* **L**ocal — Names assigned in any way within a function (`def` or `lambda`), and not declared global in that function
* **E**nclosing-function — Names assigned in the local scope of any and all statically enclosing functions (`def` or `lambda`), from inner to outer
* **G**lobal (module) — Names assigned at the top-level of a module file, or by executing a `global` statement in a `def` within the file
* **B**uilt-in (Python) — Names preassigned in the built-in names module: `open`, `range`, `SyntaxError`, etc

So, in the case of

```
code1
class Foo:
    code2
    def spam():
        code3
        for code4:
            code5
            x()

```

The `for` loop does not have its own namespace. In LEGB order, the scopes would be

* L: Local in `def spam` (in `code3`, `code4`, and `code5`)
* E: Any enclosing functions (if the whole example were in another `def`)
* G: Were there any `x` declared globally in the module (in `code1`)?
* B: Any builtin `x` in Python.

`x` will never be found in `code2` (even in cases where you might expect it would, see [Antti's answer](https://stackoverflow.com/a/23471004/2810305) or [here](https://stackoverflow.com/q/13905741/2810305))."
"I am using

```
import requests
requests.post(url='https://foo.example', data={'bar':'baz'})

```

but I get a request.exceptions.SSLError.
The website has an expired certficate, but I am not sending sensitive data, so it doesn't matter to me.
I would imagine there is an argument like 'verifiy=False' that I could use, but I can't seem to find it.","From [the documentation](https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification):

> `requests` can also ignore verifying the SSL certificate if you set
> `verify` to False.
>
> ```
> >>> requests.get('https://kennethreitz.com', verify=False)
> <Response [200]>
>
> ```

If you're using a third-party module and want to disable the checks, here's a context manager that monkey patches `requests` and changes it so that `verify=False` is the default and suppresses the warning.

```
import warnings
import contextlib

import requests
from urllib3.exceptions import InsecureRequestWarning

old_merge_environment_settings = requests.Session.merge_environment_settings

@contextlib.contextmanager
def no_ssl_verification():
    opened_adapters = set()

    def merge_environment_settings(self, url, proxies, stream, verify, cert):
        # Verification happens only once per connection so we need to close
        # all the opened adapters once we're done. Otherwise, the effects of
        # verify=False persist beyond the end of this context manager.
        opened_adapters.add(self.get_adapter(url))

        settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert)
        settings['verify'] = False

        return settings

    requests.Session.merge_environment_settings = merge_environment_settings

    try:
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', InsecureRequestWarning)
            yield
    finally:
        requests.Session.merge_environment_settings = old_merge_environment_settings

        for adapter in opened_adapters:
            try:
                adapter.close()
            except:
                pass

```

Here's how you use it:

```
with no_ssl_verification():
    requests.get('https://wrong.host.badssl.example/')
    print('It works')

    requests.get('https://wrong.host.badssl.example/', verify=True)
    print('Even if you try to force it to')

requests.get('https://wrong.host.badssl.example/', verify=False)
print('It resets back')

session = requests.Session()
session.verify = True

with no_ssl_verification():
    session.get('https://wrong.host.badssl.example/', verify=True)
    print('Works even here')

try:
    requests.get('https://wrong.host.badssl.example/')
except requests.exceptions.SSLError:
    print('It breaks')

try:
    session.get('https://wrong.host.badssl.example/')
except requests.exceptions.SSLError:
    print('It breaks here again')

```

Note that this code closes all open adapters that handled a patched request once you leave the context manager. This is because requests maintains a per-session connection pool and certificate validation happens only once per connection so unexpected things like this will happen:

```
>>> import requests
>>> session = requests.Session()
>>> session.get('https://wrong.host.badssl.example/', verify=False)
/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
<Response [200]>
>>> session.get('https://wrong.host.badssl.example/', verify=True)
/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
<Response [200]>

```"
"I need to produce a screencast of an IPython session, and to avoid confusing viewers, I want to disable all warnings emitted by `warnings.warn` calls from different packages. Is there a way to configure the *ipythonrc* file to automatically disable all such warnings?","Place:

```
import warnings
warnings.filterwarnings('ignore')

```

inside `~/.ipython/profile_default/startup/disable-warnings.py`.

Quite often it is useful to see a warning once. This can be set by:

```
warnings.filterwarnings(action='once')

```"
"On my local machine, I run a python script which contains this line

```
bashCommand = ""cwm --rdf test.rdf --ntriples > test.nt""
os.system(bashCommand)

```

This works fine.

Then I run the same code on a server and I get the following error message

```
'import site' failed; use -v for traceback
Traceback (most recent call last):
File ""/usr/bin/cwm"", line 48, in <module>
from swap import  diag
ImportError: No module named swap

```

So what I did then is I inserted a `print bashCommand` which prints me than the command in the terminal before it runs it with `os.system()`.

Of course, I get again the error (caused by `os.system(bashCommand)`) but before that error it prints the command in the terminal. Then I just copied that output and did a copy paste into the terminal and hit enter and it works...

Does anyone have a clue what's going on?","To somewhat expand on the earlier answers here, there are a number of details which are commonly overlooked.

* Prefer `subprocess.run()` over `subprocess.check_call()` and friends over `subprocess.call()` over `subprocess.Popen()` over `os.system()` over `os.popen()`
* Understand and probably use `text=True`, aka `universal_newlines=True`.
* Understand the meaning of `shell=True` or `shell=False` and how it changes quoting and the availability of shell conveniences.
* Understand differences between `sh` and Bash
* Understand how a subprocess is separate from its parent, and generally cannot change the parent.
* Avoid running the Python interpreter as a subprocess of Python.

These topics are covered in some more detail below.

Prefer `subprocess.run()` or `subprocess.check_call()`
======================================================

The `subprocess.Popen()` function is a low-level workhorse but it is tricky to use correctly and you end up copy/pasting multiple lines of code ... which conveniently already exist in the standard library as a set of higher-level wrapper functions for various purposes, which are presented in more detail in the following.

Here's a paragraph from the [documentation](https://docs.python.org/3/library/subprocess.html#using-the-subprocess-module):

> The recommended approach to invoking subprocesses is to use the `run()` function for all use cases it can handle. For more advanced use cases, the underlying `Popen` interface can be used directly.

Unfortunately, the availability of these wrapper functions differs between Python versions.

* `subprocess.run()` was officially introduced in Python 3.5. It is meant to replace all of the following.
* `subprocess.check_output()` was introduced in Python 2.7 / 3.1. It is basically equivalent to `subprocess.run(..., check=True, stdout=subprocess.PIPE).stdout`
* `subprocess.check_call()` was introduced in Python 2.5. It is basically equivalent to `subprocess.run(..., check=True).returncode` (where the return code is trivially always 0)
* `subprocess.call()` was introduced in Python 2.4 in the original `subprocess` module ([PEP-324](https://www.python.org/dev/peps/pep-0324/)). It is basically equivalent to `subprocess.run(...).returncode`

### High-level API vs `subprocess.Popen()`

The refactored and extended `subprocess.run()` is more logical and more versatile than the older legacy functions it replaces. It returns a [`CompletedProcess`](https://docs.python.org/3/library/subprocess.html#subprocess.CompletedProcess) object which has various methods which allow you to retrieve the exit status, the standard output, and a few other results and status indicators from the finished subprocess.

`subprocess.run()` is the way to go if you simply need a program to run and return control to Python. For more involved scenarios (background processes, perhaps with interactive I/O with the Python parent program) you still need to use `subprocess.Popen()` and take care of all the plumbing yourself. This requires a fairly intricate understanding of all the moving parts and should not be undertaken lightly. The simpler [`Popen` object](https://docs.python.org/3/library/subprocess.html#subprocess.Popen) represents the (possibly still-running) process which needs to be managed from your code for the remainder of the lifetime of the subprocess.

It should perhaps be emphasized that just `subprocess.Popen()` merely creates a process. If you leave it at that, you have a subprocess running concurrently alongside with Python, so a ""background"" process. If it doesn't need to do input or output or otherwise coordinate with you, it can do useful work in parallel with your Python program.

### Avoid `os.system()` and `os.popen()`

Since time eternal (well, since Python 2.5) the [`os` module documentation](https://docs.python.org/release/2.5.2/lib/os-process.html) has contained the recommendation to prefer `subprocess` over `os.system()`:

> The `subprocess` module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function.

The problems with `system()` are that it's obviously system-dependent and doesn't offer ways to interact with the subprocess. It simply runs, with standard output and standard error outside of Python's reach. The only information Python receives back is the exit status of the command (zero means success, though the meaning of non-zero values is also somewhat system-dependent).

[PEP-324](https://www.python.org/dev/peps/pep-0324/) (which was already mentioned above) contains a more detailed rationale for why `os.system` is problematic and how `subprocess` attempts to solve those issues.

`os.popen()` used to be even more [strongly discouraged](https://docs.python.org/2/library/os.html#os.popen):

> *Deprecated since version 2.6:* This function is obsolete. Use the `subprocess` module.

However, since sometime in Python 3, it has been reimplemented to simply use `subprocess`, and redirects to the `subprocess.Popen()` documentation for details.

### Understand and usually use `check=True`

You'll also notice that `subprocess.call()` has many of the same limitations as `os.system()`. In regular use, you should generally check whether the process finished successfully, which `subprocess.check_call()` and `subprocess.check_output()` do (where the latter also returns the standard output of the finished subprocess). Similarly, you should usually use `check=True` with `subprocess.run()` unless you specifically need to allow the subprocess to return an error status.

In practice, with `check=True` or `subprocess.check_*`, Python will throw a [`CalledProcessError` exception](https://docs.python.org/3/library/subprocess.html#subprocess.CalledProcessError) if the subprocess returns a nonzero exit status.

A common error with `subprocess.run()` is to omit `check=True` and be surprised when downstream code fails if the subprocess failed.

On the other hand, a common problem with `check_call()` and `check_output()` was that users who blindly used these functions were surprised when the exception was raised e.g. when `grep` did not find a match. (You should probably replace `grep` with native Python code anyway, as outlined below.)

All things counted, you need to understand how shell commands return an exit code, and under what conditions they will return a non-zero (error) exit code, and make a conscious decision how exactly it should be handled.

Understand and probably use `text=True` aka `universal_newlines=True`
=====================================================================

Since Python 3, strings internal to Python are Unicode strings. But there is no guarantee that a subprocess generates Unicode output, or strings at all.

(If the differences are not immediately obvious, Ned Batchelder's [Pragmatic Unicode](https://nedbatchelder.com/text/unipain.html) is recommended, if not outright obligatory, reading. There is a 36-minute video presentation behind the link if you prefer, though reading the page yourself will probably take significantly less time.)

Deep down, Python has to fetch a `bytes` buffer and interpret it somehow. If it contains a blob of binary data, it *shouldn't* be decoded into a Unicode string, because that's error-prone and bug-inducing behavior - precisely the sort of pesky behavior which riddled many Python 2 scripts, before there was a way to properly distinguish between encoded text and binary data.

With `text=True`, you tell Python that you, in fact, expect back textual data in the system's default encoding, and that it should be decoded into a Python (Unicode) string to the best of Python's ability (usually UTF-8 on any moderately up to date system, except perhaps Windows?)

If that's *not* what you request back, Python will just give you `bytes` strings in the `stdout` and `stderr` strings. Maybe at some later point you *do* know that they were text strings after all, and you know their encoding. Then, you can decode them.

```
normal = subprocess.run([external, arg],
    stdout=subprocess.PIPE, stderr=subprocess.PIPE,
    check=True,
    text=True)
print(normal.stdout)

convoluted = subprocess.run([external, arg],
    stdout=subprocess.PIPE, stderr=subprocess.PIPE,
    check=True)
# You have to know (or guess) the encoding
print(convoluted.stdout.decode('utf-8'))

```

Python 3.7 introduced the shorter and more descriptive and understandable alias `text` for the keyword argument which was previously somewhat misleadingly called `universal_newlines`.

Understand `shell=True` vs `shell=False`
========================================

With `shell=True` you pass a single string to your shell, and the shell takes it from there.

With `shell=False` you pass a list of arguments to the OS, bypassing the shell.

When you don't have a shell, you save a process and get rid of a [fairly substantial amount of hidden complexity, which may or may not harbor bugs or even security problems.](/questions/3172470/actual-meaning-of-shell-true-in-subprocess)

On the other hand, when you don't have a shell, you don't have redirection, wildcard expansion, job control, and a large number of other shell features.

A common mistake is to use `shell=True` and then still pass Python a list of tokens, or vice versa. This happens to work in some cases, but is really ill-defined and could break in interesting ways.

```
# XXX AVOID THIS BUG
buggy = subprocess.run('dig +short stackoverflow.com')

# XXX AVOID THIS BUG TOO
broken = subprocess.run(['dig', '+short', 'stackoverflow.com'],
    shell=True)

# XXX DEFINITELY AVOID THIS
pathological = subprocess.run(['dig +short stackoverflow.com'],
    shell=True)

correct = subprocess.run(['dig', '+short', 'stackoverflow.com'],
    # Probably don't forget these, too
    check=True, text=True)

# XXX Probably better avoid shell=True
# but this is nominally correct
fixed_but_fugly = subprocess.run('dig +short stackoverflow.com',
    shell=True,
    # Probably don't forget these, too
    check=True, text=True)

```

The common retort ""but it works for me"" is not a useful rebuttal unless you understand exactly under what circumstances it could stop working.

To briefly recap, correct usage looks like

```
subprocess.run(""string for 'the shell' to parse"", shell=True)
# or
subprocess.run([""list"", ""of"", ""tokenized strings""]) # shell=False

```

If you want to avoid the shell but are too lazy or unsure of how to parse a string into a list of tokens, notice that `shlex.split()` can do this for you.

```
subprocess.run(shlex.split(""no string for 'the shell' to parse""))  # shell=False
# equivalent to
# subprocess.run([""no"", ""string"", ""for"", ""the shell"", ""to"", ""parse""])

```

The regular `split()` will not work here, because it doesn't preserve quoting. In the example above, notice how `""the shell""` is a single string, and how in the shell, single quotes were used to produce that string value, where the single quotes are not part of the value itself.

### Refactoring Example

Very often, the features of the shell can be replaced with native Python code. Mundane pipelines with `grep` or `tr` or `cut` etc, and simple Awk or `sed` scripts should probably just be translated to Python instead.

To partially illustrate this, here is a typical but slightly silly example which involves many shell features.

```
cmd = '''while read -r x;
   do ping -c 3 ""$x"" | grep 'min/avg/max'
   done <hosts.txt'''

# Trivial but horrible
results = subprocess.run(
    cmd, shell=True, text=True, capture_output=True, check=True)
print(results.stdout)

# Reimplement with shell=False
with open('hosts.txt') as hosts:
    for host in hosts:
        host = host.rstrip('\n')  # drop newline
        ping = subprocess.run(
             ['ping', '-c', '3', host],
             text=True,
             stdout=subprocess.PIPE,
             check=True)
        for line in ping.stdout.split('\n'):
             if 'min/avg/max' in line:
                 print('{}: {}'.format(host, line))

```

Some things to note here:

* With `shell=False` you don't need the quoting that the shell requires around strings. Putting quotes anyway is probably an error.
* It often makes sense to run as little code as possible in a subprocess. This gives you more control over execution from within your Python code.
* Having said that, complex shell pipelines are tedious and sometimes challenging to reimplement in Python.

The refactored code also illustrates just how much the shell really does for you with a very terse syntax -- for better or for worse. Python says *explicit is better than implicit* but the Python code *is* rather verbose and arguably looks more complex than this really is. On the other hand, it offers a number of points where you can grab control in the middle of something else, as trivially exemplified by the enhancement that we can easily include the host name along with the shell command output. (This is by no means challenging to do in the shell, either, but at the expense of yet another diversion and perhaps another process.)

### Common Shell Constructs

For completeness, here are brief explanations of some of these shell features, and some notes on how they can perhaps be replaced with native Python facilities.

* Globbing aka wildcard expansion can be replaced with `glob.glob()` or very often with simple Python string comparisons like `for file in os.listdir('.'): if not file.endswith('.png'): continue`. Bash has various other expansion facilities like `.{png,jpg}` brace expansion and `{1..100}` as well as tilde expansion (`~` expands to your home directory, and more generally `~account` to the home directory of another user)
* Shell variables like `$SHELL` or `$my_exported_var` can sometimes simply be replaced with Python variables. Exported shell variables are available as e.g. `os.environ['SHELL']` (the meaning of `export` is to make the variable available to subprocesses -- a variable which is not available to subprocesses will obviously not be available to Python running as a subprocess of the shell, or vice versa. The `env=` keyword argument to `subprocess` methods allows you to define the environment of the subprocess as a dictionary, so that's one way to make a Python variable visible to a subprocess). With `shell=False` you will need to understand how to remove any quotes; for example, `cd ""$HOME""` is equivalent to `os.chdir(os.environ['HOME'])` without quotes around the directory name. (Very often `cd` is not useful or necessary anyway, and many beginners omit the double quotes around the variable and get away with it [until one day ...](/questions/10067266/when-to-wrap-quotes-around-a-shell-variable))
* Redirection allows you to read from a file as your standard input, and write your standard output to a file. `grep 'foo' <inputfile >outputfile` opens `outputfile` for writing and `inputfile` for reading, and passes its contents as standard input to `grep`, whose standard output then lands in `outputfile`. This is not generally hard to replace with native Python code.
* Pipelines are a form of redirection. `echo foo | nl` runs two subprocesses, where the standard output of `echo` is the standard input of `nl` (on the OS level, in Unix-like systems, this is a single file handle). If you cannot replace one or both ends of the pipeline with native Python code, perhaps think about using a shell after all, especially if the pipeline has more than two or three processes (though look at the [`pipes` module in the Python standard library](https://docs.python.org/3.7/library/pipes.html) or a number of more modern and versatile third-party competitors).
* Job control lets you interrupt jobs, run them in the background, return them to the foreground, etc. The basic Unix signals to stop and continue a process are of course available from Python, too. But jobs are a higher-level abstraction in the shell which involve process groups etc which you have to understand if you want to do something like this from Python.
* Quoting in the shell is potentially confusing until you understand that *everything* is basically a string. So `ls -l /` is equivalent to `'ls' '-l' '/'` but the quoting around literals is completely optional. Unquoted strings which contain shell metacharacters undergo parameter expansion, whitespace tokenization and wildcard expansion; double quotes prevent whitespace tokenization and wildcard expansion but allow parameter expansions (variable substitution, command substitution, and backslash processing). This is simple in theory but can get bewildering, especially when there are several layers of interpretation (a remote shell command, for example).

Understand differences between `sh` and Bash
============================================

`subprocess` runs your shell commands with `/bin/sh` unless you specifically request otherwise (except of course on Windows, where it uses the value of the `COMSPEC` variable). This means that [various Bash-only features like arrays, `[[` etc](/a/42666651/874188) are not available.

If you need to use Bash-only syntax, you can
pass in the path to the shell as `executable='/bin/bash'` (where of course if your Bash is installed somewhere else, you need to adjust the path).

```
subprocess.run('''
    # This for loop syntax is Bash only
    for((i=1;i<=$#;i++)); do
        # Arrays are Bash-only
        array[i]+=123
    done''',
    shell=True, check=True,
    executable='/bin/bash')

```

A `subprocess` is separate from its parent, and cannot change it
================================================================

A somewhat common mistake is doing something like

```
subprocess.run('cd /tmp', shell=True)
subprocess.run('pwd', shell=True)  # Oops, doesn't print /tmp

```

The same thing will happen if the first subprocess tries to set an environment variable, which of course will have disappeared when you run another subprocess, etc.

A child process runs completely separate from Python, and when it finishes, Python has no idea what it did (apart from the vague indicators that it can infer from the exit status and output from the child process). A child generally cannot change the parent's environment; it cannot set a variable, change the working directory, or, in so many words, communicate with its parent without cooperation from the parent.

The immediate fix in this particular case is to run both commands in a single subprocess;

```
subprocess.run('cd /tmp; pwd', shell=True)

```

though obviously this particular use case isn't very useful; instead, use the `cwd` keyword argument, or simply `os.chdir()` before running the subprocess. Similarly, for setting a variable, you can manipulate the environment of the current process (and thus also its children) via

```
os.environ['foo'] = 'bar'

```

or pass an environment setting to a child process with

```
subprocess.run('echo ""$foo""', shell=True, env={'foo': 'bar'})

```

(not to mention the obvious refactoring `subprocess.run(['echo', 'bar'])`; but `echo` is a poor example of something to run in a subprocess in the first place, of course).

Don't run Python from Python
============================

This is slightly dubious advice; there are certainly situations where it does make sense or is even an absolute requirement to run the Python interpreter as a subprocess from a Python script. But very frequently, the correct approach is simply to `import` the other Python module into your calling script and call its functions directly.

If the other Python script is under your control, and it isn't a module, consider [turning it into one](/questions/15746675/how-to-write-a-python-module-package). (This answer is too long already so I will not delve into details here.)

If you need parallelism, you can run Python functions in subprocesses with the [`multiprocessing` module.](https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing) There is also [`threading`](https://docs.python.org/3/library/threading.html#module-threading) which runs multiple tasks in a single process (which is more lightweight and gives you more control, but also more constrained in that threads within a process are tightly coupled, and bound to a single [GIL](https://docs.python.org/3/glossary.html#term-global-interpreter-lock).)"
"Simple question here: I'm trying to get the size of my legend using `matplotlib.pyplot` to be smaller (i.e., the text to be smaller). The code I'm using goes something like this:

```
plot.figure()
plot.scatter(k, sum_cf, color='black', label='Sum of Cause Fractions')
plot.scatter(k, data[:, 0],  color='b', label='Dis 1: cf = .6, var = .2')
plot.scatter(k, data[:, 1],  color='r',  label='Dis 2: cf = .2, var = .1')
plot.scatter(k, data[:, 2],  color='g', label='Dis 3: cf = .1, var = .01')
plot.legend(loc=2)

```","You can set an individual font size for the legend by adjusting the `prop` keyword.

```
plot.legend(loc=2, prop={'size': 6})

```

This takes a dictionary of keywords corresponding to `matplotlib.font_manager.FontProperties` properties. See the [documentation for legend](http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.legend):

> Keyword arguments:
>
> ```
> prop: [ None | FontProperties | dict ]
>     A matplotlib.font_manager.FontProperties instance. If prop is a 
>     dictionary, a new instance will be created with prop. If None, use
>     rc settings.
>
> ```

It is also possible, as of version [1.2.1](http://matplotlib.org/api/legend_api.html#matplotlib.legend.Legend), to use the keyword `fontsize`."
How to find out what week number is current year on June 16th (wk24) with Python?,"`datetime.date` has a `isocalendar()` method, which returns a tuple containing the calendar week:

```
>>> import datetime
>>> datetime.date(2010, 6, 16).isocalendar()[1]
24

```

[datetime.date.isocalendar()](http://docs.python.org/library/datetime.html#datetime.date.isocalendar) is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance.

In Python 3.9+ `isocalendar()` returns a [namedtuple](https://docs.python.org/3/glossary.html#term-named-tuple) with the fields `year`, `week` and `weekday` which means you can access the week explicitly using a named attribute:

```
>>> import datetime
>>> datetime.date(2010, 6, 16).isocalendar().week
24

```"
"How do I get the path of a the Python script I am running in? I was doing `dirname(sys.argv[0])`, however on Mac I only get the filename - not the full path as I do on Windows.

No matter where my application is launched from, I want to open files that are relative to my script file(s).","Use this to get the path of the current file. It will resolve any symlinks in the path.

```
import os

file_path = os.path.realpath(__file__)

```

This works fine on my mac. It won't work from the Python interpreter (you need to be executing a Python file)."
"I want to convert JSON data into a Python object.

I receive JSON data objects from the Facebook API, which I want to store in my database.

My current View in Django (Python) (`request.POST` contains the JSON):

```
response = request.POST
user = FbApiUser(user_id = response['id'])
user.name = response['name']
user.username = response['username']
user.save()

```

* This works fine, but how do I handle complex JSON data objects?
* Wouldn't it be much better if I could somehow convert this JSON object into a Python object for easy use?","**UPDATE**

With Python3, you can do it in one line, using `SimpleNamespace` and `object_hook`:

```
import json
from types import SimpleNamespace

data = '{""name"": ""John Smith"", ""hometown"": {""name"": ""New York"", ""id"": 123}}'

# Parse JSON into an object with attributes corresponding to dict keys.
x = json.loads(data, object_hook=lambda d: SimpleNamespace(**d))
# Or, in Python 3.13+:
#   json.loads(data, object_hook=SimpleNamespace)
print(x.name, x.hometown.name, x.hometown.id)

```

**OLD ANSWER (Python2)**

In Python2, you can do it in one line, using `namedtuple` and `object_hook` (but it's very slow with many nested objects):

```
import json
from collections import namedtuple

data = '{""name"": ""John Smith"", ""hometown"": {""name"": ""New York"", ""id"": 123}}'

# Parse JSON into an object with attributes corresponding to dict keys.
x = json.loads(data, object_hook=lambda d: namedtuple('X', d.keys())(*d.values()))
print x.name, x.hometown.name, x.hometown.id

```

or, to reuse this easily:

```
def _json_object_hook(d): return namedtuple('X', d.keys())(*d.values())
def json2obj(data): return json.loads(data, object_hook=_json_object_hook)

x = json2obj(data)

```

If you want it to handle keys that aren't good attribute names, check out `namedtuple`'s [`rename` parameter](http://docs.python.org/2/library/collections.html#collections.namedtuple)."
"I would like to include image in a jupyter notebook.

If I did the following, it works :

```
from IPython.display import Image
Image(""img/picture.png"")

```

But I would like to include the images in a markdown cell and the following code gives a 404 error :

```
![title](""img/picture.png"")

```

I also tried

```
![texte](""http://localhost:8888/img/picture.png"")

```

But I still get the same error :

```
404 GET /notebooks/%22/home/user/folder/img/picture.png%22 (127.0.0.1) 2.74ms referer=http://localhost:8888/notebooks/notebook.ipynb

```","You mustn't use quotation marks around the name of the image files in markdown!

If you carefully read your error message, you will see the two `%22` parts in the link. That is the html encoded quotation mark.

You have to change the line

```
![title](""img/picture.png"")

```

to

```
![title](img/picture.png)
```

**UPDATE**

It is assumed, that you have the following file structure and that you run the `jupyter notebook` command in the directory where the file `example.ipynb` (<-- contains the markdown for the image) is stored:

```
/
+-- example.ipynb
+-- img
    +-- picture.png

```"
"How do I concatenate two lists in Python?

Example:

```
listone = [1, 2, 3]
listtwo = [4, 5, 6]

```

Expected outcome:

```
>>> joinedlist
[1, 2, 3, 4, 5, 6]

```","Use the `+` operator to combine the lists:

```
listone = [1, 2, 3]
listtwo = [4, 5, 6]

joinedlist = listone + listtwo

```

Output:

```
>>> joinedlist
[1, 2, 3, 4, 5, 6]

```

NOTE: This will create a new list with a shallow copy of the items in the first list, followed by a shallow copy of the items in the second list. Use [copy.deepcopy()](https://docs.python.org/3/library/copy.html#copy.deepcopy) to get deep copies of lists."
"Suppose I have a dataframe with columns `a`, `b` and `c`. I want to sort the dataframe by column `b` in ascending order, and by column `c` in descending order. How do I do this?","As of the 0.17.0 release, the [`sort`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sort.html) method was deprecated in favor of [`sort_values`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html). `sort` was completely removed in the 0.20.0 release. The arguments (and results) remain the same:

```
df.sort_values(['a', 'b'], ascending=[True, False])

```

---

You can use the ascending argument of [`sort`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html):

```
df.sort(['a', 'b'], ascending=[True, False])

```

For example:

```
In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])

In [12]: df1.sort(['a', 'b'], ascending=[True, False])
Out[12]:
   a  b
2  1  4
7  1  3
1  1  2
3  1  2
4  3  2
6  4  4
0  4  3
9  4  3
5  4  1
8  4  1

```

---

As commented by @renadeen

> Sort isn't in place by default! So you should assign result of the sort method to a variable or add inplace=True to method call.

that is, if you want to reuse df1 as a sorted DataFrame:

```
df1 = df1.sort(['a', 'b'], ascending=[True, False])

```

or

```
df1.sort(['a', 'b'], ascending=[True, False], inplace=True)

```"
"How can I get the current system status (current CPU, RAM, free disk space, etc.) in Python? Ideally, it would work for both Unix and Windows platforms.

There seems to be a few possible ways of extracting that from my search:

1. Using a library such as [PSI](http://www.psychofx.com/psi/trac/wiki/) (that currently seems not actively developed and not supported on multiple platforms) or something like [pystatgrab](http://www.i-scream.org/pystatgrab/) (again no activity since 2007 it seems and no support for Windows).
2. Using platform specific code such as using a `os.popen(""ps"")` or similar for the \*nix systems and `MEMORYSTATUS` in `ctypes.windll.kernel32` (see [this recipe on ActiveState](http://code.activestate.com/recipes/511491/)) for the Windows platform. One could put a Python class together with all those code snippets.

It's not that those methods are bad but is there already a well-supported, multi-platform way of doing the same thing?","[The psutil library](https://pypi.python.org/pypi/psutil) gives you information about CPU, RAM, etc., on a variety of platforms:

> psutil is a module providing an interface for retrieving information on running processes and system utilization (CPU, memory) in a portable way by using Python, implementing many functionalities offered by tools like ps, top and Windows task manager.
>
> It currently supports Linux, Windows, OSX, Sun Solaris, FreeBSD, OpenBSD and NetBSD, both 32-bit and 64-bit architectures, with Python versions from 2.6 to 3.5 (users of Python 2.4 and 2.5 may use 2.1.3 version).

---

Some examples:

```
#!/usr/bin/env python
import psutil
# gives a single float value
psutil.cpu_percent()
# gives an object with many fields
psutil.virtual_memory()
# you can convert that object to a dictionary 
dict(psutil.virtual_memory()._asdict())
# you can have the percentage of used RAM
psutil.virtual_memory().percent
79.2
# you can calculate percentage of available memory
psutil.virtual_memory().available * 100 / psutil.virtual_memory().total
20.8

```

Here's other documentation that provides more concepts and interest concepts:

* <https://psutil.readthedocs.io/en/latest/>"
"What's the easiest way to add an empty column to a pandas DataFrame object? The best I've stumbled upon is something like

```
df['foo'] = df.apply(lambda _: '', axis=1)

```

Is there a less perverse method?","If I understand correctly, assignment should fill:

```
>>> import numpy as np
>>> import pandas as pd
>>> df = pd.DataFrame({""A"": [1,2,3], ""B"": [2,3,4]})
>>> df
   A  B
0  1  2
1  2  3
2  3  4
>>> df[""C""] = """"
>>> df[""D""] = np.nan
>>> df
   A  B C   D
0  1  2   NaN
1  2  3   NaN
2  3  4   NaN

```"
"I'd simply like to convert a base-2 binary number string into an int, something like this:

```
>>> '11111111'.fromBinaryToInt()
255

```

Is there a way to do this in Python?","You use the built-in `int()` function, and pass it the base of the input number, i.e. `2` for a binary number:

```
>>> int('11111111', 2)
255

```

Here is documentation for [Python 2](https://docs.python.org/2/library/functions.html#int), and for [Python 3](https://docs.python.org/3/library/functions.html#int)."
"In **Python 2.6**, I want to do:

```
f = lambda x: if x==2 print x else raise Exception()
f(2) #should print ""2""
f(3) #should throw an exception

```

This clearly isn't the syntax. Is it possible to perform an `if` in `lambda` and if so how to do it?","The syntax you're looking for:

```
lambda x: True if x % 2 == 0 else False

```

But you can't use `print` or `raise` in a lambda."
"I just started Python and I've got no idea what [memoization](http://en.wikipedia.org/wiki/Memoization) is and how to use it. Also, may I have a simplified example?","Memoization effectively refers to remembering (""memoization"" → ""memorandum"" → to be remembered) results of method calls based on the method inputs and then returning the remembered result rather than computing the result again. You can think of it as a cache for method results. For further details, see page 387 for the definition in ***Introduction To Algorithms*** (3e), Cormen et al.

A simple example for computing factorials using memoization in Python would be something like this:

```
factorial_memo = {}
def factorial(k):
    if k < 2: return 1
    if k not in factorial_memo:
        factorial_memo[k] = k * factorial(k-1)
    return factorial_memo[k]

```

You can get more complicated and encapsulate the memoization process into a class:

```
class Memoize:
    def __init__(self, f):
        self.f = f
        self.memo = {}
    def __call__(self, *args):
        if not args in self.memo:
            self.memo[args] = self.f(*args)
        #Warning: You may wish to do a deepcopy here if returning objects
        return self.memo[args]

```

Then:

```
def factorial(k):
    if k < 2: return 1
    return k * factorial(k - 1)

factorial = Memoize(factorial)

```

A feature known as ""[decorators](https://www.python.org/dev/peps/pep-0318/)"" was added in Python 2.4 which allow you to now simply write the following to accomplish the same thing:

```
@Memoize
def factorial(k):
    if k < 2: return 1
    return k * factorial(k - 1)

```

The [Python Decorator Library](https://wiki.python.org/moin/PythonDecoratorLibrary) has a similar decorator called [`memoized`](https://wiki.python.org/moin/PythonDecoratorLibrary#Memoize) that is slightly more robust than the `Memoize` class shown here."
"I query a model:

```
Members.objects.all()

```

And it returns:

```
Eric, Salesman, X-Shop
Freddie, Manager, X2-Shop
Teddy, Salesman, X2-Shop
Sean, Manager, X2-Shop

```

What I want is to know the best Django way to fire
a `group_by` query to my database, like:

```
Members.objects.all().group_by('designation')

```

Which doesn't work, of course.
I know we can do some tricks on `django/db/models/query.py`, but I am just curious to know how to do it without patching.","If you mean to do aggregation you can use the [aggregation features of the ORM](http://docs.djangoproject.com/en/stable/topics/db/aggregation/#topics-db-aggregation):

```
from django.db.models import Count
result = (Members.objects
    .values('designation')
    .annotate(dcount=Count('designation'))
    .order_by()
)

```

This results in a query similar to

```
SELECT designation, COUNT(designation) AS dcount
FROM members GROUP BY designation

```

and the output would be of the form

```
[{'designation': 'Salesman', 'dcount': 2}, 
 {'designation': 'Manager', 'dcount': 2}]

```

If you don't include the `order_by()`, you may get incorrect results if the default sorting is not what you expect.

If you want to include multiple fields in the results, just add them as arguments to `values`, for example:

```
    .values('designation', 'first_name', 'last_name')

```

### References:

* Django documentation: [`values()`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.values), [`annotate()`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.annotate), and [`Count`](https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.Count)
* Django documentation: [Aggregation](https://docs.djangoproject.com/en/stable/topics/db/aggregation), and in particular the section entitled [Interaction with default ordering or `order_by()`](https://docs.djangoproject.com/en/stable/topics/db/aggregation/#interaction-with-default-ordering-or-order-by)"
"I want to write a test to establish that an Exception is not raised in a given circumstance.

It's straightforward to test if an Exception **is** raised ...

```
sInvalidPath=AlwaysSuppliesAnInvalidPath()
self.assertRaises(PathIsNotAValidOne, MyObject, sInvalidPath) 

```

... but how can you do the **opposite**.

Something like this i what I'm after ...

```
sValidPath=AlwaysSuppliesAValidPath()
self.assertNotRaises(PathIsNotAValidOne, MyObject, sValidPath) 

```","```
def run_test(self):
    try:
        myFunc()
    except ExceptionType:
        self.fail(""myFunc() raised ExceptionType unexpectedly!"")

```"
"When I try to install `odoo-server`, I got the following error:

```
error: Setup script exited with error: command 'x86_64-linux-gnu-gcc' failed with exit status 1

```

Could anyone help me to solve this issue?","I encountered the same problem in college having installed *Linux Mint* for the main project of my final year, the third solution below worked for me.

When encountering this error please note before the error it may say you are ***missing*** a package or header file — you should find those and install them and verify if it works (e.g. ssl → libssl).

For **Python 2.x** use:

```
sudo apt-get install python-dev

```

For **Python 2.7** use:

```
sudo apt-get install libffi-dev

```

For **Python 3.x** use:

```
sudo apt-get install python3-dev

```

or for a specific version of Python 3, replace *`x`* with the minor version in

```
sudo apt-get install python3.x-dev

```"
"I need some help on declaring a regex. My inputs are like the following:

```
this is a paragraph with<[1> in between</[1> and then there are cases ... where the<[99> number ranges from 1-100</[99>. 
and there are many other lines in the txt files
with<[3> such tags </[3>

```

The required output is:

```
this is a paragraph with in between and then there are cases ... where the number ranges from 1-100. 
and there are many other lines in the txt files
with such tags

```

I've tried this:

```
#!/usr/bin/python
import os, sys, re, glob
for infile in glob.glob(os.path.join(os.getcwd(), '*.txt')):
    for line in reader: 
        line2 = line.replace('<[1> ', '')
        line = line2.replace('</[1> ', '')
        line2 = line.replace('<[1>', '')
        line = line2.replace('</[1>', '')
        
        print line

```

I've also tried this (but it seems like I'm using the wrong regex syntax):

```
        line2 = line.replace('<[*> ', '')
        line = line2.replace('</[*> ', '')
        line2 = line.replace('<[*>', '')
        line = line2.replace('</[*>', '')

```

I dont want to hard-code the `replace` from 1 to 99.","This tested snippet should do it:

```
import re
line = re.sub(r""</?\[\d+>"", """", line)

```

**Edit:** Here's a commented version explaining how it works:

```
line = re.sub(r""""""
  (?x) # Use free-spacing mode.
  <    # Match a literal '<'
  /?   # Optionally match a '/'
  \[   # Match a literal '['
  \d+  # Match one or more digits
  >    # Match a literal '>'
  """""", """", line)

```

Regexes are *fun!* But I would strongly recommend spending an hour or two studying the basics. For starters, you need to learn which characters are special: *""metacharacters""* which need to be escaped (i.e. with a backslash placed in front - and the rules are different inside and outside character classes.) There is an excellent online tutorial at: [www.regular-expressions.info](http://www.regular-expressions.info/). The time you spend there will pay for itself many times over. Happy regexing!"
"For example, if passed the following:

```
a = []

```

How do I check to see if `a` is empty?","```
if not a:
    print(""List is empty"")

```

Using the [implicit booleanness](https://docs.python.org/library/stdtypes.html#truth-value-testing) of the empty `list` is quite Pythonic."
How can I check if a Python object is a string (either regular or Unicode)?,"Python 2
========

Use `isinstance(obj, basestring)` for an object-to-test `obj`.

[Docs](https://docs.python.org/2.7/library/functions.html#basestring)."
"Are there situations in which `sys.stdout.write()` is preferable to `print`?

(**Examples:** better performance; code that makes more sense)","`print` is just a thin wrapper that formats the inputs (modifiable, but by default with a space between args and newline at the end) and calls the write function of a given object. By default this object is `sys.stdout`, but you can pass a file using the ""chevron"" form. For example:

```
print >> open('file.txt', 'w'), 'Hello', 'World', 2+3

```

See: <https://docs.python.org/2/reference/simple_stmts.html?highlight=print#the-print-statement>

---

In Python 3.x, `print` becomes a function, but it is still possible to pass something other than `sys.stdout` thanks to the `file`argument.

```
print('Hello', 'World', 2+3, file=open('file.txt', 'w'))

```

See <https://docs.python.org/3/library/functions.html#print>

---

In Python 2.6+, `print` is still a statement, but it can be used as a function with

```
from __future__ import print_function

```

---

Update: Bakuriu commented to point out that there is a small difference between the print function and the print statement (and more generally between a function and a statement).

In case of an error when evaluating arguments:

```
print ""something"", 1/0, ""other"" #prints only something because 1/0 raise an Exception

print(""something"", 1/0, ""other"") #doesn't print anything. The function is not called

```"
"I need to install a package from [PyPI](https://en.wikipedia.org/wiki/Python_Package_Index) straight within my script.

Is there maybe some module or `distutils` (`distribute`, `pip`, etc.) feature which allows me to just execute something like `pypi.install('requests')` and requests will be installed into my virtualenv?","The officially recommended way to install packages from a script is by calling pip's command-line interface via a subprocess. [Most other answers presented here are not supported by pip](https://pip.pypa.io/en/latest/user_guide/#using-pip-from-your-program). Furthermore since pip v10, all code has been moved to `pip._internal` precisely in order to make it clear to users that programmatic use of pip is not allowed.

Use `sys.executable` to ensure that you will call the same `pip` associated with the current runtime.

```
import subprocess
import sys

def install(package):
    subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])

```"
"I came across the Python `with` statement for the first time today. I've been using Python lightly for several months and didn't even know of its existence! Given its somewhat obscure status, I thought it would be worth asking:

1. What is the Python `with` statement
   designed to be used for?
2. What do
   you use it for?
3. Are there any
   gotchas I need to be aware of, or
   common anti-patterns associated with
   its use? Any cases where it is better use `try..finally` than `with`?
4. Why isn't it used more widely?
5. Which standard library classes are compatible with it?","1. I believe this has already been answered by other users before me, so I only add it for the sake of completeness: the `with` statement simplifies exception handling by encapsulating common preparation and cleanup tasks in so-called [context managers](http://docs.python.org/release/2.5.2/lib/typecontextmanager.html). More details can be found in [PEP 343](http://www.python.org/dev/peps/pep-0343/). For instance, the `open` statement is a context manager in itself, which lets you open a file, keep it open as long as the execution is in the context of the `with` statement where you used it, and close it as soon as you leave the context, no matter whether you have left it because of an exception or during regular control flow. The `with` statement can thus be used in ways similar to the [RAII pattern](http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization) in C++: some resource is acquired by the `with` statement and released when you leave the `with` context.
2. Some examples are: opening files using `with open(filename) as fp:`, acquiring locks using `with lock:` (where `lock` is an instance of `threading.Lock`). You can also construct your own context managers using the `contextmanager` decorator from `contextlib`. For instance, I often use this when I have to change the current directory temporarily and then return to where I was:

   ```
   from contextlib import contextmanager
   import os

   @contextmanager
   def working_directory(path):
       current_dir = os.getcwd()
       os.chdir(path)
       try:
           yield
       finally:
           os.chdir(current_dir)

   with working_directory(""data/stuff""):
       # do something within data/stuff
   # here I am back again in the original working directory

   ```

   Here's another example that temporarily redirects `sys.stdin`, `sys.stdout` and `sys.stderr` to some other file handle and restores them later:

   ```
   from contextlib import contextmanager
   import sys

   @contextmanager
   def redirected(**kwds):
       stream_names = [""stdin"", ""stdout"", ""stderr""]
       old_streams = {}
       try:
           for sname in stream_names:
               stream = kwds.get(sname, None)
               if stream is not None and stream != getattr(sys, sname):
                   old_streams[sname] = getattr(sys, sname)
                   setattr(sys, sname, stream)
           yield
       finally:
           for sname, stream in old_streams.iteritems():
               setattr(sys, sname, stream)

   with redirected(stdout=open(""/tmp/log.txt"", ""w"")):
        # these print statements will go to /tmp/log.txt
        print ""Test entry 1""
        print ""Test entry 2""
   # back to the normal stdout
   print ""Back to normal stdout again""

   ```

   And finally, another example that creates a temporary folder and cleans it up when leaving the context:

   ```
   from tempfile import mkdtemp
   from shutil import rmtree

   @contextmanager
   def temporary_dir(*args, **kwds):
       name = mkdtemp(*args, **kwds)
       try:
           yield name
       finally:
           shutil.rmtree(name)

   with temporary_dir() as dirname:
       # do whatever you want

   ```"
"How do I change the size of my image so it's suitable for printing?

For example, I'd like to use an A4 paper, whose dimensions are 11.7 inches by 8.27 inches in landscape orientation.","You can also set figure size by passing dictionary to `rc` parameter with key `'figure.figsize'` in seaborn `set_theme` method (which replaces the `set` method, [deprecated in v0.11.0 (September 2020)](https://seaborn.pydata.org/whatsnew/v0.11.0.html#theming))

```
import seaborn as sns

sns.set_theme(rc={'figure.figsize':(11.7,8.27)})

```

Other alternative may be to use `figure.figsize` of `rcParams` to set figure size as below:

```
from matplotlib import rcParams

# figure size in inches
rcParams['figure.figsize'] = 11.7,8.27

```

More details can be found in [matplotlib documentation](https://matplotlib.org/users/customizing.html#matplotlib-rcparams)"
"In a matplotlib figure, how can I make the font size for the tick labels using `ax1.set_xticklabels()` smaller?

Further, how can one rotate it from horizontal to vertical?","There is a simpler way actually. I just found:

```
import matplotlib.pyplot as plt
# We prepare the plot  
fig, ax = plt.subplots()

# We change the fontsize of minor ticks label 
ax.tick_params(axis='both', which='major', labelsize=10)
ax.tick_params(axis='both', which='minor', labelsize=8)

```

This only answers to the size of `label` part of your question though."
"Do you know if there is a built-in function to build a dictionary from an arbitrary object? I'd like to do something like this:

```
>>> class Foo:
...     bar = 'hello'
...     baz = 'world'
...
>>> f = Foo()
>>> props(f)
{ 'bar' : 'hello', 'baz' : 'world' }

```

**NOTE:** It should not include methods. Only fields.","Note that best practice in Python 2.7 is to use *[new-style](https://www.python.org/doc/newstyle/)* classes (not needed with Python 3), i.e.

```
class Foo(object):
   ...

```

Also, there's a difference between an 'object' and a 'class'. To build a dictionary from an arbitrary *object*, it's sufficient to use `__dict__`. Usually, you'll declare your methods at class level and your attributes at instance level, so `__dict__` should be fine. For example:

```
>>> class A(object):
...   def __init__(self):
...     self.b = 1
...     self.c = 2
...   def do_nothing(self):
...     pass
...
>>> a = A()
>>> a.__dict__
{'c': 2, 'b': 1}

```

A better approach (suggested by [robert](https://stackoverflow.com/users/409638/robert) in comments) is the builtin [`vars`](https://docs.python.org/3/library/functions.html#vars) function:

```
>>> vars(a)
{'c': 2, 'b': 1}

```

Alternatively, depending on what you want to do, it might be nice to inherit from `dict`. Then your class is *already* a dictionary, and if you want you can override `getattr` and/or `setattr` to call through and set the dict. For example:

```
class Foo(dict):
    def __init__(self):
        pass
    def __getattr__(self, attr):
        return self[attr]

    # etc...

```"
"I am trying to subtract one date value from the value of `datetime.datetime.today()` to calculate how long ago something was. But it complains:

```
TypeError: can't subtract offset-naive and offset-aware datetimes

```

The return value from `datetime.datetime.today()` doesn't seem to be ""timezone aware"", while my other date value is. How do I get a return value from `datetime.datetime.today()` that is timezone aware?

The ideal solution would be for it to automatically know the timezone.

Right now, it's giving me the time in local time, which happens to be PST, i.e. UTC - 8 hours. Worst case, is there a way I can manually enter a timezone value into the `datetime` object returned by `datetime.datetime.today()` and set it to UTC-8?","In the standard library, there is no cross-platform way to create aware timezones without creating your own timezone class. (**Edit:** Python 3.9 introduces [`zoneinfo`](https://docs.python.org/3/library/zoneinfo.html) in the standard library which does provide this functionality.)

On Windows, there's `win32timezone.utcnow()`, but that's part of pywin32. I would rather suggest to use the [pytz library](http://pytz.sourceforge.net/), which has a constantly updated database of most timezones.

Working with local timezones can be very tricky (see ""Further reading"" links below), so you may rather want to use UTC throughout your application, especially for arithmetic operations like calculating the difference between two time points.

You can get the current date/time like so:

```
import pytz
from datetime import datetime
datetime.utcnow().replace(tzinfo=pytz.utc)

```

Mind that `datetime.today()` and `datetime.now()` return the *local* time, not the UTC time, so applying `.replace(tzinfo=pytz.utc)` to them would not be correct.

Another nice way to do it is:

```
datetime.now(pytz.utc)

```

which is a bit shorter and does the same.

---

Further reading/watching why to prefer UTC in many cases:

* [pytz documentation](https://pythonhosted.org/pytz/)
* [What Every Developer Should Know About Time](http://web.archive.org/web/20160803154621/http://www.windward.net/blogs/every-developer-know-time/) – development hints for many real-life use cases
* [The Problem with Time & Timezones - Computerphile](https://www.youtube.com/watch?v=-5wpm-gesOY) – funny, eye-opening explanation about the complexity of working with timezones (video)"
"I am trying to run cv2, but when I try to import it, I get the following error:

```
ImportError: libGL.so.1: cannot open shared object file: No such file or directory

```

My suggested solution online is to install

```
apt install libgl1-mesa-glx

```

but this is already installed and the latest version.

NB: I am actually running this on Docker, and I am not able to check the OpenCV version. I tried importing matplotlib and that imports fine.","Add the following lines to your Dockerfile:

```
RUN apt-get update && apt-get install ffmpeg libsm6 libxext6  -y

```

These commands install the cv2 dependencies that are normally present on the local machine, but might be missing in your Docker container causing the issue.

[minor update on 20 Jan 2022: as Docker recommends, never put `RUN apt-get update` alone, causing cache issue]"
"I have a `dataframe` with over 200 columns. The issue is as they were generated the order is

```
['Q1.3','Q6.1','Q1.2','Q1.1',......]

```

I need to *sort* the columns as follows:

```
['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]

```

Is there some way for me to do this within Python?","```
df = df.reindex(sorted(df.columns), axis=1)

```

This assumes that sorting the column names will give the order you want. If your column names won't sort lexicographically (e.g., if you want column Q10.3 to appear after Q9.1), you'll need to sort differently, but that has nothing to do with pandas."
"How do I check whether a file exists or not, without using the [`try`](https://docs.python.org/3.6/reference/compound_stmts.html#try) statement?","If the reason you're checking is so you can do something like `if file_exists: open_it()`, it's safer to use a `try` around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it.

If you're not planning to open the file immediately, you can use [`os.path.isfile`](https://docs.python.org/library/os.path.html#os.path.isfile) if you need to be sure it's a file.

> Return `True` if path is an existing regular file. This follows symbolic links, so both [islink()](https://docs.python.org/library/os.path.html#os.path.islink) and [isfile()](https://docs.python.org/library/os.path.html#os.path.isfile) can be true for the same path.

```
import os.path
os.path.isfile(fname)

```

### `pathlib`

Starting with Python 3.4, the [`pathlib` module](https://docs.python.org/3/library/pathlib.html#pathlib.Path.is_file) offers an object-oriented approach (backported to `pathlib2` in Python 2.7):

```
from pathlib import Path

my_file = Path(""/path/to/file"")
if my_file.is_file():
    # file exists

```

To check a directory, do:

```
if my_file.is_dir():
    # directory exists

```

To check whether a `Path` object exists independently of whether is it a file or directory, use `exists()`:

```
if my_file.exists():
    # path exists

```

You can also use `resolve(strict=True)` in a `try` block:

```
try:
    my_abs_path = my_file.resolve(strict=True)
except FileNotFoundError:
    # doesn't exist
else:
    # exists

```"
"How do I make two decorators in Python that would do the following?

```
@make_bold
@make_italic
def say():
   return ""Hello""

```

Calling `say()` should return:

```
""<b><i>Hello</i></b>""

```","If you are not into long explanations, see [Paolo Bergantino’s answer](https://stackoverflow.com/questions/739654/understanding-python-decorators#answer-739665).

Decorator Basics
================

Python’s functions are objects
------------------------------

To understand decorators, you must first understand that functions are objects in Python. This has important consequences. Let’s see why with a simple example :

```
def shout(word=""yes""):
    return word.capitalize()+""!""

print(shout())
# outputs : 'Yes!'

# As an object, you can assign the function to a variable like any other object 
scream = shout

# Notice we don't use parentheses: we are not calling the function,
# we are putting the function ""shout"" into the variable ""scream"".
# It means you can then call ""shout"" from ""scream"":

print(scream())
# outputs : 'Yes!'

# More than that, it means you can remove the old name 'shout',
# and the function will still be accessible from 'scream'

del shout
try:
    print(shout())
except NameError as e:
    print(e)
    #outputs: ""name 'shout' is not defined""

print(scream())
# outputs: 'Yes!'

```

Keep this in mind. We’ll circle back to it shortly.

Another interesting property of Python functions is they can be defined inside another function!

```
def talk():

    # You can define a function on the fly in ""talk"" ...
    def whisper(word=""yes""):
        return word.lower()+""...""

    # ... and use it right away!
    print(whisper())

# You call ""talk"", that defines ""whisper"" EVERY TIME you call it, then
# ""whisper"" is called in ""talk"". 
talk()
# outputs: 
# ""yes...""

# But ""whisper"" DOES NOT EXIST outside ""talk"":

try:
    print(whisper())
except NameError as e:
    print(e)
    #outputs : ""name 'whisper' is not defined""*
    #Python's functions are objects

```

Functions references
--------------------

Okay, still here? Now the fun part...

You’ve seen that functions are objects. Therefore, functions:

* can be assigned to a variable
* can be defined in another function

That means that **a function can `return` another function**.

```
def getTalk(kind=""shout""):

    # We define functions on the fly
    def shout(word=""yes""):
        return word.capitalize()+""!""

    def whisper(word=""yes"") :
        return word.lower()+""...""

    # Then we return one of them
    if kind == ""shout"":
        # We don't use ""()"", we are not calling the function,
        # we are returning the function object
        return shout  
    else:
        return whisper

# How do you use this strange beast?

# Get the function and assign it to a variable
talk = getTalk()      

# You can see that ""talk"" is here a function object:
print(talk)
#outputs : <function shout at 0xb7ea817c>

# The object is the one returned by the function:
print(talk())
#outputs : Yes!

# And you can even use it directly if you feel wild:
print(getTalk(""whisper"")())
#outputs : yes...

```

There’s more!

If you can `return` a function, you can pass one as a parameter:

```
def doSomethingBefore(func): 
    print(""I do something before then I call the function you gave me"")
    print(func())

doSomethingBefore(scream)
#outputs: 
#I do something before then I call the function you gave me
#Yes!

```

Well, you just have everything needed to understand decorators. You see, decorators are “wrappers”, which means that **they let you execute code before and after the function they decorate** without modifying the function itself.

Handcrafted decorators
----------------------

How you’d do it manually:

```
# A decorator is a function that expects ANOTHER function as parameter
def my_shiny_new_decorator(a_function_to_decorate):

    # Inside, the decorator defines a function on the fly: the wrapper.
    # This function is going to be wrapped around the original function
    # so it can execute code before and after it.
    def the_wrapper_around_the_original_function():

        # Put here the code you want to be executed BEFORE the original function is called
        print(""Before the function runs"")

        # Call the function here (using parentheses)
        a_function_to_decorate()

        # Put here the code you want to be executed AFTER the original function is called
        print(""After the function runs"")

    # At this point, ""a_function_to_decorate"" HAS NEVER BEEN EXECUTED.
    # We return the wrapper function we have just created.
    # The wrapper contains the function and the code to execute before and after. It’s ready to use!
    return the_wrapper_around_the_original_function

# Now imagine you create a function you don't want to ever touch again.
def a_stand_alone_function():
    print(""I am a stand alone function, don't you dare modify me"")

a_stand_alone_function() 
#outputs: I am a stand alone function, don't you dare modify me

# Well, you can decorate it to extend its behavior.
# Just pass it to the decorator, it will wrap it dynamically in 
# any code you want and return you a new function ready to be used:

a_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)
a_stand_alone_function_decorated()
#outputs:
#Before the function runs
#I am a stand alone function, don't you dare modify me
#After the function runs

```

Now, you probably want that every time you call `a_stand_alone_function`, `a_stand_alone_function_decorated` is called instead. That’s easy, just overwrite `a_stand_alone_function` with the function returned by `my_shiny_new_decorator`:

```
a_stand_alone_function = my_shiny_new_decorator(a_stand_alone_function)
a_stand_alone_function()
#outputs:
#Before the function runs
#I am a stand alone function, don't you dare modify me
#After the function runs

# That’s EXACTLY what decorators do!

```

Decorators demystified
----------------------

The previous example, using the decorator syntax:

```
@my_shiny_new_decorator
def another_stand_alone_function():
    print(""Leave me alone"")

another_stand_alone_function()  
#outputs:  
#Before the function runs
#Leave me alone
#After the function runs

```

Yes, that’s all, it’s that simple. `@decorator` is just a shortcut to:

```
another_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)

```

Decorators are just a pythonic variant of the [decorator design pattern](http://en.wikipedia.org/wiki/Decorator_pattern). There are several classic design patterns embedded in Python to ease development (like iterators).

Of course, you can accumulate decorators:

```
def bread(func):
    def wrapper():
        print(""</''''''\>"")
        func()
        print(""<\______/>"")
    return wrapper

def ingredients(func):
    def wrapper():
        print(""#tomatoes#"")
        func()
        print(""~salad~"")
    return wrapper

def sandwich(food=""--ham--""):
    print(food)

sandwich()
#outputs: --ham--
sandwich = bread(ingredients(sandwich))
sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>

```

Using the Python decorator syntax:

```
@bread
@ingredients
def sandwich(food=""--ham--""):
    print(food)

sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>

```

The order you set the decorators MATTERS:

```
@ingredients
@bread
def strange_sandwich(food=""--ham--""):
    print(food)

strange_sandwich()
#outputs:
##tomatoes#
#</''''''\>
# --ham--
#<\______/>
# ~salad~

```

---

Now: to answer the question...
==============================

As a conclusion, you can easily see how to answer the question:

```
# The decorator to make it bold
def makebold(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return ""<b>"" + fn() + ""</b>""
    return wrapper

# The decorator to make it italic
def makeitalic(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return ""<i>"" + fn() + ""</i>""
    return wrapper

@makebold
@makeitalic
def say():
    return ""hello""

print(say())
#outputs: <b><i>hello</i></b>

# This is the exact equivalent to 
def say():
    return ""hello""
say = makebold(makeitalic(say))

print(say())
#outputs: <b><i>hello</i></b>

```

You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.

---

Taking decorators to the next level
===================================

Passing arguments to the decorated function
-------------------------------------------

```
# It’s not black magic, you just have to let the wrapper 
# pass the argument:

def a_decorator_passing_arguments(function_to_decorate):
    def a_wrapper_accepting_arguments(arg1, arg2):
        print(""I got args! Look: {0}, {1}"".format(arg1, arg2))
        function_to_decorate(arg1, arg2)
    return a_wrapper_accepting_arguments

# Since when you are calling the function returned by the decorator, you are
# calling the wrapper, passing arguments to the wrapper will let it pass them to 
# the decorated function

@a_decorator_passing_arguments
def print_full_name(first_name, last_name):
    print(""My name is {0} {1}"".format(first_name, last_name))
    
print_full_name(""Peter"", ""Venkman"")
# outputs:
#I got args! Look: Peter Venkman
#My name is Peter Venkman

```

Decorating methods
------------------

One nifty thing about Python is that methods and functions are really the same. The only difference is that methods expect that their first argument is a reference to the current object (`self`).

That means you can build a decorator for methods the same way! Just remember to take `self` into consideration:

```
def method_friendly_decorator(method_to_decorate):
    def wrapper(self, lie):
        lie = lie - 3 # very friendly, decrease age even more :-)
        return method_to_decorate(self, lie)
    return wrapper
    
    
class Lucy(object):
    
    def __init__(self):
        self.age = 32
    
    @method_friendly_decorator
    def sayYourAge(self, lie):
        print(""I am {0}, what did you think?"".format(self.age + lie))
        
l = Lucy()
l.sayYourAge(-3)
#outputs: I am 26, what did you think?

```

If you’re making general-purpose decorator--one you’ll apply to any function or method, no matter its arguments--then just use `*args, **kwargs`:

```
def a_decorator_passing_arbitrary_arguments(function_to_decorate):
    # The wrapper accepts any arguments
    def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):
        print(""Do I have args?:"")
        print(args)
        print(kwargs)
        # Then you unpack the arguments, here *args, **kwargs
        # If you are not familiar with unpacking, check:
        # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/
        function_to_decorate(*args, **kwargs)
    return a_wrapper_accepting_arbitrary_arguments

@a_decorator_passing_arbitrary_arguments
def function_with_no_argument():
    print(""Python is cool, no argument here."")

function_with_no_argument()
#outputs
#Do I have args?:
#()
#{}
#Python is cool, no argument here.

@a_decorator_passing_arbitrary_arguments
def function_with_arguments(a, b, c):
    print(a, b, c)
    
function_with_arguments(1,2,3)
#outputs
#Do I have args?:
#(1, 2, 3)
#{}
#1 2 3 
 
@a_decorator_passing_arbitrary_arguments
def function_with_named_arguments(a, b, c, platypus=""Why not ?""):
    print(""Do {0}, {1} and {2} like platypus? {3}"".format(a, b, c, platypus))

function_with_named_arguments(""Bill"", ""Linus"", ""Steve"", platypus=""Indeed!"")
#outputs
#Do I have args ? :
#('Bill', 'Linus', 'Steve')
#{'platypus': 'Indeed!'}
#Do Bill, Linus and Steve like platypus? Indeed!

class Mary(object):
    
    def __init__(self):
        self.age = 31
    
    @a_decorator_passing_arbitrary_arguments
    def sayYourAge(self, lie=-3): # You can now add a default value
        print(""I am {0}, what did you think?"".format(self.age + lie))

m = Mary()
m.sayYourAge()
#outputs
# Do I have args?:
#(<__main__.Mary object at 0xb7d303ac>,)
#{}
#I am 28, what did you think?

```

Passing arguments to the decorator
----------------------------------

Great, now what would you say about passing arguments to the decorator itself?

This can get somewhat twisted, since a decorator must accept a function as an argument. Therefore, you cannot pass the decorated function’s arguments directly to the decorator.

Before rushing to the solution, let’s write a little reminder:

```
# Decorators are ORDINARY functions
def my_decorator(func):
    print(""I am an ordinary function"")
    def wrapper():
        print(""I am function returned by the decorator"")
        func()
    return wrapper

# Therefore, you can call it without any ""@""

def lazy_function():
    print(""zzzzzzzz"")

decorated_function = my_decorator(lazy_function)
#outputs: I am an ordinary function
            
# It outputs ""I am an ordinary function"", because that’s just what you do:
# calling a function. Nothing magic.

@my_decorator
def lazy_function():
    print(""zzzzzzzz"")
    
#outputs: I am an ordinary function

```

It’s exactly the same. ""`my_decorator`"" is called. So when you `@my_decorator`, you are telling Python to call the function 'labelled by the variable ""`my_decorator`""'.

This is important! The label you give can point directly to the decorator—**or not**.

Let’s get evil. ☺

```
def decorator_maker():
    
    print(""I make decorators! I am executed only once: ""
          ""when you make me create a decorator."")
            
    def my_decorator(func):
        
        print(""I am a decorator! I am executed only when you decorate a function."")
               
        def wrapped():
            print(""I am the wrapper around the decorated function. ""
                  ""I am called when you call the decorated function. ""
                  ""As the wrapper, I return the RESULT of the decorated function."")
            return func()
        
        print(""As the decorator, I return the wrapped function."")
        
        return wrapped
    
    print(""As a decorator maker, I return a decorator"")
    return my_decorator
            
# Let’s create a decorator. It’s just a new function after all.
new_decorator = decorator_maker()       
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator

# Then we decorate the function
            
def decorated_function():
    print(""I am the decorated function."")
   
decorated_function = new_decorator(decorated_function)
#outputs:
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function
     
# Let’s call the function:
decorated_function()
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.

```

No surprise here.

Let’s do EXACTLY the same thing, but skip all the pesky intermediate variables:

```
def decorated_function():
    print(""I am the decorated function."")
decorated_function = decorator_maker()(decorated_function)
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

# Finally:
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.

```

Let’s make it *even shorter*:

```
@decorator_maker()
def decorated_function():
    print(""I am the decorated function."")
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

#Eventually: 
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.

```

Hey, did you see that? We used a function call with the ""`@`"" syntax! :-)

So, back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?

```
def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):
    
    print(""I make decorators! And I accept arguments: {0}, {1}"".format(decorator_arg1, decorator_arg2))
            
    def my_decorator(func):
        # The ability to pass arguments here is a gift from closures.
        # If you are not comfortable with closures, you can assume it’s ok,
        # or read: https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python
        print(""I am the decorator. Somehow you passed me arguments: {0}, {1}"".format(decorator_arg1, decorator_arg2))
               
        # Don't confuse decorator arguments and function arguments!
        def wrapped(function_arg1, function_arg2) :
            print(""I am the wrapper around the decorated function.\n""
                  ""I can access all the variables\n""
                  ""\t- from the decorator: {0} {1}\n""
                  ""\t- from the function call: {2} {3}\n""
                  ""Then I can pass them to the decorated function""
                  .format(decorator_arg1, decorator_arg2,
                          function_arg1, function_arg2))
            return func(function_arg1, function_arg2)
        
        return wrapped
    
    return my_decorator

@decorator_maker_with_arguments(""Leonard"", ""Sheldon"")
def decorated_function_with_arguments(function_arg1, function_arg2):
    print(""I am the decorated function and only knows about my arguments: {0}""
           "" {1}"".format(function_arg1, function_arg2))
          
decorated_function_with_arguments(""Rajesh"", ""Howard"")
#outputs:
#I make decorators! And I accept arguments: Leonard Sheldon
#I am the decorator. Somehow you passed me arguments: Leonard Sheldon
#I am the wrapper around the decorated function. 
#I can access all the variables 
#   - from the decorator: Leonard Sheldon 
#   - from the function call: Rajesh Howard 
#Then I can pass them to the decorated function
#I am the decorated function and only knows about my arguments: Rajesh Howard

```

Here it is: a decorator with arguments. Arguments can be set as variable:

```
c1 = ""Penny""
c2 = ""Leslie""

@decorator_maker_with_arguments(""Leonard"", c1)
def decorated_function_with_arguments(function_arg1, function_arg2):
    print(""I am the decorated function and only knows about my arguments:""
           "" {0} {1}"".format(function_arg1, function_arg2))

decorated_function_with_arguments(c2, ""Howard"")
#outputs:
#I make decorators! And I accept arguments: Leonard Penny
#I am the decorator. Somehow you passed me arguments: Leonard Penny
#I am the wrapper around the decorated function. 
#I can access all the variables 
#   - from the decorator: Leonard Penny 
#   - from the function call: Leslie Howard 
#Then I can pass them to the decorated function
#I am the decorated function and only know about my arguments: Leslie Howard

```

As you can see, you can pass arguments to the decorator like any function using this trick. You can even use `*args, **kwargs` if you wish. But remember decorators are called **only once**. Just when Python imports the script. You can't dynamically set the arguments afterwards. When you do ""import x"", **the function is already decorated**, so you can't
change anything.

---

Let’s practice: decorating a decorator
======================================

Okay, as a bonus, I'll give you a snippet to make any decorator accept generically any argument. After all, in order to accept arguments, we created our decorator using another function.

We wrapped the decorator.

Anything else we saw recently that wrapped function?

Oh yes, decorators!

Let’s have some fun and write a decorator for the decorators:

```
def decorator_with_args(decorator_to_enhance):
    """""" 
    This function is supposed to be used as a decorator.
    It must decorate an other function, that is intended to be used as a decorator.
    Take a cup of coffee.
    It will allow any decorator to accept an arbitrary number of arguments,
    saving you the headache to remember how to do that every time.
    """"""
    
    # We use the same trick we did to pass arguments
    def decorator_maker(*args, **kwargs):
       
        # We create on the fly a decorator that accepts only a function
        # but keeps the passed arguments from the maker.
        def decorator_wrapper(func):
       
            # We return the result of the original decorator, which, after all, 
            # IS JUST AN ORDINARY FUNCTION (which returns a function).
            # Only pitfall: the decorator must have this specific signature or it won't work:
            return decorator_to_enhance(func, *args, **kwargs)
        
        return decorator_wrapper
    
    return decorator_maker
       

```

It can be used as follows:

```
# You create the function you will use as a decorator. And stick a decorator on it :-)
# Don't forget, the signature is ""decorator(func, *args, **kwargs)""
@decorator_with_args 
def decorated_decorator(func, *args, **kwargs): 
    def wrapper(function_arg1, function_arg2):
        print(""Decorated with {0} {1}"".format(args, kwargs))
        return func(function_arg1, function_arg2)
    return wrapper
    
# Then you decorate the functions you wish with your brand new decorated decorator.

@decorated_decorator(42, 404, 1024)
def decorated_function(function_arg1, function_arg2):
    print(""Hello {0} {1}"".format(function_arg1, function_arg2))

decorated_function(""Universe and"", ""everything"")
#outputs:
#Decorated with (42, 404, 1024) {}
#Hello Universe and everything

# Whoooot!

```

I know, the last time you had this feeling, it was after listening a guy saying: ""before understanding recursion, you must first understand recursion"". But now, don't you feel good about mastering this?

---

Best practices: decorators
==========================

* Decorators were introduced in Python 2.4, so be sure your code will be run on >= 2.4.
* Decorators slow down the function call. Keep that in mind.
* **You cannot un-decorate a function.** (There *are* hacks to create decorators that can be removed, but nobody uses them.) So once a function is decorated, it’s decorated *for all the code*.
* Decorators wrap functions, which can make them hard to debug. (This gets better from Python >= 2.5; see below.)

The `functools` module was introduced in Python 2.5. It includes the function `functools.wraps()`, which copies the name, module, and docstring of the decorated function to its wrapper.

(Fun fact: `functools.wraps()` is a decorator! ☺)

```
# For debugging, the stacktrace prints you the function __name__
def foo():
    print(""foo"")
    
print(foo.__name__)
#outputs: foo
    
# With a decorator, it gets messy    
def bar(func):
    def wrapper():
        print(""bar"")
        return func()
    return wrapper

@bar
def foo():
    print(""foo"")

print(foo.__name__)
#outputs: wrapper

# ""functools"" can help for that

import functools

def bar(func):
    # We say that ""wrapper"", is wrapping ""func""
    # and the magic begins
    @functools.wraps(func)
    def wrapper():
        print(""bar"")
        return func()
    return wrapper

@bar
def foo():
    print(""foo"")

print(foo.__name__)
#outputs: foo

```

---

How can the decorators be useful?
=================================

**Now the big question:** What can I use decorators for?

Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities. Classic uses are extending a function behavior from an external lib (you can't modify it), or for debugging (you don't want to modify it because it’s temporary).

You can use them to extend several functions in a DRY’s way, like so:

```
def benchmark(func):
    """"""
    A decorator that prints the time a function takes
    to execute.
    """"""
    import time
    def wrapper(*args, **kwargs):
        t = time.clock()
        res = func(*args, **kwargs)
        print(""{0} {1}"".format(func.__name__, time.clock()-t))
        return res
    return wrapper


def logging(func):
    """"""
    A decorator that logs the activity of the script.
    (it actually just prints it, but it could be logging!)
    """"""
    def wrapper(*args, **kwargs):
        res = func(*args, **kwargs)
        print(""{0} {1} {2}"".format(func.__name__, args, kwargs))
        return res
    return wrapper


def counter(func):
    """"""
    A decorator that counts and prints the number of times a function has been executed
    """"""
    def wrapper(*args, **kwargs):
        wrapper.count = wrapper.count + 1
        res = func(*args, **kwargs)
        print(""{0} has been used: {1}x"".format(func.__name__, wrapper.count))
        return res
    wrapper.count = 0
    return wrapper

@counter
@benchmark
@logging
def reverse_string(string):
    return str(reversed(string))

print(reverse_string(""Able was I ere I saw Elba""))
print(reverse_string(""A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!""))

#outputs:
#reverse_string ('Able was I ere I saw Elba',) {}
#wrapper 0.0
#wrapper has been used: 1x 
#ablE was I ere I saw elbA
#reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}
#wrapper 0.0
#wrapper has been used: 2x
#!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A

```

Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:

```
@counter
@benchmark
@logging
def get_random_futurama_quote():
    from urllib import urlopen
    result = urlopen(""http://subfusion.net/cgi-bin/quote.pl?quote=futurama"").read()
    try:
        value = result.split(""<br><b><hr><br>"")[1].split(""<br><br><hr>"")[0]
        return value.strip()
    except:
        return ""No, I'm ... doesn't!""

    
print(get_random_futurama_quote())
print(get_random_futurama_quote())

#outputs:
#get_random_futurama_quote () {}
#wrapper 0.02
#wrapper has been used: 1x
#The laws of science be a harsh mistress.
#get_random_futurama_quote () {}
#wrapper 0.01
#wrapper has been used: 2x
#Curse you, merciful Poseidon!

```

Python itself provides several decorators: `property`, `staticmethod`, etc.

* Django uses decorators to manage caching and view permissions.
* Twisted to fake inlining asynchronous functions calls.

This really is a large playground."
"I have a timezone aware `timestamptz` field in PostgreSQL. When I pull data from the table, I then want to subtract the time right now so I can get it's age.

The problem I'm having is that both `datetime.datetime.now()` and `datetime.datetime.utcnow()` seem to return timezone unaware timestamps, which results in me getting this error:

```
TypeError: can't subtract offset-naive and offset-aware datetimes 

```

Is there a way to avoid this (preferably without a third-party module being used).

EDIT: Thanks for the suggestions, however trying to adjust the timezone seems to give me errors.. so I'm just going to use timezone unaware timestamps in PG and always insert using:

```
NOW() AT TIME ZONE 'UTC'

```

That way all my timestamps are UTC by default (even though it's more annoying to do this).","Have you tried to remove the timezone awareness?

From <http://pytz.sourceforge.net/>

```
naive = dt.replace(tzinfo=None)

```

may have to add time zone conversion as well.

edit: Please be aware the age of this answer. An answer involving ADDing the timezone info instead of removing it in python 3 is below. <https://stackoverflow.com/a/25662061/93380>"
"How do I convert a string to a date object in python?

The string would be: `""24052010""` (corresponding to the format: `""%d%m%Y""`)

I don't want a datetime.datetime object, but rather a datetime.date.","You can use [`strptime`](http://docs.python.org/library/datetime.html#strftime-and-strptime-behavior) in the [`datetime`](http://docs.python.org/library/datetime.html) package of Python:

```
>>> import datetime
>>> datetime.datetime.strptime('24052010', ""%d%m%Y"").date()
datetime.date(2010, 5, 24)

```"
"I have a dataset

```
category
cat a
cat b
cat a

```

I'd like to return something like the following which shows the unique values and their frequencies

```
category   freq 
cat a       2
cat b       1

```","Use [`value_counts()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) as @DSM commented.

```
In [37]:
df = pd.DataFrame({'a':list('abssbab')})
df['a'].value_counts()

Out[37]:

b    3
a    2
s    2
dtype: int64

```

Also `groupby` and `count`. Many ways to skin a cat here.

```
In [38]:
df.groupby('a').count()

Out[38]:

   a
a   
a  2
b  3
s  2

[3 rows x 1 columns]

```

See [the online docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html).

If you wanted to add frequency back to the original dataframe use `transform` to return an aligned index:

```
In [41]:
df['freq'] = df.groupby('a')['a'].transform('count')
df

Out[41]:

   a freq
0  a    2
1  b    3
2  s    2
3  s    2
4  b    3
5  a    2
6  b    3

[7 rows x 2 columns]

```"
"How can one create a legend for a line graph in Matplotlib's PyPlot without creating any extra variables?

Please consider the graphing script below:

```
if __name__ == '__main__':
    PyPlot.plot(length, bubble, 'b-',
                length, ins, 'r-',
                length, merge_r, 'g+',
                length, merge_i, 'p-', )
    PyPlot.title(""Combined Statistics"")
    PyPlot.xlabel(""Length of list (number)"")
    PyPlot.ylabel(""Time taken (seconds)"")
    PyPlot.show()

```

As you can see, this is a very basic use of matplotlib's PyPlot. This generates the following graph:

![Graph](https://i.sstatic.net/sRcuYm.png)

However, it is unclear which line is which. Thus, I need a legend; however, taking a look at the following example below ([from the official site](http://matplotlib.org/users/legend_guide.html)):

```
ax = subplot(1,1,1)
p1, = ax.plot([1,2,3], label=""line 1"")
p2, = ax.plot([3,2,1], label=""line 2"")
p3, = ax.plot([2,3,1], label=""line 3"")

handles, labels = ax.get_legend_handles_labels()

# reverse the order
ax.legend(handles[::-1], labels[::-1])

# or sort them by labels
import operator
hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
handles2, labels2 = zip(*hl)

ax.legend(handles2, labels2)

```

You will see that I need to create an extra variable `ax`. How can I add a legend to my graph *without* having to create this extra variable and retaining the simplicity of my current script?","Add a `label=` to each of your [`plot()`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) calls, and then call [`legend(loc='upper left')`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend).

Consider this sample (tested with Python 3.8.0):

```
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0, 20, 1000)
y1 = np.sin(x)
y2 = np.cos(x)

plt.plot(x, y1, ""-b"", label=""sine"")
plt.plot(x, y2, ""-r"", label=""cosine"")
plt.legend(loc=""upper left"")
plt.ylim(-1.5, 2.0)
plt.show()

```

[![enter image description here](https://i.sstatic.net/GyMXH.png)](https://i.sstatic.net/GyMXH.png)
Slightly modified from this tutorial: <http://jakevdp.github.io/mpl_tutorial/tutorial_pages/tut1.html>"
"I am trying to do a grab everything after the `'</html>'` tag and delete it, but my code doesn't seem to be doing anything. Does `.replace()` not support regex?

```
z.write(article.replace('</html>.+', '</html>'))

```","No. Regular expressions in Python are handled by the [`re`](http://docs.python.org/library/re.html) module.

```
article = re.sub(r'(?is)</html>.+', '</html>', article)

```

In general:

```
str_output = re.sub(regex_search_term, regex_replacement, str_input)

```"
"```
as3:~/ngokevin-site# nano content/blog/20140114_test-chinese.mkd
as3:~/ngokevin-site# wok
Traceback (most recent call last):
  File ""/usr/local/bin/wok"", line 4, in
    Engine()
  File ""/usr/local/lib/python2.7/site-packages/wok/engine.py"", line 104, in init
    self.load_pages()
  File ""/usr/local/lib/python2.7/site-packages/wok/engine.py"", line 238, in load_pages
    p = Page.from_file(os.path.join(root, f), self.options, self, renderer)
  File ""/usr/local/lib/python2.7/site-packages/wok/page.py"", line 111, in from_file
    page.meta['content'] = page.renderer.render(page.original)
  File ""/usr/local/lib/python2.7/site-packages/wok/renderers.py"", line 46, in render
    return markdown(plain, Markdown.plugins)
  File ""/usr/local/lib/python2.7/site-packages/markdown/init.py"", line 419, in markdown
    return md.convert(text)
  File ""/usr/local/lib/python2.7/site-packages/markdown/init.py"", line 281, in convert
    source = unicode(source)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 1: ordinal not in range(128). -- Note: Markdown only accepts unicode input!

```

How to fix it?

In some other python-based static blog apps, Chinese post can be published successfully.
Such as this app: <http://github.com/vrypan/bucket3>. In my site <http://bc3.brite.biz/>, Chinese post can be published successfully.","tl;dr / quick fix
=================

* Don't decode/encode willy nilly
* Don't assume your strings are UTF-8 encoded
* Try to convert strings to Unicode strings as soon as possible in your code
* Fix your locale: [How to solve UnicodeDecodeError in Python 3.6?](https://stackoverflow.com/questions/51026315/how-to-solve-unicodedecodeerror-in-python-3-6/51027262#51027262)
* Don't be tempted to use quick `reload` hacks

Unicode Zen in Python 2.x - The Long Version
============================================

Without seeing the source it's difficult to know the root cause, so I'll have to speak generally.

`UnicodeDecodeError: 'ascii' codec can't decode byte` generally happens when you try to convert a Python 2.x `str` that contains non-ASCII to a Unicode string without specifying the encoding of the original string.

In brief, Unicode strings are an entirely separate type of Python string that does not contain any encoding. They only hold Unicode **point codes** and therefore can hold any Unicode point from across the entire spectrum. Strings contain encoded text, beit UTF-8, UTF-16, ISO-8895-1, GBK, Big5 etc. **Strings are decoded to Unicode** and **Unicodes are encoded to strings**. Files and text data are always transferred in encoded strings.

The Markdown module authors probably use `unicode()` (where the exception is thrown) as a quality gate to the rest of the code - it will convert ASCII or re-wrap existing Unicodes strings to a new Unicode string. The Markdown authors can't know the encoding of the incoming string so will rely on you to decode strings to Unicode strings before passing to Markdown.

Unicode strings can be declared in your code using the `u` prefix to strings. E.g.

```
>>> my_u = u'my Ã¼nicÃ´dÃ© strÄ¯ng'
>>> type(my_u)
<type 'unicode'>

```

Unicode strings may also come from file, databases and network modules. When this happens, you don't need to worry about the encoding.

Gotchas
=======

Conversion from `str` to Unicode can happen even when you don't explicitly call `unicode()`.

The following scenarios cause `UnicodeDecodeError` exceptions:

```
# Explicit conversion without encoding
unicode('â‚¬')

# New style format string into Unicode string
# Python will try to convert value string to Unicode first
u""The currency is: {}"".format('â‚¬')

# Old style format string into Unicode string
# Python will try to convert value string to Unicode first
u'The currency is: %s' % 'â‚¬'

# Append string to Unicode
# Python will try to convert string to Unicode first
u'The currency is: ' + 'â‚¬'         

```

Examples
--------

In the following diagram, you can see how the word `cafÃ©` has been encoded in either ""UTF-8"" or ""Cp1252"" encoding depending on the terminal type. In both examples, `caf` is just regular ascii. In UTF-8, `Ã©` is encoded using two bytes. In ""Cp1252"", Ã© is 0xE9 (which is also happens to be the Unicode point value (it's no coincidence)). The correct `decode()` is invoked and conversion to a Python Unicode is successfull:
[![Diagram of a string being converted to a Python Unicode string](https://i.sstatic.net/uUUBd.jpg)](https://i.sstatic.net/uUUBd.jpg)

In this diagram, `decode()` is called with `ascii` (which is the same as calling `unicode()` without an encoding given). As ASCII can't contain bytes greater than `0x7F`, this will throw a `UnicodeDecodeError` exception:

[![Diagram of a string being converted to a Python Unicode string with the wrong encoding](https://i.sstatic.net/oyk7O.jpg)](https://i.sstatic.net/oyk7O.jpg)

The Unicode Sandwich
====================

It's good practice to form a Unicode sandwich in your code, where you decode all incoming data to Unicode strings, work with Unicodes, then encode to `str`s on the way out. This saves you from worrying about the encoding of strings in the middle of your code.

Input / Decode
--------------

### Source code

If you need to bake non-ASCII into your source code, just create Unicode strings by prefixing the string with a `u`. E.g.

```
u'ZÃ¼rich'

```

To allow Python to decode your source code, you will need to add an encoding header to match the actual encoding of your file. For example, if your file was encoded as 'UTF-8', you would use:

```
# encoding: utf-8

```

This is only necessary when you have non-ASCII in your **source code**.

### Files

Usually non-ASCII data is received from a file. The `io` module provides a TextWrapper that decodes your file on the fly, using a given `encoding`. You must use the correct encoding for the file - it can't be easily guessed. For example, for a UTF-8 file:

```
import io
with io.open(""my_utf8_file.txt"", ""r"", encoding=""utf-8"") as my_file:
     my_unicode_string = my_file.read() 

```

`my_unicode_string` would then be suitable for passing to Markdown. If a `UnicodeDecodeError` from the `read()` line, then you've probably used the wrong encoding value.

### CSV Files

The Python 2.7 CSV module does not support non-ASCII characters ðŸ˜©. Help is at hand, however, with <https://pypi.python.org/pypi/backports.csv>.

Use it like above but pass the opened file to it:

```
from backports import csv
import io
with io.open(""my_utf8_file.txt"", ""r"", encoding=""utf-8"") as my_file:
    for row in csv.reader(my_file):
        yield row

```

### Databases

Most Python database drivers can return data in Unicode, but usually require a little configuration. Always use Unicode strings for SQL queries.

MySQL

In the connection string add:

```
charset='utf8',
use_unicode=True

```

E.g.

```
>>> db = MySQLdb.connect(host=""localhost"", user='root', passwd='passwd', db='sandbox', use_unicode=True, charset=""utf8"")

```

PostgreSQL

Add:

```
psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)
psycopg2.extensions.register_type(psycopg2.extensions.UNICODEARRAY)

```

### HTTP

Web pages can be encoded in just about any encoding. The `Content-type` header should contain a `charset` field to hint at the encoding. The content can then be decoded manually against this value. Alternatively, [Python-Requests](https://requests.readthedocs.io/ ""Python-Requests"") returns Unicodes in `response.text`.

### Manually

If you must decode strings manually, you can simply do `my_string.decode(encoding)`, where `encoding` is the appropriate encoding. Python 2.x supported codecs are given here: [Standard Encodings](https://docs.python.org/2/library/codecs.html#standard-encodings ""Standard Encodings""). Again, if you get `UnicodeDecodeError` then you've probably got the wrong encoding.

The meat of the sandwich
------------------------

Work with Unicodes as you would normal strs.

Output
------

### stdout / printing

`print` writes through the stdout stream. Python tries to configure an encoder on stdout so that Unicodes are encoded to the console's encoding. For example, if a Linux shell's `locale` is `en_GB.UTF-8`, the output will be encoded to `UTF-8`. On Windows, you will be limited to an 8bit code page.

An incorrectly configured console, such as corrupt locale, can lead to unexpected print errors. `PYTHONIOENCODING` environment variable can force the encoding for stdout.

### Files

Just like input, `io.open` can be used to transparently convert Unicodes to encoded byte strings.

### Database

The same configuration for reading will allow Unicodes to be written directly.

Python 3
========

Python 3 is no more Unicode capable than Python 2.x is, however it is slightly less confused on the topic. E.g the regular `str` is now a Unicode string and the old `str` is now `bytes`.

The default encoding is UTF-8, so if you `.decode()` a byte string without giving an encoding, Python 3 uses UTF-8 encoding. This probably fixes 50% of people's Unicode problems.

Further, `open()` operates in text mode by default, so returns decoded `str` (Unicode ones). The encoding is derived from your locale, which tends to be UTF-8 on Un\*x systems or an 8-bit code page, such as windows-1251, on Windows boxes.

Why you shouldn't use `sys.setdefaultencoding('utf8')`
======================================================

It's a nasty hack (there's a reason you have to use `reload`) that will only mask problems and hinder your migration to Python 3.x. Understand the problem, fix the root cause and enjoy Unicode zen.
See [Why should we NOT use sys.setdefaultencoding(""utf-8"") in a py script?](https://stackoverflow.com/questions/3828723/why-should-we-not-use-sys-setdefaultencodingutf-8-in-a-py-script/34378962#34378962) for further details"
"I have this string:

```
mystring = 'Here is  some   text   I      wrote   '

```

How can I substitute the double, triple (...) whitespace chracters with a single space, so that I get:

```
mystring = 'Here is some text I wrote'

```","A simple possibility (if you'd rather avoid REs) is

```
' '.join(mystring.split())

```

The split and join perform the task you're explicitly asking about -- plus, they also do the extra one that you don't talk about but is seen in your example, removing trailing spaces;-)."
"Why is the below item failing? Why does it succeed with ""latin-1"" codec?

```
o = ""a test of \xe9 char"" #I want this to remain a string as this is what I am receiving
v = o.decode(""utf-8"")

```

Which results in:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python27\lib\encodings\utf_8.py"", line 16, in decode
     return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xe9 in position 10: invalid continuation byte

```","I had the same error when I tried to open a CSV file by `pandas.read_csv`
method.

The solution was change the encoding to **`latin-1`**:

```
pd.read_csv('ml-100k/u.item', sep='|', names=m_cols , encoding='latin-1')

```"
"I have a semilogx plot and I would like to remove the xticks. I tried:

```
plt.gca().set_xticks([])
plt.xticks([])
ax.set_xticks([])

```

The grid disappears (ok), but small ticks (at the place of the main ticks) remain. How to remove them?","The [`plt.tick_params`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tick_params.html) method is very useful for stuff like this. This code turns off major and minor ticks and removes the labels from the x-axis.

Note that there is also [`ax.tick_params`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.tick_params.html#matplotlib.axes.Axes.tick_params) for `matplotlib.axes.Axes` objects.

```
from matplotlib import pyplot as plt
plt.plot(range(10))
plt.tick_params(
    axis='x',          # changes apply to the x-axis
    which='both',      # both major and minor ticks are affected
    bottom=False,      # ticks along the bottom edge are off
    top=False,         # ticks along the top edge are off
    labelbottom=False) # labels along the bottom edge are off
plt.show()
plt.savefig('plot')
plt.clf()

```

![enter image description here](https://i.sstatic.net/0emBv.png)"
"How do I sort a NumPy array by its nth column?

For example, given:

```
a = array([[9, 2, 3],
           [4, 5, 6],
           [7, 0, 5]])

```

I want to sort the rows of `a` by the second column to obtain:

```
array([[7, 0, 5],
       [9, 2, 3],
       [4, 5, 6]])

```","To sort by the second column of `a`:

```
a[a[:, 1].argsort()]

```"
"How do I split a list of arbitrary length into equal sized chunks?

---

See also: [How to iterate over a list in chunks](https://stackoverflow.com/q/434287).  
To chunk strings, see [Split string every nth character?](https://stackoverflow.com/questions/9475241).","Here's a generator that yields evenly-sized chunks:

```
def chunks(lst, n):
    """"""Yield successive n-sized chunks from lst.""""""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

```

```
import pprint
pprint.pprint(list(chunks(range(10, 75), 10)))
[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
 [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
 [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
 [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
 [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
 [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
 [70, 71, 72, 73, 74]]

```

For Python 2, using `xrange` instead of `range`:

```
def chunks(lst, n):
    """"""Yield successive n-sized chunks from lst.""""""
    for i in xrange(0, len(lst), n):
        yield lst[i:i + n]

```

---

Below is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:

```
[lst[i:i + n] for i in range(0, len(lst), n)]

```

For Python 2:

```
[lst[i:i + n] for i in xrange(0, len(lst), n)]

```"
"I am getting a `ValueError: cannot reindex from a duplicate axis` when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.

Here is my session inside of `ipdb` trace. I have a DataFrame with string index, and integer columns, float values. However when I try to create `sum` index for sum of all columns I am getting `ValueError: cannot reindex from a duplicate axis` error. I created a small DataFrame with the same characteristics, but was not able to reproduce the problem, what could I be missing?

I don't really understand what `ValueError: cannot reindex from a duplicate axis`means, what does this error message mean? Maybe this will help me diagnose the problem, and this is most answerable part of my question.

```
ipdb> type(affinity_matrix)
<class 'pandas.core.frame.DataFrame'>
ipdb> affinity_matrix.shape
(333, 10)
ipdb> affinity_matrix.columns
Int64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')
ipdb> affinity_matrix.index
Index([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')

ipdb> affinity_matrix.values.dtype
dtype('float64')
ipdb> 'sums' in affinity_matrix.index
False

```

Here is the error:

```
ipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)
*** ValueError: cannot reindex from a duplicate axis

```

I tried to reproduce this with a simple example, but I failed

```
In [32]: import pandas as pd

In [33]: import numpy as np

In [34]: a = np.arange(35).reshape(5,7)

In [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))

In [36]: df.values.dtype
Out[36]: dtype('int64')

In [37]: df.loc['sums'] = df.sum(axis=0)

In [38]: df
Out[38]: 
      10  11  12  13  14  15   16
x      0   1   2   3   4   5    6
y      7   8   9  10  11  12   13
u     14  15  16  17  18  19   20
z     21  22  23  24  25  26   27
w     28  29  30  31  32  33   34
sums  70  75  80  85  90  95  100

```","This error usually rises when you join / assign to a column when the index has duplicate values. Since you are assigning to a row, I suspect that there is a duplicate value in `affinity_matrix.columns`, perhaps not shown in your question."
"I'm trying to download and save an image from the web using python's `requests` module.

Here is the (working) code I used:

```
img = urllib2.urlopen(settings.STATICMAP_URL.format(**data))
with open(path, 'w') as f:
    f.write(img.read())

```

Here is the new (non-working) code using `requests`:

```
r = requests.get(settings.STATICMAP_URL.format(**data))
if r.status_code == 200:
    img = r.raw.read()
    with open(path, 'w') as f:
        f.write(img)

```

Can you help me on what attribute from the response to use from `requests`?","You can either use the [`response.raw` file object](https://requests.readthedocs.io/en/latest/api/#requests.Response.raw), or iterate over the response.

To use the `response.raw` file-like object will not, by default, decode compressed responses (with GZIP or deflate). You can force it to decompress for you anyway by setting the `decode_content` attribute to `True` (`requests` sets it to `False` to control decoding itself). You can then use [`shutil.copyfileobj()`](https://docs.python.org/2/library/shutil.html#shutil.copyfileobj) to have Python stream the data to a file object:

```
import requests
import shutil

r = requests.get(settings.STATICMAP_URL.format(**data), stream=True)
if r.status_code == 200:
    with open(path, 'wb') as f:
        r.raw.decode_content = True
        shutil.copyfileobj(r.raw, f)        

```

To iterate over the response use a loop; iterating like this ensures that data is decompressed by this stage:

```
r = requests.get(settings.STATICMAP_URL.format(**data), stream=True)
if r.status_code == 200:
    with open(path, 'wb') as f:
        for chunk in r:
            f.write(chunk)

```

This'll read the data in 128 byte chunks; if you feel another chunk size works better, use the [`Response.iter_content()` method](https://requests.readthedocs.io/en/latest/api/#requests.Response.iter_content) with a custom chunk size:

```
r = requests.get(settings.STATICMAP_URL.format(**data), stream=True)
if r.status_code == 200:
    with open(path, 'wb') as f:
        for chunk in r.iter_content(1024):
            f.write(chunk)

```

Note that you need to open the destination file in binary mode to ensure python doesn't try and translate newlines for you. We also set `stream=True` so that `requests` doesn't download the whole image into memory first."
"I am trying to get my program to print out `""banana""` from the dictionary. What would be the simplest way to do this?

This is my dictionary:

```
prices = {
    ""banana"" : 4,
    ""apple"" : 2,
    ""orange"" : 1.5,
    ""pear"" : 3
}

```","On a Python version where dicts actually are ordered, you can do

```
my_dict = {'foo': 'bar', 'spam': 'eggs'}
next(iter(my_dict)) # outputs 'foo'

```

For dicts to be ordered, you need Python 3.7+, or 3.6+ if you're okay with relying on the technically-an-implementation-detail ordered nature of dicts on CPython 3.6.

For earlier Python versions, there is no ""first key"", but this will give you ""a key"", especially useful if there is only one."
"I'm trying to build a simple API using Flask, in which I now want to read some POSTed JSON. I do the POST with the Postman Chrome extension, and the JSON I POST is simply `{""text"":""hello world""}`. I try to read the JSON using the following method:

```
@app.route('/api/add_message/<uuid>', methods=['GET', 'POST'])
def add_message(uuid):
    content = request.json
    print(content)
    return uuid

```

On the browser it correctly returns the UUID I put in the GET, but on the console, it just prints out `None` (where I expect it to print out the `{""text"":""hello world""}`. Does anybody know how I can get the posted JSON from within the Flask method?","First of all, the `.json` attribute is a property that delegates to the [`request.get_json()` method](https://flask.palletsprojects.com/api/#flask.Request.get_json), which documents why you see `None` here.

You need to set the request content type to `application/json` for the `.json` property and `.get_json()` method (with no arguments) to work as either will produce `None` otherwise. See the [Flask `Request` documentation](https://flask.palletsprojects.com/api/#flask.Request.json):

> The parsed JSON data if `mimetype` indicates JSON (`application/json`, see [`.is_json`](https://flask.palletsprojects.com/api/#flask.Request.is_json)).

You can tell `request.get_json()` to skip the content type requirement by passing it the `force=True` keyword argument.

Note that if an *exception* is raised at this point (possibly resulting in a 415 Unsupported Media Type response), your JSON *data* is invalid. It is in some way malformed; you may want to check it with a JSON validator."
"I am using Python 3.6. When I try to install ""modules"" using `pip3`, I face this issue:

```
pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available

```","For Windows 10
if you want use pip in normal cmd, not only in Anaconda prompt. you need add 3 environment paths.
like the followings:

```
D:\Anaconda3 
D:\Anaconda3\Scripts
D:\Anaconda3\Library\bin 

```

most people only add D:\Anaconda3\Scripts"
"I'd like to call a function in python using a dictionary with matching key-value pairs for the parameters.

Here is some code:

```
d = dict(param='test')

def f(param):
    print(param)

f(d)

```

This prints `{'param': 'test'}` but I'd like it to just print `test`.

I'd like it to work similarly for more parameters:

```
d = dict(p1=1, p2=2)
def f2(p1, p2):
    print(p1, p2)
f2(d)

```

Is this possible?","Figured it out for myself in the end. It is simple, I was just missing the \*\* operator to unpack the dictionary

So my example becomes:

```
d = dict(p1=1, p2=2)
def f2(p1,p2):
    print(p1, p2)
f2(**d)

```"
"What does `nonlocal` do in Python 3.x?

---

To close debugging questions where OP needs `nonlocal` and doesn't realize it, please use [Is it possible to modify variable in python that is in outer, but not global, scope?](https://stackoverflow.com/questions/8447947) instead.

Although Python 2 is [officially unsupported as of January 1, 2020](https://www.python.org/doc/sunset-python-2/), if for some reason you are forced to maintain a Python 2.x codebase and need an equivalent to `nonlocal`, see [nonlocal keyword in Python 2.x](https://stackoverflow.com/questions/3190706).","Compare this, without using `nonlocal`:

```
x = 0
def outer():
    x = 1
    def inner():
        x = 2
        print(""inner:"", x)

    inner()
    print(""outer:"", x)

outer()
print(""global:"", x)

# inner: 2
# outer: 1
# global: 0

```

To this, using **`nonlocal`**, where `inner()`'s `x` is now also `outer()`'s `x`:

```
x = 0
def outer():
    x = 1
    def inner():
        nonlocal x
        x = 2
        print(""inner:"", x)

    inner()
    print(""outer:"", x)

outer()
print(""global:"", x)

# inner: 2
# outer: 2
# global: 0

```

If we were to use **`global`**, it would bind `x` to the properly ""global"" value:

```
x = 0
def outer():
    x = 1
    def inner():
        global x
        x = 2
        print(""inner:"", x)
        
    inner()
    print(""outer:"", x)

outer()
print(""global:"", x)

# inner: 2
# outer: 1
# global: 2

```"
"I've heard it said that multiline lambdas can't be added in Python because they would clash syntactically with the other syntax constructs in Python. I was thinking about this on the bus today and realized I couldn't think of a single Python construct that multiline lambdas clash with. Given that I know the language pretty well, this surprised me.

Now, I'm sure Guido had a reason for not including multiline lambdas in the language, but out of curiosity: what's a situation where including a multiline lambda would be ambiguous? Is what I've heard true, or is there some other reason that Python doesn't allow multiline lambdas?","Guido van Rossum (the inventor of Python) answers this exact question himself in [an old blog post](http://www.artima.com/weblogs/viewpost.jsp?thread=147358).  
Basically, he admits that it's theoretically possible, but that any proposed solution would be un-Pythonic:

> ""But the complexity of any proposed solution for this puzzle is immense, to me: it requires the parser (or more precisely, the lexer) to be able to switch back and forth between indent-sensitive and indent-insensitive modes, keeping a stack of previous modes and indentation level. Technically that can all be solved (there's already a stack of indentation levels that could be generalized). But none of that takes away my gut feeling that it is all an elaborate [Rube Goldberg contraption](http://en.wikipedia.org/wiki/Rube_Goldberg_Machine)."""
"I have a Python program that uses YAML. I attempted to install it on a new server using `pip install yaml` and it returns the following:

```
$ sudo pip install yaml
Downloading/unpacking yaml
  Could not find any downloads that satisfy the requirement yaml
No distributions at all found for yaml
Storing complete log in /home/pa/.pip/pip.log

```

How do I install the yaml package for Python? I'm running Python 2.7. (OS: Debian Wheezy)","You could try the search the feature on <https://pypi.org/search> (via a browser) and look for packages in PyPI with yaml in the short description. That reveals various packages, including PyYaml, yamltools, and PySyck, among others (Note that [PySyck docs](http://pyyaml.org/wiki/PySyck) recommend using PyYaml, since syck is out of date). Now you know a specific package name, you can install it:

```
$ pip install pyyaml

```

If you want to install python yaml system-wide in linux, you can also use a package manager, like `aptitude` or `yum`:

```
$ sudo apt-get install python-yaml
$ sudo yum install python-yaml

```"
"What do these python file extensions mean?

* `.pyc`
* `.pyd`
* `.pyo`

What are the differences between them and how are they generated from a \*.py file?","1. `.py`: This is normally the input source code that you've written.
2. `.pyc`: This is the compiled bytecode. If you import a module, python will build a `*.pyc` file that contains the bytecode to make importing it again later easier (and faster).
3. `.pyo`: This was a file format used before Python 3.5 for `*.pyc` files that were created with optimizations (`-O`) flag. (see the note below)
4. `.pyd`: This is basically a windows dll file. <http://docs.python.org/faq/windows.html#is-a-pyd-file-the-same-as-a-dll>

Also for some further discussion on `.pyc` vs `.pyo`, take a look at: [http://www.network-theory.co.uk/docs/pytut/CompiledPythonfiles.html](https://web.archive.org/web/20160130165632/http://www.network-theory.co.uk/docs/pytut/CompiledPythonfiles.html) (I've copied the important part below)

> * When the Python interpreter is invoked with the -O flag, optimized code is generated and stored in ‘.pyo’ files. The optimizer currently doesn't help much; it only removes assert statements. When -O is used, all bytecode is optimized; .pyc files are ignored and .py files are compiled to optimized bytecode.
> * Passing two -O flags to the Python interpreter (-OO) will cause the bytecode compiler to perform optimizations that could in some rare cases result in malfunctioning programs. Currently only `__doc__` strings are removed from the bytecode, resulting in more compact ‘.pyo’ files. Since some programs may rely on having these available, you should only use this option if you know what you're doing.
> * A program doesn't run any faster when it is read from a ‘.pyc’ or ‘.pyo’ file than when it is read from a ‘.py’ file; the only thing that's faster about ‘.pyc’ or ‘.pyo’ files is the speed with which they are loaded.
> * When a script is run by giving its name on the command line, the bytecode for the script is never written to a ‘.pyc’ or ‘.pyo’ file. Thus, the startup time of a script may be reduced by moving most of its code to a module and having a small bootstrap script that imports that module. It is also possible to name a ‘.pyc’ or ‘.pyo’ file directly on the command line.

**Note:**

On 2015-09-15 the [Python 3.5 release](https://docs.python.org/3/whatsnew/3.5.html#pep-488-elimination-of-pyo-files) implemented PEP-488 and eliminated `.pyo` files.
This means that `.pyc` files represent both unoptimized and optimized bytecode."
What's the difference between the list methods `append()` and `extend()`?,"[`.append()`](https://docs.python.org/3/library/stdtypes.html#mutable-sequence-types) appends a **single object** at the end of the list:

```
>>> x = [1, 2, 3]
>>> x.append([4, 5])
>>> print(x)
[1, 2, 3, [4, 5]]

```

[`.extend()`](https://docs.python.org/3/library/stdtypes.html#mutable-sequence-types) appends **multiple objects** that are taken from inside the specified iterable:

```
>>> x = [1, 2, 3]
>>> x.extend([4, 5])
>>> print(x)
[1, 2, 3, 4, 5]

```"
"I'm using [`argparse` in Python 2.7](http://docs.python.org/library/argparse.html) for parsing input options. One of my options is a multiple choice. I want to make a list in its help text, e.g.

```
from argparse import ArgumentParser

parser = ArgumentParser(description='test')

parser.add_argument('-g', choices=['a', 'b', 'g', 'd', 'e'], default='a',
    help=""Some option, where\n""
         "" a = alpha\n""
         "" b = beta\n""
         "" g = gamma\n""
         "" d = delta\n""
         "" e = epsilon"")

parser.parse_args()

```

However, `argparse` strips all newlines and consecutive spaces. The result looks like

```
~/Downloads:52$ python2.7 x.py -h
usage: x.py [-h] [-g {a,b,g,d,e}]

test

optional arguments:
  -h, --help      show this help message and exit
  -g {a,b,g,d,e}  Some option, where a = alpha b = beta g = gamma d = delta e
                  = epsilon

```

How to insert newlines in the help text?","Try using [`RawTextHelpFormatter`](https://docs.python.org/2/library/argparse.html#argparse.RawTextHelpFormatter) to preserve all of your formatting:

```
from argparse import RawTextHelpFormatter
parser = ArgumentParser(description='test', formatter_class=RawTextHelpFormatter)

```

It's similar to [`RawDescriptionHelpFormatter`](https://docs.python.org/2/library/argparse.html#argparse.RawDescriptionHelpFormatter) but instead of only applying to the description and epilog, `RawTextHelpFormatter` also applies to all help text (including arguments)."
"How do I get the probability of a string being similar to another string in Python?

I want to get a decimal value like 0.9 (meaning 90%) etc. Preferably with standard Python and library.

e.g.

```
similar(""Apple"",""Appel"") #would have a high prob.

similar(""Apple"",""Mango"") #would have a lower prob.

```","There is a built in.

```
from difflib import SequenceMatcher

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

```

Using it:

```
>>> similar(""Apple"",""Appel"")
0.8
>>> similar(""Apple"",""Mango"")
0.0

```"
"I have issue after installing the **matplotlib** package unable to **import matplotlib.pyplot as plt**. Any suggestion will be greatly appreciate.

```
>>> import matplotlib.pyplot as plt
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""//anaconda/lib/python2.7/site-packages/matplotlib-1.3.1-py2.7-macosx-10.5-x86_64.egg/matplotlib/pyplot.py"", line 98, in <module>
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""//anaconda/lib/python2.7/site-packages/matplotlib-1.3.1-py2.7-macosx-10.5-x86_64.egg/matplotlib/backends/__init__.py"", line 28, in pylab_setup
    globals(),locals(),[backend_name],0)
  File ""//anaconda/lib/python2.7/site-packages/matplotlib-1.3.1-py2.7-macosx-10.5-x86_64.egg/matplotlib/backends/backend_macosx.py"", line 21, in <module>
    from matplotlib.backends import _macosx
**RuntimeError**: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends.

```","**Problem Cause**

In mac os image rendering back end of matplotlib (what-is-a-backend to render using the API of Cocoa by default). There are Qt4Agg and GTKAgg and as a back-end is not the default. Set the back end of macosx that is differ compare with other windows or linux os.

**Solution**

* I assume you have installed the pip matplotlib, there is a directory in your root called `~/.matplotlib`.
* Create a file `~/.matplotlib/matplotlibrc` there and add the following code: `backend: TkAgg`

From this [link](http://matplotlib.org/examples/index.html) you can try different diagrams."
"How do I format a floating number to a fixed width with the following requirements:

1. Leading zero if n < 1
2. Add trailing decimal zero(s) to fill up fixed width
3. Truncate decimal digits past fixed width
4. Align all decimal points

For example:

```
% formatter something like '{:06}'
numbers = [23.23, 0.123334987, 1, 4.223, 9887.2]

for number in numbers:
    print formatter.format(number)

```

The output would be like

```
  23.2300
   0.1233
   1.0000
   4.2230
9887.2000

```","```
numbers = [23.23, 0.1233, 1.0, 4.223, 9887.2]                                                                                                                                                   
                                                                                                                                                                                                
for x in numbers:                                                                                                                                                                               
    print(""{:10.4f}"".format(x)) 

```

prints

```
   23.2300
    0.1233
    1.0000
    4.2230
 9887.2000

```

The format specifier inside the curly braces follows the [Python format string syntax](https://docs.python.org/3/library/string.html#format-string-syntax). Specifically, in this case, it consists of the following parts:

* The *empty string* before the colon means ""take the next provided argument to `format()`"" â€“ in this case the `x` as the only argument.
* The `10.4f` part after the colon is the [format specification](https://docs.python.org/3/library/string.html#format-specification-mini-language).
* The `f` denotes fixed-point notation.
* The `10` is the total width of the field being printed, lefted-padded by spaces.
* The `4` is the number of digits after the decimal point."
"What does a bare asterisk in the parameters of a function do?

When I looked at the [pickle module](http://docs.python.org/3.3/library/pickle.html#pickle.dump), I see this:

```
pickle.dump(obj, file, protocol=None, *, fix_imports=True)

```

I know about a single and double asterisks preceding parameters (for variable number of parameters), but this precedes nothing. And I'm pretty sure this has nothing to do with pickle. That's probably just an example of this happening. I only learned its name when I sent this to the interpreter:

```
>>> def func(*):
...     pass
...
  File ""<stdin>"", line 1
SyntaxError: named arguments must follow bare *

```

If it matters, I'm on python 3.3.0.","Bare `*` is used to force the caller to use named arguments - so you cannot define a function with `*` as an argument when you have no following keyword arguments.

See [this answer](https://stackoverflow.com/a/14298976/180174) or [Python 3 documentation](http://docs.python.org/3/reference/compound_stmts.html#function-definitions) for more details."
"Is the a short syntax for joining a list of lists into a single list( or iterator) in python?

For example I have a list as follows and I want to iterate over a,b and c.

```
x = [[""a"",""b""], [""c""]]

```

The best I can come up with is as follows.

```
result = []
[ result.extend(el) for el in x] 

for el in result:
  print el

```","```
import itertools
a = [['a','b'], ['c']]
print(list(itertools.chain.from_iterable(a)))

```

This gives

```
['a', 'b', 'c']

```"
"I converted a Pandas dataframe to an HTML output using the `DataFrame.to_html` function. When I save this to a separate HTML file, the file shows truncated output.

For example, in my TEXT column,

`df.head(1)` will show

*The film was an excellent effort...*

instead of

*The film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.*

This rendition is fine in the case of a screen-friendly format of a massive Pandas dataframe, but I need an HTML file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet.

How would I be able to show the complete, non-truncated text data for each element in my TEXT column in the HTML version of the information? I would imagine that the HTML table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the `DataFrame.to_html` function.","Set the `display.max_colwidth` option to `None` (or `-1` before version 1.0):

```
pd.set_option('display.max_colwidth', None)

```

[`set_option` documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html#pandas-set-option)

For example, in [IPython](https://en.wikipedia.org/wiki/IPython), we see that the information is truncated to 50 characters. Anything in excess is ellipsized:

[![Truncated result](https://i.sstatic.net/hANGS.png)](https://i.sstatic.net/hANGS.png)

If you set the `display.max_colwidth` option, the information will be displayed fully:

[![Non-truncated result](https://i.sstatic.net/Nxg2q.png)](https://i.sstatic.net/Nxg2q.png)"
"Like most Python developers, I typically keep a console window open with the Python interpreter running to test commands, `dir()` stuff, `help() stuff`, etc.

Like any console, after a while the visible backlog of past commands and prints gets to be cluttered, and sometimes confusing when re-running the same command several times. I'm wondering if, and how, to clear the Python interpreter console.

I've heard about doing a system call and either calling `cls` on Windows or `clear` on Linux, but I was hoping there was something I could command the interpreter itself to do.

**Note:** I'm running on Windows, so `Ctrl` + `L` doesn't work.","As you mentioned, you can do a system call:

For Windows:

```
>>> import os
>>> clear = lambda: os.system('cls')
>>> clear()

```

For Linux it would be:

```
>>> import os
>>> clear = lambda: os.system('clear')
>>> clear()

```"
"How do I add a `color` column to the following dataframe so that `color='green'` if `Set == 'Z'`, and `color='red'` otherwise?

```
   Type  Set
1     A    Z
2     B    Z           
3     B    X
4     C    Y

```","**If you only have two choices to select from then use [`np.where`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html):**

```
df['color'] = np.where(df['Set']=='Z', 'green', 'red')

```

For example,

```
import pandas as pd
import numpy as np

df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
df['color'] = np.where(df['Set']=='Z', 'green', 'red')
print(df)

```

yields

```
  Set Type  color
0   Z    A  green
1   Z    B  green
2   X    B    red
3   Y    C    red

```

---

**If you have more than two conditions then use [`np.select`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html)**. For example, if you want `color` to be

* `yellow` when `(df['Set'] == 'Z') & (df['Type'] == 'A')`
* otherwise `blue` when `(df['Set'] == 'Z') & (df['Type'] == 'B')`
* otherwise `purple` when `(df['Type'] == 'B')`
* otherwise `black`,

then use

```
df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
conditions = [
    (df['Set'] == 'Z') & (df['Type'] == 'A'),
    (df['Set'] == 'Z') & (df['Type'] == 'B'),
    (df['Type'] == 'B')]
choices = ['yellow', 'blue', 'purple']
df['color'] = np.select(conditions, choices, default='black')
print(df)

```

which yields

```
  Set Type   color
0   Z    A  yellow
1   Z    B    blue
2   X    B  purple
3   Y    C   black

```"
"What would be the best way to split a string on the first occurrence of a delimiter?

For example:

```
""123mango abcd mango kiwi peach""

```

splitting on the first `mango` to get:

```
"" abcd mango kiwi peach""

```

---

To split on the **last** occurrence instead, see [Partition string in Python and get value of last segment after colon](https://stackoverflow.com/questions/6169324).","From [the docs](http://docs.python.org/library/stdtypes.html#str.split):

> `str.split([sep[, maxsplit]])`
>
> Return a list of the words in the string, using *sep* as the delimiter string. If *maxsplit* is given, at most *maxsplit* splits are done (thus, the list will have at most `maxsplit+1` elements).

```
s.split('mango', 1)[1]

```"
"How do I determine:

1. the current directory (where I was in the shell when I ran the Python script), and
2. where the Python file I am executing is?","To get the full path to the directory a Python file is contained in, write this in that file:

```
import os 
dir_path = os.path.dirname(os.path.realpath(__file__))

```

(Note that the incantation above won't work if you've already used `os.chdir()` to change your current working directory, since the value of the `__file__` constant is relative to the current working directory and is not changed by an `os.chdir()` call.)

---

To get the current working directory use

```
import os
cwd = os.getcwd()

```

---

Documentation references for the modules, constants and functions used above:

* The [`os`](https://docs.python.org/library/os.html) and [`os.path`](https://docs.python.org/library/os.path.html#module-os.path) modules.
* The [`__file__`](https://docs.python.org/reference/datamodel.html) constant
* [`os.path.realpath(path)`](https://docs.python.org/library/os.path.html#os.path.realpath) (returns *""the canonical path of the specified filename, eliminating any symbolic links encountered in the path""*)
* [`os.path.dirname(path)`](https://docs.python.org/library/os.path.html#os.path.dirname) (returns *""the directory name of pathname `path`""*)
* [`os.getcwd()`](https://docs.python.org/library/os.html#os.getcwd) (returns *""a string representing the current working directory""*)
* [`os.chdir(path)`](https://docs.python.org/library/os.html#os.chdir) (*""change the current working directory to `path`""*)"
"What does [`np.random.seed`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html) do?

```
np.random.seed(0)

```","`np.random.seed(0)` makes the random numbers predictable

```
>>> numpy.random.seed(0) ; numpy.random.rand(4)
array([ 0.55,  0.72,  0.6 ,  0.54])
>>> numpy.random.seed(0) ; numpy.random.rand(4)
array([ 0.55,  0.72,  0.6 ,  0.54])

```

With the seed reset (every time), the *same* set of numbers will appear every time.

If the random seed is not reset, *different* numbers appear with every invocation:

```
>>> numpy.random.rand(4)
array([ 0.42,  0.65,  0.44,  0.89])
>>> numpy.random.rand(4)
array([ 0.96,  0.38,  0.79,  0.53])

```

(pseudo-)random numbers work by starting with a number (the seed), multiplying it by a large number, adding an offset, then taking modulo of that sum. The resulting number is then used as the seed to generate the next ""random"" number. When you set the seed (every time), it does the same thing every time, giving you the same numbers.

If you want seemingly random numbers, do not set the seed. If you have code that uses random numbers that you want to debug, however, it can be very helpful to set the seed before each run so that the code does the same thing every time you run it.

To get the most random numbers for each run, call `numpy.random.seed()`. [This](https://docs.scipy.org/doc/numpy-1.3.x/reference/generated/numpy.random.seed.html) will cause numpy to set the seed to a random number obtained from `/dev/urandom` or its Windows analog or, if neither of those is available, it will use the clock.

For more information on using seeds to generate pseudo-random numbers, see [wikipedia](https://en.wikipedia.org/wiki/Random_number_generation#Computational_methods)."
"I have a Python file which might have to support Python versions < 3.x and >= 3.x. Is there a way to introspect the Python runtime to know the version which it is running (for example, `2.6 or 3.2.x`)?","Sure, take a look at [`sys.version`](http://docs.python.org/library/sys.html#sys.version) and [`sys.version_info`](http://docs.python.org/library/sys.html#sys.version_info).

For example, to check that you are running Python 3.x, use

```
import sys
if sys.version_info[0] < 3:
    raise Exception(""Must be using Python 3"")

```

Here, `sys.version_info[0]` is the major version number. `sys.version_info[1]` would give you the minor version number.

In Python 2.7 and later, the components of `sys.version_info` can also be accessed by name, so the major version number is `sys.version_info.major`.

See also [How can I check for Python version in a program that uses new language features?](https://stackoverflow.com/questions/446052/python-best-way-to-check-for-python-version-in-program-that-uses-new-language-f)"
"How do I find all rows in a pandas DataFrame which have the max value for `count` column, after grouping by `['Sp','Mt']` columns?

**Example 1:** the following DataFrame:

```
   Sp   Mt Value   count
0  MM1  S1   a     **3**
1  MM1  S1   n       2
2  MM1  S3   cb    **5**
3  MM2  S3   mk    **8**
4  MM2  S4   bg    **10**
5  MM2  S4   dgd     1
6  MM4  S2   rd      2
7  MM4  S2   cb      2
8  MM4  S2   uyi   **7**

```

Expected output is to get the result rows whose count is max in each group, like this:

```
   Sp   Mt   Value  count
0  MM1  S1   a      **3**
2  MM1  S3   cb     **5**
3  MM2  S3   mk     **8**
4  MM2  S4   bg     **10** 
8  MM4  S2   uyi    **7**

```

**Example 2:**

```
   Sp   Mt   Value  count
4  MM2  S4   bg     10
5  MM2  S4   dgd    1
6  MM4  S2   rd     2
7  MM4  S2   cb     8
8  MM4  S2   uyi    8

```

Expected output:

```
   Sp   Mt   Value  count
4  MM2  S4   bg     10
7  MM4  S2   cb     8
8  MM4  S2   uyi    8

```","Firstly, we can get the max count for each group like this:

```
In [1]: df
Out[1]:
    Sp  Mt Value  count
0  MM1  S1     a      3
1  MM1  S1     n      2
2  MM1  S3    cb      5
3  MM2  S3    mk      8
4  MM2  S4    bg     10
5  MM2  S4   dgd      1
6  MM4  S2    rd      2
7  MM4  S2    cb      2
8  MM4  S2   uyi      7

In [2]: df.groupby(['Sp', 'Mt'])['count'].max()
Out[2]:
Sp   Mt
MM1  S1     3
     S3     5
MM2  S3     8
     S4    10
MM4  S2     7
Name: count, dtype: int64

```

To get the indices of the original DF you can do:

```
In [3]: idx = df.groupby(['Sp', 'Mt'])['count'].transform(max) == df['count']

In [4]: df[idx]
Out[4]:
    Sp  Mt Value  count
0  MM1  S1     a      3
2  MM1  S3    cb      5
3  MM2  S3    mk      8
4  MM2  S4    bg     10
8  MM4  S2   uyi      7

```

Note that if you have multiple max values per group, all will be returned.

---

**Update**

On a Hail Mary chance that this is what the OP is requesting:

```
In [5]: df['count_max'] = df.groupby(['Sp', 'Mt'])['count'].transform(max)

In [6]: df
Out[6]:
    Sp  Mt Value  count  count_max
0  MM1  S1     a      3          3
1  MM1  S1     n      2          3
2  MM1  S3    cb      5          5
3  MM2  S3    mk      8          8
4  MM2  S4    bg     10         10
5  MM2  S4   dgd      1         10
6  MM4  S2    rd      2          7
7  MM4  S2    cb      2          7
8  MM4  S2   uyi      7          7

```"
"The goal is to simplify using many arguments in a Python program by writing a config (settings) file that dynamically can add an item.

What is the best practice for using a settings (config) file or importing a library in Python?

use case:

* Rather than using `pickle` I would like it to be a straightforward text file that can easily be read and edited.
* I want to be able to add data in it(like a dictionary) (i.e.: some form of nesting should be supported).

An example file:

```
truck:
    color: blue
    brand: ford
city: new york
cabriolet:
    color: black
    engine:
        cylinders: 8
        placement: mid
    doors: 2

```","You can have a regular Python module, say config.py, like this:

```
truck = dict(
    color = 'blue',
    brand = 'ford',
)
city = 'new york'
cabriolet = dict(
    color = 'black',
    engine = dict(
        cylinders = 8,
        placement = 'mid',
    ),
    doors = 2,
)

```

and use it like this:

```
import config
print(config.truck['color'])  

```"
"I'm running Mountain Lion and the basic default Python version is 2.7. I downloaded Python 3.3 and want to set it as default.

Currently:

```
$ python
    version 2.7.5
$ python3.3
    version 3.3

```

How do I set it so that every time I run `$ python` it opens 3.3?","Changing the default python executable's version system-wide could break some applications that depend on python2.

However, you can alias the commands in most shells, Since the default shells in macOS (bash in 10.14 and below; zsh in 10.15) share a similar syntax. You could put
`alias python='python3'`
in your `~/.profile`, and then source `~/.profile` in your `~/.bash_profile` and/or your`~/.zsh_profile` with a line like:

```
[ -e ~/.profile ] && . ~/.profile

```

This way, your alias will work across shells.

With this, `python` command now invokes `python3`. If you want to invoke the ""original"" python (that refers to *python2*) on occasion, you can use `command python`, which will leaving the alias untouched, and works in all shells.

If you launch interpreters more often *(I do)*, you can always create more aliases to add as well, i.e.:

```
alias 2='python2'
alias 3='python3'

```

---

Tip: For scripts, instead of using a shebang like:

```
#!/usr/bin/env python

```

use:

```
#!/usr/bin/env python3

```

This way, the system will use python3 for running python ***executables***."
"In order to test some functionality I would like to create a `DataFrame` from a string. Let's say my test data looks like:

```
TESTDATA=""""""col1;col2;col3
1;4.4;99
2;4.5;200
3;4.7;65
4;3.2;140
""""""

```

What is the simplest way to read that data into a Pandas `DataFrame`?","A simple way to do this is to use [`StringIO.StringIO` (python2)](https://docs.python.org/2/library/io.html#io.StringIO) or [`io.StringIO` (python3)](https://docs.python.org/3/library/io.html#io.StringIO) and pass that to the [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv) function. E.g:

```
import sys
if sys.version_info[0] < 3: 
    from StringIO import StringIO
else:
    from io import StringIO

import pandas as pd

TESTDATA = StringIO(""""""col1;col2;col3
    1;4.4;99
    2;4.5;200
    3;4.7;65
    4;3.2;140
    """""")

df = pd.read_csv(TESTDATA, sep="";"")

```"
"Whenever I am trying to install any package using pip, I am getting this import error:

```
guru@guru-notebook:~$ pip3 install numpy
Traceback (most recent call last):
  File ""/usr/bin/pip3"", line 9, in <module>
    from pip import main
ImportError: cannot import name 'main'

```

```
guru@guru-notebook:~$ cat `which pip3`
#!/usr/bin/python3
# GENERATED BY DEBIAN

import sys

# Run the main entry point, similarly to how setuptools does it, but because
# we didn't install the actual entry point from setup.py, don't use the
# pkg_resources API.
from pip import main
if __name__ == '__main__':
    sys.exit(main())

```

It was working fine earlier, I am not sure why it is throwing this error.
I have searched about this error, but can't find anything to fix it.

Please let me know if you need any further detail, I will update my question.","You must have inadvertently upgraded your system pip (probably through something like `sudo pip install pip --upgrade`)

pip 10.x adjusts where its internals are situated. The `pip3` command you're seeing is one provided by your package maintainer (presumably debian based here?) and is not a file managed by pip.

You can read more about this on [pip's issue tracker](https://github.com/pypa/pip/issues/5221)

You'll probably want to *not* upgrade your system pip and instead use a virtualenv.

To recover the `pip3` binary you'll need to `sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall`

If you want to continue in ""unsupported territory"" (upgrading a system package outside of the system package manager), you can probably get away with `python3 -m pip ...` instead of `pip3`."
"I want to repeatedly execute a function in Python every 60 seconds forever (just like an [NSTimer](http://web.archive.org/web/20090823012700/http://developer.apple.com:80/DOCUMENTATION/Cocoa/Reference/Foundation/Classes/NSTimer_Class/Reference/NSTimer.html) in Objective C or setTimeout in JS). This code will run as a daemon and is effectively like calling the python script every minute using a cron, but without requiring that to be set up by the user.

In [this question about a cron implemented in Python](https://stackoverflow.com/questions/373335/suggestions-for-a-cron-like-scheduler-in-python), the solution appears to effectively just [sleep()](http://docs.python.org/library/time.html#time.sleep) for x seconds. I don't need such advanced functionality so perhaps something like this would work

```
while True:
    # Code executed here
    time.sleep(60)

```

Are there any foreseeable problems with this code?","If your program doesn't have a event loop already, use the [sched](http://docs.python.org/library/sched.html) module, which implements a general purpose event scheduler.

```
import sched, time

def do_something(scheduler): 
    # schedule the next call first
    scheduler.enter(60, 1, do_something, (scheduler,))
    print(""Doing stuff..."")
    # then do your stuff

my_scheduler = sched.scheduler(time.time, time.sleep)
my_scheduler.enter(60, 1, do_something, (my_scheduler,))
my_scheduler.run()

```

If you're already using an event loop library like `asyncio`, `trio`, `tkinter`, `PyQt5`, `gobject`, `kivy`, and many others - just schedule the task using your existing event loop library's methods, instead."
"On

* [Conda](https://en.wikipedia.org/wiki/Conda_(package_manager)) 4.2.13
* [Mac OS X v10.12.1](https://en.wikipedia.org/wiki/MacOS_Sierra) (Sierra)

I am trying to install packages from `pip` to a fresh environment (virtual) created using anaconda. [In the Anaconda documentation](http://conda.pydata.org/docs/using/pkgs.html#install-non-conda-packages) it says this is perfectly fine. It is done the same way as for virtualenv.

> Activate the environment where you want to put the program, then pip install a program...

I created an empty environment in Anaconda like this:

```
conda create -n shrink_venv

```

Activate it:

```
source activate shrink_venv

```

I then can see in the terminal that I am working in my environment `(shrink_venv)`. A problem is coming up when I try to install a package using `pip`:

```
pip install Pillow

```

Output:

```
Requirement already satisfied (use --upgrade to upgrade): Pillow in /Library/Python/2.7/site-packages

```

So I can see it thinks the requirement is satisfied from the system-wide package. So it seems the environment is not working correctly, definitely not like it said in the documentation. Am I doing something wrong here?

Just a note, I know you can use `conda install` for the packages, but I have had an issue with Pillow from anaconda, so I wanted to get it from `pip`, and since the docs say that is fine.

Output of `which -a pip`:

```
/usr/local/bin/pip
/Users/my_user/anaconda/bin/pip

```

I see this is pretty common issue. I have found that the Conda environment doesn't play well with the `PYTHONPATH`. The system seems to always look in the `PYTHONPATH` locations even when you're using a Conda environment. Now, I always run `unset PYTHONPATH` when using a Conda environment, and it works much better. I'm on a Mac.","For others who run into this situation, I found this to be the most straightforward solution:

1. Run `conda create -n venv_name` and `conda activate venv_name`, where `venv_name` is the name of your virtual environment.
2. Run `conda install pip`. This will install pip to your venv directory.
3. Find your anaconda directory, and find the actual venv folder. It should be somewhere like `/anaconda/envs/venv_name/`; or, you could also run `conda activate venv_name`.
4. Install new packages by doing `/anaconda/envs/venv_name/bin/pip install package_name`; or, simply run `pip install package_name`.

This should now successfully install packages using that virtual environment's pip!"
"I am trying to plot a simple graph using pyplot, e.g.:

```
import matplotlib.pyplot as plt
plt.plot([1,2,3],[5,7,4])
plt.show()

```

but the figure does not appear and I get the following message:

```
UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.

```

I found and tried some advice to re-configure the ""backend"" mentioned in that warning, like so:

```
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

```

but this gives me an error message:

```
ModuleNotFoundError: No module named 'tkinter'

```

I assumed that I had to install this module separately, but `pip install tkinter` does not work:

```
Collecting tkinter
  Could not find a version that satisfies the requirement tkinter (from versions: )
No matching distribution found for tkinter

```

How can I make Matplotlib display the graph?

---

**See also**: [Why does tkinter (or turtle) seem to be missing or broken? Shouldn't it be part of the standard library?](https://stackoverflow.com/questions/76105218) . This question is not a duplicate, because the answers discuss other backends besides the Tkinter one.

Also see [\_tkinter.TclError: no display name and no $DISPLAY environment variable](https://stackoverflow.com/questions/37604289) for issues with attempts to use Matplotlib remotely.","### Solution 1: is to install the GUI backend `tk`

I found a solution to my problem (thanks to the help of [ImportanceOfBeingErnest](https://stackoverflow.com/users/4124317/importanceofbeingernest)).

All I had to do was to install `tkinter` through the Linux bash terminal using the following command:

```
sudo apt-get install python3-tk

```

instead of installing it with `pip` or directly in the virtual environment in Pycharm.

### Solution 2: install any of the `matplotlib` supported GUI backends

* solution 1 works fine because you get a GUI backend... in this case the `TkAgg`
* however you can also fix the issue by installing any of the matplolib GUI backends like `Qt5Agg`, `GTKAgg`, `Qt4Agg`, etc
  + for example `pip install pyqt5` will fix the issue also

NOTE:

* usually this error appears when you pip install matplotlib and you are trying to display a plot in a GUI window and you do not have a python module for GUI display.
* The authors of `matplotlib` made the pypi software deps not depend on any GUI backend because some people **need** `matplotlib` without any GUI backend."
"It is my understanding that the `range()` function, which is actually [an object type in Python 3](https://docs.python.org/3/library/stdtypes.html#typesseq-range), generates its contents on the fly, similar to a generator.

This being the case, I would have expected the following line to take an inordinate amount of time because, in order to determine whether 1 quadrillion is in the range, a quadrillion values would have to be generated:

```
1_000_000_000_000_000 in range(1_000_000_000_000_001)

```

Furthermore: it seems that no matter how many zeroes I add on, the calculation more or less takes the same amount of time (basically instantaneous).

I have also tried things like this, but the calculation is still almost instant:

```
# count by tens
1_000_000_000_000_000_000_000 in range(0,1_000_000_000_000_000_000_001,10)

```

If I try to implement my own range function, the result is not so nice!

```
def my_crappy_range(N):
    i = 0
    while i < N:
        yield i
        i += 1
    return

```

What is the `range()` object doing under the hood that makes it so fast?

---

[Martijn Pieters's answer](https://stackoverflow.com/a/30081318/2437514) was chosen for its completeness, but also see [abarnert's first answer](https://stackoverflow.com/a/30081894/2437514) for a good discussion of what it means for `range` to be a full-fledged *sequence* in Python 3, and some information/warning regarding potential inconsistency for `__contains__` function optimization across Python implementations. [abarnert's other answer](https://stackoverflow.com/a/30088140/2437514) goes into some more detail and provides links for those interested in the history behind the optimization in Python 3 (and lack of optimization of `xrange` in Python 2). Answers [by poke](https://stackoverflow.com/a/30081467/2437514) and [by wim](https://stackoverflow.com/a/30081470/2437514) provide the relevant C source code and explanations for those who are interested.","The Python 3 `range()` object doesn't produce numbers immediately; it is a smart [sequence object](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence) that produces numbers *on demand*. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.

The object also implements the [`object.__contains__` hook](https://docs.python.org/3/reference/datamodel.html#object.__contains__), and *calculates* if your number is part of its range. Calculating is a (near) constant time operation \*. There is never a need to scan through all possible integers in the range.

From the [`range()` object documentation](https://docs.python.org/3/library/stdtypes.html#range):

> The advantage of the `range` type over a regular `list` or `tuple` is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the `start`, `stop` and `step` values, calculating individual items and subranges as needed).

So at a minimum, your `range()` object would do:

```
class my_range:
    def __init__(self, start, stop=None, step=1, /):
        if stop is None:
            start, stop = 0, start
        self.start, self.stop, self.step = start, stop, step
        if step < 0:
            lo, hi, step = stop, start, -step
        else:
            lo, hi = start, stop
        self.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1

    def __iter__(self):
        current = self.start
        if self.step < 0:
            while current > self.stop:
                yield current
                current += self.step
        else:
            while current < self.stop:
                yield current
                current += self.step

    def __len__(self):
        return self.length

    def __getitem__(self, i):
        if i < 0:
            i += self.length
        if 0 <= i < self.length:
            return self.start + i * self.step
        raise IndexError('my_range object index out of range')

    def __contains__(self, num):
        if self.step < 0:
            if not (self.stop < num <= self.start):
                return False
        else:
            if not (self.start <= num < self.stop):
                return False
        return (num - self.start) % self.step == 0

```

This is still missing several things that a real `range()` supports (such as the `.index()` or `.count()` methods, hashing, equality testing, or slicing), but should give you an idea.

I also simplified the `__contains__` implementation to only focus on integer tests; if you give a real `range()` object a non-integer value (including subclasses of `int`), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original [Python issue](http://bugs.python.org/issue1766304) that implemented the containment test.

---

\* *Near* constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since it’s all executed in optimised C code and Python stores integer values in 30-bit chunks, you’d run out of memory before you saw any performance impact due to the size of the integers involved here."
"When debugging in PHP, I frequently find it useful to simply stick a [var\_dump()](http://php.net/var-dump) in my code to show me what a variable is, what its value is, and the same for anything that it contains.

What is a good Python equivalent for this?","I think the best equivalent to PHP's `var_dump($foo, $bar)` is combine [`print`](https://docs.python.org/library/functions.html#print) with [`vars`](https://docs.python.org/library/functions.html#vars):

```
print vars(foo),vars(bar)

```"
What is the best way of creating an alphabetically sorted list in Python?,"Basic answer:

```
mylist = [""b"", ""C"", ""A""]
mylist.sort()

```

This modifies your original list (i.e. sorts in-place). To get a sorted copy of the list, without changing the original, use the [`sorted()`](http://docs.python.org/library/functions.html#sorted) function:

```
for x in sorted(mylist):
    print x

```

However, the examples above are a bit naive, because they don't take locale into account, and perform a case-sensitive sorting. You can take advantage of the optional parameter `key` to specify custom sorting order (the alternative, using `cmp`, is a deprecated solution, as it has to be evaluated multiple times - `key` is only computed once per element).

So, to sort according to the current locale, taking language-specific rules into account ([`cmp_to_key`](http://docs.python.org/library/functools.html#functools.cmp_to_key) is a helper function from functools):

```
sorted(mylist, key=cmp_to_key(locale.strcoll))

```

And finally, if you need, you can specify a [custom locale](http://docs.python.org/library/locale.html) for sorting:

```
import locale
locale.setlocale(locale.LC_ALL, 'en_US.UTF-8') # vary depending on your lang/locale
assert sorted((u'Ab', u'ad', u'aa'),
  key=cmp_to_key(locale.strcoll)) == [u'aa', u'Ab', u'ad']

```

Last note: you will see examples of case-insensitive sorting which use the `lower()` method - those are incorrect, because they work only for the ASCII subset of characters. Those two are wrong for any non-English data:

```
# this is incorrect!
mylist.sort(key=lambda x: x.lower())
# alternative notation, a bit faster, but still wrong
mylist.sort(key=str.lower)

```"
"What is the purpose of [`np.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html)? I know it creates some kind of grid of coordinates for plotting, but I can't see the direct benefit of it.

The official documentation gives the following example, but its output doesn't make sense to me:

```
x = np.arange(-5, 5, 1)
y = np.arange(-5, 5, 1)
xx, yy = np.meshgrid(x, y, sparse=True)
z = np.sin(xx**2 + yy**2) / (xx**2 + yy**2)
h = plt.contourf(x,y,z)

```","The purpose of `meshgrid` is to create a rectangular grid out of an array of x values and an array of y values.

So, for example, if we want to create a grid where we have a point at each integer value between 0 and 4 in both the x and y directions. To create a rectangular grid, we need every combination of the `x` and `y` points.

This is going to be 25 points, right? So if we wanted to create an x and y array for all of these points, we *could* do the following.

```
x[0,0] = 0    y[0,0] = 0
x[0,1] = 1    y[0,1] = 0
x[0,2] = 2    y[0,2] = 0
x[0,3] = 3    y[0,3] = 0
x[0,4] = 4    y[0,4] = 0
x[1,0] = 0    y[1,0] = 1
x[1,1] = 1    y[1,1] = 1
...
x[4,3] = 3    y[4,3] = 4
x[4,4] = 4    y[4,4] = 4

```

This would result in the following `x` and `y` matrices, such that the pairing of the corresponding element in each matrix gives the x and y coordinates of a point in the grid.

```
x =   0 1 2 3 4        y =   0 0 0 0 0
      0 1 2 3 4              1 1 1 1 1
      0 1 2 3 4              2 2 2 2 2
      0 1 2 3 4              3 3 3 3 3
      0 1 2 3 4              4 4 4 4 4

```

We can then plot these to verify that they are a grid:

```
plt.plot(x,y, marker='.', color='k', linestyle='none')

```

[![enter image description here](https://i.sstatic.net/kZNzz.png)](https://i.sstatic.net/kZNzz.png)

Obviously, this gets very tedious especially for large ranges of `x` and `y`. Instead, `meshgrid` can actually generate this for us: all we have to specify are the unique `x` and `y` values.

```
xvalues = np.array([0, 1, 2, 3, 4]);
yvalues = np.array([0, 1, 2, 3, 4]);

```

Now, when we call `meshgrid`, we get the previous output automatically.

```
xx, yy = np.meshgrid(xvalues, yvalues)

plt.plot(xx, yy, marker='.', color='k', linestyle='none')

```

[![enter image description here](https://i.sstatic.net/1xeW8.png)](https://i.sstatic.net/1xeW8.png)

Creation of these rectangular grids is useful for a number of tasks. In the example that you have provided in your post, it is simply a way to sample a function (`sin(x**2 + y**2) / (x**2 + y**2)`) over a range of values for `x` and `y`.

Because this function has been sampled on a rectangular grid, the function can now be visualized as an ""image"".

[![enter image description here](https://i.sstatic.net/K5BCm.png)](https://i.sstatic.net/K5BCm.png)

Additionally, the result can now be passed to functions which expect data on rectangular grid (i.e. `contourf`)"
"Given a plot of a signal in time representation, how can I draw lines marking the corresponding time index?

Specifically, given a signal plot with a time index ranging from 0 to 2.6 (seconds), I want to draw vertical red lines indicating the corresponding time index for the list `[0.22058956, 0.33088437, 2.20589566]`. How can I do it?","The standard way to add vertical lines that will cover your entire plot window without you having to specify their actual height is `plt.axvline`

```
import matplotlib.pyplot as plt

plt.axvline(x=0.22058956)
plt.axvline(x=0.33088437)
plt.axvline(x=2.20589566)

```

OR

```
xcoords = [0.22058956, 0.33088437, 2.20589566]
for xc in xcoords:
    plt.axvline(x=xc)

```

You can use many of the keywords available for other plot commands (e.g. `color`, `linestyle`, `linewidth` ...). You can pass in keyword arguments `ymin` and `ymax` if you like in axes corrdinates (e.g. `ymin=0.25`, `ymax=0.75` will cover the middle half of the plot). There are corresponding functions for horizontal lines (`axhline`) and rectangles (`axvspan`)."
"I would like to apply colormap to an image, and write the resulting image, without using axes, labels, titles, or anything automatically added by matplotlib. Here is what I did:

```
def make_image(inputname,outputname):
    data = mpimg.imread(inputname)[:,:,0]
    fig = plt.imshow(data)
    fig.set_cmap('hot')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    plt.savefig(outputname)

```

It successfully removes the axis of the figure, but the figure saved, presents a white padding, and a frame around the actual image.

How can I remove them (at least the white padding)?","The [`axis('off')`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axis.html) method resolves one of the problems more succinctly than separately changing each axis and border. It still leaves the white space around the border however. Adding `bbox_inches='tight'` to the `savefig` command almost gets you there; you can see in the example below that the white space left is much smaller, but still present.

Newer versions of matplotlib may require `bbox_inches=0` instead of the string `'tight'` (via @episodeyang and @kadrach)

```
from numpy import random
import matplotlib.pyplot as plt

data = random.random((5,5))
img = plt.imshow(data, interpolation='nearest')
img.set_cmap('hot')
plt.axis('off')
plt.savefig(""test.png"", bbox_inches='tight')

```

![enter image description here](https://i.sstatic.net/4ZMuW.png)"
"I am new to Jupyter Notebook, what is the key difference between the Jupyter Notebook and JupyterLab, suggest me to choose the best one, which should be used in future.","Jupyter Notebook is a web-based interactive computational environment for creating Jupyter notebook documents. It supports several languages like Python (IPython), Julia, R etc. and is largely used for data analysis, data visualization and further interactive, exploratory computing.

JupyterLab is the next-generation user interface **including notebooks**. It has a modular structure, where you can open several notebooks or files (e.g. HTML, Text, Markdowns etc) as tabs in the same window. It offers more of an IDE-like experience.

For a beginner I would suggest starting with Jupyter Notebook as it just consists of a filebrowser and an (notebook) editor view. It might be easier to use.
If you want more features, switch to JupyterLab. JupyterLab offers much more features and an enhanced interface, which can be extended through extensions:
[JupyterLab Extensions (GitHub)](https://github.com/search?q=topic%3Ajupyterlab-extension&type=Repositories)"
"Is there a way to have a `defaultdict(defaultdict(int))` in order to make the following code work?

```
for x in stuff:
    d[x.a][x.b] += x.c_int

```

`d` needs to be built ad-hoc, depending on `x.a` and `x.b` elements.

I could use:

```
for x in stuff:
    d[x.a,x.b] += x.c_int

```

but then I wouldn't be able to use:

```
d.keys()
d[x.a].keys()

```","Yes like this:

```
defaultdict(lambda: defaultdict(int))

```

The argument of a `defaultdict` (in this case is `lambda: defaultdict(int)`) will be called when you try to access a key that doesn't exist. The return value of it will be set as the new value of this key, which means in our case the value of `d[Key_doesnt_exist]` will be `defaultdict(int)`.

If you try to access a key from this last defaultdict i.e. `d[Key_doesnt_exist][Key_doesnt_exist]` it will return 0, which is the return value of the argument of the last defaultdict i.e. `int()`."
"How can I order by descending my query set in django by date?

```
Reserved.objects.all().filter(client=client_id).order_by('check_in')

```

I just want to filter from descending all the Reserved by check\_in date.","```
Reserved.objects.filter(client=client_id).order_by('-check_in')

```

Notice the `-` before `check_in`.

**`-` before column name mean ""descending order"", while without `-` mean ""ascending"".**

[Django Documentation](https://docs.djangoproject.com/en/dev/ref/models/querysets/#order-by ""Documentation"")"
"I have a directory that contains my Python unit tests. Each unit test module is of the form **test\_\*.py**. I am attempting to make a file called **all\_test.py** that will, you guessed it, run all files in the aforementioned test form and return the result. I have tried two methods so far; both have failed. I will show the two methods, and I hope someone out there knows how to actually do this correctly.

For my first valiant attempt, I thought ""If I just import all my testing modules in the file, and then call this `unittest.main()` doodad, it will work, right?"" Well, turns out I was wrong.

```
import glob
import unittest

testSuite = unittest.TestSuite()
test_file_strings = glob.glob('test_*.py')
module_strings = [str[0:len(str)-3] for str in test_file_strings]

if __name__ == ""__main__"":
     unittest.main()

```

This did not work, the result I got was:

```
$ python all_test.py 

----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK

```

For my second try, I though, ok, maybe I will try to do this whole testing thing in a more ""manual"" fashion. So I attempted to do that below:

```
import glob
import unittest

testSuite = unittest.TestSuite()
test_file_strings = glob.glob('test_*.py')
module_strings = [str[0:len(str)-3] for str in test_file_strings]
[__import__(str) for str in module_strings]
suites = [unittest.TestLoader().loadTestsFromName(str) for str in module_strings]
[testSuite.addTest(suite) for suite in suites]
print testSuite 

result = unittest.TestResult()
testSuite.run(result)
print result

#Ok, at this point I have a result
#How do I display it as the normal unit test command line output?
if __name__ == ""__main__"":
    unittest.main()

```

This also did not work, but it seems so close!

```
$ python all_test.py 
<unittest.TestSuite tests=[<unittest.TestSuite tests=[<unittest.TestSuite tests=[<test_main.TestMain testMethod=test_respondes_to_get>]>]>]>
<unittest.TestResult run=1 errors=0 failures=0>

----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK

```

I seem to have a suite of some sort, and I can execute the result. I am a little concerned about the fact that it says I have only `run=1`, seems like that should be `run=2`, but it is progress. But how do I pass and display the result to main? Or how do I basically get it working so I can just run this file, and in doing so, run all the unit tests in this directory?","With Python 2.7 and higher you don't have to write new code or use third-party tools to do this; recursive test execution via the command line is built-in. Put an `__init__.py` in your test directory and:

```
python -m unittest discover <test_directory>
# or
python -m unittest discover -s <directory> -p '*_test.py'

```

You can read more in the [python 2.7](http://docs.python.org/2/library/unittest.html#command-line-interface)
or [python 3.x](http://docs.python.org/3/library/unittest.html#command-line-interface) unittest documentation."
"Updated MRE with subplots
-------------------------

* I'm not sure of the usefulness of the original question and MRE. The margin padding seems to be properly adjusted for large x and y labels.
* The issue is reproducible with subplots.
* Using `matplotlib 3.4.2`

```
fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))
axes = axes.flatten()

for ax in axes:
    ax.set_ylabel(r'$\ln\left(\frac{x_a-x_b}{x_a-x_c}\right)$')
    ax.set_xlabel(r'$\ln\left(\frac{x_a-x_d}{x_a-x_e}\right)$')

plt.show()

```

[![enter image description here](https://i.sstatic.net/RZ0QA.png)](https://i.sstatic.net/RZ0QA.png)

Original
--------

I am plotting a dataset using `matplotlib` where I have an xlabel that is quite ""tall"" (it's a formula rendered in TeX that contains a fraction and is therefore has the height equivalent of a couple of lines of text).

In any case, the bottom of the formula is always cut off when I draw the figures. Changing figure size doesn't seem to help this, and I haven't been able to figure out how to shift the x-axis ""up"" to make room for the xlabel. Something like that would be a reasonable temporary solution, but what would be nice would be to have a way to make matplotlib recognize automatically that the label is cut off and resize accordingly.

Here's an example of what I mean:

```
import matplotlib.pyplot as plt

plt.figure()
plt.ylabel(r'$\ln\left(\frac{x_a-x_b}{x_a-x_c}\right)$')
plt.xlabel(r'$\ln\left(\frac{x_a-x_d}{x_a-x_e}\right)$', fontsize=50)
plt.title('Example with matplotlib 3.4.2\nMRE no longer an issue')
plt.show()

```

[![enter image description here](https://i.sstatic.net/pJ9qx.png)](https://i.sstatic.net/pJ9qx.png)

The entire ylabel is visible, however, the xlabel is cut off at the bottom.

In the case this is a machine-specific problem, I am running this on OSX 10.6.8 with matplotlib 1.0.0","Use:

```
import matplotlib.pyplot as plt

plt.gcf().subplots_adjust(bottom=0.15)

# alternate option without .gcf
plt.subplots_adjust(bottom=0.15)

```

to make room for the label, where [`plt.gcf()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.gcf.html) means **get the current figure**. [`plt.gca()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.gca.html), which gets the current `Axes`, can also be used.

Edit:

Since I gave the answer, `matplotlib` has added the **[`plt.tight_layout()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html#matplotlib.pyplot.tight_layout)** function.

[See matplotlib Tutorials: Tight Layout Guide](https://matplotlib.org/stable/tutorials/intermediate/tight_layout_guide.html)

So I suggest using it:

```
fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))
axes = axes.flatten()

for ax in axes:
    ax.set_ylabel(r'$\ln\left(\frac{x_a-x_b}{x_a-x_c}\right)$')
    ax.set_xlabel(r'$\ln\left(\frac{x_a-x_d}{x_a-x_e}\right)$')

plt.tight_layout()
plt.show()

```

[![enter image description here](https://i.sstatic.net/uTnEP.png)](https://i.sstatic.net/uTnEP.png)"
"I want to change the column labels of a Pandas DataFrame from

```
['$a', '$b', '$c', '$d', '$e']

```

to

```
['a', 'b', 'c', 'd', 'e']

```","Rename Specific Columns
-----------------------

Use the [`df.rename()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) function and refer the columns to be renamed. Not all the columns have to be renamed:

```
df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})

# Or rename the existing DataFrame (rather than creating a copy) 
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)

```

**Minimal Code Example**

```
df = pd.DataFrame('x', index=range(3), columns=list('abcde'))
df

   a  b  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

```

The following methods all work and produce the same output:

```
df2 = df.rename({'a': 'X', 'b': 'Y'}, axis=1)
df2 = df.rename({'a': 'X', 'b': 'Y'}, axis='columns')
df2 = df.rename(columns={'a': 'X', 'b': 'Y'}) 

df2

   X  Y  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

```

Remember to assign the result back, as the modification is not-inplace. Alternatively, specify `inplace=True`:

```
df.rename({'a': 'X', 'b': 'Y'}, axis=1, inplace=True)
df

   X  Y  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
 

```

You can specify `errors='raise'` to raise errors if an invalid column-to-rename is specified.

---

Reassign Column Headers
-----------------------

Use [`df.set_axis()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_axis.html) with `axis=1`.

```
df2 = df.set_axis(['V', 'W', 'X', 'Y', 'Z'], axis=1)
df2

   V  W  X  Y  Z
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

```

Headers can be assigned directly:

```
df.columns = ['V', 'W', 'X', 'Y', 'Z']
df

   V  W  X  Y  Z
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

```"
"[PEP 557](https://www.python.org/dev/peps/pep-0557/) introduces data classes into the Python standard library. It says that by applying the [`@dataclass`](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass) decorator shown below, it will generate ""among other things, an `__init__()`"".

> ```
> from dataclasses import dataclass
>
> @dataclass
> class InventoryItem:
>     """"""Class for keeping track of an item in inventory.""""""
>     name: str
>     unit_price: float
>     quantity_on_hand: int = 0
>
>     def total_cost(self) -> float:
>         return self.unit_price * self.quantity_on_hand
>
> ```

It also says dataclasses are ""mutable namedtuples with default"", but I don't understand what this means, nor how data classes are different from common classes.

What are data classes and when is it best to use them?","Data classes are just regular classes that are geared towards storing state, rather than containing a lot of logic. Every time you create a class that mostly consists of attributes, you make a data class.

What the `dataclasses` module does is to make it **easier** to create data classes. It takes care of a lot of boilerplate for you.

This is especially useful when your data class must be hashable; because this requires a `__hash__` method as well as an `__eq__` method. If you add a custom `__repr__` method for ease of debugging, that can become quite verbose:

```
class InventoryItem:
    '''Class for keeping track of an item in inventory.'''
    name: str
    unit_price: float
    quantity_on_hand: int = 0

    def __init__(
            self, 
            name: str, 
            unit_price: float,
            quantity_on_hand: int = 0
        ) -> None:
        self.name = name
        self.unit_price = unit_price
        self.quantity_on_hand = quantity_on_hand

    def total_cost(self) -> float:
        return self.unit_price * self.quantity_on_hand
    
    def __repr__(self) -> str:
        return (
            'InventoryItem('
            f'name={self.name!r}, unit_price={self.unit_price!r}, '
            f'quantity_on_hand={self.quantity_on_hand!r})'
        )

    def __hash__(self) -> int:
        return hash((self.name, self.unit_price, self.quantity_on_hand))

    def __eq__(self, other) -> bool:
        if not isinstance(other, InventoryItem):
            return NotImplemented
        return (
            (self.name, self.unit_price, self.quantity_on_hand) == 
            (other.name, other.unit_price, other.quantity_on_hand))

```

With `dataclasses` you can reduce it to:

```
from dataclasses import dataclass

@dataclass(unsafe_hash=True)
class InventoryItem:
    '''Class for keeping track of an item in inventory.'''
    name: str
    unit_price: float
    quantity_on_hand: int = 0

    def total_cost(self) -> float:
        return self.unit_price * self.quantity_on_hand

```

(Example based on [the PEP example](https://peps.python.org/pep-0557/#abstract)).

The same class decorator can also generate comparison methods (`__lt__`, `__gt__`, etc.) and handle immutability.

`namedtuple` classes are also data classes, but are immutable by default (as well as being sequences). `dataclasses` are much more flexible in this regard, and can easily be structured such that they can [fill the same role as a `namedtuple` class](https://stackoverflow.com/questions/44287623/a-way-to-subclass-namedtuple-for-purposes-of-typechecking/50369898#50369898).

The PEP was inspired by the [`attrs` project](http://www.attrs.org/en/stable/), which can do even more (including slots, validators, converters, metadata, etc.).

If you want to see some examples, I recently used `dataclasses` for several of my [Advent of Code](http://adventofcode.com/) solutions, see the solutions for [day 7](https://github.com/mjpieters/adventofcode/blob/master/2017/Day%2007.ipynb), [day 8](https://github.com/mjpieters/adventofcode/blob/master/2017/Day%2008.ipynb), [day 11](https://github.com/mjpieters/adventofcode/blob/master/2017/Day%2011.ipynb) and [day 20](https://github.com/mjpieters/adventofcode/blob/master/2017/Day%2020.ipynb).

If you want to use `dataclasses` module in Python versions < 3.7, then you could install the [backported module](https://pypi.org/p/dataclasses/) (requires 3.6) or use the `attrs` project mentioned above."
"How do I find the *nearest value* in a numpy array? Example:

```
np.find_nearest(array, value)

```","```
import numpy as np
def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return array[idx]

```

Example usage:

```
array = np.random.random(10)
print(array)
# [ 0.21069679  0.61290182  0.63425412  0.84635244  0.91599191  0.00213826
#   0.17104965  0.56874386  0.57319379  0.28719469]

print(find_nearest(array, value=0.5))
# 0.568743859261

```"
"I saw this in someone's code:

```
y = img_index // num_images

```

where `img_index` is a running index and `num_images` is 3.

When I mess around with `//` in [IPython](https://en.wikipedia.org/wiki/IPython), it seems to act just like a division sign (i.e. one forward slash). I was just wondering if there is any reason for having double forward slashes?","In Python 3, they made the `/` operator do a floating-point division, and added the `//` operator to do integer division (i.e., quotient without remainder); whereas in Python 2, the `/` operator was simply integer division, unless one of the operands was already a floating point number.

In Python 2.X:

```
>>> 10/3
3
>>> # To get a floating point number from integer division:
>>> 10.0/3
3.3333333333333335
>>> float(10)/3
3.3333333333333335

```

In Python 3:

```
>>> 10/3
3.3333333333333335
>>> 10//3
3

```

For further reference, see [PEP238](http://www.python.org/dev/peps/pep-0238/)."
"I have one field in a pandas DataFrame that was imported as string format.

It should be a datetime variable. How do I convert it to a datetime column, and then filter based on date?

Example:

```
raw_data = pd.DataFrame({'Mycol': ['05SEP2014:00:00:00.000']})

```","Use the [`to_datetime`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) function, specifying a [format](http://strftime.org/) to match your data.

```
df['Mycol'] = pd.to_datetime(df['Mycol'], format='%d%b%Y:%H:%M:%S.%f')

```"
"I want to delete a particular record like:

```
delete from table_name where id = 1;

```

How can I do this in a django model?","There are a couple of ways:

To delete it directly:

```
SomeModel.objects.filter(id=id).delete()

```

To delete it from an instance:

```
instance = SomeModel.objects.get(id=id)
instance.delete()

```"
"If I have a list of chars:

```
a = ['a','b','c','d']

```

How do I convert it into a single string?

```
a = 'abcd'

```","Use the `join` method of the empty string to join all of the strings together with the empty string in between, like so:

```
>>> a = ['a', 'b', 'c', 'd']
>>> ''.join(a)
'abcd'

```"
"What's the difference between `raise` and `raise from` in Python?

```
try:
    raise ValueError
except Exception as e:
    raise IndexError

```

which yields

```
Traceback (most recent call last):
  File ""tmp.py"", line 2, in <module>
    raise ValueError
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tmp.py"", line 4, in <module>
    raise IndexError
IndexError

```

and

```
try:
    raise ValueError
except Exception as e:
    raise IndexError from e

```

which yields

```
Traceback (most recent call last):
  File ""tmp.py"", line 2, in <module>
    raise ValueError
ValueError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""tmp.py"", line 4, in <module>
    raise IndexError from e
IndexError

```","The difference is that when you use `from`, the *`__cause__` attribute* is set and the message states that the exception was *directly caused by*. If you omit the `from` then no `__cause__` is set, but the *`__context__` attribute* may be set as well, and the traceback then shows the context as *during handling something else happened*.

Setting the `__context__` happens if you used `raise` in an exception handler; if you used `raise` anywhere else no `__context__` is set either.

If a `__cause__` is set, a `__suppress_context__ = True` flag is also set on the exception; when `__suppress_context__` is set to `True`, the `__context__` is ignored when printing a traceback.

When raising from a exception handler where you *don't* want to show the context (don't want a *during handling another exception happened* message), then use `raise ... from None` to set `__suppress_context__` to `True`.

In other words, Python sets a *context* on exceptions so you can introspect where an exception was raised, letting you see if another exception was replaced by it. You can also add a *cause* to an exception, making the traceback explicit about the other exception (use different wording), and the context is ignored (but can still be introspected when debugging). Using `raise ... from None` lets you suppress the context being printed.

See the [`raise` statement documenation](https://docs.python.org/3/reference/simple_stmts.html#the-raise-statement):

> The `from` clause is used for exception chaining: if given, the second *expression* must be another exception class or instance, which will then be attached to the raised exception as the `__cause__` attribute (which is writable). If the raised exception is not handled, both exceptions will be printed:
>
> ```
> >>> try:
> ...     print(1 / 0)
> ... except Exception as exc:
> ...     raise RuntimeError(""Something bad happened"") from exc
> ...
> Traceback (most recent call last):
>   File ""<stdin>"", line 2, in <module>
> ZeroDivisionError: int division or modulo by zero
>
> The above exception was the direct cause of the following exception:
>
> Traceback (most recent call last):
>   File ""<stdin>"", line 4, in <module>
> RuntimeError: Something bad happened
>
> ```
>
> A similar mechanism works implicitly if an exception is raised inside an exception handler or a `finally` clause: the previous exception is then attached as the new exceptionâ€™s `__context__` attribute:
>
> ```
> >>> try:
> ...     print(1 / 0)
> ... except:
> ...     raise RuntimeError(""Something bad happened"")
> ...
> Traceback (most recent call last):
>   File ""<stdin>"", line 2, in <module>
> ZeroDivisionError: int division or modulo by zero
>
> During handling of the above exception, another exception occurred:
>
> Traceback (most recent call last):
>   File ""<stdin>"", line 4, in <module>
> RuntimeError: Something bad happened
>
> ```

Also see the [Built-in Exceptions documentation](https://docs.python.org/3/library/exceptions.html#built-in-exceptions) for details on the context and cause information attached to exceptions."
"Suppose I have the following argparse snippet:

```
diags.cmdln_parser.add_argument( '--scan-time',
                     action  = 'store',
                     nargs   = '?',
                     type    = int,
                     default = 5,
                     help    = ""Wait SCAN-TIME seconds between status checks."")

```

Currently, `--help` returns:

```
usage: connection_check.py [-h]
                             [--version] [--scan-time [SCAN_TIME]]

          Test the reliability/uptime of a connection.



optional arguments:
-h, --help            show this help message and exit
--version             show program's version number and exit
--scan-time [SCAN_TIME]
                    Wait SCAN-TIME seconds between status checks.

```

I would prefer something like:

```
--scan-time [SCAN_TIME]
                    Wait SCAN-TIME seconds between status checks.
                    (Default = 5)

```

Peeking at the help formatter code revealed limited options. Is there a clever way to get `argparse` to print the default value for `--scan-time` in a similar fashion, or should I just subclass the `help` formatter?","Use the [`argparse.ArgumentDefaultsHelpFormatter` formatter](http://docs.python.org/library/argparse.html#formatter-class):

```
parser = argparse.ArgumentParser(
    # ... other options ...
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

```

To quote the [documentation](http://docs.python.org/library/argparse.html#formatter-class):

> The other formatter class available, `ArgumentDefaultsHelpFormatter`, will add information about the default value of each of the arguments.

Note that **this only applies to arguments that have help text defined**; with no `help` value for an argument, there is no help message to add information about the default value *to*.

The exact output for your scan-time option then becomes:

```
  --scan-time [SCAN_TIME]
                        Wait SCAN-TIME seconds between status checks.
                        (default: 5)

```"
"I have a Python `datetime` object that I want to convert to unix time, or seconds/milliseconds since the 1970 epoch.

How do I do this?","It appears to me that the simplest way to do this is

```
import datetime

epoch = datetime.datetime.utcfromtimestamp(0)

def unix_time_millis(dt):
    return (dt - epoch).total_seconds() * 1000.0

```"
"How do I execute a string containing Python code in Python?

---

Editor's note: [**Never use `eval` (or `exec`) on data that could possibly come from outside the program in any form. It is a critical security risk. You allow the author of the data to run arbitrary code on your computer.**](https://stackoverflow.com/questions/1832940/why-is-using-eval-a-bad-practice) If you are here because you want to create multiple variables in your Python program following a pattern, **you almost certainly have an [XY problem](https://xyproblem.info)**. Do not create those variables at all - instead, [use a list or dict appropriately](https://stackoverflow.com/questions/1373164).","For statements, use [`exec(string)`](https://docs.python.org/library/functions.html#exec) (Python 3) or [`exec string`](https://docs.python.org/2/reference/simple_stmts.html#grammar-token-exec-stmt) (Python 2):

```
>>> my_code = 'print(""Hello world"")'
>>> exec(my_code)
Hello world

```

When you need the value of an expression, use [`eval(string)`](http://docs.python.org/library/functions.html#eval):

```
>>> x = eval(""2+2"")
>>> x
4

```

However, the first step should be to ask yourself if you really need to. Executing code should generally be the position of last resort: It's slow, ugly and dangerous if it can contain user-entered code. You should always look at alternatives first, such as higher order functions, to see if these can better meet your needs."
"I have a huge list of datetime strings like the following

```
[""Jun 1 2005 1:33PM"", ""Aug 28 1999 12:00AM""]

```

How do I convert them into [`datetime`](https://docs.python.org/3/library/datetime.html#datetime-objects) objects?","[`datetime.strptime`](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime) parses an input string in the user-specified format into a *timezone-naive* [`datetime`](https://docs.python.org/3/library/datetime.html#datetime-objects) object:

```
>>> from datetime import datetime
>>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')
datetime.datetime(2005, 6, 1, 13, 33)

```

To obtain a [`date`](https://docs.python.org/3/library/datetime.html#date-objects) object using an existing `datetime` object, convert it using `.date()`:

```
>>> datetime.strptime('Jun 1 2005', '%b %d %Y').date()
date(2005, 6, 1)

```

---

**Links:**

* `strptime` docs: [Python 2](https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime ""datetime.datetime.strptime""), [Python 3](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime)
* `strptime`/`strftime` format string docs: [Python 2](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior ""strftime-and-strptime-behavior""), [Python 3](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)
* [strftime.org](http://strftime.org/) format string cheatsheet

**Notes:**

* `strptime` = ""string parse time""
* `strftime` = ""string format time"""
"I'm trying to add items to an array in Python.

I run

```
array = {}

```

Then, I try to add something to this array by doing:

```
array.append(valueToBeInserted)

```

There doesn't seem to be an `.append` method for this. How do I add items to an array?","`{}` represents an empty dictionary, not an array/list. For lists or arrays, you need `[]`.

To initialize an empty list do this:

```
my_list = []

```

or

```
my_list = list()

```

To add elements to the list, use `append`

```
my_list.append(12)

```

To `extend` the list to include the elements from another list use `extend`

```
my_list.extend([1,2,3,4])
my_list
--> [12,1,2,3,4]

```

To remove an element from a list use `remove`

```
my_list.remove(2)

```

Dictionaries represent a collection of key/value pairs also known as an associative array or a map.

To initialize an empty dictionary use `{}` or `dict()`

Dictionaries have keys and values

```
my_dict = {'key':'value', 'another_key' : 0}

```

To extend a dictionary with the contents of another dictionary you may use the `update` method

```
my_dict.update({'third_key' : 1})

```

To remove a value from a dictionary

```
del my_dict['key']

```"
"This is what I normally do in order to ascertain that the input is a `list`/`tuple` - but not a `str`. Because many times I stumbled upon bugs where a function passes a `str` object by mistake, and the target function does `for x in lst` assuming that `lst` is actually a `list` or `tuple`.

```
assert isinstance(lst, (list, tuple))

```

My question is: is there a better way of achieving this?","In python 2 only (not python 3):

```
assert not isinstance(lst, basestring)

```

Is actually what you want, otherwise you'll miss out on a lot of things which act like lists, but aren't subclasses of `list` or `tuple`."
"How do I convert a PIL `Image` back and forth to a NumPy array so that I can do faster pixel-wise transformations than PIL's `PixelAccess` allows? I can convert it to a NumPy array via:

```
pic = Image.open(""foo.jpg"")
pix = numpy.array(pic.getdata()).reshape(pic.size[0], pic.size[1], 3)

```

But how do I load it back into the PIL `Image` after I've modified the array? `pic.putdata()` isn't working well.","You're not saying how exactly `putdata()` is not behaving. I'm assuming you're doing

```
>>> pic.putdata(a)
Traceback (most recent call last):
  File ""...blablabla.../PIL/Image.py"", line 1185, in putdata
    self.im.putdata(data, scale, offset)
SystemError: new style getargs format but argument is not a tuple

```

This is because `putdata` expects a sequence of tuples and you're giving it a numpy array. This

```
>>> data = list(tuple(pixel) for pixel in pix)
>>> pic.putdata(data)

```

will work but it is very slow.

As of PIL 1.1.6, the [""proper"" way to convert between images and numpy arrays](https://web.archive.org/web/20081225061956/http://effbot.org/zone/pil-changes-116.htm) is simply

```
>>> pix = numpy.array(pic)

```

although the resulting array is in a different format than yours (3-d array or rows/columns/rgb in this case).

Then, after you make your changes to the array, you should be able to do either `pic.putdata(pix)` or create a new image with `Image.fromarray(pix)`."
"How do I make Python dictionary members accessible via a dot "".""?

For example, instead of writing `mydict['val']`, I'd like to write `mydict.val`.

Also I'd like to access nested dicts this way. For example

```
mydict.mydict2.val 

```

would refer to

```
mydict = { 'mydict2': { 'val': ... } }

```","I've always kept this around in a util file. You can use it as a mixin on your own classes too.

```
class dotdict(dict):
    """"""dot.notation access to dictionary attributes""""""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

mydict = {'val':'it works'}
nested_dict = {'val':'nested works too'}
mydict = dotdict(mydict)
mydict.val
# 'it works'

mydict.nested = dotdict(nested_dict)
mydict.nested.val
# 'nested works too'

```"
"I have a plot with two y-axes, using `twinx()`. I also give labels to the lines, and want to show them with `legend()`, but I only succeed to get the labels of one axis in the legend:

```
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
rc('mathtext', default='regular')

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(time, Swdown, '-', label = 'Swdown')
ax.plot(time, Rn, '-', label = 'Rn')
ax2 = ax.twinx()
ax2.plot(time, temp, '-r', label = 'temp')
ax.legend(loc=0)
ax.grid()
ax.set_xlabel(""Time (h)"")
ax.set_ylabel(r""Radiation ($MJ\,m^{-2}\,d^{-1}$)"")
ax2.set_ylabel(r""Temperature ($^\circ$C)"")
ax2.set_ylim(0, 35)
ax.set_ylim(-20,100)
plt.show()

```

So I only get the labels of the first axis in the legend, and not the label 'temp' of the second axis. How could I add this third label to the legend?

![enter image description here](https://i.sstatic.net/MdCYW.png)","You can easily add a second legend by adding the line:

```
ax2.legend(loc=0)

```

You'll get this:

![enter image description here](https://i.sstatic.net/DLZkF.png)

But if you want all labels on one legend then you should do something like this:

```
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
rc('mathtext', default='regular')

time = np.arange(10)
temp = np.random.random(10)*30
Swdown = np.random.random(10)*100-10
Rn = np.random.random(10)*100-10

fig = plt.figure()
ax = fig.add_subplot(111)

lns1 = ax.plot(time, Swdown, '-', label = 'Swdown')
lns2 = ax.plot(time, Rn, '-', label = 'Rn')
ax2 = ax.twinx()
lns3 = ax2.plot(time, temp, '-r', label = 'temp')

# added these three lines
lns = lns1+lns2+lns3
labs = [l.get_label() for l in lns]
ax.legend(lns, labs, loc=0)

ax.grid()
ax.set_xlabel(""Time (h)"")
ax.set_ylabel(r""Radiation ($MJ\,m^{-2}\,d^{-1}$)"")
ax2.set_ylabel(r""Temperature ($^\circ$C)"")
ax2.set_ylim(0, 35)
ax.set_ylim(-20,100)
plt.show()

```

Which will give you this:

![enter image description here](https://i.sstatic.net/Z8pg4.png)"
"If I do this:

```
>>> False in [False, True]
True

```

That returns `True`. Simply because `False` is in the list.

But if I do:

```
>>> not(True) in [False, True]
False

```

That returns `False`. Whereas `not(True)` is equal to `False`:

```
>>> not(True)
False

```

Why?","**Operator precedence** [2.x](https://docs.python.org/2/reference/expressions.html#operator-precedence), [3.x](https://docs.python.org/3/reference/expressions.html#operator-precedence). The precedence of `not` is lower than that of `in`. So it is equivalent to:

```
>>> not ((True) in [False, True])
False

```

This is what you want:

```
>>> (not True) in [False, True]
True

```

---

As @Ben points out: It's recommended to never write `not(True)`, prefer `not True`. The former makes it look like a function call, while `not` is an operator, not a function."
"I have a script named `test1.py` which is not in a module. It just has code that should execute when the script itself is run. There are no functions, classes, methods, etc. I have another script which runs as a service. I want to call `test1.py` from the script running as a service.

For example:

File **`test1.py`**:

```
print ""I am a test""
print ""see! I do nothing productive.""

```

File **`service.py`**:

```
# Lots of stuff here
test1.py # do whatever is in test1.py

```","The usual way to do this is something like the following.

test1.py

```
def some_func():
    print 'in test 1, unproductive'

if __name__ == '__main__':
    # test1.py executed as script
    # do something
    some_func()

```

service.py

```
import test1

def service_func():
    print 'service func'

if __name__ == '__main__':
    # service.py executed as script
    # do something
    service_func()
    test1.some_func()

```"
"I got this error:

```
AttributeError: 'NoneType' object has no attribute 'something'

```

What general scenarios might cause such an `AttributeError`, and how can I identify the problem?

---

This is a special case of `AttributeError`s. It merits separate treatment because there are a lot of ways to get an unexpected `None` value from the code, so it's typically a different problem; for other `AttributeError`s, the problem might just as easily be the attribute name.

See also [What is a None value?](https://stackoverflow.com/questions/19473185) and [What is a 'NoneType' object?](https://stackoverflow.com/questions/21095654) for an understanding of `None` and its type, `NoneType`.","NoneType means that instead of an instance of whatever Class or Object you think you're working with, you've actually got `None`. That usually means that an assignment or function call up above failed or returned an unexpected result."
"I want to iterate through the methods in a class, or handle class or instance objects differently based on the methods present. How do I get a list of class methods?

Also see:

* [How can I list the methods in a
  Python 2.5 module?](https://stackoverflow.com/questions/1280787/how-can-i-list-the-methods-in-a-python-2-5-module)
* [Looping over
  a Python / IronPython Object
  Methods](https://stackoverflow.com/questions/928990/looping-over-a-python-ironpython-object-methods)
* [Finding the methods an
  object has](https://stackoverflow.com/questions/34439/finding-the-methods-an-object-has)
* [How do I look inside
  a Python object?](https://stackoverflow.com/questions/1006169/how-do-i-look-inside-a-python-object)
* [How Do I
  Perform Introspection on an Object in
  Python 2.x?](https://stackoverflow.com/questions/546337/how-do-i-perform-introspection-on-an-object-in-python-2-x)
* [How to get a
  complete list of objectâ€™s methods and
  attributes?](https://stackoverflow.com/questions/191010/how-to-get-a-complete-list-of-objects-methods-and-attributes)
* [Finding out which
  functions are available from a class
  instance in python?](https://stackoverflow.com/questions/955533/finding-out-which-functions-are-available-from-a-class-instance-in-python)","An example (listing the methods of the `optparse.OptionParser` class):

```
>>> from optparse import OptionParser
>>> import inspect
#python2
>>> inspect.getmembers(OptionParser, predicate=inspect.ismethod)
[([('__init__', <unbound method OptionParser.__init__>),
...
 ('add_option', <unbound method OptionParser.add_option>),
 ('add_option_group', <unbound method OptionParser.add_option_group>),
 ('add_options', <unbound method OptionParser.add_options>),
 ('check_values', <unbound method OptionParser.check_values>),
 ('destroy', <unbound method OptionParser.destroy>),
 ('disable_interspersed_args',
  <unbound method OptionParser.disable_interspersed_args>),
 ('enable_interspersed_args',
  <unbound method OptionParser.enable_interspersed_args>),
 ('error', <unbound method OptionParser.error>),
 ('exit', <unbound method OptionParser.exit>),
 ('expand_prog_name', <unbound method OptionParser.expand_prog_name>),
 ...
 ]
# python3
>>> inspect.getmembers(OptionParser, predicate=inspect.isfunction)
...

```

Notice that `getmembers` returns a list of 2-tuples. The first item is the name of the member, the second item is the value.

You can also pass an instance to `getmembers`:

```
>>> parser = OptionParser()
>>> inspect.getmembers(parser, predicate=inspect.ismethod)
...

```"
"How does one convert a django Model object to a dict with **all** of its fields? All ideally includes foreign keys and fields with editable=False.

Let me elaborate. Let's say I have a django model like the following:

```
from django.db import models

class OtherModel(models.Model): pass

class SomeModel(models.Model):
    normal_value = models.IntegerField()
    readonly_value = models.IntegerField(editable=False)
    auto_now_add = models.DateTimeField(auto_now_add=True)
    foreign_key = models.ForeignKey(OtherModel, related_name=""ref1"")
    many_to_many = models.ManyToManyField(OtherModel, related_name=""ref2"")

```

In the terminal, I have done the following:

```
other_model = OtherModel()
other_model.save()
instance = SomeModel()
instance.normal_value = 1
instance.readonly_value = 2
instance.foreign_key = other_model
instance.save()
instance.many_to_many.add(other_model)
instance.save()

```

I want to convert this to the following dictionary:

```
{'auto_now_add': datetime.datetime(2015, 3, 16, 21, 34, 14, 926738, tzinfo=<UTC>),
 'foreign_key': 1,
 'id': 1,
 'many_to_many': [1],
 'normal_value': 1,
 'readonly_value': 2}

```

---

Questions with unsatisfactory answers:

[Django: Converting an entire set of a Model's objects into a single dictionary](https://stackoverflow.com/questions/1123337/django-converting-an-entire-set-of-a-models-objects-into-a-single-dictionary)

[How can I turn Django Model objects into a dictionary and still have their foreign keys?](https://stackoverflow.com/questions/12382546/how-can-i-turn-django-model-objects-into-a-dictionary-and-still-have-their-forei)","There are many ways to convert an instance to a dictionary, with varying degrees of corner case handling and closeness to the desired result.

---

1. `instance.__dict__`
----------------------

```
instance.__dict__

```

which returns

```
{'_foreign_key_cache': <OtherModel: OtherModel object>,
 '_state': <django.db.models.base.ModelState at 0x7ff0993f6908>,
 'auto_now_add': datetime.datetime(2018, 12, 20, 21, 34, 29, 494827, tzinfo=<UTC>),
 'foreign_key_id': 2,
 'id': 1,
 'normal_value': 1,
 'readonly_value': 2}

```

This is by far the simplest, but is missing `many_to_many`, `foreign_key` is misnamed, and it has two unwanted extra things in it.

---

2. `model_to_dict`
------------------

```
from django.forms.models import model_to_dict
model_to_dict(instance)

```

which returns

```
{'foreign_key': 2,
 'id': 1,
 'many_to_many': [<OtherModel: OtherModel object>],
 'normal_value': 1}

```

This is the only one with `many_to_many`, but is missing the uneditable fields.

---

3. `model_to_dict(..., fields=...)`
-----------------------------------

```
from django.forms.models import model_to_dict
model_to_dict(instance, fields=[field.name for field in instance._meta.fields])

```

which returns

```
{'foreign_key': 2, 'id': 1, 'normal_value': 1}

```

This is strictly worse than the standard `model_to_dict` invocation.

---

4. `query_set.values()`
-----------------------

```
SomeModel.objects.filter(id=instance.id).values()[0]

```

which returns

```
{'auto_now_add': datetime.datetime(2018, 12, 20, 21, 34, 29, 494827, tzinfo=<UTC>),
 'foreign_key_id': 2,
 'id': 1,
 'normal_value': 1,
 'readonly_value': 2}

```

This is the same output as `instance.__dict__` but without the extra fields.
`foreign_key_id` is still wrong and `many_to_many` is still missing.

---

5. Custom Function
------------------

The code for django's `model_to_dict` had most of the answer. It explicitly removed non-editable fields, so removing that check and getting the ids of foreign keys for many to many fields results in the following code which behaves as desired:

```
from itertools import chain

def to_dict(instance):
    opts = instance._meta
    data = {}
    for f in chain(opts.concrete_fields, opts.private_fields):
        data[f.name] = f.value_from_object(instance)
    for f in opts.many_to_many:
        data[f.name] = [i.id for i in f.value_from_object(instance)]
    return data

```

While this is the most complicated option, calling `to_dict(instance)` gives us exactly the desired result:

```
{'auto_now_add': datetime.datetime(2018, 12, 20, 21, 34, 29, 494827, tzinfo=<UTC>),
 'foreign_key': 2,
 'id': 1,
 'many_to_many': [2],
 'normal_value': 1,
 'readonly_value': 2}

```

---

6. Use Serializers
------------------

[Django Rest Framework](https://www.django-rest-framework.org/)'s ModelSerializer allows you to build a serializer automatically from a model.

```
from rest_framework import serializers
class SomeModelSerializer(serializers.ModelSerializer):
    class Meta:
        model = SomeModel
        fields = ""__all__""

SomeModelSerializer(instance).data

```

returns

```
{'auto_now_add': '2018-12-20T21:34:29.494827Z',
 'foreign_key': 2,
 'id': 1,
 'many_to_many': [2],
 'normal_value': 1,
 'readonly_value': 2}

```

This is almost as good as the custom function, but auto\_now\_add is a string instead of a datetime object.

---

Bonus Round: better model printing
----------------------------------

If you want a django model that has a better python command-line display, have your models child-class the following:

```
from django.db import models
from itertools import chain

class PrintableModel(models.Model):
    def __repr__(self):
        return str(self.to_dict())

    def to_dict(instance):
        opts = instance._meta
        data = {}
        for f in chain(opts.concrete_fields, opts.private_fields):
            data[f.name] = f.value_from_object(instance)
        for f in opts.many_to_many:
            data[f.name] = [i.id for i in f.value_from_object(instance)]
        return data

    class Meta:
        abstract = True

```

So, for example, if we define our models as such:

```
class OtherModel(PrintableModel): pass

class SomeModel(PrintableModel):
    normal_value = models.IntegerField()
    readonly_value = models.IntegerField(editable=False)
    auto_now_add = models.DateTimeField(auto_now_add=True)
    foreign_key = models.ForeignKey(OtherModel, related_name=""ref1"")
    many_to_many = models.ManyToManyField(OtherModel, related_name=""ref2"")

```

Calling `SomeModel.objects.first()` now gives output like this:

```
{'auto_now_add': datetime.datetime(2018, 12, 20, 21, 34, 29, 494827, tzinfo=<UTC>),
 'foreign_key': 2,
 'id': 1,
 'many_to_many': [2],
 'normal_value': 1,
 'readonly_value': 2}

```"
"I want to remove a key from a dictionary if it is present. I currently use this code:

```
if key in my_dict:
    del my_dict[key]

```

Without the `if` statement, the code will raise `KeyError` if the key is *not* present. How can I handle this more simply?

---

See [Delete an element from a dictionary](https://stackoverflow.com/questions/5844672) for more general approaches to the problem of removing a key from a dict (including ones which produce a modified copy).","To delete a key regardless of whether it is in the dictionary, use the two-argument form of [`dict.pop()`](http://docs.python.org/library/stdtypes.html#dict.pop):

```
my_dict.pop('key', None)

```

This will return `my_dict[key]` if `key` exists in the dictionary, and `None` otherwise. If the second parameter is not specified (i.e. `my_dict.pop('key')`) and `key` does not exist, a `KeyError` is raised.

To delete a key that is guaranteed to exist, you can also use

```
del my_dict['key']

```

This will raise a `KeyError` if the key is not in the dictionary."
"How do I print formatted NumPy arrays in a way similar to this:

```
x = 1.23456
print('%.3f' % x)

```

If I want to print the `numpy.ndarray` of floats, it prints several decimals, often in 'scientific' format, which is rather hard to read even for low-dimensional arrays. However, `numpy.ndarray` apparently has to be printed as a string, i.e., with `%s`. Is there a solution for this?","Use [`numpy.set_printoptions`](https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html) to set the precision of the output:

```
import numpy as np
x = np.random.random(10)
print(x)
# [ 0.07837821  0.48002108  0.41274116  0.82993414  0.77610352  0.1023732
#   0.51303098  0.4617183   0.33487207  0.71162095]

np.set_printoptions(precision=3)
print(x)
# [ 0.078  0.48   0.413  0.83   0.776  0.102  0.513  0.462  0.335  0.712]

```

And `suppress` suppresses the use of scientific notation for small numbers:

```
y = np.array([1.5e-10, 1.5, 1500])
print(y)
# [  1.500e-10   1.500e+00   1.500e+03]

np.set_printoptions(suppress=True)
print(y)
# [    0.      1.5  1500. ]

```

---

**To apply print options locally**, using NumPy 1.15.0 or later, you could use the [`numpy.printoptions`](https://numpy.org/doc/stable/reference/generated/numpy.printoptions.html) context manager.
For example, inside the `with-suite` `precision=3` and `suppress=True` are set:

```
x = np.random.random(10)
with np.printoptions(precision=3, suppress=True):
    print(x)
    # [ 0.073  0.461  0.689  0.754  0.624  0.901  0.049  0.582  0.557  0.348]

```

But outside the `with-suite` the print options are back to default settings:

```
print(x)    
# [ 0.07334334  0.46132615  0.68935231  0.75379645  0.62424021  0.90115836
#   0.04879837  0.58207504  0.55694118  0.34768638]

```

If you are using an earlier version of NumPy, you can create the context manager
yourself. For example,

```
import numpy as np
import contextlib

@contextlib.contextmanager
def printoptions(*args, **kwargs):
    original = np.get_printoptions()
    np.set_printoptions(*args, **kwargs)
    try:
        yield
    finally: 
        np.set_printoptions(**original)

x = np.random.random(10)
with printoptions(precision=3, suppress=True):
    print(x)
    # [ 0.073  0.461  0.689  0.754  0.624  0.901  0.049  0.582  0.557  0.348]

```

---

**To prevent zeros from being stripped from the end of floats:**

`np.set_printoptions` now has a `formatter` parameter which allows you to specify a format function for each type.

```
np.set_printoptions(formatter={'float': '{: 0.3f}'.format})
print(x)

```

which prints

```
[ 0.078  0.480  0.413  0.830  0.776  0.102  0.513  0.462  0.335  0.712]

```

instead of

```
[ 0.078  0.48   0.413  0.83   0.776  0.102  0.513  0.462  0.335  0.712]

```"
"```
datetime.datetime.utcnow()

```

Why does this `datetime` not have any timezone info given that it is explicitly a UTC `datetime`?

I would expect that this would contain `tzinfo`.","Note that for Python 3.2 onwards, the [`datetime`](https://docs.python.org/3/library/datetime.html) module contains [`datetime.timezone`](https://docs.python.org/3/library/datetime.html#datetime.timezone). The documentation for [`datetime.utcnow()`](https://docs.python.org/3/library/datetime.html#datetime.datetime.utcnow) says:

> An aware current UTC datetime can be obtained by calling `datetime.now(timezone.utc)`.

So, `datetime.utcnow()` doesn't set `tzinfo` to indicate that it is UTC, but `datetime.now(datetime.timezone.utc)` does return UTC time *with* `tzinfo` set.

So you can do:

```
>>> import datetime
>>> datetime.datetime.now(datetime.timezone.utc)
datetime.datetime(2014, 7, 10, 2, 43, 55, 230107, tzinfo=datetime.timezone.utc)

```

Since Python 3.11, there also exists [`datetime.UTC`](https://docs.python.org/3/library/datetime.html#datetime.UTC) which is equivalent to `datetime.timezone.utc`. So you can also do `datetime.datetime.now(datetime.UTC)`."
"I have a dataframe in pandas where each column has different value range. For example:

df:

```
A     B   C
1000  10  0.5
765   5   0.35
800   7   0.09

```

Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?

My desired output is:

```
A     B    C
1     1    1
0.765 0.5  0.7
0.8   0.7  0.18(which is 0.09/0.5)

```","one easy way by using **Pandas**: (here I want to use mean normalization)

```
normalized_df=(df-df.mean())/df.std()

```

to use min-max normalization:

```
normalized_df=(df-df.min())/(df.max()-df.min())

```

Edit: To address some concerns, need to say that Pandas automatically applies colomn-wise function in the code above."
"What is the best way to strip all non alphanumeric characters from a string, using Python?

The solutions presented in the [PHP variant of this question](https://stackoverflow.com/questions/840948) will probably work with some minor adjustments, but don't seem very 'pythonic' to me.

For the record, I don't just want to strip periods and commas (and other punctuation), but also quotes, brackets, etc.","I just timed some functions out of curiosity. In these tests I'm removing non-alphanumeric characters from the string `string.printable` (part of the built-in `string` module). The use of compiled `'[\W_]+'` and `pattern.sub('', str)` was found to be fastest.

```
$ python -m timeit -s \
     ""import string"" \
     ""''.join(ch for ch in string.printable if ch.isalnum())"" 
10000 loops, best of 3: 57.6 usec per loop

$ python -m timeit -s \
    ""import string"" \
    ""filter(str.isalnum, string.printable)""                 
10000 loops, best of 3: 37.9 usec per loop

$ python -m timeit -s \
    ""import re, string"" \
    ""re.sub('[\W_]', '', string.printable)""
10000 loops, best of 3: 27.5 usec per loop

$ python -m timeit -s \
    ""import re, string"" \
    ""re.sub('[\W_]+', '', string.printable)""                
100000 loops, best of 3: 15 usec per loop

$ python -m timeit -s \
    ""import re, string; pattern = re.compile('[\W_]+')"" \
    ""pattern.sub('', string.printable)"" 
100000 loops, best of 3: 11.2 usec per loop

```"
Right now I'm importing a fairly large `CSV` as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?,"The easiest way is to [pickle](https://docs.python.org/3/library/pickle.html) it using [`to_pickle`](http://pandas.pydata.org/pandas-docs/stable/io.html#pickling):

```
df.to_pickle(file_name)  # where to save it, usually as a .pkl

```

Then you can load it back using:

```
df = pd.read_pickle(file_name)

```

*Note: before 0.11.1 `save` and `load` were the only way to do this (they are now deprecated in favor of `to_pickle` and `read_pickle` respectively).*

---

Another popular choice is to use [HDF5](http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables) ([pytables](http://www.pytables.org)) which offers [very fast](https://stackoverflow.com/questions/16628329/hdf5-and-sqlite-concurrency-compression-i-o-performance) access times for large datasets:

```
import pandas as pd
store = pd.HDFStore('store.h5')

store['df'] = df  # save it
store['df']  # load it

```

*More advanced strategies are discussed in the [cookbook](http://pandas-docs.github.io/pandas-docs-travis/#pandas-powerful-python-data-analysis-toolkit).*

---

Since 0.13 there's also [msgpack](http://pandas.pydata.org/pandas-docs/stable/io.html#msgpack-experimental) which may be be better for interoperability, as a faster alternative to JSON, or if you have python object/text-heavy data (see [this question](https://stackoverflow.com/q/30651724/1240268))."
I used to use `perl -c <filename>` to check the syntax of a Perl program and then exit without executing it. Is there an equivalent way to do this for a Python script?,"You can check the syntax by compiling it:

```
python -m py_compile script.py

```"
"How can I sort this list in descending order?

```
timestamps = [
    ""2010-04-20 10:07:30"",
    ""2010-04-20 10:07:38"",
    ""2010-04-20 10:07:52"",
    ""2010-04-20 10:08:22"",
    ""2010-04-20 10:08:22"",
    ""2010-04-20 10:09:46"",
    ""2010-04-20 10:10:37"",
    ""2010-04-20 10:10:58"",
    ""2010-04-20 10:11:50"",
    ""2010-04-20 10:12:13"",
    ""2010-04-20 10:12:13"",
    ""2010-04-20 10:25:38""
]

```","This will give you a sorted version of the array.

```
sorted(timestamps, reverse=True)

```

If you want to sort in-place:

```
timestamps.sort(reverse=True)

```

Check the docs at [Sorting HOW TO](https://docs.python.org/howto/sorting.html)"
"I'm looking for a library in Python which will provide `at` and `cron` like functionality.

I'd quite like have a pure Python solution, rather than relying on tools installed on the box; this way I run on machines with no cron.

For those unfamiliar with `cron`: you can schedule tasks based upon an expression like:

```
 0 2 * * 7 /usr/bin/run-backup # run the backups at 0200 on Every Sunday
 0 9-17/2 * * 1-5 /usr/bin/purge-temps # run the purge temps command, every 2 hours between 9am and 5pm on Mondays to Fridays.

```

The cron time expression syntax is less important, but I would like to have something with this sort of flexibility.

If there isn't something that does this for me out-the-box, any suggestions for the building blocks to make something like this would be gratefully received.

**Edit**
I'm not interested in launching processes, just ""jobs"" also written in Python - python functions. By necessity I think this would be a different thread, but not in a different process.

To this end, I'm looking for the expressivity of the cron time expression, but in Python.

Cron *has* been around for years, but I'm trying to be as portable as possible. I cannot rely on its presence.","If you're looking for something lightweight checkout [schedule](https://github.com/dbader/schedule):

```
import schedule
import time

def job():
    print(""I'm working..."")

schedule.every(10).minutes.do(job)
schedule.every().hour.do(job)
schedule.every().day.at(""10:30"").do(job)

while 1:
    schedule.run_pending()
    time.sleep(1)

```

*Disclosure*: I'm the author of that library."
"In the [tensorflow API docs](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax) they use a keyword called `logits`. What is it? A lot of methods are written like:

```
tf.nn.softmax(logits, name=None)

```

If `logits` is just a generic `Tensor` input, why is it named `logits`?

---

Secondly, what is the difference between the following two methods?

```
tf.nn.softmax(logits, name=None)
tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)

```

I know what `tf.nn.softmax` does, but not the other. An example would be really helpful.","The softmax+logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear. It means, in particular, the sum of the inputs may not equal 1, that the values are *not* probabilities (you might have an input of 5). Internally, it first applies softmax to the unscaled output, and then computes the cross entropy of those values vs. what they ""should"" be as defined by the labels.

`tf.nn.softmax` produces the result of applying the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to an input tensor. The softmax ""squishes"" the inputs so that `sum(input) = 1`, and it does the mapping by interpreting the inputs as log-probabilities (logits) and then converting them back into raw probabilities between 0 and 1. The shape of output of a softmax is the same as the input:

```
a = tf.constant(np.array([[.1, .3, .5, .9]]))
print s.run(tf.nn.softmax(a))
[[ 0.16838508  0.205666    0.25120102  0.37474789]]

```

See [this answer](https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization) for more about why softmax is used extensively in DNNs.

`tf.nn.softmax_cross_entropy_with_logits` combines the softmax step with the calculation of the cross-entropy loss after applying the softmax function, but it does it all together in a more mathematically careful way. It's similar to the result of:

```
sm = tf.nn.softmax(x)
ce = cross_entropy(sm)

```

The cross entropy is a summary metric: it sums across the elements. The output of `tf.nn.softmax_cross_entropy_with_logits` on a shape `[2,5]` tensor is of shape `[2,1]` (the first dimension is treated as the batch).

If you want to do optimization to minimize the cross entropy **AND** you're softmaxing after your last layer, you should use `tf.nn.softmax_cross_entropy_with_logits` instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way. Otherwise, you'll end up hacking it by adding little epsilons here and there.

**Edited 2016-02-07:**
If you have single-class labels, where an object can only belong to one class, you might now consider using `tf.nn.sparse_softmax_cross_entropy_with_logits` so that you don't have to convert your labels to a dense one-hot array. This function was added after release 0.6.0."
How can I convert a DataFrame column of strings (in ***dd/mm/yyyy*** format) to datetime dtype?,"The easiest way is to use [`to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html):

```
df['col'] = pd.to_datetime(df['col'])

```

It also offers a `dayfirst` argument for European times (but beware [this isn't strict](https://github.com/pydata/pandas/issues/3341)).

Here it is in action:

```
In [11]: pd.to_datetime(pd.Series(['05/23/2005']))
Out[11]:
0   2005-05-23 00:00:00
dtype: datetime64[ns]

```

You can pass a specific [format](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior):

```
In [12]: pd.to_datetime(pd.Series(['05/23/2005']), format=""%m/%d/%Y"")
Out[12]:
0   2005-05-23
dtype: datetime64[ns]

```"
"Is it possible to upgrade all Python packages at one time with **[`pip`](https://pypi.python.org/pypi/pip)**?

**Note**: that there is [a feature request](https://github.com/pypa/pip/issues/4551) for this on the official issue tracker.","There isn't a built-in flag yet. Starting with pip version 22.3, the `--outdated` and `--format=freeze` have become [mutually exclusive](https://pip.pypa.io/en/stable/news/#v22-3). Use Python, to parse the JSON output:

```
pip --disable-pip-version-check list --outdated --format=json | python -c ""import json, sys; print('\n'.join([x['name'] for x in json.load(sys.stdin)]))"" | xargs -n1 pip install -U

```

If you are using `pip<22.3` you can use:

```
pip list --outdated --format=freeze | grep -v '^\-e' | cut -d = -f 1  | xargs -n1 pip install -U

```

For older versions of `pip`:

```
pip freeze --local | grep -v '^\-e' | cut -d = -f 1  | xargs -n1 pip install -U

```

---

* The `grep` is to skip editable (""-e"") package definitions, as suggested by [@jawache](https://stackoverflow.com/questions/2720014/how-to-upgrade-all-python-packages-with-pip#comment13279323_3452888). (Yes, you could replace `grep`+`cut` with `sed` or `awk` or `perl` or...).
* The `-n1` flag for `xargs` prevents stopping everything if updating one package fails (thanks [@andsens](https://stackoverflow.com/users/339505/andsens)).

---

**Note:** there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!"
"I often test my module in the Python Interpreter, and when I see an error, I quickly update the .py file. But how do I make it reflect on the Interpreter ? So, far I have been exiting and reentering the Interpreter because re importing the file again is not working for me.","**Update for Python3**: (quoted from the [already-answered answer](https://stackoverflow.com/a/437591/3358272), since the last edit/comment here suggested a deprecated method)

> In Python 3, `reload` was moved to the [`imp`](https://docs.python.org/3.2/library/imp.html) module. In 3.4, `imp` was deprecated in favor of [`importlib`](https://docs.python.org/3.4/library/importlib.html), and [`reload`](https://docs.python.org/3.4/library/importlib.html#importlib.reload) was added to the latter. When targeting 3 or later, either reference the appropriate module when calling `reload` or import it.

Takeaway:

* Python3 >= 3.4: `importlib.reload(packagename)`
* Python3 < 3.4: `imp.reload(packagename)`
* Python2: continue below

---

Use the `reload` builtin function:

<https://docs.python.org/2/library/functions.html#reload>

> When `reload(module)` is executed:
>
> * Python modules’ code is recompiled and the module-level code reexecuted, defining a new set of objects which are bound to names in the module’s dictionary. The init function of extension modules is not called a second time.
> * As with all other objects in Python the old objects are only reclaimed after their reference counts drop to zero.
> * The names in the module namespace are updated to point to any new or changed objects.
> * Other references to the old objects (such as names external to the module) are not rebound to refer to the new objects and must be updated in each namespace where they occur if that is desired.

Example:

```
# Make a simple function that prints ""version 1""
shell1$ echo 'def x(): print ""version 1""' > mymodule.py

# Run the module
shell2$ python
>>> import mymodule
>>> mymodule.x()
version 1

# Change mymodule to print ""version 2"" (without exiting the python REPL)
shell2$ echo 'def x(): print ""version 2""' > mymodule.py

# Back in that same python session
>>> reload(mymodule)
<module 'mymodule' from 'mymodule.pyc'>
>>> mymodule.x()
version 2

```"
"I can't really think of any reason why Python needs the `del` keyword (and most languages seem to not have a similar keyword). For instance, rather than deleting a variable, one could just assign `None` to it. And when deleting from a dictionary, a `del` method could be added.

Is there a reason to keep `del` in Python, or is it a vestige of Python's pre-garbage collection days?","Firstly, you can del other things besides local variables

```
del list_item[4]
del dictionary[""alpha""]

```

Both of which should be clearly useful. Secondly, using `del` on a local variable makes the intent clearer. Compare:

```
del foo

```

to

```
foo = None

```

I know in the case of `del foo` that the intent is to remove the variable from scope. It's not clear that `foo = None` is doing that. If somebody just assigned `foo = None` I might think it was dead code. But I instantly know what somebody who codes `del foo` was trying to do."
"I'm a little bit confused with JSON in Python.
To me, it seems like a dictionary, and for that reason
I'm trying to do that:

```
{
    ""glossary"":
    {
        ""title"": ""example glossary"",
        ""GlossDiv"":
        {
            ""title"": ""S"",
            ""GlossList"":
            {
                ""GlossEntry"":
                {
                    ""ID"": ""SGML"",
                    ""SortAs"": ""SGML"",
                    ""GlossTerm"": ""Standard Generalized Markup Language"",
                    ""Acronym"": ""SGML"",
                    ""Abbrev"": ""ISO 8879:1986"",
                    ""GlossDef"":
                    {
                        ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",
                        ""GlossSeeAlso"": [""GML"", ""XML""]
                    },
                    ""GlossSee"": ""markup""
                }
            }
        }
    }
}

```

But when I do `print(dict(json))`, I get an error.

How can I transform this string into a structure and then call `json[""title""]` to obtain `""example glossary""`?","[`json.loads()`](http://docs.python.org/library/json.html#json.loads)

```
import json

d = json.loads(j)
print d['glossary']['title']

```"
"Unless I'm mistaken, creating a function in Python works like this:

```
def my_func(param1, param2):
    # stuff

```

However, you don't actually give the types of those parameters. Also, if I remember, Python is a strongly typed language, as such, it seems like Python shouldn't let you pass in a parameter of a different type than the function creator expected. However, how does Python know that the user of the function is passing in the proper types? Will the program just die if it's the wrong type, assuming the function actually uses the parameter? Do you have to specify the type?","The other answers have done a good job at explaining duck typing and [the simple answer by tzot](https://stackoverflow.com/questions/2489669/function-parameter-types-in-python/21384492#comment2574436_2489692):

> Python does not have variables, like other languages where variables have a type and a value; it has names pointing to objects, which know their type.

**However**, one interesting thing has changed since 2010 (when the question was first asked), namely the implementation of [PEP 3107](http://www.python.org/dev/peps/pep-3107/) (implemented in Python 3). You can now actually specify the type of a parameter and the type of the return type of a function like this:

```
def pick(l: list, index: int) -> int:
    return l[index]

```

Here we can see that `pick` takes 2 parameters, a list `l` and an integer `index`. It should also return an integer.

So here it is implied that `l` is a list of integers which we can see without much effort, but for more complex functions it can be a bit confusing as to what the list should contain. We also want the default value of `index` to be 0. To solve this you may choose to write `pick` like this instead:

```
def pick(l: ""list of ints"", index: int = 0) -> int:
    return l[index]

```

Note that we now put in a string as the type of `l`, which is syntactically allowed, but it is not good for parsing programmatically (which we'll come back to later).

It is important to note that Python won't raise a `TypeError` if you pass a float into `index`, the reason for this is one of the main points in Python's design philosophy: *""We're all consenting adults here""*, which means you are expected to be aware of what you can pass to a function and what you can't. If you really want to write code that throws TypeErrors you can use the `isinstance` function to check that the passed argument is of the proper type or a subclass of it like this:

```
def pick(l: list, index: int = 0) -> int:
    if not isinstance(l, list):
        raise TypeError
    return l[index]

```

More on why you should rarely do this and what you should do instead is talked about in the next section and in the comments.

[PEP 3107](http://www.python.org/dev/peps/pep-3107/) does not only improve code readability but also has several fitting use cases which you can read about [**here**](http://www.python.org/dev/peps/pep-3107/#use-cases).

---

Type annotation got a lot more attention in Python 3.5 with the introduction of [PEP 484](https://www.python.org/dev/peps/pep-0484/) which introduces a standard module `typing` for type hints.

These type hints came from the type checker [mypy](http://mypy-lang.org/) ([GitHub](https://github.com/JukkaL/mypy)), which is now [PEP 484](https://www.python.org/dev/peps/pep-0484/) compliant.

The `typing` module comes with a pretty comprehensive collection of type hints, including:

* `List`, `Tuple`, `Set`, `Dict` - for `list`, `tuple`, `set` and `dict` respectively.
* `Iterable` - useful for generators.
* `Any` - when it could be anything.
* `Union` - when it could be anything within a specified set of types, as opposed to `Any`.
* `Optional` - when it **might** be None. Shorthand for `Union[T, None]`.
* `TypeVar` - used with generics.
* `Callable` - used primarily for functions, but could be used for other callables.

These are the most common type hints. A complete listing can be found in the [documentation for the typing module](https://docs.python.org/3/library/typing.html).

Here is the old example using the annotation methods introduced in the typing module:

```
from typing import List

def pick(l: List[int], index: int) -> int:
    return l[index]

```

One powerful feature is the `Callable` which allows you to type annotate methods that take a function as an argument. For example:

```
from typing import Callable, Any, Iterable

def imap(f: Callable[[Any], Any], l: Iterable[Any]) -> List[Any]:
    """"""An immediate version of map, don't pass it any infinite iterables!""""""
    return list(map(f, l))

```

The above example could become more precise with the usage of `TypeVar` instead of `Any`, but this has been left as an exercise to the reader since I believe I've already filled my answer with too much information about the wonderful new features enabled by type hinting.

---

Previously when one documented Python code with for example [Sphinx](http://sphinx-doc.org/) some of the above functionality could be obtained by writing docstrings formatted like this:

```
def pick(l, index):
    """"""
    :param l: list of integers
    :type l: list
    :param index: index at which to pick an integer from *l*
    :type index: int
    :returns: integer at *index* in *l*
    :rtype: int
    """"""
    return l[index]

```

As you can see, this takes a number of extra lines (the exact number depends on how explicit you want to be and how you format your docstring). But it should now be clear to you how [PEP 3107](http://www.python.org/dev/peps/pep-3107/) provides an alternative that is in many (all?) ways superior. This is especially true in combination with [PEP 484](https://www.python.org/dev/peps/pep-0484/) which, as we have seen, provides a standard module that defines a syntax for these type hints/annotations that can be used in such a way that it is unambiguous and precise yet flexible, making for a powerful combination.

In my personal opinion, this is one of the greatest features in Python ever. I can't wait for people to start harnessing the power of it. Sorry for the long answer, but this is what happens when I get excited.

---

An example of Python code which heavily uses type hinting can be found [here](https://github.com/ActivityWatch/aw-core/blob/master/aw_core/models.py)."
"I have a directory which hosts all of my Django apps (`C:\My_Projects`). I want to add this directory to my `PYTHONPATH` so I can call the apps directly.

I tried adding `C:\My_Projects\;` to my Windows `Path` variable from the Windows GUI (`My Computer > Properties > Advanced System Settings > Environment Variables`). But it still doesn't read the coltrane module and generates this error:

> Error: No module named coltrane","You know what has worked for me really well on windows.

`My Computer > Properties > Advanced System Settings > Environment Variables >`

Just add the path as C:\Python27 (or wherever you installed python)

OR

Then under system variables I create a new Variable called `PythonPath`. In this variable I have `C:\Python27\Lib;C:\Python27\DLLs;C:\Python27\Lib\lib-tk;C:\other-folders-on-the-path`

![enter image description here](https://i.sstatic.net/ZGp36.png)

This is the best way that has worked for me which I hadn't found in any of the docs offered.

**EDIT:** For those who are not able to get it,
Please add

> C:\Python27;

along with it. Else it will *never work*."
"I want to build a query for sunburnt (solr interface) using class inheritance and therefore adding key-value pairs together. The sunburnt interface takes keyword arguments. How can I transform a dict (`{'type':'Event'}`) into keyword arguments (`type='Event'`)?

---

See also: [What do \*\* (double star/asterisk) and \* (star/asterisk) mean in a function call?](https://stackoverflow.com/questions/2921847) - the corresponding question for people who encounter the syntax and are confused by it.","Use the [double-star](http://docs.python.org/tutorial/controlflow.html#unpacking-argument-lists) (aka [double-splat?](https://stackoverflow.com/questions/2322355/proper-name-for-python-operator/2322384#2322384)) operator:

```
func(**{'type':'Event'})

```

is equivalent to

```
func(type='Event')

```"
"How do I get the number of elements in a list in jinja2 template?

For example, in Python:

```
print(template.render(products=[???]))

```

and in jinja2

```
<span>You have {{what goes here?}} products</span>

```","```
<span>You have {{products|length}} products</span>

```

You can also use this syntax in expressions like

```
{% if products|length > 1 %}

```

jinja2's builtin filters are documented [here](http://jinja.pocoo.org/docs/templates/#builtin-filters); and specifically, as you've already found, [`length`](http://jinja.pocoo.org/docs/templates/#length) (and its synonym `count`) is documented to:

> Return the number of items of a sequence or mapping.

So, again as you've found, `{{products|count}}` (or equivalently `{{products|length}}`) in your template will give the ""number of products"" (""length of list"")"
"I would like to use the .replace function to replace multiple strings.

I currently have

```
string.replace(""condition1"", """")

```

but would like to have something like

```
string.replace(""condition1"", """").replace(""condition2"", ""text"")

```

although that does not feel like good syntax

what is the proper way to do this? kind of like how in grep/regex you can do `\1` and `\2` to replace fields to certain search strings","Here is a short example that should do the trick with regular expressions:

```
import re

rep = {""condition1"": """", ""condition2"": ""text""} # define desired replacements here

# use these three lines to do the replacement
rep = dict((re.escape(k), v) for k, v in rep.items()) 
pattern = re.compile(""|"".join(rep.keys()))
text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)

```

For example:

```
>>> pattern.sub(lambda m: rep[re.escape(m.group(0))], ""(condition1) and --condition2--"")
'() and --text--'

```"
"Is there any simple way of generating (and checking) MD5 checksums of a list of files in Python? (I have a small program I'm working on, and I'd like to confirm the checksums of the files).","You can use [hashlib.md5()](http://docs.python.org/library/hashlib.html)

Note that sometimes you won't be able to fit the whole file in memory. In that case, you'll have to read chunks sequentially and feed them to the md5 update method.

In this example I'm using a 4096-byte (4 KiB) buffer, but you can adjust this size

```
import hashlib
def md5(fname):
    hash_md5 = hashlib.md5()
    with open(fname, ""rb"") as f:
        for chunk in iter(lambda: f.read(4096), b""""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

```

**Note:** `hash_md5.hexdigest()` will return the *hex string* representation for the digest, if you just need the packed bytes use `return hash_md5.digest()`, so you don't have to convert back."
"How to remove rows with duplicate index values?

In the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.

I'm reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like:

```
                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress
Date                                                                                      
2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31
2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30
2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30
2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30
2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28

```

Example of a duplicate case:

```
import pandas as pd
import datetime

startdate = datetime.datetime(2001, 1, 1, 0, 0)
enddate = datetime.datetime(2001, 1, 1, 5, 0)
index = pd.date_range(start=startdate, end=enddate, freq='H')
data1 = {'A' : range(6), 'B' : range(6)}
data2 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}
df1 = pd.DataFrame(data=data1, index=index)
df2 = pd.DataFrame(data=data2, index=index[:3])
df3 = df2.append(df1)

df3
                       A   B
2001-01-01 00:00:00   20 -50
2001-01-01 01:00:00  -30  60
2001-01-01 02:00:00   40 -70
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2

```

And so I need `df3` to eventually become:

```
                       A   B
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5

```

I thought that adding a column of row numbers (`df3['rownum'] = range(df3.shape[0])`) would help me select the bottom-most row for any value of the `DatetimeIndex`, but I am stuck on figuring out the `group_by` or `pivot` (or ???) statements to make that work.","I would suggest using the [duplicated](https://pandas.pydata.org/docs/reference/api/pandas.Index.duplicated.html) method on the Pandas Index itself:

```
df3 = df3[~df3.index.duplicated(keep='first')]

```

While all the other methods work, `.drop_duplicates` is by far the least performant for the provided example. Furthermore, while the [groupby method](https://stackoverflow.com/a/13036848/3622349) is only slightly less performant, I find the duplicated method to be more readable.

Using the sample data provided:

```
>>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')
1000 loops, best of 3: 1.54 ms per loop

>>> %timeit df3.groupby(df3.index).first()
1000 loops, best of 3: 580 µs per loop

>>> %timeit df3[~df3.index.duplicated(keep='first')]
1000 loops, best of 3: 307 µs per loop

```

Note that you can keep the last element by changing the keep argument to `'last'`.

It should also be noted that this method works with `MultiIndex` as well (using df1 as specified in [Paul's example](https://stackoverflow.com/a/13036848/3622349)):

```
>>> %timeit df1.groupby(level=df1.index.names).last()
1000 loops, best of 3: 771 µs per loop

>>> %timeit df1[~df1.index.duplicated(keep='last')]
1000 loops, best of 3: 365 µs per loop

```"
"How do I sort a list of dictionaries by a specific key's value? Given:

```
[{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]

```

When sorted by `name`, it should become:

```
[{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]

```","The [`sorted()`](https://docs.python.org/library/functions.html#sorted) function takes a `key=` parameter

```
newlist = sorted(list_to_be_sorted, key=lambda d: d['name'])

```

Alternatively, you can use [`operator.itemgetter`](https://docs.python.org/library/operator.html#operator.itemgetter) instead of defining the function yourself

```
from operator import itemgetter
newlist = sorted(list_to_be_sorted, key=itemgetter('name'))

```

For completeness, add `reverse=True` to sort in descending order

```
newlist = sorted(list_to_be_sorted, key=itemgetter('name'), reverse=True)

```"
"In Python, how can I print the current call stack from within a method (for debugging purposes).","Here's an example of getting the stack via the [traceback](http://docs.python.org/library/traceback.html) module, and printing it:

```
import traceback

def f():
    g()

def g():
    for line in traceback.format_stack():
        print(line.strip())

f()

# Prints:
# File ""so-stack.py"", line 10, in <module>
#     f()
# File ""so-stack.py"", line 4, in f
#     g()
# File ""so-stack.py"", line 7, in g
#     for line in traceback.format_stack():

```

If you really only want to print the stack to stderr, you can use:

```
traceback.print_stack()

```

Or to print to stdout (useful if want to keep redirected output together), use:

```
traceback.print_stack(file=sys.stdout)

```

But getting it via `traceback.format_stack()` lets you do whatever you like with it."
"I have a big dictionary object that has several key value pairs (about 16), but I am only interested in 3 of them. What is the best way (shortest/efficient/most elegant) to subset such dictionary?

The best I know is:

```
bigdict = {'a':1,'b':2,....,'z':26} 
subdict = {'l':bigdict['l'], 'm':bigdict['m'], 'n':bigdict['n']}

```

I am sure there is a more elegant way than this.","You could try:

```
dict((k, bigdict[k]) for k in ('l', 'm', 'n'))

```

... or in Python versions 2.7 or later:

```
{k: bigdict[k] for k in ('l', 'm', 'n')}

```

I'm assuming that you know the keys are going to be in the dictionary. See the [answer](https://stackoverflow.com/questions/5352546/best-way-to-extract-subset-of-key-value-pairs-from-python-dictionary-object/5352649#5352649) by [HÃ¥vard S](https://stackoverflow.com/users/94237/havard-s) if you don't.

Alternatively, as [timbo](https://stackoverflow.com/users/127660/timbo) points out in the comments, if you want a key that's missing in `bigdict` to map to `None`, you can do:

```
{k: bigdict.get(k, None) for k in ('l', 'm', 'n')}

```

If you're using Python 3, and you *only* want keys in the new dict that actually exist in the original one, you can use the fact to view objects implement some set operations:

```
{k: bigdict[k] for k in bigdict.keys() & {'l', 'm', 'n'}}

```"
"I'm learning to use `matplotlib` by studying examples, and a lot of examples seem to include a line like the following before creating a single plot...

```
fig, ax = plt.subplots()

```

Here are some examples...

* [Modify tick label text](https://stackoverflow.com/questions/11244514/modify-tick-label-text)
* <http://matplotlib.org/examples/pylab_examples/boxplot_demo2.html>

I see this function used a lot, even though the example is only attempting to create a single chart. Is there some other advantage? The official demo for `subplots()` also uses `f, ax = subplots` when creating a single chart, and it only ever references ax after that. This is the code they use.

```
# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

```","`plt.subplots()` is a function that returns a tuple containing a figure and axes object(s). Thus when using `fig, ax = plt.subplots()` you unpack this tuple into the variables `fig` and `ax`. Having `fig` is useful if you want to change figure-level attributes or save the figure as an image file later (e.g. with `fig.savefig('yourfilename.png')`). You certainly don't have to use the returned figure object but many people do use it later so it's common to see. Also, all axes objects (the objects that have plotting methods), have a parent figure object anyway, thus:

```
fig, ax = plt.subplots()

```

is more concise than this:

```
fig = plt.figure()
ax = fig.add_subplot(111)

```"
"I am getting the following error:

```
Exception in thread Thread-3:
Traceback (most recent call last):
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 810, in        __bootstrap_inner
self.run()
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 763, in  run
self.__target(*self.__args, **self.__kwargs)
File ""/Users/Matthew/Desktop/Skypebot 2.0/bot.py"", line 271, in process
info = urllib2.urlopen(req).read()
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 154, in urlopen
return opener.open(url, data, timeout)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 431, in open
response = self._open(req, data)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 449, in _open
'_open', req)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 409, in _call_chain
result = func(*args)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 1240, in https_open
context=self._context)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 1197, in do_open
raise URLError(err)
URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)>

```

This is the code that is causing this error:

```
if input.startswith(""!web""):
    input = input.replace(""!web "", """")      
    url = ""https://domainsearch.p.mashape.com/index.php?name="" + input
    req = urllib2.Request(url, headers={ 'X-Mashape-Key': 'XXXXXXXXXXXXXXXXXXXX' })
    info = urllib2.urlopen(req).read()
    Message.Chat.SendMessage ("""" + info)

```

The API I'm using requires me to use HTTPS. How can I make it bypass the verification?

---

For this error message **when using Pip**, see [pip install fails with ""connection error: [SSL: CERTIFICATE\_VERIFY\_FAILED] certificate verify failed (\_ssl.c:598)""](https://stackoverflow.com/questions/25981703) .","This isn't a solution to your specific problem, but I'm putting it here because this thread is the top Google result for ""SSL: CERTIFICATE\_VERIFY\_FAILED"", and it lead me on a wild goose chase.

If you have installed Python 3.6 on OSX and are getting the ""SSL: CERTIFICATE\_VERIFY\_FAILED"" error when trying to connect to an https:// site, it's probably because Python 3.6 on OSX has no certificates at all, and can't validate any SSL connections. This is a change for 3.6 on OSX, and requires a post-install step, which installs the `certifi` package of certificates. This is documented in the file [ReadMe.rtf](https://github.com/python/cpython/blob/560ea272b01acaa6c531cc7d94331b2ef0854be6/Mac/BuildScript/resources/ReadMe.rtf#L35), which you can find at `/Applications/Python\ 3.6/ReadMe.rtf` (see also the file [Conclusion.rtf](https://github.com/python/cpython/blob/560ea272b01acaa6c531cc7d94331b2ef0854be6/Mac/BuildScript/resources/Conclusion.rtf#L15-L19), and the script [`build-installer.py`](https://github.com/python/cpython/blob/560ea272b01acaa6c531cc7d94331b2ef0854be6/Mac/BuildScript/build-installer.py#L207-L220) that generates the macOS installer).

The ReadMe will have you run the post-install script at

`/Applications/Python\ 3.10/Install\ Certificates.command` (Terminal App, this command alone should, fix the issue. Be sure to update the file path using your current subversion.)

(its source is [`install_certificates.command`](https://github.com/python/cpython/blob/560ea272b01acaa6c531cc7d94331b2ef0854be6/Mac/BuildScript/resources/install_certificates.command)), which:

* first [installs the Python package `certifi`](https://github.com/python/cpython/blob/560ea272b01acaa6c531cc7d94331b2ef0854be6/Mac/BuildScript/resources/install_certificates.command#L27-L28), and
* then [creates a symbolic link](https://github.com/python/cpython/blob/560ea272b01acaa6c531cc7d94331b2ef0854be6/Mac/BuildScript/resources/install_certificates.command#L41) from the OpenSSL certificates file to the certificates file installed by the package `certifi`.

Release notes have some more info: <https://www.python.org/downloads/release/python-360/>

On newer versions of Python, there is more documentation about this:

* <https://github.com/python/cpython/blob/e05a703848473b0365886dcc593cbddc46609f29/Mac/BuildScript/resources/ReadMe.rtf#L22-L34>
* <https://github.com/python/cpython/blob/e05a703848473b0365886dcc593cbddc46609f29/Mac/BuildScript/resources/Conclusion.rtf#L15-L19>
* <https://github.com/python/cpython/blob/e05a703848473b0365886dcc593cbddc46609f29/Mac/BuildScript/resources/Welcome.rtf#L23-L25>
* <https://github.com/python/cpython/blob/e05a703848473b0365886dcc593cbddc46609f29/Mac/BuildScript/resources/install_certificates.command>
* <https://github.com/python/cpython/blob/e05a703848473b0365886dcc593cbddc46609f29/Mac/BuildScript/README.rst>
* <https://github.com/python/cpython/blob/e05a703848473b0365886dcc593cbddc46609f29/Mac/BuildScript/build-installer.py#L239-L246>"
"How do I convert a `numpy.datetime64` object to a `datetime.datetime` (or `Timestamp`)?

In the following code, I create a datetime, timestamp and datetime64 objects.

```
import datetime
import numpy as np
import pandas as pd
dt = datetime.datetime(2012, 5, 1)
# A strange way to extract a Timestamp object, there's surely a better way?
ts = pd.DatetimeIndex([dt])[0]
dt64 = np.datetime64(dt)

In [7]: dt
Out[7]: datetime.datetime(2012, 5, 1, 0, 0)

In [8]: ts
Out[8]: <Timestamp: 2012-05-01 00:00:00>

In [9]: dt64
Out[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')

```

*Note: it's easy to get the datetime from the Timestamp:*

```
In [10]: ts.to_datetime()
Out[10]: datetime.datetime(2012, 5, 1, 0, 0)

```

But how do we extract the `datetime` or `Timestamp` from a `numpy.datetime64` (`dt64`)?

.

Update: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:

```
dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')

```

which should be `datetime.datetime(2002, 6, 28, 1, 0)`, and not a long (!) (`1025222400000000000L`)...","You can just use the pd.Timestamp constructor. The following diagram may be useful for this and related questions.

![Conversions between time representations](https://i.sstatic.net/uiXQd.png)"
"I'm using Enum4 library to create an enum class as follows:

```
class Color(Enum):
    RED = 1
    BLUE = 2

```

I want to print `[1, 2]` as a list somewhere. How can I achieve this?","You can do the following:

```
[e.value for e in Color]

```"
"I have a list of tuples in [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29), and I have a conditional where I want to take the branch ONLY if the tuple is not in the list (if it is in the list, then I don't want to take the if branch)

```
if curr_x -1 > 0 and (curr_x-1 , curr_y) not in myList: 

    # Do Something

```

This is not really working for me though. What have I done wrong?","The bug is probably somewhere else in your code, because it should work fine:

```
>>> 3 not in [2, 3, 4]
False
>>> 3 not in [4, 5, 6]
True

```

Or with tuples:

```
>>> (2, 3) not in [(2, 3), (5, 6), (9, 1)]
False
>>> (2, 3) not in [(2, 7), (7, 3), ""hi""]
True

```"
Does Python support short-circuiting in boolean expressions?,"Yep, both `and` and `or` operators short-circuit -- see [the docs](http://docs.python.org/library/stdtypes.html?highlight=short%20circuit#boolean-operations-and-or-not)."
"If I have a Python dictionary, how do I get the key to the entry which contains the minimum value?

I was thinking about something to do with the `min()` function...

Given the input:

```
{320:1, 321:0, 322:3}

```

It would return `321`.","Best: `min(d, key=d.get)` -- no reason to interpose a useless `lambda` indirection layer or extract items or keys!

```
>>> d = {320: 1, 321: 0, 322: 3}
>>> min(d, key=d.get)
321

```"
"I have a dict like this:

```
sample = {'ObjectInterpolator': 1629,  'PointInterpolator': 1675, 'RectangleInterpolator': 2042}

```

I can't figure out how to dump the dict to a JSON file as showed below:

```
{      
    ""name"": ""interpolator"",
    ""children"": [
      {""name"": ""ObjectInterpolator"", ""size"": 1629},
      {""name"": ""PointInterpolator"", ""size"": 1675},
      {""name"": ""RectangleInterpolator"", ""size"": 2042}
     ]
}

```

Is there a pythonic way to do this?

You may guess that I want to generate a `d3` treemap.","```
import json
with open('result.json', 'w') as fp:
    json.dump(sample, fp)

```

This is an easier way to do it.

In the second line of code the file `result.json` gets created and opened as the variable `fp`.

In the third line your dict `sample` gets written into the `result.json`!"
"I want to merge two dictionaries into a new dictionary.

```
x = {'a': 1, 'b': 2}
y = {'b': 3, 'c': 4}
z = merge(x, y)

>>> z
{'a': 1, 'b': 3, 'c': 4}

```

Whenever a key `k` is present in both dictionaries, only the value `y[k]` should be kept.","How can I merge two Python dictionaries in a single expression?
---------------------------------------------------------------

For dictionaries `x` and `y`, their shallowly-merged dictionary `z` takes values from `y`, replacing those from `x`.

* In Python 3.9.0 or greater (released 17 October 2020, [`PEP-584`](https://www.python.org/dev/peps/pep-0584/), [discussed here](https://bugs.python.org/issue36144)):

  ```
  z = x | y

  ```
* In Python 3.5 or greater:

  ```
  z = {**x, **y}

  ```
* In Python 2, (or 3.4 or lower) write a function:

  ```
  def merge_two_dicts(x, y):
      z = x.copy()   # start with keys and values of x
      z.update(y)    # modifies z with keys and values of y
      return z

  ```

  and now:

  ```
  z = merge_two_dicts(x, y)

  ```

### Explanation

Say you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:

```
x = {'a': 1, 'b': 2}
y = {'b': 3, 'c': 4}

```

The desired result is to get a new dictionary (`z`) with the values merged, and the second dictionary's values overwriting those from the first.

```
>>> z
{'a': 1, 'b': 3, 'c': 4}

```

A new syntax for this, proposed in [PEP 448](https://www.python.org/dev/peps/pep-0448) and [available as of Python 3.5](https://mail.python.org/pipermail/python-dev/2015-February/138564.html), is

```
z = {**x, **y}

```

And it is indeed a single expression.

Note that we can merge in with literal notation as well:

```
z = {**x, 'foo': 1, 'bar': 2, **y}

```

and now:

```
>>> z
{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}

```

It is now showing as implemented in the [release schedule for 3.5, PEP 478](https://www.python.org/dev/peps/pep-0478/#features-for-3-5), and it has now made its way into the [What's New in Python 3.5](https://docs.python.org/dev/whatsnew/3.5.html#pep-448-additional-unpacking-generalizations) document.

However, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:

```
z = x.copy()
z.update(y) # which returns None since it mutates z

```

In both approaches, `y` will come second and its values will replace `x`'s values, thus `b` will point to `3` in our final result.

Not yet on Python 3.5, but want a *single expression*
-----------------------------------------------------

If you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a *single expression*, the most performant while the correct approach is to put it in a function:

```
def merge_two_dicts(x, y):
    """"""Given two dictionaries, merge them into a new dict as a shallow copy.""""""
    z = x.copy()
    z.update(y)
    return z

```

and then you have a single expression:

```
z = merge_two_dicts(x, y)

```

You can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:

```
def merge_dicts(*dict_args):
    """"""
    Given any number of dictionaries, shallow copy and merge into a new dict,
    precedence goes to key-value pairs in latter dictionaries.
    """"""
    result = {}
    for dictionary in dict_args:
        result.update(dictionary)
    return result

```

This function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries `a` to `g`:

```
z = merge_dicts(a, b, c, d, e, f, g) 

```

and key-value pairs in `g` will take precedence over dictionaries `a` to `f`, and so on.

Critiques of Other Answers
--------------------------

Don't use what you see in the formerly accepted answer:

```
z = dict(x.items() + y.items())

```

In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. **In Python 3, this will fail** because you're adding two `dict_items` objects together, not two lists -

```
>>> c = dict(a.items() + b.items())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'

```

and you would have to explicitly create them as lists, e.g. `z = dict(list(x.items()) + list(y.items()))`. This is a waste of resources and computation power.

Similarly, taking the union of `items()` in Python 3 (`viewitems()` in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, **since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:**

```
>>> c = dict(a.items() | b.items())

```

This example demonstrates what happens when values are unhashable:

```
>>> x = {'a': []}
>>> y = {'b': []}
>>> dict(x.items() | y.items())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'list'

```

Here's an example where `y` should have precedence, but instead the value from `x` is retained due to the arbitrary order of sets:

```
>>> x = {'a': 2}
>>> y = {'a': 1}
>>> dict(x.items() | y.items())
{'a': 2}

```

Another hack you should not use:

```
z = dict(x, **y)

```

This uses the `dict` constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic.

Here's an example of the usage being [remediated in django](https://code.djangoproject.com/attachment/ticket/13357/django-pypy.2.diff).

Dictionaries are intended to take hashable keys (e.g. `frozenset`s or tuples), but **this method fails in Python 3 when keys are not strings.**

```
>>> c = dict(a, **b)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: keyword arguments must be strings

```

From the [mailing list](https://mail.python.org/pipermail/python-dev/2010-April/099459.html), Guido van Rossum, the creator of the language, wrote:

> I am fine with
> declaring dict({}, \*\*{1:3}) illegal, since after all it is abuse of
> the \*\* mechanism.

and

> Apparently dict(x, \*\*y) is going around as ""cool hack"" for ""call
> x.update(y) and return x"". Personally, I find it more despicable than
> cool.

It is my understanding (as well as the understanding of the [creator of the language](https://mail.python.org/pipermail/python-dev/2010-April/099485.html)) that the intended usage for `dict(**y)` is for creating dictionaries for readability purposes, e.g.:

```
dict(a=1, b=10, c=11)

```

instead of

```
{'a': 1, 'b': 10, 'c': 11}

```

Response to comments
--------------------

> Despite what Guido says, `dict(x, **y)` is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the \*\* operator in this place an abuse of the mechanism, in fact, \*\* was designed precisely to pass dictionaries as keywords.

Again, it doesn't work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. `dict` broke this consistency in Python 2:

```
>>> foo(**{('a', 'b'): None})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: foo() keywords must be strings
>>> dict(**{('a', 'b'): None})
{('a', 'b'): None}

```

This inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.

I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.

More comments:

> `dict(x.items() + y.items())` is still the most readable solution for Python 2. Readability counts.

My response: `merge_two_dicts(x, y)` actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.

> `{**x, **y}` does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word ""merging"" these answers describe ""updating one dict with another"", and not merging.

Yes. I must refer you back to the question, which is asking for a *shallow* merge of ***two*** dictionaries, with the first's values being overwritten by the second's - in a single expression.

Assuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:

```
from copy import deepcopy

def dict_of_dicts_merge(x, y):
    z = {}
    overlapping_keys = x.keys() & y.keys()
    for key in overlapping_keys:
        z[key] = dict_of_dicts_merge(x[key], y[key])
    for key in x.keys() - overlapping_keys:
        z[key] = deepcopy(x[key])
    for key in y.keys() - overlapping_keys:
        z[key] = deepcopy(y[key])
    return z

```

Usage:

```
>>> x = {'a':{1:{}}, 'b': {2:{}}}
>>> y = {'b':{10:{}}, 'c': {11:{}}}
>>> dict_of_dicts_merge(x, y)
{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}

```

Coming up with contingencies for other value types is far beyond the scope of this question, so I will point you at [my answer to the canonical question on a ""Dictionaries of dictionaries merge""](https://stackoverflow.com/a/24088493/541136).

Less Performant But Correct Ad-hocs
-----------------------------------

These approaches are less performant, but they will provide correct behavior.
They will be *much less* performant than `copy` and `update` or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they *do* respect the order of precedence (latter dictionaries have precedence)

You can also chain the dictionaries manually inside a [dict comprehension](https://www.python.org/dev/peps/pep-0274/):

```
{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7

```

or in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):

```
dict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2

```

`itertools.chain` will chain the iterators over the key-value pairs in the correct order:

```
from itertools import chain
z = dict(chain(x.items(), y.items())) # iteritems in Python 2

```

Performance Analysis
--------------------

I'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)

```
from timeit import repeat
from itertools import chain

x = dict.fromkeys('abcdefg')
y = dict.fromkeys('efghijk')

def merge_two_dicts(x, y):
    z = x.copy()
    z.update(y)
    return z

min(repeat(lambda: {**x, **y}))
min(repeat(lambda: merge_two_dicts(x, y)))
min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))
min(repeat(lambda: dict(chain(x.items(), y.items()))))
min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))

```

In Python 3.8.1, NixOS:

```
>>> min(repeat(lambda: {**x, **y}))
1.0804965235292912
>>> min(repeat(lambda: merge_two_dicts(x, y)))
1.636518670246005
>>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))
3.1779992282390594
>>> min(repeat(lambda: dict(chain(x.items(), y.items()))))
2.740647904574871
>>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))
4.266070580109954

```

```
$ uname -a
Linux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux

```

Resources on Dictionaries
-------------------------

* [My explanation of Python's **dictionary implementation**, updated for 3.6.](https://stackoverflow.com/questions/327311/how-are-pythons-built-in-dictionaries-implemented/44509302#44509302)
* [Answer on how to add new keys to a dictionary](https://stackoverflow.com/questions/1024847/add-new-keys-to-a-dictionary/27208535#27208535)
* [Mapping two lists into a dictionary](https://stackoverflow.com/questions/209840/map-two-lists-into-a-dictionary-in-python/33737067#33737067)
* [The official Python docs on dictionaries](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)
* [The Dictionary Even Mightier](https://www.youtube.com/watch?v=66P5FMkWoVU) - talk by Brandon Rhodes at Pycon 2017
* [Modern Python Dictionaries, A Confluence of Great Ideas](https://www.youtube.com/watch?v=npw4s1QTmPg) - talk by Raymond Hettinger at Pycon 2017"
"What is the quickest way to HTTP GET in Python if I know the content will be a string? I am searching the documentation for a quick one-liner like:

```
contents = url.get(""http://example.com/foo/bar"")

```

But all I can find using Google are `httplib` and `urllib` - and I am unable to find a shortcut in those libraries.

Does standard Python 2.5 have a shortcut in some form as above, or should I write a function `url_get`?

1. I would prefer not to capture the output of shelling out to `wget` or `curl`.","Python 3:

```
import urllib.request
contents = urllib.request.urlopen(""http://example.com/foo/bar"").read()

```

Python 2:

```
import urllib2
contents = urllib2.urlopen(""http://example.com/foo/bar"").read()

```

Documentation for [`urllib.request`](https://docs.python.org/library/urllib.request.html) and [`read`](https://docs.python.org/tutorial/inputoutput.html#methods-of-file-objects)."
"I want to find out my Python installation path on Windows. For example:

```
C:\Python25

```

How can I find where Python is installed?","In your Python interpreter, type the following commands:

```
>>> import os, sys
>>> os.path.dirname(sys.executable)
'C:\\Python25'

```

Also, you can club all these and use a single line command. Open cmd and enter following command

```
python -c ""import os, sys; print(os.path.dirname(sys.executable))""

```"
"I know how to get an intersection of two flat lists:

```
b1 = [1,2,3,4,5,9,11,15]
b2 = [4,5,6,7,8]
b3 = [val for val in b1 if val in b2]

```

or

```
def intersect(a, b):
    return list(set(a) & set(b))
 
print intersect(b1, b2)

```

But when I have to find intersection for nested lists then my problems starts:

```
c1 = [1, 6, 7, 10, 13, 28, 32, 41, 58, 63]
c2 = [[13, 17, 18, 21, 32], [7, 11, 13, 14, 28], [1, 5, 6, 8, 15, 16]]

```

In the end I would like to receive:

```
c3 = [[13,32],[7,13,28],[1,6]]

```

Can you guys give me a hand with this?

### Related

* [Flattening a shallow list in python](https://stackoverflow.com/questions/406121/flattening-a-shallow-list-in-python)","You don't need to define intersection. It's already a first-class part of set.

```
>>> b1 = [1,2,3,4,5,9,11,15]
>>> b2 = [4,5,6,7,8]
>>> set(b1).intersection(b2)
set([4, 5])

```"
"Whenever I use `sys.path.append`, the new directory will be added. However, once I close python, the list will revert to the previous (default?) values. How do I permanently add a directory to `PYTHONPATH`?","If you're using bash (on a Mac or GNU/Linux distro), add this to your `~/.bashrc`

```
export PYTHONPATH=""${PYTHONPATH}:/my/other/path""

```"
Is there a way to show the SQL that Django is running while performing a query?,"See the docs FAQ: ""[How can I see the raw SQL queries Django is running?](http://docs.djangoproject.com/en/stable/faq/models/#how-can-i-see-the-raw-sql-queries-django-is-running)""

`django.db.connection.queries` contains a list of the SQL queries:

```
from django.db import connection
print(connection.queries)

```

Querysets also have a [`query` attribute](https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet) containing the query to be executed:

```
print(MyModel.objects.filter(name=""my name"").query)

```

Note that the output of the query is not valid SQL, because:

> ""Django never actually interpolates the parameters: it sends the query and the parameters separately to the database adapter, which performs the appropriate operations.""

From Django bug report [#17741](https://code.djangoproject.com/ticket/17741).

Because of that, you should not send query output directly to a database.

If you need to reset the queries to, for example, see how many queries are running in a given period, you can use `reset_queries` from `django.db`:

```
from django.db import reset_queries
from django.db import connection

reset_queries()
# Run your query here
print(connection.queries)
>>> []

```"
I want to know if it is possible to use the pandas `to_csv()` function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data.,"You can specify a python write mode in the pandas [`to_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) function. For append it is 'a'.

In your case:

```
df.to_csv('my_csv.csv', mode='a', header=False)

```

The default mode is 'w'.

If the file initially might be missing, you can make sure the header is printed at the first write using this variation:

```
output_path='my_csv.csv'
df.to_csv(output_path, mode='a', header=not os.path.exists(output_path))

```"
"After creating a NumPy array, and saving it as a Django context variable, I receive the following error when loading the webpage:

```
array([   0,  239,  479,  717,  952, 1192, 1432, 1667], dtype=int64) is not JSON serializable

```

What does this mean?","I regularly ""jsonify"" np.arrays. Try using the "".tolist()"" method on the arrays first, like this:

```
import numpy as np
import codecs, json 

a = np.arange(10).reshape(2,5) # a 2 by 5 array
b = a.tolist() # nested lists with same data, indices
file_path = ""/path.json"" ## your path variable
json.dump(b, codecs.open(file_path, 'w', encoding='utf-8'), 
          separators=(',', ':'), 
          sort_keys=True, 
          indent=4) ### this saves the array in .json format

```

In order to ""unjsonify"" the array use:

```
obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()
b_new = json.loads(obj_text)
a_new = np.array(b_new)

```"
"I find myself frequently using Python's interpreter to work with databases, files, etc -- basically a lot of manual formatting of semi-structured data. I don't properly save and clean up the useful bits as often as I would like. Is there a way to save my input into the shell (db connections, variable assignments, little for loops and bits of logic) -- some history of the interactive session? If I use something like `script` I get too much stdout noise. I don't really need to pickle all the objects -- though if there is a solution that does that, it would be OK. Ideally I would just be left with a script that ran as the one I created interactively, and I could just delete the bits I didn't need. Is there a package that does this, or a DIY approach?","[IPython](https://ipython.org/) is extremely useful if you like using interactive sessions. For example for your use-case there is [the `%save` magic command](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-save), you just input `%save my_useful_session 10-20 23` to save input lines 10 to 20 and 23 to `my_useful_session.py` (to help with this, every line is prefixed by its number).

Furthermore, the documentation states:

> This function uses the same syntax as [%history](http://ipython.readthedocs.io/en/5.x/interactive/magics.html#magic-history) for input ranges, then saves the lines to the filename you specify.

This allows for example, to reference older sessions, such as

```
%save current_session ~0/
%save previous_session ~1/

```

Look at [the videos on the presentation page](https://ipython.org/presentation.html) to get a quick overview of the features."
How do I use [`timeit`](https://docs.python.org/3/library/timeit.html) to compare the performance of my own functions such as `insertion_sort` and `tim_sort`?,"If you want to use `timeit` in an interactive Python session, there are two convenient options:

1. Use the [IPython](http://ipython.org/) shell. It features the convenient `%timeit` special function:

   ```
   In [1]: def f(x):
      ...:     return x*x
      ...: 

   In [2]: %timeit for x in range(100): f(x)
   100000 loops, best of 3: 20.3 us per loop

   ```
2. In a standard Python interpreter, you can access functions and other names you defined earlier during the interactive session by importing them from `__main__` in the setup statement:

   ```
   >>> def f(x):
   ...     return x * x 
   ... 
   >>> import timeit
   >>> timeit.repeat(""for x in range(100): f(x)"", ""from __main__ import f"",
                     number=100000)
   [2.0640320777893066, 2.0876040458679199, 2.0520210266113281]

   ```"
"I have one figure which contains many subplots.

```
fig = plt.figure(num=None, figsize=(26, 12), dpi=80, facecolor='w', edgecolor='k')
fig.canvas.set_window_title('Window Title')

# Returns the Axes instance
ax = fig.add_subplot(311) 
ax2 = fig.add_subplot(312) 
ax3 = fig.add_subplot(313) 

```

How do I add titles to the subplots?

`fig.suptitle` adds a title to all graphs and although `ax.set_title()` exists, the latter does not add any title to my subplots.

Thank you for your help.

Edit:
Corrected typo about `set_title()`. Thanks Rutger Kassies","`ax.title.set_text('My Plot Title')` seems to work too.

```
fig = plt.figure()
ax1 = fig.add_subplot(221)
ax2 = fig.add_subplot(222)
ax3 = fig.add_subplot(223)
ax4 = fig.add_subplot(224)
ax1.title.set_text('First Plot')
ax2.title.set_text('Second Plot')
ax3.title.set_text('Third Plot')
ax4.title.set_text('Fourth Plot')
plt.show()

```

[![matplotlib add titles on subplots](https://i.sstatic.net/dUp6p.png)](https://i.sstatic.net/dUp6p.png)"
"In Python, I've seen two variable values swapped using this syntax:

```
left, right = right, left

```

Is this considered the standard way to swap two variable values or is there some other means by which two variables are by convention most usually swapped?","> Python evaluates expressions from left to right. Notice that while
> evaluating an assignment, the right-hand side is evaluated before the
> left-hand side.
>
> [Python docs: Evaluation order](http://docs.python.org/3/reference/expressions.html#evaluation-order)

That means the following for the expression `a,b = b,a` :

* The right-hand side `b,a` is evaluated, that is to say, a tuple of two elements is created in the memory. The two elements are the objects designated by the identifiers `b` and `a`, that were existing before the instruction is encountered during the execution of the program.
* Just after the creation of this tuple, no assignment of this tuple object has still been made, but it doesn't matter, Python internally knows where it is.
* Then, the left-hand side is evaluated, that is to say, the tuple is assigned to the left-hand side.
* As the left-hand side is composed of two identifiers, the tuple is unpacked in order that the first identifier `a` be assigned to the first element of the tuple (which is the object that was formerly **b** before the swap because it had name `b`)  
  and the second identifier `b` is assigned to the second element of the tuple (which is the object that was formerly **a** before the swap because its identifiers was `a`)

This mechanism has effectively swapped the objects assigned to the identifiers `a` and `b`

So, to answer your question: YES, it's the standard way to swap two identifiers on two objects.  
By the way, the objects are not variables, they are objects."
"When I read Django code I often see in models what is called a ""slug"". I am not quite sure what this is, but I do know it has something to do with URLs. How and when is this slug-thing supposed to be used?

I have read its definition below in [this glossary](http://docs.djangoproject.com/en/dev/glossary/):

> **Slug**  
> A short label for something, containing only letters, numbers,
> underscores or hyphens. Theyâ€™re generally used in URLs. For example,
> in a typical blog entry URL:
>
> <https://www.djangoproject.com/weblog/2008/apr/12/spring/> the last bit
> (spring) is the slug.","A ""slug"" is a way of generating a valid URL, generally using data already obtained. For instance, a slug uses the title of an article to generate a URL. I advise to generate the slug by means of a function, given the title (or another piece of data), rather than setting it manually.

An example:

```
<title> The 46 Year Old Virgin </title>
<content> A silly comedy movie </content>
<slug> the-46-year-old-virgin </slug>

```

Now let's pretend that we have a Django model such as:

```
class Article(models.Model):
    title = models.CharField(max_length=100)
    content = models.TextField(max_length=1000)
    slug = models.SlugField(max_length=40)

```

How would you reference this object with a URL and with a meaningful name? You could for instance use Article.id so the URL would look like this:

```
www.example.com/article/23

```

Or, you might want to reference the title like this:

```
www.example.com/article/The 46 Year Old Virgin

```

Since spaces aren't valid in URLs, they must be replaced by `%20`, which results in:

```
www.example.com/article/The%2046%20Year%20Old%20Virgin

```

Both attempts are not resulting in very meaningful, easy-to-read URL. This is better:

```
www.example.com/article/the-46-year-old-virgin

```

In this example, `the-46-year-old-virgin` is a slug: it is created from the title by down-casing all letters, and replacing spaces by hyphens `-`.

Also see the URL of this very web page for another example."
"I would like to install `scipy-0.15.1-cp33-none-win_amd64.whl` that I have saved to the local drive. I am using:

```
pip 6.0.8 from C:\Python27\Lib\site-packages
python 2.7.9 (default, Dec 10 2014, 12:28:03) [MSC v.1500 64 bit (AMD64)]

```

When I run:

```
pip install scipy-0.15.1-cp33-none-win_amd64.whl

```

I get the following error:

> scipy-0.15.1-cp33-none-win\_amd64.whl is not a supported wheel on this platform

What is the problem?","`cp33` means `CPython 3.3`.

You need `scipy‑0.15.1‑cp27‑none‑win_amd64.whl` instead."
"I'm having trouble formatting a `datetime.timedelta` object.

Here's what I'm trying to do:
I have a list of objects and one of the members of the class of the object is a timedelta object that shows the duration of an event. I would like to display that duration in the format of hours:minutes.

I have tried a variety of methods for doing this and I'm having difficulty. My current approach is to add methods to the class for my objects that return hours and minutes. I can get the hours by dividing the timedelta.seconds by 3600 and rounding it. I'm having trouble with getting the remainder seconds and converting that to minutes.

By the way, I'm using Google AppEngine with Django Templates for presentation.","You can just convert the timedelta to a string with `str()`. Here's an example:

```
import datetime
start = datetime.datetime(2009,2,10,14,00)
end   = datetime.datetime(2009,2,10,16,00)
delta = end - start
print(str(delta))
# prints 2:00:00

```"
"I already read [How to get a function name as a string?](https://stackoverflow.com/questions/251464/).

How can I do the same for a variable? As opposed to functions, Python variables do not have the `__name__` attribute.

In other words, if I have a variable such as:

```
foo = dict()
foo['bar'] = 2

```

I am looking for a function/attribute, e.g. `retrieve_name`, where:

```
>>> retrieve_name(foo)
'foo'

```

This is in order to [create a DataFrame in Pandas from this list](http://pandas.pydata.org/pandas-docs/dev/dsintro.html#from-a-list-of-dicts), where **the column names** are given by the names of the actual dictionaries:

```
# List of dictionaries for my DataFrame
list_of_dicts = [n_jobs, users, queues, priorities]
columns = [retrieve_name(d) for d in list_of_dicts] 

```","With Python 3.8 one can simply use [f-string](https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals) debugging feature:

```
>>> foo = dict()
>>> f'{foo=}'.split('=')[0]
'foo' 

```

One drawback of this method is that in order to get `'foo'` printed you have to add `f'{foo=}'` yourself. In other words, you already have to know the name of the variable. In other words, the above code snippet is exactly the same as just

```
>>> 'foo'

```"
"Is there any other way to delete an item in a dictionary only if the given key exists, other than:

```
if key in mydict:
    del mydict[key]

```

The scenario is that I'm given a collection of keys to be removed from a given dictionary, but I am not certain if all of them exist in the dictionary. Just in case I miss a more efficient solution.","You can use  [`dict.pop`](http://docs.python.org/3/library/stdtypes.html#dict.pop):

```
 mydict.pop(""key"", None)

```

Note that if the second argument, i.e. `None` is not given, `KeyError` is raised if the key is not in the dictionary. Providing the second argument prevents the conditional exception."
"Is there a simple way to iterate over column name and value pairs?

My version of SQLAlchemy is 0.5.6

Here is the sample code where I tried using `dict(row)`:

```
import sqlalchemy
from sqlalchemy import *
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

print ""sqlalchemy version:"",sqlalchemy.__version__ 

engine = create_engine('sqlite:///:memory:', echo=False)
metadata = MetaData()
users_table = Table('users', metadata,
     Column('id', Integer, primary_key=True),
     Column('name', String),
)
metadata.create_all(engine) 

class User(declarative_base()):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    name = Column(String)
    
    def __init__(self, name):
        self.name = name

Session = sessionmaker(bind=engine)
session = Session()

user1 = User(""anurag"")
session.add(user1)
session.commit()

# uncommenting next line throws exception 'TypeError: 'User' object is not iterable'
#print dict(user1)
# this one also throws 'TypeError: 'User' object is not iterable'
for u in session.query(User).all():
    print dict(u)

```

Running this code on my system outputs:

```
Traceback (most recent call last):
  File ""untitled-1.py"", line 37, in <module>
    print dict(u)
TypeError: 'User' object is not iterable

```","You may access the internal `__dict__` of a SQLAlchemy object, like the following:

```
for u in session.query(User).all():
    print u.__dict__

```"
"I have some python code that splits on comma, but doesn't strip the whitespace:

```
>>> string = ""blah, lots  ,  of ,  spaces, here ""
>>> mylist = string.split(',')
>>> print mylist
['blah', ' lots  ', '  of ', '  spaces', ' here ']

```

I would rather end up with whitespace removed like this:

```
['blah', 'lots', 'of', 'spaces', 'here']

```

I am aware that I could loop through the list and strip() each item but, as this is Python, I'm guessing there's a quicker, easier and more elegant way of doing it.","Use list comprehension -- simpler, and just as easy to read as a `for` loop.

```
my_string = ""blah, lots  ,  of ,  spaces, here ""
result = [x.strip() for x in my_string.split(',')]
# result is [""blah"", ""lots"", ""of"", ""spaces"", ""here""]

```

**See:** [Python docs on List Comprehension](http://docs.python.org/tutorial/datastructures.html#list-comprehensions)  
[A good 2 second explanation of list comprehension.](https://stackoverflow.com/questions/501308/problem-in-understanding-python-list-comprehensions/501323#501323)"
"Python includes the [heapq](https://docs.python.org/3.8/library/heapq.html) module for [min-heaps](https://en.wikipedia.org/wiki/Binary_heap), but I need a [max-heap](https://en.wikipedia.org/wiki/Binary_heap). What should I use for a max-heap implementation in Python?","The easiest way is to invert the value of the keys and use heapq. For example, turn 1000.0 into -1000.0 and 5.0 into -5.0."
"For some reason, I can't use the `Tkinter` (or `tkinter`, on Python 3) module.
After running the following command in the python shell:

```
import Tkinter

```

or this, in Python 3:

```
import tkinter

```

I got this error

> ModuleNotFoundError: No module named 'Tkinter'

or this:

> ModuleNotFoundError: No module named 'tkinter'

What could be the reason for these errors and how can I solve it?","You probably need to install it using something similar to the following:

* For Ubuntu or other distros with Apt:

  ```
  sudo apt-get install python3-tk

  ```
* For Fedora:

  ```
  sudo dnf install python3-tkinter

  ```

You can also mention a Python version number like this:

* ```
  sudo apt-get install python3.7-tk

  ```
* ```
  sudo dnf install python3-tkinter-3.6.6-1.fc28.x86_64

  ```

Finally, import `tkinter` (for Python 3) or `Tkinter` (for Python 2), or choose at runtime based on the version number of the Python interpreter (for compatibility with both):

```
import sys
if sys.version_info[0] == 3:
    import tkinter as tk
else:
    import Tkinter as tk

```"
"I have installed [OpenCV](https://en.wikipedia.org/wiki/OpenCV) on the Occidentalis operating system (a variant of Raspbian) on a Raspberry Pi, using [this script](https://github.com/jayrambhia/Install-OpenCV/blob/master/Ubuntu/opencv_latest.sh) by jayrambhia. It installed version 2.4.5.

When I try `import cv2` in a Python program, I get the following message:

```
pi@raspberrypi~$ python cam.py
Traceback (most recent call last)
File ""cam.py"", line 1, in <module>
    import cv2
ImportError: No module named cv2

```

The file `cv2.so` is stored in `/usr/local/lib/python2.7/site-packages/...`

There are also folders in `/usr/local/lib` called `python3.2` and `python2.6`, in case that is relevant.

How can I fix the problem? Is this caused by a path misconfiguration?","First do run these commands inside Terminal/CMD:

```
conda update anaconda-navigator  
conda update navigator-updater  

```

Then the issue for the instruction below will be resolved

For windows if you have anaconda installed, you can simply do

```
pip install opencv-python

```

or

```
conda install -c https://conda.binstar.org/menpo opencv

```

if you are on linux you can do :

```
pip install opencv-python

```

or

```
conda install opencv 

```

[Link1](https://stackoverflow.com/questions/40872683/unable-to-install-cv2-on-windows#comment68960647_40872683) [Link2](https://stackoverflow.com/questions/19876079/opencv-cannot-find-module-cv2/41895783#comment69457795_35546806)

For python3.5+ check these links : [Link3](https://stackoverflow.com/a/42540015/2736559) , [Link4](https://stackoverflow.com/a/30686903/2736559)

Update:  
if you use anaconda, you may simply use this as well (and hence don't need to add menpo channel):

```
conda install -c conda-forge opencv

```"
How do I get a size of a pictures sides with PIL or any other Python library?,"```
from PIL import Image

im = Image.open('whatever.png')
width, height = im.size

```

According to the [documentation](https://web.archive.org/web/20201111195341/http://effbot.org/imagingbook/image.htm)."
"Matplotlib offers these functions:

```
cla()   # Clear axis
clf()   # Clear figure
close() # Close a figure window

```

When should I use each function and what exactly does it do?","They all do different things, since matplotlib uses a hierarchical order in which a figure window contains a figure which may consist of many axes. Additionally, there are functions from the pyplot interface and there are methods on the `Figure` class. I will discuss both cases below.

pyplot interface
----------------

`pyplot` is a module that collects a couple of functions that allow matplotlib to be used in a functional manner. I here assume that `pyplot` has been imported as `import matplotlib.pyplot as plt`.
In this case, there are three different commands that remove stuff:

See [`matplotlib.pyplot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html) Functions:

* **[`plt.cla()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.cla.html#matplotlib.pyplot.cla) clears an axis**, i.e. the currently active axis in the current figure. It leaves the other axes untouched.
* **[`plt.clf()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.clf.html#matplotlib.pyplot.clf) clears the entire current figure with all its axes**, but leaves the window opened, such that it may be reused for other plots.
* **[`plt.close()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.close.html#matplotlib.pyplot.close) closes a window**, which will be the current window, if not specified otherwise.

Which functions suits you best depends thus on your use-case.

The `close()` function furthermore allows one to specify which window should be closed. The argument can either be a number or name given to a window when it was created using `figure(number_or_name)` or it can be a figure instance `fig` obtained, i.e., using`fig = figure()`. If no argument is given to `close()`, the currently active window will be closed. Furthermore, there is the syntax `close('all')`, which closes all figures.

methods of the Figure class
---------------------------

Additionally, the `Figure` class provides methods for clearing figures.
I'll assume in the following that `fig` is an instance of a `Figure`:

[`fig.clf()` clears the entire figure](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.clf). This call is equivalent to `plt.clf()` only if `fig` is the current figure.

`fig.clear()` is a synonym for `fig.clf()`

Note that even `del fig` will not close the associated figure window. As far as I know the only way to close a figure window is using `plt.close(fig)` as described above."
"If you are creating a 1d array, you can implement it as a list, or else use the 'array' module in the standard library. I have always used lists for 1d arrays.

What is the reason or circumstance where I would want to use the array module instead?

Is it for performance and memory optimization, or am I missing something obvious?","Basically, Python lists are very flexible and can hold completely heterogeneous, arbitrary data, and they can be appended to very efficiently, in [amortized constant time](http://en.wikipedia.org/wiki/Dynamic_array#Geometric_expansion_and_amortized_cost). If you need to shrink and grow your list time-efficiently and without hassle, they are the way to go. But they use **a lot more space than C arrays**, in part because each item in the list requires the construction of an individual Python object, even for data that could be represented with simple C types (e.g. `float` or `uint64_t`).

The `array.array` type, on the other hand, is just a thin wrapper on C arrays. It can hold only homogeneous data (that is to say, all of the same type) and so it uses only `sizeof(one object) * length` bytes of memory. Mostly, you should use it when you need to expose a C array to an extension or a system call (for example, `ioctl` or `fctnl`).

`array.array` is also a reasonable way to represent a **mutable** string in Python 2.x (`array('B', bytes)`). However, Python 2.6+ and 3.x offer a mutable *byte* string as [`bytearray`](https://docs.python.org/3/library/stdtypes.html#bytearray).

However, if you want to do **math** on a homogeneous array of numeric data, then you're much better off using NumPy, which can automatically vectorize operations on complex multi-dimensional arrays.

**To make a long story short**: `array.array` is useful when you need a homogeneous C array of data for reasons *other than doing math*."
"Is there a way for a Python program to determine how much memory it's currently using? I've seen discussions about memory usage for a single object, but what I need is total memory usage for the process, so that I can determine when it's necessary to start discarding cached data.","[Here](http://fa.bianp.net/blog/2013/different-ways-to-get-memory-consumption-or-lessons-learned-from-memory_profiler/) is a useful solution that works for various operating systems, including Linux, Windows, etc.:

```
import psutil
process = psutil.Process()
print(process.memory_info().rss)  # in bytes 

```

Notes:

* do `pip install psutil` if it is not installed yet
* handy one-liner if you quickly want to know how many MiB your process takes:

  ```
  import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)

  ```
* with Python 2.7 and psutil 5.6.3, it was `process.memory_info()[0]` instead (there was a change in the API later)."
"Ok so I can use an OrderedDict in `json.dump`. That is, an OrderedDict can be used as an input to JSON.

But can it be used as an output? If so how? In my case I'd like to `load` into an OrderedDict so I can keep the order of the keys in the file.

If not, is there some kind of workaround?","Yes, you can. By specifying the `object_pairs_hook` argument to [JSONDecoder](http://docs.python.org/library/json.html#encoders-and-decoders). In fact, this is the exact example given in the documentation.

```
>>> json.JSONDecoder(object_pairs_hook=collections.OrderedDict).decode('{""foo"":1, ""bar"": 2}')
OrderedDict([('foo', 1), ('bar', 2)])
>>> 

```

You can pass this parameter to `json.loads` (if you don't need a Decoder instance for other purposes) like so:

```
>>> import json
>>> from collections import OrderedDict
>>> data = json.loads('{""foo"":1, ""bar"": 2}', object_pairs_hook=OrderedDict)
>>> print json.dumps(data, indent=4)
{
    ""foo"": 1,
    ""bar"": 2
}
>>> 

```

Using `json.load` is done in the same way:

```
>>> data = json.load(open('config.json'), object_pairs_hook=OrderedDict)

```"
What is the difference between NumPy's [`np.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array) and [`np.asarray`](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html#numpy.asarray)? When should I use one rather than the other? They seem to generate identical output.,"The [definition of `asarray`](https://github.com/numpy/numpy/blob/bcbed877f42ed6e9b01b2125134db4b6395f1d9d/numpy/core/numeric.py#L413-481) is:

```
def asarray(a, dtype=None, order=None):
    return array(a, dtype, copy=False, order=order)

```

So it is like `array`, except it has fewer options, and `copy=False`. `array` has `copy=True` by default.

The main difference is that `array` (by default) will make a copy of the object, while `asarray` will not unless necessary."
"When I try to use a `print` statement in Python, it gives me this error:

```
>>> print ""Hello, World!""
  File ""<stdin>"", line 1
    print ""Hello, World!""
                        ^
SyntaxError: Missing parentheses in call to 'print'

```

What does that mean?

---

See [Getting SyntaxError for print with keyword argument end=' '](https://stackoverflow.com/questions/2456148/) for the opposite problem.

See [Python 3 print without parenthesis](https://stackoverflow.com/questions/32122868) for workarounds, and confirmation that `print` cannot be *made to* work like a statement in Python 3.","The error message ***SyntaxError: Missing parentheses in call to 'print'*** occurs when you attempt to use Python 3 syntax with the Python 2 print statement.

**Example:**

~~```
print ""Hello, World!""

```~~

In Python 3, the print statement was replaced with a print() function, requiring parentheses around the value to be printed.

**Solution**

```
print(""Hello, World!"")

```

---

In Python 3, the print statement was replaced with a print() function, requiring parentheses around the value to be printed.

```
>>> print(""Hello, World!"")
Hello, World!

```

In earlier versions of Python 3, the interpreter just reports a generic syntax error, without providing any useful hints as to what might be going wrong:

```
>>> print ""Hello, World!""
  File ""<stdin>"", line 1
    print ""Hello, World!""
                        ^
SyntaxError: invalid syntax

```

As for *why* `print` became an ordinary function in Python 3, that didn't relate to the basic form of the statement, but rather to how you did more complicated things like printing multiple items to stderr with a trailing space rather than ending the line.

In Python 2:

```
>>> import sys
>>> print >> sys.stderr, 1, 2, 3,; print >> sys.stderr, 4, 5, 6
1 2 3 4 5 6

```

In Python 3:

```
>>> import sys
>>> print(1, 2, 3, file=sys.stderr, end="" ""); print(4, 5, 6, file=sys.stderr)
1 2 3 4 5 6

```

---

Starting with the Python 3.6.3 release in September 2017, some error messages related to the Python 2.x print syntax have been updated to recommend their Python 3.x counterparts:

```
>>> print ""Hello!""
  File ""<stdin>"", line 1
    print ""Hello!""
                 ^
SyntaxError: Missing parentheses in call to 'print'. Did you mean print(""Hello!"")?

```

Since the ""Missing parentheses in call to print"" case is a compile time syntax error and hence has access to the raw source code, it's able to include the full text on the rest of the line in the suggested replacement. However, it doesn't currently try to work out the appropriate quotes to place around that expression (that's not impossible, just sufficiently complicated that it hasn't been done).

The `TypeError` raised for the right shift operator has also been customised:

```
>>> print >> sys.stderr
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for >>: 'builtin_function_or_method' and '_io.TextIOWrapper'. Did you mean ""print(<message>, file=<output_stream>)""?

```

Since this error is raised when the code runs, rather than when it is compiled, it doesn't have access to the raw source code, and hence uses meta-variables (`<message>` and `<output_stream>`) in the suggested replacement expression instead of whatever the user actually typed. Unlike the syntax error case, it's straightforward to place quotes around the Python expression in the custom right shift error message."
"I'm sure this is a trivial operation, but I can't figure out how it's done.

There's got to be something smarter than this:

```
ids = [1, 3, 6, 7, 9]

for id in ids:
    MyModel.objects.filter(pk=id)

```

I'm looking to get them all in one query with something like:

```
MyModel.objects.filter(pk=[1, 3, 6, 7, 9])

```

How can I filter a Django query with a list of values?","From the [Django documentation](https://docs.djangoproject.com/en/stable/topics/db/queries/#the-pk-lookup-shortcut):

```
Blog.objects.filter(pk__in=[1, 4, 7])

```"
"I have some code like:

```
class Pump:    
    def __init__(self):
        print(""init"")

    def getPumps(self):
        pass

p = Pump.getPumps()
print(p)

```

But I get an error like:

```
Traceback (most recent call last):
  File ""C:\Users\Dom\Desktop\test\test.py"", line 7, in <module>
    p = Pump.getPumps()
TypeError: getPumps() missing 1 required positional argument: 'self'

```

Why doesn't `__init__` seem to be called, and what does this exception mean? My understanding is that `self` is passed to the constructor and methods automatically. What am I doing wrong here?

---

See [Why do I get 'takes exactly 1 argument (2 given)' when trying to call a method?](https://stackoverflow.com/questions/4909585/) for the opposite problem.","To use the class, first create an instance, like so:

```
p = Pump()
p.getPumps()

```

A full example:

```
>>> class TestClass:
...     def __init__(self):
...         print(""init"")
...     def testFunc(self):
...         print(""Test Func"")
... 
>>> testInstance = TestClass()
init
>>> testInstance.testFunc()
Test Func

```"
"Is there an easier way to break out of nested loops than throwing an exception? (In [Perl](https://en.wikipedia.org/wiki/Perl), you can give labels to each loop and at least continue an outer loop.)

```
for x in range(10):
    for y in range(10):
        print x*y
        if x*y > 50:
            ""break both loops""

```

I.e., is there a nicer way than:

```
class BreakIt(Exception): pass

try:
    for x in range(10):
        for y in range(10):
            print x*y
            if x*y > 50:
                raise BreakIt
except BreakIt:
    pass

```","```
for x in range(10):
    for y in range(10):
        print(x * y)
        if x * y > 50:
            break
    else:
        continue  # only executed if the inner loop did NOT break
    break  # only executed if the inner loop DID break

```

The same works for deeper loops:

```
for x in range(10):
    for y in range(10):
        for z in range(10):
            print(x, y, z)
            if (x * y * z) == 30:
                break
        else:
            continue
        break
    else:
        continue
    break

```"
"I have a string that I want to use as a filename, so I want to remove all characters that wouldn't be allowed in filenames, using Python.

I'd rather be strict than otherwise, so let's say I want to retain only letters, digits, and a small set of other characters like `""_-.() ""`. What's the most elegant solution?

The filename needs to be valid on multiple operating systems (Windows, Linux and Mac OS) - it's an MP3 file in my library with the song title as the filename, and is shared and backed up between 3 machines.","You can look at the [Django framework](http://www.djangoproject.com) (but take their licence into account!) for how they create a ""slug"" from arbitrary text. A slug is URL- and filename- friendly.

The Django text utils define a function, [`slugify()`](https://docs.djangoproject.com/en/4.0/ref/utils/#django.utils.text.slugify), that's probably the gold standard for this kind of thing. Essentially, their code is the following.

```
import unicodedata
import re

def slugify(value, allow_unicode=False):
    """"""
    Taken from https://github.com/django/django/blob/master/django/utils/text.py
    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated
    dashes to single dashes. Remove characters that aren't alphanumerics,
    underscores, or hyphens. Convert to lowercase. Also strip leading and
    trailing whitespace, dashes, and underscores.
    """"""
    value = str(value)
    if allow_unicode:
        value = unicodedata.normalize('NFKC', value)
    else:
        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')
    value = re.sub(r'[^\w\s-]', '', value.lower())
    return re.sub(r'[-\s]+', '-', value).strip('-_')

```

And the older version:

```
def slugify(value):
    """"""
    Normalizes string, converts to lowercase, removes non-alpha characters,
    and converts spaces to hyphens.
    """"""
    import unicodedata
    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore')
    value = unicode(re.sub('[^\w\s-]', '', value).strip().lower())
    value = unicode(re.sub('[-\s]+', '-', value))
    # ...
    return value

```

There's more, but I left it out, since it doesn't address slugification, but escaping."
"I have this error:

```
Traceback (most recent call last):
  File ""python_md5_cracker.py"", line 27, in <module>
  m.update(line)
TypeError: Unicode-objects must be encoded before hashing

```

when I try to execute this code in **Python 3.2.2**:

```
import hashlib, sys
m = hashlib.md5()
hash = """"
hash_file = input(""What is the file name in which the hash resides?  "")
wordlist = input(""What is your wordlist?  (Enter the file name)  "")
try:
  hashdocument = open(hash_file, ""r"")
except IOError:
  print(""Invalid file."")
  raw_input()
  sys.exit()
else:
  hash = hashdocument.readline()
  hash = hash.replace(""\n"", """")

try:
  wordlistfile = open(wordlist, ""r"")
except IOError:
  print(""Invalid file."")
  raw_input()
  sys.exit()
else:
  pass
for line in wordlistfile:
  # Flush the buffer (this caused a massive problem when placed 
  # at the beginning of the script, because the buffer kept getting
  # overwritten, thus comparing incorrect hashes)
  m = hashlib.md5()
  line = line.replace(""\n"", """")
  m.update(line)
  word_hash = m.hexdigest()
  if word_hash == hash:
    print(""Collision! The word corresponding to the given hash is"", line)
    input()
    sys.exit()

print(""The hash given does not correspond to any supplied word in the wordlist."")
input()
sys.exit()

```","It is probably looking for a character encoding from `wordlistfile`.

```
wordlistfile = open(wordlist,""r"",encoding='utf-8')

```

Or, if you're working on a line-by-line basis:

```
line.encode('utf-8')

```

---

EDIT
----

Per the comment below and [this answer](https://stackoverflow.com/a/22644618/57191).

My answer above assumes that the desired output is a `str` from the `wordlist` file. If you are comfortable in working in `bytes`, then you're better off using `open(wordlist, ""rb"")`. But it is important to remember that your `hashfile` should *NOT* use `rb` if you are comparing it to the output of `hexdigest`. `hashlib.md5(value).hashdigest()` outputs a `str` and that cannot be directly compared with a bytes object: `'abc' != b'abc'`. (There's a lot more to this topic, but I don't have the time ATM).

It should also be noted that this line:

```
line.replace(""\n"", """")

```

Should probably be

```
line.strip()

```

That will work for both bytes and str's. But if you decide to simply convert to `bytes`, then you can change the line to:

```
line.replace(b""\n"", b"""")

```"
"Dictionaries are insertion ordered as of Python 3.6. It is described as a CPython implementation detail rather than a language feature. The [documentation](https://docs.python.org/3.6/whatsnew/3.6.html#new-dict-implementation) states:

> `dict()` now uses a “compact” representation [pioneered by PyPy](https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html). The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. [PEP 468](https://www.python.org/dev/peps/pep-0468) (Preserving the order of \*\*kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in [issue 27350](https://bugs.python.org/issue27350). Idea [originally suggested by Raymond Hettinger](https://mail.python.org/pipermail/python-dev/2012-December/123028.html).)

How does the new dictionary implementation perform better than the older one while preserving element order?

---

Update December 2017: `dict`s retaining insertion order is [guaranteed](https://mail.python.org/pipermail/python-dev/2017-December/151283.html) for Python 3.7","> **Are dictionaries ordered in Python 3.6+?**

They are **insertion ordered****[1]**.

**As of Python 3.6**, for the CPython implementation of Python, dictionaries *remember the order of items inserted*. *This is considered an implementation detail in Python 3.6*; you need to use `OrderedDict` if you want insertion ordering that's *guaranteed* across other implementations of Python (and other ordered behavior**[1]**).

**As of Python 3.7**, this is a guaranteed language feature, not merely an implementation detail. [From a python-dev message by GvR](https://mail.python.org/pipermail/python-dev/2017-December/151283.html):

> Make it so. ""Dict keeps insertion order"" is the ruling. Thanks!

This simply means that *you can depend on it*. Other implementations of Python must also offer an insertion ordered dictionary if they wish to be a conforming implementation of Python 3.7.

---

> **How does the Python `3.6` dictionary implementation perform better[2] than the older one while preserving element order?**

Essentially, by *keeping two arrays*.

* The first array, [`dk_entries`](https://github.com/python/cpython/blob/474ef63e38726d4bcde14f6104984a742c6cb747/Objects/dictobject.c#L551), holds the entries ([of type  `PyDictKeyEntry`](https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Objects/dict-common.h#L4)) for the dictionary in the order that they were inserted. Preserving order is achieved by this being an append only array where new items are always inserted at the end (insertion order).
* The second, [`dk_indices`](https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Objects/dict-common.h#L70), holds the indices for the `dk_entries` array (that is, values that indicate the position of the corresponding entry in `dk_entries`). This array acts as the hash table. When a key is hashed it leads to one of the indices stored in `dk_indices` and the corresponding entry is fetched by indexing `dk_entries`. Since only indices are kept, the type of this array depends on the overall size of the dictionary (ranging from type [`int8_t`](https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Objects/dict-common.h#L64)(`1` byte) to [`int32_t`](https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Objects/dict-common.h#L66)/[`int64_t`](https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Objects/dict-common.h#L68) (`4`/`8` bytes) on `32`/`64` bit builds)

In the previous implementation, a sparse array of type `PyDictKeyEntry` and size `dk_size` had to be allocated; unfortunately, it also resulted in a lot of empty space since that array was not allowed to be more than `2/3 * dk_size` full [for performance reasons](https://github.com/python/cpython/blob/474ef63e38726d4bcde14f6104984a742c6cb747/Objects/dictobject.c#L375). (and the empty space *still* had `PyDictKeyEntry` size!).

This is not the case now since only the *required* entries are stored (those that have been inserted) and a sparse array of type `intX_t` (`X` depending on dict size) `2/3 * dk_size`s full is kept. The empty space changed from type `PyDictKeyEntry` to `intX_t`.

So, obviously, creating a sparse array of type `PyDictKeyEntry` is much more memory demanding than a sparse array for storing `int`s.

You can see the full conversation [on Python-Dev](https://mail.python.org/pipermail/python-dev/2016-September/146327.html) regarding this feature if interested, it is a good read.

---

[In the original proposal made by Raymond Hettinger](https://mail.python.org/pipermail/python-dev/2012-December/123028.html), a visualization of the data structures used can be seen which captures the gist of the idea.

> For example, the dictionary:
>
> ```
> d = {'timmy': 'red', 'barry': 'green', 'guido': 'blue'}
>
> ```
>
> is currently stored as [keyhash, key, value]:
>
> ```
> entries = [['--', '--', '--'],
>            [-8522787127447073495, 'barry', 'green'],
>            ['--', '--', '--'],
>            ['--', '--', '--'],
>            ['--', '--', '--'],
>            [-9092791511155847987, 'timmy', 'red'],
>            ['--', '--', '--'],
>            [-6480567542315338377, 'guido', 'blue']]
>
> ```
>
> Instead, the data should be organized as follows:
>
> ```
> indices =  [None, 1, None, None, None, 0, None, 2]
> entries =  [[-9092791511155847987, 'timmy', 'red'],
>             [-8522787127447073495, 'barry', 'green'],
>             [-6480567542315338377, 'guido', 'blue']]
>
> ```

As you can visually now see, in the original proposal, a lot of space is essentially empty to reduce collisions and make look-ups faster. With the new approach, you reduce the memory required by moving the sparseness where it's really required, in the indices.

---

[1]: I say ""insertion ordered"" and not ""ordered"" since, with the existence of OrderedDict, ""ordered"" suggests further behavior that the `dict` object \*doesn't provide\*. OrderedDicts are reversible, provide order sensitive methods and, mainly, provide an order-sensive equality tests (`==`, `!=`). `dict`s currently don't offer any of those behaviors/methods.


---

[2]: The new dictionary implementations performs better \*\*memory wise\*\* by being designed more compactly; that's the main benefit here. Speed wise, the difference isn't so drastic, there's places where the new dict might introduce slight regressions ([key-lookups, for example](https://github.com/python/cpython/blob/474ef63e38726d4bcde14f6104984a742c6cb747/Objects/dictobject.c#L375)) while in others (iteration and resizing come to mind) a performance boost should be present.
Overall, the performance of the dictionary, especially in real-life situations, improves due to the compactness introduced."
"```
a = [1,2,3,4,5]
b = [1,3,5,6]
c = a and b
print c

```

actual output: `[1,3,5,6]`
expected output: `[1,3,5]`

How can we achieve a boolean AND operation (list intersection) on two lists?","If order is not important and you don't need to worry about duplicates then you can use set intersection:

```
>>> a = [1,2,3,4,5]
>>> b = [1,3,5,6]
>>> list(set(a) & set(b))
[1, 3, 5]

```"
"I have a Python function which takes several arguments. Some of these arguments could be omitted in some scenarios.

```
def some_function (self, a, b, c, d = None, e = None, f = None, g = None, h = None):
    #code

```

The arguments `d` through `h` are strings which each have different meanings. It is important that I can choose which optional parameters to pass in any combination. For example, `(a, b, C, d, e)`, or `(a, b, C, g, h)`, or `(a, b, C, d, e, f`, or all of them (these are my choices).

It would be great if I could overload the function - but I read that Python does not support overloading. I tried to insert some of the required int arguments in the list - and got an argument mismatch error.

Right now I am sending empty strings in place of the first few missing arguments as placeholders. I would like to be able to call a function just using actual values.

Is there any way to do this? Could I pass a list instead of the argument list?

Right now the prototype using ctypes looks something like:

```
_fdll.some_function.argtypes = [c_void_p, c_char_p, c_int, c_char_p, c_char_p, c_char_p, c_char_p, c_char_p]

```","Just use the `*args` parameter, which allows you to pass as many arguments as you want after your `a,b,c`. You would have to add some logic to map `args`->`c,d,e,f` but its a ""way"" of overloading.

```
def myfunc(a,b, *args, **kwargs):
   for ar in args:
      print ar
myfunc(a,b,c,d,e,f)

```

And it will print values of `c,d,e,f`

---

Similarly you could use the `kwargs` argument and then you could name your parameters.

```
def myfunc(a,b, *args, **kwargs):
      c = kwargs.get('c', None)
      d = kwargs.get('d', None)
      #etc
myfunc(a,b, c='nick', d='dog', ...)

```

And then `kwargs` would have a dictionary of all the parameters that are key valued after `a,b`"
"Given a list of numbers such as:

```
[1, 2, 3, 4, 5, ...]

```

How do I calculate their total sum:

```
1 + 2 + 3 + 4 + 5 + ...

```

How do I calculate their pairwise averages:

```
[(1+2)/2, (2+3)/2, (3+4)/2, (4+5)/2, ...]

```","##### Question 1:

To sum a list of numbers, use [`sum`](https://docs.python.org/3/library/functions.html#sum):

```
xs = [1, 2, 3, 4, 5]
print(sum(xs))

```

This outputs:

```
15

```

---

##### Question 2:

So you want (element 0 + element 1) / 2, (element 1 + element 2) / 2, ... etc.

We make two lists: one of every element except the first, and one of every element except the last. Then the averages we want are the averages of each pair taken from the two lists. We use `zip` to take pairs from two lists.

I assume you want to see decimals in the result, even though your input values are integers. By default, Python does integer division: it discards the remainder. To divide things through all the way, we need to use floating-point numbers. Fortunately, dividing an int by a float will produce a float, so we just use `2.0` for our divisor instead of `2`.

Thus:

```
averages = [(x + y) / 2.0 for (x, y) in zip(my_list[:-1], my_list[1:])]

```"
"In Python, what is the difference between expressions and statements?","[Expressions](http://docs.python.org/reference/expressions.html) only contain [identifiers](http://docs.python.org/release/2.5.2/ref/identifiers.html), [literals](http://docs.python.org/release/2.5.2/ref/literals.html) and [operators](http://docs.python.org/release/2.5.2/ref/operators.html), where operators include arithmetic and boolean operators, the function [call operator](https://docs.python.org/3/reference/expressions.html?highlight=subscriptions#calls) `()` the [subscription operator](https://docs.python.org/3/reference/expressions.html?highlight=subscriptions#grammar-token-subscription) `[]` and similar, and can be reduced to some kind of ""value"", which can be any Python object. Examples:

```
3 + 5
map(lambda x: x*x, range(10))
[a.x for a in some_iterable]
yield 7

```

Statements (see [1](https://docs.python.org/3/reference/expressions.html?highlight=subscriptions#calls), [2](https://docs.python.org/3/reference/expressions.html?highlight=subscriptions#grammar-token-subscription)), on the other hand, are everything that can make up a line (or several lines) of Python code. Note that expressions are statements as well. Examples:

```
# all the above expressions
print 42
if x: do_y()
return
a = 7

```"
"Can I delete items from a dictionary in Python while iterating over it?

I want to remove elements that don't meet a certain condition from the dictionary, instead of creating an entirely new dictionary. Is the following a good solution, or are there better ways?

```
for k, v in mydict.items():
    if k == val:
        del mydict[k]

```","For **Python 3+**:

```
>>> mydict
{'four': 4, 'three': 3, 'one': 1}

>>> for k in list(mydict.keys()):
...     if mydict[k] == 3:
...         del mydict[k]

>>> mydict
{'four': 4, 'one': 1}

```

The other answers work fine with **Python 2** but raise a `RuntimeError` for **Python 3**:

> RuntimeError: dictionary changed size during iteration.

This happens because `mydict.keys()` returns an iterator not a list.
As pointed out in comments simply convert `mydict.keys()` to a list by `list(mydict.keys())` and it should work.

---

For **Python 2**:

A simple test in the console shows you cannot modify a dictionary while iterating over it:

```
>>> mydict = {'one': 1, 'two': 2, 'three': 3, 'four': 4}

>>> for k, v in mydict.iteritems():
...    if k == 'two':
...        del mydict[k]

------------------------------------------------------------
Traceback (most recent call last):
  File ""<ipython console>"", line 1, in <module>
RuntimeError: dictionary changed size during iteration

```

As stated in delnan's answer, deleting entries causes problems when the iterator tries to move onto the next entry. Instead, use the `keys()` method to get a list of the keys and work with that:

```
>>> for k in mydict.keys():
...    if k == 'two':
...        del mydict[k]

>>> mydict
{'four': 4, 'three': 3, 'one': 1}

```

If you need to delete based on the items value, use the `items()` method instead:

```
>>> for k, v in mydict.items():
...     if v == 3:
...         del mydict[k]

>>> mydict
{'four': 4, 'one': 1}

```"
"I'd like to add comments for a few packages in a pip requirements file. (Just to explain why that package is on the list.) Can I do this?

I'm imagining something like

```
Babel==0.9.5 # translation
CherryPy==3.2.0 # web server
Creoleparser==0.7.1 # wiki formatting
Genshi==0.5.1 # templating

```","Sure, you can, just use `#`

[`pip` docs](https://pip.pypa.io/en/latest/reference/requirements-file-format/):

> A line that begins with `#` is treated as a comment and ignored. Whitespace followed by a `#` causes the `#` and the remainder of the line to be treated as a comment."
"I need to add leading zeros to integer to make a string with defined quantity of digits ($cnt).
What the best way to translate this simple function from PHP to Python:

```
function add_nulls($int, $cnt=2) {
    $int = intval($int);
    for($i=0; $i<($cnt-strlen($int)); $i++)
        $nulls .= '0';
    return $nulls.$int;
}

```

Is there a function that can do this?","You can use the `zfill()` method to pad a string with zeros:

```
In [3]: str(1).zfill(2)
Out[3]: '01'

```"
"I would like to put an `int` into a `string`. This is what I am doing at the moment:

```
num = 40
plot.savefig('hanning40.pdf') #problem line

```

I have to run the program for several different numbers, so I'd like to do a loop. But inserting the variable like this doesn't work:

```
plot.savefig('hanning', num, '.pdf')

```

How do I insert a variable into a Python string?

---

### See also

If you are trying to *create a file path*, see [How can I create a full path to a file from parts (e.g. path to the folder, name and extension)?](https://stackoverflow.com/questions/7132861) for additional techniques. It will usually be better to use code that is specific to creating paths.

If you are trying to *construct an URL with variable data*, **do not** use ordinary string formatting, because it is error-prone and more difficult than necessary. Specialized tools are available. See [Add params to given URL in Python](https://stackoverflow.com/questions/2506379/).

If you are trying to *construct a SQL query*, **do not** use ordinary string formatting, because it is a **major security risk**. This is the cause of ""SQL injection"" which [costs real companies huge amounts of money every year](https://duckduckgo.com/?q=sql+injection+annual+financial+impact). See for example [How to use variables in SQL statement in Python?](https://stackoverflow.com/questions/902408) for proper techniques.

If you *just want to `print` (output) the string*, you can prepare it this way first, **or** if you don't need the string for anything else, print each piece of the output individually using a single call to `print`. See [How can I print multiple things (fixed text and/or variable values) on the same line, all at once?](https://stackoverflow.com/questions/15286401) for details on both approaches.

See [How can I concatenate str and int objects?](https://stackoverflow.com/questions/25675943) for bugs caused by trying to use `+` to join ""strings"" when one of them isn't a string.","Using [f-strings](https://docs.python.org/3/reference/lexical_analysis.html#f-strings):

```
plot.savefig(f'hanning{num}.pdf')

```

This was added in 3.6 and is the new preferred way.

---

Using [`str.format()`](https://docs.python.org/3/library/stdtypes.html#str.format):

```
plot.savefig('hanning{0}.pdf'.format(num))

```

---

String concatenation:

```
plot.savefig('hanning' + str(num) + '.pdf')

```

---

[Conversion Specifier](https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting):

```
plot.savefig('hanning%s.pdf' % num)

```

---

Using local variable names (neat trick):

```
plot.savefig('hanning%(num)s.pdf' % locals())

```

---

Using [`string.Template`](https://docs.python.org/3/library/string.html#string.Template):

```
plot.savefig(string.Template('hanning${num}.pdf').substitute(locals()))

```

---

See also:

* [Fancier Output Formatting - The Python Tutorial](https://docs.python.org/3/tutorial/inputoutput.html#fancier-output-formatting)
* [Python 3's f-Strings: An Improved String Formatting Syntax (Guide) - RealPython](https://realpython.com/python-f-strings/)"
"With [linq](/questions/tagged/linq ""show questions tagged 'linq'"") I would

```
var top5 = array.Take(5);

```

How to do this with Python?","Slicing a list
==============

```
top5 = array[:5]

```

* To slice a list, there's a simple syntax: `array[start:stop:step]`
* You can omit any parameter. These are all valid: `array[start:]`, `array[:stop]`, `array[::step]`

Slicing a generator
===================

```
import itertools
top5 = itertools.islice(my_list, 5) # grab the first five elements

```

* You can't slice a generator directly in Python. [`itertools.islice()`](http://docs.python.org/library/itertools.html#itertools.islice) will wrap an object in a new slicing generator using the syntax `itertools.islice(generator, start, stop, step)`
* Remember, slicing a generator will exhaust it partially. If you want to keep the entire generator intact, perhaps turn it into a tuple or list first, like: `result = tuple(generator)`"
"I would like to get the current time in Python and assign them into variables like `year`, `month`, `day`, `hour`, `minute`. How can this be done in Python 2.7?","The [`datetime`](https://docs.python.org/2/library/datetime.html#datetime.datetime.now) module is your friend:

```
import datetime
now = datetime.datetime.now()
print(now.year, now.month, now.day, now.hour, now.minute, now.second)
# 2015 5 6 8 53 40

```

You don't need separate variables, the attributes on the returned `datetime` object have all you need."
"```
try:
    r = requests.get(url, params={'s': thing})
except requests.ConnectionError, e:
    print(e)

```

Is this correct? Is there a better way to structure this? Will this cover all my bases?","Have a look at the Requests [exception docs](https://requests.readthedocs.io/en/latest/user/quickstart.html#errors-and-exceptions). In short:

> In the event of a network problem (e.g. DNS failure, refused connection, etc), Requests will raise a **`ConnectionError`** exception.
>
> In the event of the rare invalid HTTP response, Requests will raise an **`HTTPError`** exception.
>
> If a request times out, a **`Timeout`** exception is raised.
>
> If a request exceeds the configured number of maximum redirections, a **`TooManyRedirects`** exception is raised.
>
> All exceptions that Requests explicitly raises inherit from **`requests.exceptions.RequestException`**.

To answer your question, what you show will *not* cover all of your bases. You'll only catch connection-related errors, not ones that time out.

What to do when you catch the exception is really up to the design of your script/program. Is it acceptable to exit? Can you go on and try again? If the error is catastrophic and you can't go on, then yes, you may abort your program by raising [SystemExit](https://docs.python.org/3/library/exceptions.html#SystemExit) (a nice way to both print an error and call `sys.exit`).

You can either catch the base-class exception, which will handle all cases:

```
try:
    r = requests.get(url, params={'s': thing})
except requests.exceptions.RequestException as e:  # This is the correct syntax
    raise SystemExit(e)

```

Or you can catch them separately and do different things.

```
try:
    r = requests.get(url, params={'s': thing})
except requests.exceptions.Timeout:
    # Maybe set up for a retry, or continue in a retry loop
except requests.exceptions.TooManyRedirects:
    # Tell the user their URL was bad and try a different one
except requests.exceptions.RequestException as e:
    # catastrophic error. bail.
    raise SystemExit(e)

```

---

As [Christian](https://stackoverflow.com/users/456550/christian-long) pointed out:

> If you want http errors (e.g. 401 Unauthorized) to raise exceptions, you can call [`Response.raise_for_status`](https://requests.readthedocs.io/en/latest/api/#requests.Response.raise_for_status). That will raise an `HTTPError`, if the response was an http error.

An example:

```
try:
    r = requests.get('http://www.google.com/nothere')
    r.raise_for_status()
except requests.exceptions.HTTPError as err:
    raise SystemExit(err)

```

Will print:

```
404 Client Error: Not Found for url: http://www.google.com/nothere

```"
"I'm calling a bunch of methods that return a list. The list may be empty. If the list is non-empty, I want to return the first item; otherwise, I want to return `None`. This code works:

```
def main():
    my_list = get_list()
    if len(my_list) > 0:
        return my_list[0]
    return None

```

but it seems to me that there should be a simple one-line idiom for doing this. Is there?","Python 2.6+
-----------

```
next(iter(your_list), None)

```

If `your_list` can be `None`:

```
next(iter(your_list or []), None)

```

Python 2.4
----------

```
def get_first(iterable, default=None):
    if iterable:
        for item in iterable:
            return item
    return default

```

Example:

```
x = get_first(get_first_list())
if x:
    ...
y = get_first(get_second_list())
if y:
    ...

```

Another option is to inline the above function:

```
for x in get_first_list() or []:
    # process x
    break # process at most one item
for y in get_second_list() or []:
    # process y
    break

```

To avoid `break` you could write:

```
for x in yield_first(get_first_list()):
    x # process x
for y in yield_first(get_second_list()):
    y # process y

```

Where:

```
def yield_first(iterable):
    for item in iterable or []:
        yield item
        return

```"
"Given the following 2D array:

```
a = np.array([
    [1, 2, 3],
    [2, 3, 4],
])

```

I want to add a column of zeros along the second axis to get:

```
b = np.array([
    [1, 2, 3, 0],
    [2, 3, 4, 0],
])

```","`np.r_[...]` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.r_.html)) and `np.c_[...]` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.c_.html))
are useful alternatives to `np.vstack` and `np.hstack`.
Note that they use square brackets [] instead of parentheses ().

Some examples:

```
: import numpy as np
: N = 3
: A = np.eye(N)

: np.c_[ A, np.ones(N) ]              # add a column
array([[ 1.,  0.,  0.,  1.],
       [ 0.,  1.,  0.,  1.],
       [ 0.,  0.,  1.,  1.]])

: np.c_[ np.ones(N), A, np.ones(N) ]  # or two
array([[ 1.,  1.,  0.,  0.,  1.],
       [ 1.,  0.,  1.,  0.,  1.],
       [ 1.,  0.,  0.,  1.,  1.]])

: np.r_[ A, [A[1]] ]              # add a row
array([[ 1.,  0.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  1.,  0.]])
: # not np.r_[ A, A[1] ]

: np.r_[ A[0], 1, 2, 3, A[1] ]    # mix vecs and scalars
  array([ 1.,  0.,  0.,  1.,  2.,  3.,  0.,  1.,  0.])

: np.r_[ A[0], [1, 2, 3], A[1] ]  # lists
  array([ 1.,  0.,  0.,  1.,  2.,  3.,  0.,  1.,  0.])

: np.r_[ A[0], (1, 2, 3), A[1] ]  # tuples
  array([ 1.,  0.,  0.,  1.,  2.,  3.,  0.,  1.,  0.])

: np.r_[ A[0], 1:4, A[1] ]        # same, 1:4 == arange(1,4) == 1,2,3
  array([ 1.,  0.,  0.,  1.,  2.,  3.,  0.,  1.,  0.])

```

The reason for square brackets [] instead of round ()
is that Python converts `1:4` to slice objects in square brackets."
"I am creating a program that will download a .jar (java) file from a web server, by reading the URL that is specified in the .jad file of the same game/application. I'm using Python 3.2.1

I've managed to extract the URL of the JAR file from the JAD file (every JAD file contains the URL to the JAR file), but as you may imagine, the extracted value is type() string.

Here's the relevant function:

```
def downloadFile(URL=None):
    import httplib2
    h = httplib2.Http("".cache"")
    resp, content = h.request(URL, ""GET"")
    return content

downloadFile(URL_from_file)

```

However I always get an error saying that the type in the function above has to be bytes, and not string. I've tried using the URL.encode('utf-8'), and also bytes(URL,encoding='utf-8'), but I'd always get the same or similar error.

So basically my question is how to download a file from a server when the URL is stored in a string type?","If you want to obtain the contents of a web page into a variable, just `read` the response of [**`urllib.request.urlopen`**](http://docs.python.org/dev/library/urllib.request.html#urllib.request.urlopen):

```
import urllib.request
...
url = 'http://example.com/'
response = urllib.request.urlopen(url)
data = response.read()      # a `bytes` object
text = data.decode('utf-8') # a `str`; this step can't be used if data is binary

```

---

The easiest way to download and save a file is to use the [**`urllib.request.urlretrieve`**](http://docs.python.org/dev/library/urllib.request.html#urllib.request.urlretrieve) function:

```
import urllib.request
...
# Download the file from `url` and save it locally under `file_name`:
urllib.request.urlretrieve(url, file_name)

```

```
import urllib.request
...
# Download the file from `url`, save it in a temporary directory and get the
# path to it (e.g. '/tmp/tmpb48zma.txt') in the `file_name` variable:
file_name, headers = urllib.request.urlretrieve(url)

```

But keep in mind that `urlretrieve` is considered [legacy](http://docs.python.org/dev/library/urllib.request.html#legacy-interface) and might become deprecated (not sure why, though).

So the most *correct* way to do this would be to use the [**`urllib.request.urlopen`**](http://docs.python.org/dev/library/urllib.request.html#urllib.request.urlopen) function to return a file-like object that represents an HTTP response and copy it to a real file using [**`shutil.copyfileobj`**](http://docs.python.org/dev/library/shutil.html#shutil.copyfileobj).

```
import urllib.request
import shutil
...
# Download the file from `url` and save it locally under `file_name`:
with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:
    shutil.copyfileobj(response, out_file)

```

If this seems too complicated, you may want to go simpler and store the whole download in a `bytes` object and then write it to a file. But this works well only for small files.

```
import urllib.request
...
# Download the file from `url` and save it locally under `file_name`:
with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:
    data = response.read() # a `bytes` object
    out_file.write(data)

```

---

It is possible to extract `.gz` (and maybe other formats) compressed data on the fly, but such an operation probably requires the HTTP server to support random access to the file.

```
import urllib.request
import gzip
...
# Read the first 64 bytes of the file inside the .gz archive located at `url`
url = 'http://example.com/something.gz'
with urllib.request.urlopen(url) as response:
    with gzip.GzipFile(fileobj=response) as uncompressed:
        file_header = uncompressed.read(64) # a `bytes` object
        # Or do anything shown above using `uncompressed` instead of `response`.

```"
"Consider:

```
$ cat bla.py 
u = unicode('dâ€¦')
s = u.encode('utf-8')
print s
$ python bla.py 
  File ""bla.py"", line 1
SyntaxError: Non-ASCII character '\xe2' in file bla.py on line 1, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

```

How can I declare UTF-8 strings in source code?","In Python 3, UTF-8 is the default source encoding (see [PEP 3120](https://www.python.org/dev/peps/pep-3120/)), so Unicode characters can be used anywhere.

In Python 2, you can declare in the source code header:

```
# -*- coding: utf-8 -*-
....

```

This is described in [PEP 0263](http://www.python.org/dev/peps/pep-0263/).

Then you can use UTF-8 in strings:

```
# -*- coding: utf-8 -*-

u = 'idzie wąż wąską dróżką'
uu = u.decode('utf8')
s = uu.encode('cp1250')
print(s)

```"
"What is the simplest way to compare two NumPy arrays for equality (where equality is defined as: A = B iff for all indices i: `A[i]Â == B[i]`)?

Simply using `==` gives me a boolean array:

```
 >>> numpy.array([1,1,1]) == numpy.array([1,1,1])

array([ True,  True,  True], dtype=bool)

```

Do I have to `and` the elements of this array to determine if the arrays are equal, or is there a simpler way to compare?","```
(A==B).all()

```

test if all values of array (A==B) are True.

Note: maybe you also want to test A and B shape, such as `A.shape == B.shape`

**Special cases and alternatives** (from dbaupp's answer and yoavram's comment)

It should be noted that:

* this solution can have a strange behavior in a particular case: if either `A` or `B` is empty and the other one contains a single element, then it return `True`. For some reason, the comparison `A==B` returns an empty array, for which the `all` operator returns `True`.
* Another risk is if `A` and `B` don't have the same shape and aren't broadcastable, then this approach will raise an error.

In conclusion, if you have a doubt about `A` and `B` shape or simply want to be safe: use one of the specialized functions:

```
np.array_equal(A,B)  # test if same shape, same elements values
np.array_equiv(A,B)  # test if broadcastable shape, same elements values
np.allclose(A,B,...) # test if same shape, elements have close enough values

```"
"I have a tuple of tuples containing strings:

```
T1 = (('13', '17', '18', '21', '32'),
      ('07', '11', '13', '14', '28'),
      ('01', '05', '06', '08', '15', '16'))

```

I want to convert all the string elements into integers and put them back into a list of lists:

```
T2 = [[13, 17, 18, 21, 32],
      [7, 11, 13, 14, 28],
      [1, 5, 6, 8, 15, 16]]

```","[`int()`](https://docs.python.org/3/library/functions.html#int) is the Python standard built-in function to convert a string into an integer value. You call it with a string containing a number as the argument, and it returns the number converted to an integer:

```
>>> int(""1"") + 1
2

```

If you know the structure of your list, T1 (that it simply contains lists, only one level), you could do this in Python 3:

```
T2 = [list(map(int, x)) for x in T1]

```

In Python 2:

```
T2 = [map(int, x) for x in T1]

```"
"It seems like in Python 3 they've removed all of the easy ways to quickly load a script, by removing `execfile()`.

What alternative is there to include a Python file in another one, and execute it?","According to the [Python documentation](https://docs.python.org/3.3/whatsnew/3.0.html?highlight=execfile#builtins), instead of this:

```
execfile(""./filename"") 

```

Use this:

```
exec(open(""./filename"").read())

```

See Python's docs for:

* [Whatâ€™s New In Python 3.0](http://docs.python.org/3.3/whatsnew/3.0.html?highlight=execfile#builtins)
* [`execfile`](https://docs.python.org/2/library/functions.html?highlight=exec#execfile)
* [`exec`](https://docs.python.org/3/library/functions.html?highlight=exec#exec)"
"How can I make a class or method abstract in Python?

I tried redefining `__new__()` like so:

```
class F:
    def __new__(cls):
        raise Exception(""Unable to create an instance of abstract class %s"" %cls)

```

But now, if I create a class `G` that inherits from `F` like so:

```
class G(F):
    pass

```

Then, I can't instantiate `G` either, since it calls its super class's `__new__` method.

Is there a better way to define an abstract class?","Use the [`abc`](http://docs.python.org/library/abc.html) module to create abstract classes. Use the [`abstractmethod`](https://docs.python.org/library/abc.html#abc.abstractmethod) decorator to declare a method abstract, and declare a class abstract using one of three ways, depending upon your Python version.

In Python 3.4 and above, you can inherit from [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC). In earlier versions of Python, you need to specify your class's metaclass as [`ABCMeta`](https://docs.python.org/library/abc.html#abc.ABCMeta). Specifying the metaclass has different syntax in Python 3 and Python 2. The three possibilities are shown below:

```
# Python 3.4+
from abc import ABC, abstractmethod
class Abstract(ABC):
    @abstractmethod
    def foo(self):
        pass

```

```
# Python 3.0+
from abc import ABCMeta, abstractmethod
class Abstract(metaclass=ABCMeta):
    @abstractmethod
    def foo(self):
        pass

```

```
# Python 2
from abc import ABCMeta, abstractmethod
class Abstract:
    __metaclass__ = ABCMeta

    @abstractmethod
    def foo(self):
        pass

```

Whichever way you use, you won't be able to instantiate an abstract class that has abstract methods, but will be able to instantiate a subclass that provides concrete definitions of those methods:

```
>>> Abstract()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Can't instantiate abstract class Abstract with abstract methods foo
>>> class StillAbstract(Abstract):
...     pass
... 
>>> StillAbstract()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Can't instantiate abstract class StillAbstract with abstract methods foo
>>> class Concrete(Abstract):
...     def foo(self):
...         print('Hello, World')
... 
>>> Concrete()
<__main__.Concrete object at 0x7fc935d28898>

```"
"This is the first time I've really sat down and tried python 3, and seem to be failing miserably. I have the following two files:

1. test.py
2. config.py

config.py has a few functions defined in it as well as a few variables. I've stripped it down to the following:

**config.py**

```
debug = True

```

**test.py**

```
import config
print (config.debug)

```

I also have an `__init__.py`

However, I'm getting the following error:

```
ModuleNotFoundError: No module named 'config'

```

I'm aware that the py3 convention is to use absolute imports:

```
from . import config

```

However, this leads to the following error:

```
ImportError: cannot import name 'config'

```

So I'm at a loss as to what to do here... Any help is greatly appreciated. :)","**TL;DR:** You can't do relative imports from the file you **execute** since `__main__` module is not a part of a package.

*Absolute imports* - import something available on `sys.path`

*Relative imports* - import something relative to the current module, must be a part of a package

If you're running both variants in exactly the same way, one of them should work. Here is an example that should help you understand what's going on. Let's add another `main.py` file with the overall directory structure like this:

```
.
./main.py
./ryan/__init__.py
./ryan/config.py
./ryan/test.py

```

And let's update `test.py` to see what's going on:

```
# config.py
debug = True

```

```
# test.py
print(__name__)

try:
    # Trying to find module in the parent package
    from . import config
    print(config.debug)
    del config
except ImportError:
    print('Relative import failed')

try:
    # Trying to find module on sys.path
    import config
    print(config.debug)
except ModuleNotFoundError:
    print('Absolute import failed')

```

```
# main.py
import ryan.test

```

Let's run `test.py` first:

```
$ python ryan/test.py
__main__
Relative import failed
True

```

Here ""test"" *is* the `__main__` module and doesn't know anything about belonging to a package. However `import config` should work, since the `ryan` folder will be added to `sys.path`.

Let's run `main.py` instead:

```
$ python main.py
ryan.test
True
Absolute import failed

```

And here test is inside of the ""ryan"" package and can perform relative imports. `import config` fails since implicit relative imports are not allowed in Python 3.

Hope this helped.

P.S.: If you're sticking with Python 3 there is no more need for `__init__.py` files."
"Consider the following:

```
with open(path, mode) as f:
    return [line for line in f if condition]

```

Will the file be closed properly, or does using `return` somehow bypass the [context manager](https://docs.python.org/library/contextlib.html)?","Yes, it acts like the `finally` block after a `try` block, i.e. it always executes (unless the python process terminates in an unusual way of course).

It is also mentioned in one of the examples of [PEP-343](http://www.python.org/dev/peps/pep-0343/) which is the specification for the `with` statement:

```
with locked(myLock):
    # Code here executes with myLock held.  The lock is
    # guaranteed to be released when the block is left (even
    # if via return or by an uncaught exception).

```

Something worth mentioning is however, that you cannot easily catch exceptions thrown by the `open()` call without putting the whole `with` block inside a `try..except` block which is usually not what one wants."
"How do I efficiently append one string to another? Are there any faster alternatives to:

```
var1 = ""foo""
var2 = ""bar""
var3 = var1 + var2

```

---

For handling multiple strings in a list, see [How to concatenate (join) items in a list to a single string](https://stackoverflow.com/questions/12453580/).

See [How do I put a variableâ€™s value inside a string (interpolate it into the string)?](https://stackoverflow.com/questions/2960772) if some inputs are **not** strings, but the *result* should still be a string.","If you only have one reference to a string and you concatenate another string to the end, CPython now special cases this and tries to extend the string in place.

The end result is that the operation is amortized O(n).

e.g.

```
s = """"
for i in range(n):
    s += str(i)

```

used to be O(n^2), but now it is O(n).

### More information

From the source (bytesobject.c):

```
void
PyBytes_ConcatAndDel(register PyObject **pv, register PyObject *w)
{
    PyBytes_Concat(pv, w);
    Py_XDECREF(w);
}


/* The following function breaks the notion that strings are immutable:
   it changes the size of a string.  We get away with this only if there
   is only one module referencing the object.  You can also think of it
   as creating a new string object and destroying the old one, only
   more efficiently.  In any case, don't use this if the string may
   already be known to some other part of the code...
   Note that if there's not enough memory to resize the string, the original
   string object at *pv is deallocated, *pv is set to NULL, an ""out of
   memory"" exception is set, and -1 is returned.  Else (on success) 0 is
   returned, and the value in *pv may or may not be the same as on input.
   As always, an extra byte is allocated for a trailing \0 byte (newsize
   does *not* include that), and a trailing \0 byte is stored.
*/

int
_PyBytes_Resize(PyObject **pv, Py_ssize_t newsize)
{
    register PyObject *v;
    register PyBytesObject *sv;
    v = *pv;
    if (!PyBytes_Check(v) || Py_REFCNT(v) != 1 || newsize < 0) {
        *pv = 0;
        Py_DECREF(v);
        PyErr_BadInternalCall();
        return -1;
    }
    /* XXX UNREF/NEWREF interface should be more symmetrical */
    _Py_DEC_REFTOTAL;
    _Py_ForgetReference(v);
    *pv = (PyObject *)
        PyObject_REALLOC((char *)v, PyBytesObject_SIZE + newsize);
    if (*pv == NULL) {
        PyObject_Del(v);
        PyErr_NoMemory();
        return -1;
    }
    _Py_NewReference(*pv);
    sv = (PyBytesObject *) *pv;
    Py_SIZE(sv) = newsize;
    sv->ob_sval[newsize] = '\0';
    sv->ob_shash = -1;          /* invalidate cached hash value */
    return 0;
}

```

It's easy enough to verify empirically.

```
$ python -m timeit -s""s=''"" ""for i in xrange(10):s+='a'""
1000000 loops, best of 3: 1.85 usec per loop
$ python -m timeit -s""s=''"" ""for i in xrange(100):s+='a'""
10000 loops, best of 3: 16.8 usec per loop
$ python -m timeit -s""s=''"" ""for i in xrange(1000):s+='a'""
10000 loops, best of 3: 158 usec per loop
$ python -m timeit -s""s=''"" ""for i in xrange(10000):s+='a'""
1000 loops, best of 3: 1.71 msec per loop
$ python -m timeit -s""s=''"" ""for i in xrange(100000):s+='a'""
10 loops, best of 3: 14.6 msec per loop
$ python -m timeit -s""s=''"" ""for i in xrange(1000000):s+='a'""
10 loops, best of 3: 173 msec per loop

```

**It's important** however to note that this optimisation isn't part of the Python spec. It's only in the cPython implementation as far as I know. The same empirical testing on pypy or jython for example might show the older O(n\*\*2) performance.

```
$ pypy -m timeit -s""s=''"" ""for i in xrange(10):s+='a'""
10000 loops, best of 3: 90.8 usec per loop
$ pypy -m timeit -s""s=''"" ""for i in xrange(100):s+='a'""
1000 loops, best of 3: 896 usec per loop
$ pypy -m timeit -s""s=''"" ""for i in xrange(1000):s+='a'""
100 loops, best of 3: 9.03 msec per loop
$ pypy -m timeit -s""s=''"" ""for i in xrange(10000):s+='a'""
10 loops, best of 3: 89.5 msec per loop

```

So far so good, but then,

```
$ pypy -m timeit -s""s=''"" ""for i in xrange(100000):s+='a'""
10 loops, best of 3: 12.8 sec per loop

```

ouch even worse than quadratic. So pypy is doing something that works well with short strings, but performs poorly for larger strings."
"How do I get the last element of a list? Which way is preferred?

```
alist[-1]
alist[len(alist) - 1]

```","`some_list[-1]` is the shortest and most Pythonic.

In fact, you can do much more with this syntax. The `some_list[-n]` syntax gets the nth-to-last element. So `some_list[-1]` gets the last element, `some_list[-2]` gets the second to last, etc, all the way down to `some_list[-len(some_list)]`, which gives you the first element.

You can also set list elements in this way. For instance:

```
>>> some_list = [1, 2, 3]
>>> some_list[-1] = 5 # Set the last element
>>> some_list[-2] = 3 # Set the second to last element
>>> some_list
[1, 3, 5]

```

Note that getting a list item by index will raise an [`IndexError`](https://docs.python.org/3/library/exceptions.html#IndexError) if the expected item doesn't exist. This means that `some_list[-1]` will raise an exception if `some_list` is empty, because an empty list can't have a last element."
"I'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the `workon` command.

```
me@mymachine:~$ workon env1
(env1)me@mymachine:~$ workon env2
(env2)me@mymachine:~$ workon env1
(env1)me@mymachine:~$ 

```

How do I exit all virtual environments and work on my system environment again? Right now, the only way I have of getting back to `me@mymachine:~$` is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on ""nothing"", and if so, what is it? If such a command does not exist, how would I go about creating it?","Usually, activating a virtualenv gives you a shell function named:

```
$ deactivate

```

which puts things back to normal.

I have just looked specifically again at the code for `virtualenvwrapper`, and, yes, it too supports `deactivate` as the way to escape from all virtualenvs.

If you are trying to leave an [*Anaconda*](https://en.wikipedia.org/wiki/Anaconda_(Python_distribution)) environment, the command depends upon your version of `conda`. Recent versions (like 4.6) install a `conda` function directly in your shell, in which case you run:

```
conda deactivate

```

Older conda versions instead implement deactivation using a stand-alone script:

```
source deactivate

```"
[`pip`](https://pip.pypa.io/en/stable/) is a replacement for [`easy_install`](http://setuptools.readthedocs.io/en/latest/easy_install.html). But should I install `pip` using `easy_install` on Windows? Is there a better way?,"Python 3.4+ and 2.7.9+
----------------------

Good news! [Python 3.4](https://docs.python.org/3/whatsnew/3.4.html) (released March 2014) and [Python 2.7.9](https://docs.python.org/2/whatsnew/2.7.html#pep-477-backport-ensurepip-pep-453-to-python-2-7) (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins [Ruby](http://en.wikipedia.org/wiki/Ruby_%28programming_language%29), [Node.js](http://en.wikipedia.org/wiki/Node.js), [Haskell](http://en.wikipedia.org/wiki/Haskell_%28programming_language%29), [Perl](http://en.wikipedia.org/wiki/Perl), [Go](http://en.wikipedia.org/wiki/Go_%28programming_language%29)—almost every other contemporary language with a majority open-source community. Thank you, Python.

If you do find that pip is not available, simply run [`ensurepip`](https://docs.python.org/library/ensurepip.html).

* On Windows:

  ```
  py -3 -m ensurepip

  ```
* Otherwise:

  ```
  python3 -m ensurepip

  ```

Of course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this [in the Stack Overflow question *Does Python have a package/module management system?*](https://stackoverflow.com/questions/2436731/does-python-have-a-package-module-management-system/13445719#13445719).

Python 3 ≤ 3.3 and 2 ≤ 2.7.8
----------------------------

Flying in the face of its ['batteries included'](http://www.python.org/about/) motto, Python ships without a package manager. To make matters worse, Pip was—until recently—ironically difficult to install.

### Official instructions

Per <https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip>:

Download [`get-pip.py`](https://bootstrap.pypa.io/get-pip.py), being careful to save it as a `.py` file rather than `.txt`. Then, run it from the command prompt:

```
python get-pip.py

```

You possibly need an administrator command prompt to do this. Follow *[Start a Command Prompt as an Administrator](http://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspx)* (Microsoft TechNet).

This installs the pip package, which (in Windows) contains ...\Scripts\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,

### Alternative instructions

The official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.

For our sake, Christoph Gohlke prepares Windows installers (`.msi`) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:

1. [Install setuptools](http://www.lfd.uci.edu/%7Egohlke/pythonlibs/#setuptools)
2. [Install pip](http://www.lfd.uci.edu/%7Egohlke/pythonlibs/#pip)

For me, this installed Pip at `C:\Python27\Scripts\pip.exe`. Find `pip.exe` on your computer, then add its folder (for example, `C:\Python27\Scripts`) to your path (Start / Edit environment variables). Now you should be able to run `pip` from the command line. Try installing a package:

```
pip install httpie

```

There you go (hopefully)! Solutions for common problems are given below:

### Proxy problems

If you work in an office, you might be behind an HTTP proxy. If so, set the environment variables [`http_proxy` and `https_proxy`](http://docs.python.org/2/library/urllib.html). Most Python applications (and other free software) respect these. Example syntax:

```
http://proxy_url:port
http://username:password@proxy_url:port

```

If you're really unlucky, your proxy might be a Microsoft [NTLM](https://en.wikipedia.org/wiki/NT_LAN_Manager) proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. <http://cntlm.sourceforge.net/>

### Unable to find vcvarsall.bat

Python modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.

> Error: Unable to find vcvarsall.bat

You can fix that by [installing a C++ compiler](https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat) such as [MinGW](http://en.wikipedia.org/wiki/MinGW) or [Visual C++](http://en.wikipedia.org/wiki/Visual_C%2B%2B#32-bit_versions). Microsoft actually ships one specifically for use with Python. Or try *[Microsoft Visual C++ Compiler for Python 2.7](http://aka.ms/vcpython27)*.

Often though it's easier to check [Christoph's site](http://www.lfd.uci.edu/%7Egohlke/pythonlibs/) for your package."
"How can I convert an `str` to a `float`?

```
""545.2222"" -> 545.2222

```

Or an `str` to a `int`?

```
""31"" -> 31

```

---

For the reverse, see [Convert integer to string in Python](https://stackoverflow.com/questions/961632) and [Converting a float to a string without rounding it](https://stackoverflow.com/questions/1317558).

Please instead use [How can I read inputs as numbers?](https://stackoverflow.com/questions/20449427) to close duplicate questions where OP received a string *from user input* and immediately wants to convert it, or was hoping for `input` (in 3.x) to convert the type automatically.","```
>>> a = ""545.2222""
>>> float(a)
545.22220000000004
>>> int(float(a))
545

```"
"I want to get a new string from the third character to the end of the string, e.g. `myString[2:end]`. If omitting the second part means 'to the end', and if you omit the first part, does it start from the start?","```
>>> x = ""Hello World!""
>>> x[2:]
'llo World!'
>>> x[:2]
'He'
>>> x[:-2]
'Hello Worl'
>>> x[-2:]
'd!'
>>> x[2:-2]
'llo Worl'

```

Python calls this concept ""slicing"" and it works on more than just strings. Take a look [here](https://stackoverflow.com/questions/509211/good-primer-for-python-slice-notation) for a comprehensive introduction."
"Non-working example:

```
print("" \{ Hello \} {0} "".format(42))

```

Desired output:

```
 {Hello} 42 

```","You need to double the `{{` and `}}`:

```
>>> x = "" {{ Hello }} {0} ""
>>> print(x.format(42))
' { Hello } 42 '

```

Here's the relevant part of the [Python documentation for format string syntax](https://docs.python.org/library/string.html#formatstrings):

> Format strings contain “replacement fields” surrounded by curly braces `{}`. Anything that is not contained in braces is considered literal text, which is copied unchanged to the output. If you need to include a brace character in the literal text, it can be escaped by doubling: `{{` and `}}`."
"I wanted to test if a key exists in a dictionary before updating the value for the key.
I wrote the following code:

```
if 'key1' in dict.keys():
  print ""blah""
else:
  print ""boo""

```

I think this is not the best way to accomplish this task. Is there a better way to test for a key in the dictionary?","[`in`](https://docs.python.org/reference/expressions.html#membership-test-operations) tests for the existence of a key in a [`dict`](https://docs.python.org/library/stdtypes.html#dict):

```
d = {""key1"": 10, ""key2"": 23}

if ""key1"" in d:
    print(""this will execute"")

if ""nonexistent key"" in d:
    print(""this will not"")

```

---

Use [`dict.get()`](https://docs.python.org/library/stdtypes.html#dict.get) to provide a default value when the key does not exist:

```
d = {}

for i in range(100):
    key = i % 10
    d[key] = d.get(key, 0) + 1

```

---

To provide a default value for *every* key, either use [`dict.setdefault()`](https://docs.python.org/library/stdtypes.html#dict.setdefault) on each assignment:

```
d = {}

for i in range(100):
    d[i % 10] = d.setdefault(i % 10, 0) + 1    

```

...or better, use [`defaultdict`](https://docs.python.org/library/collections.html#collections.defaultdict) from the [`collections`](https://docs.python.org/library/collections.html) module:

```
from collections import defaultdict

d = defaultdict(int)

for i in range(100):
    d[i % 10] += 1

```"
"I have this folder structure:

```
application
├── app
│   └── folder
│       └── file.py
└── app2
    └── some_folder
        └── some_file.py

```

How can I import a function from `file.py`, from within `some_file.py`? I tried:

```
from application.app.folder.file import func_name

```

but it doesn't work.","Note: This answer was intended for a very specific question. For most programmers coming here from a search engine, this is not the answer you are looking for. Typically you would structure your files into packages (see other answers) instead of modifying the search path.

---

By default, you can't. When importing a file, Python only searches the directory that the entry-point script is running from and `sys.path` which includes locations such as the package installation directory (it's actually [a little more complex](https://docs.python.org/3/library/sys.html#sys.path) than this, but this covers most cases).

However, you can add to the Python path at runtime:

```
# some_file.py
import sys
# caution: path[0] is reserved for script path (or '' in REPL)
sys.path.insert(1, '/path/to/application/app/folder')

import file

```"
How do I create class (i.e. [static](https://en.wikipedia.org/wiki/Method_(computer_programming)#Static_methods)) variables or methods in Python?,"Variables declared inside the class definition, but not inside a method are class or static variables:

```
>>> class MyClass:
...     i = 3
...
>>> MyClass.i
3 

```

As @[millerdev](https://stackoverflow.com/questions/68645/static-class-variables-in-python#answer-69067) points out, this creates a class-level `i` variable, but this is distinct from any instance-level `i` variable, so you could have

```
>>> m = MyClass()
>>> m.i = 4
>>> MyClass.i, m.i
>>> (3, 4)

```

This is different from C++ and Java, but not so different from C#, where a static member can't be accessed using a reference to an instance.

See [what the Python tutorial has to say on the subject of classes and class objects](https://docs.python.org/3/tutorial/classes.html#class-objects).

@Steve Johnson has already answered regarding [static methods](http://web.archive.org/web/20090214211613/http://pyref.infogami.com/staticmethod), also documented under [""Built-in Functions"" in the Python Library Reference](https://docs.python.org/3/library/functions.html#staticmethod).

```
class C:
    @staticmethod
    def f(arg1, arg2, ...): ...

```

@beidy recommends [classmethod](https://docs.python.org/3/library/functions.html#classmethod)s over staticmethod, as the method then receives the class type as the first argument."
"Is there a way to convert a string to lowercase?

```
""Kilometers""  â†’  ""kilometers""

```

---

See [How to change a string into uppercase?](https://stackoverflow.com/questions/9257094) for the opposite.","Use [`str.lower()`](https://docs.python.org/library/stdtypes.html#str.lower):

```
""Kilometer"".lower()

```"
"I use python to create my project settings setup, but I need help getting the command line arguments.

I tried this on the terminal:

```
$python myfile.py var1 var2 var3

```

In my Python file, I want to use all variables that are input.","[Python tutorial explains it](http://docs.python.org/tutorial/stdlib.html#command-line-arguments):

```
import sys

print(sys.argv)

```

More specifically, if you run `python example.py one two three`:

```
>>> import sys
>>> print(sys.argv)
['example.py', 'one', 'two', 'three']

```"
"How do I check if an object has some attribute? For example:

```
>>> a = SomeClass()
>>> a.property
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: SomeClass instance has no attribute 'property'

```

How do I tell if `a` has the attribute `property` before using it?","Try [`hasattr()`](https://docs.python.org/3/library/functions.html#hasattr):

```
if hasattr(a, 'property'):
    a.property

```

See [zweiterlinde's answer](https://stackoverflow.com/a/610923/117030), which offers good advice about asking forgiveness! It is a very Pythonic approach!

The general practice in Python is that, if the property is likely to be there most of the time, simply call it and either let the exception propagate, or trap it with a try/except block. This will likely be faster than `hasattr`. If the property is likely to not be there most of the time, or you're not sure, using `hasattr` will probably be faster than repeatedly falling into an exception block."
"Consider these examples using `print` in Python:

```
>>> for i in range(4): print('.')
.
.
.
.
>>> print('.', '.', '.', '.')
. . . .

```

Either a newline or a space is added between each value. How can I avoid that, so that the output is `....` instead? In other words, how can I ""append"" strings to the standard output stream?","In Python 3, you can use the `sep=` and `end=` parameters of the [`print`](https://docs.python.org/library/functions.html#print) function:

To not add a newline to the end of the string:

```
print('.', end='')

```

To not add a space between all the function arguments you want to print:

```
print('a', 'b', 'c', sep='')

```

You can pass any string to either parameter, and you can use both parameters at the same time.

If you are having trouble with buffering, you can flush the output by adding `flush=True` keyword argument:

```
print('.', end='', flush=True)

```

Python 2.6 and 2.7
------------------

From Python 2.6 you can either import the `print` function from Python 3 using the [`__future__` module](https://docs.python.org/2/library/__future__.html):

```
from __future__ import print_function

```

which allows you to use the Python 3 solution above.

However, note that the `flush` keyword is not available in the version of the `print` function imported from `__future__` in Python 2; it only works in Python 3, more specifically 3.3 and later. In earlier versions you'll still need to flush manually with a call to `sys.stdout.flush()`. You'll also have to rewrite all other print statements in the file where you do this import.

Or you can use [`sys.stdout.write()`](https://docs.python.org/library/sys.html#sys.stdout)

```
import sys
sys.stdout.write('.')

```

You may also need to call

```
sys.stdout.flush()

```

to ensure `stdout` is flushed immediately."
"How do I call a function, using a string with the function's name? For example:

```
import foo
func_name = ""bar""
call(foo, func_name)  # calls foo.bar()

```","Given a module `foo` with method `bar`:

```
import foo
bar = getattr(foo, 'bar')
result = bar()

```

[`getattr`](https://docs.python.org/library/functions.html#getattr) can similarly be used on class instance bound methods, module-level methods, class methods... the list goes on."
"I want `a` to be rounded to *13.95*. I tried using [`round`](https://docs.python.org/2/library/functions.html#round), but I get:

```
>>> a
13.949999999999999
>>> round(a, 2)
13.949999999999999

```

---

For the analogous issue with the standard library Decimal class, see [How can I format a decimal to always show 2 decimal places?](https://stackoverflow.com/questions/1995615).","You are running into the [old problem](https://en.wikipedia.org/wiki/IEEE_754) with floating point numbers that not all numbers can be represented exactly. The command line is just showing you the full floating point form from memory.

With floating point representation, your rounded version is the same number. Since computers are binary, they store floating point numbers as an integer and then divide it by a power of two so 13.95 will be represented in a similar fashion to 125650429603636838/(2\*\*53).

Double precision numbers have 53 bits (16 digits) of precision and regular floats have 24 bits (8 digits) of precision. The [floating point type in Python uses double precision](http://docs.python.org/tutorial/floatingpoint.html) to store the values.

For example,

```
>>> 125650429603636838/(2**53)
13.949999999999999

>>> 234042163/(2**24)
13.949999988079071

>>> a = 13.946
>>> print(a)
13.946
>>> print(""%.2f"" % a)
13.95
>>> round(a,2)
13.949999999999999
>>> print(""%.2f"" % round(a, 2))
13.95
>>> print(""{:.2f}"".format(a))
13.95
>>> print(""{:.2f}"".format(round(a, 2)))
13.95
>>> print(""{:.15f}"".format(round(a, 2)))
13.949999999999999

```

If you are after only two decimal places (to display a currency value, for example), then you have a couple of better choices:

1. Use integers and store values in cents, not dollars and then divide by 100 to convert to dollars.
2. Or use a fixed point number like [decimal](https://docs.python.org/library/decimal.html)."
"How do I retrieve an item at random from the following list?

```
foo = ['a', 'b', 'c', 'd', 'e']

```","Use [`random.choice()`](https://docs.python.org/library/random.html#random.choice):

```
import random

foo = ['a', 'b', 'c', 'd', 'e']
print(random.choice(foo))

```

For [cryptographically secure](https://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator) random choices (e.g., for generating a passphrase from a wordlist), use [`secrets.choice()`](https://docs.python.org/library/secrets.html#secrets.choice):

```
import secrets

foo = ['battery', 'correct', 'horse', 'staple']
print(secrets.choice(foo))

```

`secrets` is new in Python 3.6. On older versions of Python you can use the [`random.SystemRandom`](https://docs.python.org/library/random.html#random.SystemRandom) class:

```
import random

secure_random = random.SystemRandom()
print(secure_random.choice(foo))

```"
"How do I remove an element from a list **by index**?

I found `list.remove()`, but this slowly scans the list for an item **by value**.","Use `del` and specify the index of the element you want to delete:

```
>>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> del a[-1]
>>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8]

```

Also supports slices:

```
>>> del a[2:4]
>>> a
[0, 1, 4, 5, 6, 7, 8, 9]

```

[Here](http://docs.python.org/tutorial/datastructures.html#the-del-statement) is the section from the tutorial."
"To delete a column in a DataFrame, I can successfully use:

```
del df['column_name']

```

But why can't I use the following?

```
del df.column_name

```

Since it is possible to access the Series via `df.column_name`, I expected this to work.","The best way to do this in Pandas is to use [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html):

```
df = df.drop('column_name', axis=1)

```

where `1` is the *axis* number (`0` for rows and `1` for columns.)

Or, the `drop()` method accepts `index`/`columns` keywords as an alternative to specifying the axis. So we can now just do:

```
df = df.drop(columns=['column_nameA', 'column_nameB'])

```

* *This was [introduced in v0.21.0](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.21.0.html#method-drop-now-also-accepts-index-columns-keywords) (October 27, 2017)*

To delete the column without having to reassign `df` you can do:

```
df.drop('column_name', axis=1, inplace=True)

```

Finally, to drop by column *number* instead of by column *label*, try this to delete, e.g. the 1st, 2nd and 4th columns:

```
df = df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index

```

Also working with ""text"" syntax for the columns:

```
df.drop(['column_nameA', 'column_nameB'], axis=1, inplace=True)

```"
"How do I get the number of elements in the list `items`?

```
items = [""apple"", ""orange"", ""banana""]

# There are 3 items.

```","The [`len()`](https://docs.python.org/3/library/functions.html#len) function can be used with several different types in Python - both built-in types and library types. For example:

```
>>> len([1, 2, 3])
3

```"
"How do I pad a numeric string with zeroes to the left, so that the string has a specific length?","To pad strings:

```
>>> n = '4'
>>> print(n.zfill(3))
004

```

To pad numbers:

```
>>> n = 4
>>> print(f'{n:03}') # Preferred method, python >= 3.6
004
>>> print('%03d' % n)
004
>>> print(format(n, '03')) # python >= 2.6
004
>>> print('{0:03d}'.format(n))  # python >= 2.6 + python 3
004
>>> print('{foo:03d}'.format(foo=n))  # python >= 2.6 + python 3
004
>>> print('{:03d}'.format(n))  # python >= 2.7 + python3
004

```

[String formatting documentation](https://docs.python.org/3/library/string.html#format-string-syntax)."
"How do I see the type of a variable? (e.g., unsigned 32 bit)","Use the [`type()`](https://docs.python.org/library/functions.html#type) built-in function:

```
>>> i = 123
>>> type(i)
<type 'int'>
>>> type(i) is int
True
>>> i = 123.456
>>> type(i)
<type 'float'>
>>> type(i) is float
True

```

To check if a variable is of a given type, use [`isinstance`](https://docs.python.org/library/functions.html#isinstance):

```
>>> i = 123
>>> isinstance(i, int)
True
>>> isinstance(i, (float, str, set, dict))
False

```

Note that Python doesn't have the same types as C/C++, which appears to be your question."
"I am writing a program that accepts user input.

```
#note: Python 2.7 users should use `raw_input`, the equivalent of 3.X's `input`
age = int(input(""Please enter your age: ""))
if age >= 18: 
    print(""You are able to vote in the United States!"")
else:
    print(""You are not able to vote in the United States."")

```

The program works as expected as long as the the user enters meaningful data.

```
Please enter your age: 23
You are able to vote in the United States!

```

But it fails if the user enters invalid data:

```
Please enter your age: dickety six
Traceback (most recent call last):
  File ""canyouvote.py"", line 1, in <module>
    age = int(input(""Please enter your age: ""))
ValueError: invalid literal for int() with base 10: 'dickety six'

```

Instead of crashing, I would like the program to ask for the input again. Like this:

```
Please enter your age: dickety six
Sorry, I didn't understand that.
Please enter your age: 26
You are able to vote in the United States!

```

How do I ask for valid input instead of crashing or accepting invalid values (e.g. `-1`)?","The simplest way to accomplish this is to put the `input` method in a while loop. Use [`continue`](https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops) when you get bad input, and `break` out of the loop when you're satisfied.

When Your Input Might Raise an Exception
----------------------------------------

Use [`try` and `except`](https://docs.python.org/3/tutorial/errors.html#handling-exceptions) to detect when the user enters data that can't be parsed.

```
while True:
    try:
        # Note: Python 2.x users should use raw_input, the equivalent of 3.x's input
        age = int(input(""Please enter your age: ""))
    except ValueError:
        print(""Sorry, I didn't understand that."")
        #better try again... Return to the start of the loop
        continue
    else:
        #age was successfully parsed!
        #we're ready to exit the loop.
        break
if age >= 18: 
    print(""You are able to vote in the United States!"")
else:
    print(""You are not able to vote in the United States."")

```

Implementing Your Own Validation Rules
--------------------------------------

If you want to reject values that Python can successfully parse, you can add your own validation logic.

```
while True:
    data = input(""Please enter a loud message (must be all caps): "")
    if not data.isupper():
        print(""Sorry, your response was not loud enough."")
        continue
    else:
        #we're happy with the value given.
        #we're ready to exit the loop.
        break

while True:
    data = input(""Pick an answer from A to D:"")
    if data.lower() not in ('a', 'b', 'c', 'd'):
        print(""Not an appropriate choice."")
    else:
        break

```

Combining Exception Handling and Custom Validation
--------------------------------------------------

Both of the above techniques can be combined into one loop.

```
while True:
    try:
        age = int(input(""Please enter your age: ""))
    except ValueError:
        print(""Sorry, I didn't understand that."")
        continue

    if age < 0:
        print(""Sorry, your response must not be negative."")
        continue
    else:
        #age was successfully parsed, and we're happy with its value.
        #we're ready to exit the loop.
        break
if age >= 18: 
    print(""You are able to vote in the United States!"")
else:
    print(""You are not able to vote in the United States."")

```

Encapsulating it All in a Function
----------------------------------

If you need to ask your user for a lot of different values, it might be useful to put this code in a function, so you don't have to retype it every time.

```
def get_non_negative_int(prompt):
    while True:
        try:
            value = int(input(prompt))
        except ValueError:
            print(""Sorry, I didn't understand that."")
            continue

        if value < 0:
            print(""Sorry, your response must not be negative."")
            continue
        else:
            break
    return value

age = get_non_negative_int(""Please enter your age: "")
kids = get_non_negative_int(""Please enter the number of children you have: "")
salary = get_non_negative_int(""Please enter your yearly earnings, in dollars: "")

```

### Putting It All Together

You can extend this idea to make a very generic input function:

```
def sanitised_input(prompt, type_=None, min_=None, max_=None, range_=None):
    if min_ is not None and max_ is not None and max_ < min_:
        raise ValueError(""min_ must be less than or equal to max_."")
    while True:
        ui = input(prompt)
        if type_ is not None:
            try:
                ui = type_(ui)
            except ValueError:
                print(""Input type must be {0}."".format(type_.__name__))
                continue
        if max_ is not None and ui > max_:
            print(""Input must be less than or equal to {0}."".format(max_))
        elif min_ is not None and ui < min_:
            print(""Input must be greater than or equal to {0}."".format(min_))
        elif range_ is not None and ui not in range_:
            if isinstance(range_, range):
                template = ""Input must be between {0.start} and {0.stop}.""
                print(template.format(range_))
            else:
                template = ""Input must be {0}.""
                if len(range_) == 1:
                    print(template.format(*range_))
                else:
                    expected = "" or "".join((
                        "", "".join(str(x) for x in range_[:-1]),
                        str(range_[-1])
                    ))
                    print(template.format(expected))
        else:
            return ui

```

With usage such as:

```
age = sanitised_input(""Enter your age: "", int, 1, 101)
answer = sanitised_input(""Enter your answer: "", str.lower, range_=('a', 'b', 'c', 'd'))

```

Common Pitfalls, and Why you Should Avoid Them
----------------------------------------------

### The Redundant Use of Redundant `input` Statements

This method works but is generally considered poor style:

```
data = input(""Please enter a loud message (must be all caps): "")
while not data.isupper():
    print(""Sorry, your response was not loud enough."")
    data = input(""Please enter a loud message (must be all caps): "")

```

It might look attractive initially because it's shorter than the `while True` method, but it violates the [Don't Repeat Yourself](http://en.wikipedia.org/wiki/Don%27t_repeat_yourself) principle of software development. This increases the likelihood of bugs in your system. What if you want to backport to 2.7 by changing `input` to `raw_input`, but accidentally change only the first `input` above? It's a `SyntaxError` just waiting to happen.

### Recursion Will Blow Your Stack

If you've just learned about recursion, you might be tempted to use it in `get_non_negative_int` so you can dispose of the while loop.

```
def get_non_negative_int(prompt):
    try:
        value = int(input(prompt))
    except ValueError:
        print(""Sorry, I didn't understand that."")
        return get_non_negative_int(prompt)

    if value < 0:
        print(""Sorry, your response must not be negative."")
        return get_non_negative_int(prompt)
    else:
        return value

```

This appears to work fine most of the time, but if the user enters invalid data enough times, the script will terminate with a `RuntimeError: maximum recursion depth exceeded`. You may think ""no fool would make 1000 mistakes in a row"", but you're underestimating the ingenuity of fools!"
"1. How do I delete an item from a dictionary in Python?
2. **Without** modifying the original dictionary, how do I obtain **another** dictionary with the item removed?

---

See also [How can I remove a key from a Python dictionary?](https://stackoverflow.com/questions/11277432) for the specific issue of removing an item (by key) that may not already be present.","The [`del` statement](http://docs.python.org/reference/simple_stmts.html#the-del-statement) removes an element:

```
del d[key]

```

Note that this mutates the existing dictionary, so the contents of the dictionary changes for anybody else who has a reference to the same instance. To return a *new* dictionary, make a copy of the dictionary:

```
def removekey(d, key):
    r = dict(d)
    del r[key]
    return r

```

The `dict()` constructor makes a *shallow copy*. To make a deep copy, see the [`copy` module](https://docs.python.org/library/copy.html).

---

Note that making a copy for every dict `del`/assignment/etc. means you're going from constant time to linear time, and also using linear space. For small dicts, this is not a problem. But if you're planning to make lots of copies of large dicts, you probably want a different data structure, like a HAMT (as described in [this answer](https://stackoverflow.com/a/50341031/908494))."
"Python 3.3 includes in its standard library the new package `venv`. What does it do, and how does it differ from all the other packages that match the regex `(py)?(v|virtual|pip)?env`?","**This is my personal recommendation for beginners:** start by learning [`virtualenv`](https://pypi.org/project/virtualenv/) and [`pip`](https://pypi.org/project/pip/), tools which work with both Python 2 and 3 and in a variety of situations, and pick up other tools once you start needing them.

Now on to answer the question: what is the difference between these similarly named things: venv, virtualenv, etc?

PyPI packages not in the standard library:
==========================================

* **[`virtualenv`](https://pypi.python.org/pypi/virtualenv)** is a very popular tool that creates isolated Python environments for Python libraries. If you're not familiar with this tool, I highly recommend learning it, as it is a very useful tool.

  It works by installing a bunch of files in a directory (eg: `env/`), and then modifying the `PATH` environment variable to prefix it with a custom `bin` directory (eg: `env/bin/`). An exact copy of the `python` or `python3` binary is placed in this directory, but Python is programmed to look for libraries relative to its path first, in the environment directory. It's not part of Python's standard library, but is officially blessed by the PyPA (Python Packaging Authority). Once activated, you can install packages in the virtual environment using `pip`.
* **[`pyenv`](https://github.com/pyenv/pyenv)** is used to isolate Python versions. For example, you may want to test your code against Python 2.7, 3.6, 3.7 and 3.8, so you'll need a way to switch between them. Once activated, it prefixes the `PATH` environment variable with `~/.pyenv/shims`, where there are special files matching the Python commands (`python`, `pip`). These are not copies of the Python-shipped commands; they are special scripts that decide on the fly which version of Python to run based on the `PYENV_VERSION` environment variable, or the `.python-version` file, or the `~/.pyenv/version` file. `pyenv` also makes the process of downloading and installing multiple Python versions easier, using the command `pyenv install`.
* **[`pyenv-virtualenv`](https://github.com/pyenv/pyenv-virtualenv)** is a plugin for `pyenv` by the same author as `pyenv`, to allow you to use `pyenv` and `virtualenv` at the same time conveniently. However, if you're using Python 3.3 or later, `pyenv-virtualenv` will try to run `python -m venv` if it is available, instead of `virtualenv`. You can use `virtualenv` and `pyenv` together without `pyenv-virtualenv`, if you don't want the convenience features.
* **[`virtualenvwrapper`](https://pypi.python.org/pypi/virtualenvwrapper)** is a set of extensions to `virtualenv` (see [docs](http://virtualenvwrapper.readthedocs.io/en/latest/)). It gives you commands like `mkvirtualenv`, `lssitepackages`, and especially `workon` for switching between different `virtualenv` directories. This tool is especially useful if you want multiple `virtualenv` directories.
* **[`pyenv-virtualenvwrapper`](https://github.com/pyenv/pyenv-virtualenvwrapper)** is a plugin for `pyenv` by the same author as `pyenv`, to conveniently integrate `virtualenvwrapper` into `pyenv`.
* **[`pipenv`](https://pypi.python.org/pypi/pipenv)** aims to combine `Pipfile`, `pip` and `virtualenv` into one command on the command-line. The `virtualenv` directory typically gets placed in `~/.local/share/virtualenvs/XXX`, with `XXX` being a hash of the path of the project directory. This is different from `virtualenv`, where the directory is typically in the current working directory. `pipenv` is meant to be used when developing Python applications (as opposed to libraries). There are alternatives to `pipenv`, such as `poetry`, which I won't list here since this question is only about the packages that are similarly named.

Standard library:
=================

* **`pyvenv`** (not to be confused with **[`pyenv`](https://github.com/pyenv/pyenv)** in the previous section) is a script shipped with Python 3.3 to 3.7. It was [removed from Python 3.8](https://docs.python.org/3/whatsnew/3.8.html#api-and-feature-removals) as it had problems (not to mention the confusing name). Running `python3 -m venv` has exactly the same effect as `pyvenv`.
* **[`venv`](https://docs.python.org/3/library/venv.html)** is a package shipped with Python 3, which you can run using `python3 -m venv` (although for some reason some distros separate it out into a separate distro package, such as `python3-venv` on Ubuntu/Debian). It serves the same purpose as `virtualenv`, but only has a subset of its features ([see a comparison here](https://virtualenv.pypa.io/en/latest/)). `virtualenv` continues to be more popular than `venv`, especially since the former supports both Python 2 and 3."
"Given a single item, how do I count occurrences of it in a list, in Python?

---

A related but different problem is counting occurrences of **each different element** in a collection, getting a dictionary or list as a histogram result instead of a single integer. For that problem, see [Using a dictionary to count the items in a list](https://stackoverflow.com/questions/3496518).","If you only want a single item's count, use the `count` method:

```
>>> [1, 2, 3, 4, 1, 4, 1].count(1)
3

```

---

#### **Important: this is very slow if you are counting *multiple* different items**

Each `count` call goes over the entire list of `n` elements. Calling `count` in a loop `n` times means `n * n` total checks, which can be catastrophic for performance.

If you want to count multiple items, use [`Counter`](https://stackoverflow.com/a/5829377/365102), which only does `n` total checks."
"Is there a simple way to determine if a variable is a list, dictionary, or something else?","There are two built-in functions that help you identify the type of an object. You can use [`type()`](http://docs.python.org/3/library/functions.html#type) if you need the exact type of an object, and [`isinstance()`](http://docs.python.org/3/library/functions.html#isinstance) to *check* an object’s type against something. Usually, you want to use `isinstance()` most of the times since it is very robust and also supports type inheritance.

---

To get the actual type of an object, you use the built-in [`type()`](http://docs.python.org/3/library/functions.html#type) function. Passing an object as the only parameter will return the type object of that object:

```
>>> type([]) is list
True
>>> type({}) is dict
True
>>> type('') is str
True
>>> type(0) is int
True

```

This of course also works for custom types:

```
>>> class Test1 (object):
        pass
>>> class Test2 (Test1):
        pass
>>> a = Test1()
>>> b = Test2()
>>> type(a) is Test1
True
>>> type(b) is Test2
True

```

Note that `type()` will only return the immediate type of the object, but won’t be able to tell you about type inheritance.

```
>>> type(b) is Test1
False

```

To cover that, you should use the [`isinstance`](http://docs.python.org/3/library/functions.html#isinstance) function. This of course also works for built-in types:

```
>>> isinstance(b, Test1)
True
>>> isinstance(b, Test2)
True
>>> isinstance(a, Test1)
True
>>> isinstance(a, Test2)
False
>>> isinstance([], list)
True
>>> isinstance({}, dict)
True

```

`isinstance()` is usually the preferred way to ensure the type of an object because it will also accept derived types. So unless you actually need the type object (for whatever reason), using `isinstance()` is preferred over `type()`.

The second parameter of `isinstance()` also accepts a tuple of types, so it’s possible to check for multiple types at once. `isinstance` will then return true, if the object is of any of those types:

```
>>> isinstance([], (tuple, list, set))
True

```"
"Does Python have something like an empty string variable where you can do:

```
if myString == string.empty:

```

Regardless, what's the most elegant way to check for empty string values? I find hard coding `""""` every time for checking an empty string not as good.","Empty strings are ""falsy"" ([python 2](http://docs.python.org/2/library/stdtypes.html#truth-value-testing) or [python 3](https://docs.python.org/3/library/stdtypes.html#truth-value-testing) reference), which means they are considered false in a Boolean context, so you can just do this:

```
if not myString:

```

This is the preferred way if you know that your variable is a string. If your variable could also be some other type then you should use:

```
if myString == """":

```

See the documentation on [Truth Value Testing](http://docs.python.org/library/stdtypes.html#truth-value-testing) for other values that are false in Boolean contexts."
"I wanted to compare reading lines of string input from stdin using Python and C++ and was shocked to see my C++ code run an order of magnitude slower than the equivalent Python code. Since my C++ is rusty and I'm not yet an expert Pythonista, please tell me if I'm doing something wrong or if I'm misunderstanding something.

---

(**TLDR answer:** include the statement: `cin.sync_with_stdio(false)` or just use `fgets` instead.

**TLDR results:** scroll all the way down to the bottom of my question and look at the table.)

---

**C++ code:**

```
#include <iostream>
#include <time.h>

using namespace std;

int main() {
    string input_line;
    long line_count = 0;
    time_t start = time(NULL);
    int sec;
    int lps;

    while (cin) {
        getline(cin, input_line);
        if (!cin.eof())
            line_count++;
    };

    sec = (int) time(NULL) - start;
    cerr << ""Read "" << line_count << "" lines in "" << sec << "" seconds."";
    if (sec > 0) {
        lps = line_count / sec;
        cerr << "" LPS: "" << lps << endl;
    } else
        cerr << endl;
    return 0;
}

// Compiled with:
// g++ -O3 -o readline_test_cpp foo.cpp

```

**Python Equivalent:**

```
#!/usr/bin/env python
import time
import sys

count = 0
start = time.time()

for line in  sys.stdin:
    count += 1

delta_sec = int(time.time() - start_time)
if delta_sec >= 0:
    lines_per_sec = int(round(count/delta_sec))
    print(""Read {0} lines in {1} seconds. LPS: {2}"".format(count, delta_sec,
       lines_per_sec))

```

**Here are my results:**

```
$ cat test_lines | ./readline_test_cpp
Read 5570000 lines in 9 seconds. LPS: 618889

$ cat test_lines | ./readline_test.py
Read 5570000 lines in 1 seconds. LPS: 5570000

```

*I should note that I tried this both under Mac OS X v10.6.8 (Snow Leopard) and Linux 2.6.32 (Red Hat Linux 6.2). The former is a MacBook Pro, and the latter is a very beefy server, not that this is too pertinent.*

```
$ for i in {1..5}; do echo ""Test run $i at `date`""; echo -n ""CPP:""; cat test_lines | ./readline_test_cpp ; echo -n ""Python:""; cat test_lines | ./readline_test.py ; done

```

```
Test run 1 at Mon Feb 20 21:29:28 EST 2012
CPP:   Read 5570001 lines in 9 seconds. LPS: 618889
Python:Read 5570000 lines in 1 seconds. LPS: 5570000
Test run 2 at Mon Feb 20 21:29:39 EST 2012
CPP:   Read 5570001 lines in 9 seconds. LPS: 618889
Python:Read 5570000 lines in 1 seconds. LPS: 5570000
Test run 3 at Mon Feb 20 21:29:50 EST 2012
CPP:   Read 5570001 lines in 9 seconds. LPS: 618889
Python:Read 5570000 lines in 1 seconds. LPS: 5570000
Test run 4 at Mon Feb 20 21:30:01 EST 2012
CPP:   Read 5570001 lines in 9 seconds. LPS: 618889
Python:Read 5570000 lines in 1 seconds. LPS: 5570000
Test run 5 at Mon Feb 20 21:30:11 EST 2012
CPP:   Read 5570001 lines in 10 seconds. LPS: 557000
Python:Read 5570000 lines in  1 seconds. LPS: 5570000

```

---

Tiny benchmark addendum and recap

For completeness, I thought I'd update the read speed for the same file on the same box with the original (synced) C++ code. Again, this is for a 100M line file on a fast disk. Here's the comparison, with several solutions/approaches:

| Implementation | Lines per second |
| --- | --- |
| python (default) | 3,571,428 |
| cin (default/naive) | 819,672 |
| cin (no sync) | 12,500,000 |
| fgets | 14,285,714 |
| wc (not fair comparison) | 54,644,808 |","### tl;dr: Because of different default settings in C++ requiring more system calls.

By default, `cin` is synchronized with stdio, which causes it to avoid any input buffering. If you add this to the top of your main, you should see much better performance:

```
std::ios_base::sync_with_stdio(false);

```

Normally, when an input stream is buffered, instead of reading one character at a time, the stream will be read in larger chunks. This reduces the number of system calls, which are typically relatively expensive. However, since the `FILE*` based `stdio` and `iostreams` often have separate implementations and therefore separate buffers, this could lead to a problem if both were used together. For example:

```
int myvalue1;
cin >> myvalue1;
int myvalue2;
scanf(""%d"",&myvalue2);

```

If more input was read by `cin` than it actually needed, then the second integer value wouldn't be available for the `scanf` function, which has its own independent buffer. This would lead to unexpected results.

To avoid this, by default, streams are synchronized with `stdio`. One common way to achieve this is to have `cin` read each character one at a time as needed using `stdio` functions. Unfortunately, this introduces a lot of overhead. For small amounts of input, this isn't a big problem, but when you are reading millions of lines, the performance penalty is significant.

Fortunately, the library designers decided that you should also be able to disable this feature to get improved performance if you knew what you were doing, so they provided the [`sync_with_stdio`](http://en.cppreference.com/w/cpp/io/ios_base/sync_with_stdio) method. From this link (emphasis added):

> If the synchronization is turned off, the C++ standard streams are allowed to buffer their I/O independently, **which may be considerably faster in some cases**."
"I want to measure the time it took to execute a function. I couldn't get `timeit` to work:

```
import timeit
start = timeit.timeit()
print(""hello"")
end = timeit.timeit()
print(end - start)

```","Use [`time.time()`](http://docs.python.org/library/time.html#time.time) to measure the elapsed wall-clock time between two points:

```
import time

start = time.time()
print(""hello"")
end = time.time()
print(end - start)

```

This gives the execution time in seconds.

---

Another option since Python 3.3 might be to use [`perf_counter`](https://docs.python.org/3/library/time.html#time.perf_counter) or [`process_time`](https://docs.python.org/3/library/time.html#time.process_time), depending on your requirements. Before 3.3 it was recommended to use [`time.clock`](http://docs.python.org/library/time.html#time.clock) (thanks [Amber](https://stackoverflow.com/users/148870/amber)). However, it is currently deprecated:

> On Unix, return the current processor time as a floating point number
> expressed in seconds. The precision, and in fact the very definition
> of the meaning of “processor time”, depends on that of the C function
> of the same name.
>
> On Windows, this function returns wall-clock seconds elapsed since the
> first call to this function, as a floating point number, based on the
> Win32 function `QueryPerformanceCounter()`. The resolution is typically
> better than one microsecond.
>
> *Deprecated since version 3.3*: The behaviour of this function depends
> on the platform: **use `perf_counter()` or `process_time()` instead**,
> depending on your requirements, to have a well defined behaviour."
"This has always confused me. It seems like this would be nicer:

```
[""Hello"", ""world""].join(""-"")

```

Than this:

```
""-"".join([""Hello"", ""world""])

```

Is there a specific reason it is like this?","It's because any iterable can be joined (e.g, list, tuple, dict, set), but its contents and the ""joiner"" *must be* strings.

For example:

```
'_'.join(['welcome', 'to', 'stack', 'overflow'])
'_'.join(('welcome', 'to', 'stack', 'overflow'))

```

```
'welcome_to_stack_overflow'

```

Using something other than strings will raise the following error:

> TypeError: sequence item 0: expected str instance, int found"
How do I append to a file instead of overwriting it?,"Set the mode in [`open()`](https://docs.python.org/3/library/functions.html#open) to `""a""` (append) instead of `""w""` (write):

```
with open(""test.txt"", ""a"") as myfile:
    myfile.write(""appended text"")

```

The [documentation](https://docs.python.org/3/library/functions.html#open) lists all the available modes."
"We are working on an [S60](http://en.wikipedia.org/wiki/S60_%28software_platform%29) version and this platform has a nice Python API..

However, there is nothing official about Python on Android, but since [Jython](http://en.wikipedia.org/wiki/Jython) exists, is there a way to let the snake and the robot work together??","One way is to use [Kivy](http://kivy.org/):

> Open source Python library for rapid development of applications
> that make use of innovative user interfaces, such as multi-touch apps.

> Kivy runs on Linux, Windows, OS X, Android and iOS. You can run the same [python] code on all supported platforms.

[Kivy Showcase app](https://play.google.com/store/apps/details?id=org.kivy.showcase)"
"I have some code spread across multiple files that try to `import` from each other, as follows:

main.py:

```
from entity import Ent

```

entity.py:

```
from physics import Physics
class Ent:
    ...

```

physics.py:

```
from entity import Ent
class Physics:
    ...

```

I then run from `main.py` and I get the following error:

```
Traceback (most recent call last):
File ""main.py"", line 2, in <module>
    from entity import Ent
File "".../entity.py"", line 5, in <module>
    from physics import Physics
File "".../physics.py"", line 2, in <module>
    from entity import Ent
ImportError: cannot import name Ent

```

I'm assume the error is due to importing `entity` twice - once in `main.py` and later in `physics.py` - but how can I work around the problem?

---

See also [What happens when using mutual or circular (cyclic) imports in Python?](https://stackoverflow.com/questions/744373) for a general overview of what is allowed and what causes a problem WRT circular imports. See [Why do circular imports seemingly work further up in the call stack but then raise an ImportError further down?](https://stackoverflow.com/questions/22187279) for technical details on **why and how** the problem occurs.",You have circular dependent imports. `physics.py` is imported from `entity` before class `Ent` is defined and `physics` tries to import `entity` that is already initializing. Remove the dependency to `physics` from `entity` module.
"How can I remove the last character of a string if it is a newline?

```
""abc\n""  -->  ""abc""

```","Try the method `rstrip()` (see doc [Python 2](http://docs.python.org/2/library/stdtypes.html#str.rstrip) and [Python 3](https://docs.python.org/3/library/stdtypes.html#str.rstrip))

```
>>> 'test string\n'.rstrip()
'test string'

```

Python's `rstrip()` method strips *all* kinds of trailing whitespace by default, not just one newline as Perl does with [`chomp`](http://perldoc.perl.org/functions/chomp.html).

```
>>> 'test string \n \r\n\n\r \n\n'.rstrip()
'test string'

```

To strip only newlines:

```
>>> 'test string \n \r\n\n\r \n\n'.rstrip('\n')
'test string \n \r\n\n\r '

```

In addition to `rstrip()`, there are also the methods `strip()` and `lstrip()`. Here is an example with the three of them:

```
>>> s = ""   \n\r\n  \n  abc   def \n\r\n  \n  ""
>>> s.strip()
'abc   def'
>>> s.lstrip()
'abc   def \n\r\n  \n  '
>>> s.rstrip()
'   \n\r\n  \n  abc   def'

```"
"Can I define a [static method](https://en.wikipedia.org/wiki/Method_(computer_programming)#Static_methods) which I can call directly on the class instance? e.g.,

```
MyClass.the_static_method()

```","Yep, using the [`staticmethod`](https://docs.python.org/3/library/functions.html#staticmethod ""staticmethod"") decorator:

```
class MyClass(object):
    @staticmethod
    def the_static_method(x):
        print(x)

MyClass.the_static_method(2)  # outputs 2

```

Note that some code might use the old method of defining a static method, using `staticmethod` as a function rather than a decorator. This should only be used if you have to support ancient versions of Python (2.2 and 2.3):

```
class MyClass(object):
    def the_static_method(x):
        print(x)
    the_static_method = staticmethod(the_static_method)

MyClass.the_static_method(2)  # outputs 2

```

This is entirely identical to the first example (using `@staticmethod`), just not using the nice decorator syntax.

Finally, use [`staticmethod`](https://docs.python.org/3/library/functions.html#staticmethod ""staticmethod"") sparingly! There are very few situations where static-methods are necessary in Python, and I've seen them used many times where a separate ""top-level"" function would have been clearer.

---

[The following is verbatim from the documentation:](https://docs.python.org/3/library/functions.html#staticmethod ""staticmethod""):

> A static method does not receive an implicit first argument. To declare a static method, use this idiom:
>
> ```
> class C:
>     @staticmethod
>     def f(arg1, arg2, ...): ...
>
> ```
>
> The @staticmethod form is a function [*decorator*](https://docs.python.org/3/glossary.html#term-decorator ""term-decorator"") â€“ see the description of function definitions in [*Function definitions*](https://docs.python.org/3/reference/compound_stmts.html#function ""Function definitions"") for details.
>
> It can be called either on the class (such as `C.f()`) or on an instance (such as `C().f()`). The instance is ignored except for its class.
>
> Static methods in Python are similar to those found in Java or C++. For a more advanced concept, see [`classmethod()`](https://docs.python.org/3/library/functions.html#classmethod ""classmethod"").
>
> For more information on static methods, consult the documentation on the standard type hierarchy in [*The standard type hierarchy*](https://docs.python.org/3/reference/datamodel.html#types ""types"").
>
> New in version 2.2.
>
> Changed in version 2.4: Function decorator syntax added."
"I am trying to install version 1.2.2 of `MySQL_python`, using a fresh virtualenv created with the `--no-site-packages` option. The current version shown in PyPi is [1.2.3](http://pypi.python.org/pypi/MySQL-python/1.2.3). Is there a way to install the older version? I have tried:

```
pip install MySQL_python==1.2.2

```

However, when installed, it still shows `MySQL_python-1.2.3-py2.6.egg-info` in the site packages. Is this a problem specific to this package, or am I doing something wrong?","**TL;DR**:

**Update as of 2022-12-28**:

`pip install --force-reinstall -v`

For example: `pip install --force-reinstall -v ""MySQL_python==1.2.2""`

What these options mean:

* `--force-reinstall` is an option to reinstall all packages even if they are already up-to-date.
* `-v` is for verbose. You can combine for even more verbosity (i.e. `-vv`) up to 3 times (e.g. `--force-reinstall -vvv`).

Thanks to [@Peter](https://stackoverflow.com/users/968132/peter) for highlighting this (and it seems that the context of the question has broadened given the time when the question was first asked!), [the documentation for Python](https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-I) discusses a caveat with using `-I`, in that it can break your installation if it was installed with a different package manager or if if your package is/was a different version.

---

Original answer:

* `pip install -Iv` (i.e. `pip install -Iv MySQL_python==1.2.2`)

---

What these options mean:

* `-I` stands for `--ignore-installed` which will ignore the installed packages, overwriting them.
* `-v` is for verbose. You can combine for even more verbosity (i.e. `-vv`) up to 3 times (e.g. `-Ivvv`).

For more information, see `pip install --help`

First, I see two issues with what you're trying to do. Since you already have an installed version, you should either uninstall the current existing driver or use `pip install -I MySQL_python==1.2.2`

However, you'll soon find out that this doesn't work. If you look at pip's installation log, or if you do a `pip install -Iv MySQL_python==1.2.2` you'll find that the PyPI URL link does not work for MySQL\_python v1.2.2. You can verify this here: <http://pypi.python.org/pypi/MySQL-python/1.2.2>

The download link 404s and the fallback URL links are re-directing infinitely due to sourceforge.net's recent upgrade and PyPI's stale URL.

So to properly install the driver, you can follow these steps:

```
pip uninstall MySQL_python
pip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download

```"
"I have a very long query. I would like to split it in several lines in Python. A way to do it in JavaScript would be using several sentences and joining them with a `+` operator (I know, maybe it's not the most efficient way to do it, but I'm not really concerned about performance in this stage, just code readability). Example:

```
var long_string = 'some text not important. just garbage to' +
                      'illustrate my example';

```

I tried doing something similar in Python, but it didn't work, so I used `\` to split the long string. However, I'm not sure if this is the only/best/pythonicest way of doing it. It looks awkward.
Actual code:

```
query = 'SELECT action.descr as ""action"", '\
    'role.id as role_id,'\
    'role.descr as role'\
    'FROM '\
    'public.role_action_def,'\
    'public.role,'\
    'public.record_def, '\
    'public.action'\
    'WHERE role.id = role_action_def.role_id AND'\
    'record_def.id = role_action_def.def_id AND'\
    'action.id = role_action_def.action_id AND'\
    'role_action_def.account_id = ' + account_id + ' AND'\
    'record_def.account_id=' + account_id + ' AND'\
    'def_id=' + def_id

```

---

See also: [How can I do a line break (line continuation) in Python (split up a long line of source code)?](https://stackoverflow.com/questions/53162) when the overall line of code is long but doesn't contain a long string literal.","Are you talking about multi-line strings? Easy, use triple quotes to start and end them.

```
s = """""" this is a very
        long string if I had the
        energy to type more and more ...""""""

```

You can use single quotes too (3 of them of course at start and end) and treat the resulting string `s` just like any other string.

**NOTE**: Just as with any string, anything between the starting and ending quotes becomes part of the string, so this example has a leading blank (as pointed out by @root45). This string will also contain both blanks and newlines.

I.e.,:

```
' this is a very\n        long string if I had the\n        energy to type more and more ...'

```

Finally, one can also construct long lines in Python like this:

```
 s = (""this is a very""
      ""long string too""
      ""for sure ...""
     )

```

which will **not** include any extra blanks or newlines (this is a deliberate example showing what the effect of skipping blanks will result in):

```
'this is a verylong string toofor sure ...'

```

No commas required, simply place the strings to be joined together into a pair of parenthesis and be sure to account for any needed blanks and newlines."
How do I pretty-print a JSON file in Python?,"Use the `indent=` parameter of [`json.dump()`](https://docs.python.org/3/library/json.html#json.dump) or [`json.dumps()`](https://docs.python.org/3/library/json.html#json.dumps) to specify how many spaces to indent by:

```
>>> import json
>>> your_json = '[""foo"", {""bar"": [""baz"", null, 1.0, 2]}]'
>>> parsed = json.loads(your_json)
>>> print(json.dumps(parsed, indent=4))
[
    ""foo"",
    {
        ""bar"": [
            ""baz"",
            null,
            1.0,
            2
        ]
    }
]

```

To parse a file, use [`json.load()`](https://docs.python.org/3/library/json.html#json.load):

```
with open('filename.txt', 'r') as handle:
    parsed = json.load(handle)

```"
"How do I read every line of a file in Python and store each line as an element in a list?

I want to read the file line by line and append each line to the end of the list.","This code will read the entire file into memory and remove all whitespace characters (newlines and spaces) from the end of each line:

```
with open(filename) as file:
    lines = [line.rstrip() for line in file]

```

If you're working with a large file, then you should instead read and process it line-by-line:

```
with open(filename) as file:
    for line in file:
        print(line.rstrip())

```

In Python 3.8 and up you can use a while loop with the [walrus operator](https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions) like so:

```
with open(filename) as file:
    while line := file.readline():
        print(line.rstrip())

```

Depending on what you plan to do with your file and how it was encoded, you may also want to manually set the [access mode](https://www.tutorialspoint.com/python/python_files_io.htm) and character encoding:

```
with open(filename, 'r', encoding='UTF-8') as file:
    while line := file.readline():
        print(line.rstrip())

```"
"How do I find out the name of the class used to create an instance of an object in Python?

I'm not sure if I should use the [`inspect`](https://docs.python.org/2/library/inspect.html ""inspect â€” Inspect live objects"") module or parse the `__class__` attribute.","Have you tried the [`__name__` attribute](https://docs.python.org/library/stdtypes.html#definition.__name__) of the class? ie `type(x).__name__` will give you the name of the class, which I think is what you want.

```
>>> import itertools
>>> x = itertools.count(0)
>>> type(x).__name__
'count'

```

If you're still using Python 2, note that the above method works with [new-style classes](https://wiki.python.org/moin/NewClassVsClassicClass) only (in Python 3+ all classes are ""new-style"" classes). Your code might use some old-style classes. The following works for both:

```
x.__class__.__name__

```"
"How do I check if a string represents a numeric value in Python?

```
def is_number(s):
    try:
        float(s)
        return True
    except ValueError:
        return False

```

The above works, but it seems clunky.

---

If what you are testing comes from user input, it is *still* a string *even if it represents* an `int` or a `float`. See [How can I read inputs as numbers?](https://stackoverflow.com/questions/20449427/) for converting the input, and [Asking the user for input until they give a valid response](https://stackoverflow.com/questions/23294658/) for ensuring that the input represents an `int` or `float` (or other requirements) before proceeding.","For non-negative (unsigned) integers only, use [`isdigit()`](https://docs.python.org/3/library/stdtypes.html#str.isdigit ""str.isdigit""):

```
>>> a = ""03523""
>>> a.isdigit()
True
>>> b = ""963spam""
>>> b.isdigit()
False

```

---

Documentation for `isdigit()`: [Python2](https://docs.python.org/2/library/stdtypes.html#str.isdigit), [Python3](https://docs.python.org/3/library/stdtypes.html#str.isdigit)

For Python 2 Unicode strings:
[`isnumeric()`](https://docs.python.org/2/library/stdtypes.html#unicode.isnumeric ""unicode.isnumeric"")."
How do I get the number of rows of a pandas dataframe `df`?,"For a dataframe `df`, one can use any of the following:

* `len(df.index)`
* `df.shape[0]`
* `df[df.columns[0]].count()` (== [number of non-NaN values](https://pandas.pydata.org/docs/reference/api/pandas.Series.count.html) in first column)

[![Performance plot](https://i.sstatic.net/wEzue.png)](https://i.sstatic.net/wEzue.png)

---

Code to reproduce the plot:

```
import numpy as np
import pandas as pd
import perfplot

perfplot.save(
    ""out.png"",
    setup=lambda n: pd.DataFrame(np.arange(n * 3).reshape(n, 3)),
    n_range=[2**k for k in range(25)],
    kernels=[
        lambda df: len(df.index),
        lambda df: df.shape[0],
        lambda df: df[df.columns[0]].count(),
    ],
    labels=[""len(df.index)"", ""df.shape[0]"", ""df[df.columns[0]].count()""],
    xlabel=""Number of rows"",
)

```"
"What do `@classmethod` and `@staticmethod` mean in Python, and how are they different? *When* should I use them, *why* should I use them, and *how* should I use them?

As far as I understand, `@classmethod` tells a class that it's a method which should be inherited into subclasses, or... something. However, what's the point of that? Why not just define the class method without adding `@classmethod` or `@staticmethod` or any `@` definitions?","Though `classmethod` and `staticmethod` are quite similar, there's a slight difference in usage for both entities: `classmethod` must have a reference to a class object as the first parameter, whereas `staticmethod` can have no parameters at all.

Example
-------

```
class Date(object):
    
    def __init__(self, day=0, month=0, year=0):
        self.day = day
        self.month = month
        self.year = year

    @classmethod
    def from_string(cls, date_as_string):
        day, month, year = map(int, date_as_string.split('-'))
        date1 = cls(day, month, year)
        return date1

    @staticmethod
    def is_date_valid(date_as_string):
        day, month, year = map(int, date_as_string.split('-'))
        return day <= 31 and month <= 12 and year <= 3999

date2 = Date.from_string('11-09-2012')
is_date = Date.is_date_valid('11-09-2012')

```

Explanation
-----------

Let's assume an example of a class, dealing with date information (this will be our boilerplate):

```
class Date(object):
    
    def __init__(self, day=0, month=0, year=0):
        self.day = day
        self.month = month
        self.year = year

```

This class obviously could be used to store information about certain dates (without timezone information; let's assume all dates are presented in UTC).

Here we have `__init__`, a typical initializer of Python class instances, which receives arguments as a typical instance method, having the first non-optional argument (`self`) that holds a reference to a newly created instance.

**Class Method**

We have some tasks that can be nicely done using `classmethod`s.

*Let's assume that we want to create a lot of `Date` class instances having date information coming from an outer source encoded as a string with format 'dd-mm-yyyy'. Suppose we have to do this in different places in the source code of our project.*

So what we must do here is:

1. Parse a string to receive day, month and year as three integer variables or a 3-item tuple consisting of that variable.
2. Instantiate `Date` by passing those values to the initialization call.

This will look like:

```
day, month, year = map(int, string_date.split('-'))
date1 = Date(day, month, year)

```

For this purpose, C++ can implement such a feature with overloading, but Python lacks this overloading. Instead, we can use `classmethod`. Let's create another *constructor*.

```
    @classmethod
    def from_string(cls, date_as_string):
        day, month, year = map(int, date_as_string.split('-'))
        date1 = cls(day, month, year)
        return date1

date2 = Date.from_string('11-09-2012')

```

Let's look more carefully at the above implementation, and review what advantages we have here:

1. We've implemented date string parsing in one place and it's reusable now.
2. Encapsulation works fine here (if you think that you could implement string parsing as a single function elsewhere, this solution fits the OOP paradigm far better).
3. `cls` is the **class itself**, not an instance of the class. It's pretty cool because if we inherit our `Date` class, all children will have `from_string` defined also.

**Static method**

What about `staticmethod`? It's pretty similar to `classmethod` but doesn't take any obligatory parameters (like a class method or instance method does).

Let's look at the next use case.

*We have a date string that we want to validate somehow. This task is also logically bound to the `Date` class we've used so far, but doesn't require instantiation of it.*

Here is where `staticmethod` can be useful. Let's look at the next piece of code:

```
    @staticmethod
    def is_date_valid(date_as_string):
        day, month, year = map(int, date_as_string.split('-'))
        return day <= 31 and month <= 12 and year <= 3999

# usage:
is_date = Date.is_date_valid('11-09-2012')

```

So, as we can see from usage of `staticmethod`, we don't have any access to what the class is---it's basically just a function, called syntactically like a method, but without access to the object and its internals (fields and other methods), which `classmethod` does have."
"I have a problem with the transfer of the variable `insurance_mode` by the decorator. I would do it by the following decorator statement:

```
@execute_complete_reservation(True)
def test_booking_gta_object(self):
    self.test_select_gta_object()

```

but unfortunately, this statement does not work. Perhaps maybe there is better way to solve this problem.

```
def execute_complete_reservation(test_case,insurance_mode):
    def inner_function(self,*args,**kwargs):
        self.test_create_qsf_query()
        test_case(self,*args,**kwargs)
        self.test_select_room_option()
        if insurance_mode:
            self.test_accept_insurance_crosseling()
        else:
            self.test_decline_insurance_crosseling()
        self.test_configure_pax_details()
        self.test_configure_payer_details

    return inner_function

```","The syntax for decorators with arguments is a bit different - the decorator with arguments should return a function that will *take a function* and return another function. So it should really return a normal decorator. A bit confusing, right? What I mean is:

```
def decorator_factory(argument):
    def decorator(function):
        def wrapper(*args, **kwargs):
            funny_stuff()
            something_with_argument(argument)
            result = function(*args, **kwargs)
            more_funny_stuff()
            return result
        return wrapper
    return decorator

```

[Here](https://www.artima.com/weblogs/viewpost.jsp?thread=240845#decorator-functions-with-decorator-arguments) you can read more on the subject - it's also possible to implement this using callable objects and that is also explained there.

Usage:

```
@decorator_factory(""Some argument"")
def function_to_be_decorated(args):
    print(f""Do something with '{args}'."")

```

`decorator_factory(""Some argument"")` uses the given argument to create a standard, argumentless decorator. So the following block is functionally identical to the one above:

```
created_decorator = decorator_factory(""Some argument"")

@created_decorator
def function_to_be_decorated(args):
    print(f""Do something with '{args}'."")

```"
"How do I check if an object is of a given type, or if it inherits from a given type?

How do I check if the object `o` is of type `str`?

---

**Editor's note**: Beginners often wrongly expect a string to already be ""a number"" â€“ either expecting Python 3.x `input` to convert type, or expecting that a string like `'1'` is *also simultaneously* an integer. This question does not address those types of questions. Instead, see [How do I check if a string represents a number (float or int)?](https://stackoverflow.com/questions/354038), [How can I read inputs as numbers?](https://stackoverflow.com/questions/20449427/) and/or [Asking the user for input until they give a valid response](https://stackoverflow.com/questions/23294658/) as appropriate.","Use [`isinstance`](https://docs.python.org/library/functions.html#isinstance) to check if `o` is an instance of `str` or any subclass of `str`:

```
if isinstance(o, str):

```

To check if the type of `o` is exactly `str`, *excluding subclasses of `str`*:

```
if type(o) is str:

```

See [Built-in Functions](http://docs.python.org/library/functions.html) in the Python Library Reference for relevant information.

---

#### Checking for strings in Python 2

For Python 2, this is a better way to check if `o` is a string:

```
if isinstance(o, basestring):

```

because this will also catch Unicode strings. [`unicode`](https://docs.python.org/2/library/functions.html#unicode) is not a subclass of `str`; both `str` and `unicode` are subclasses of [`basestring`](https://docs.python.org/2/library/functions.html#basestring). In Python 3, `basestring` no longer exists since there's [a strict separation](https://docs.python.org/whatsnew/3.0.html#text-vs-data-instead-of-unicode-vs-8-bit) of strings ([`str`](https://docs.python.org/3/library/functions.html#func-str)) and binary data ([`bytes`](https://docs.python.org/3/library/functions.html#func-bytes)).

Alternatively, `isinstance` accepts a tuple of classes. This will return `True` if `o` is an instance of any subclass of any of `(str, unicode)`:

```
if isinstance(o, (str, unicode)):

```"
"What's the proper way to declare custom exception classes in modern Python? My primary goal is to follow whatever standard other exception classes have, so that (for instance) any extra string I include in the exception is printed out by whatever tool caught the exception.

By ""modern Python"" I mean something that will run in Python 2.5 but be 'correct' for the Python 2.6 and Python 3.\* way of doing things. And by ""custom"" I mean an `Exception` object that can include extra data about the cause of the error: a string, maybe also some other arbitrary object relevant to the exception.

I was tripped up by the following deprecation warning in Python 2.6.2:

```
>>> class MyError(Exception):
...     def __init__(self, message):
...         self.message = message
... 
>>> MyError(""foo"")
_sandbox.py:3: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6

```

It seems crazy that `BaseException` has a special meaning for attributes named `message`. I gather from [PEP-352](http://www.python.org/dev/peps/pep-0352/) that attribute did have a special meaning in 2.5 they're trying to deprecate away, so I guess that name (and that one alone) is now forbidden? Ugh.

I'm also fuzzily aware that `Exception` has some magic parameter `args`, but I've never known how to use it. Nor am I sure it's the right way to do things going forward; a lot of the discussion I found online suggested they were trying to do away with args in Python 3.

Update: two answers have suggested overriding `__init__`, and `__str__`/`__unicode__`/`__repr__`. That seems like a lot of typing, is it necessary?","Maybe I missed the question, but why not:

```
class MyException(Exception):
    pass

```

To override something (or pass extra args), do this:

```
class ValidationError(Exception):
    def __init__(self, message, errors):            
        # Call the base class constructor with the parameters it needs
        super().__init__(message)
            
        # Now for your custom code...
        self.errors = errors

```

That way you could pass dict of error messages to the second param, and get to it later with `e.errors`.

In Python 2, you have to use this slightly more complex form of `super()`:

```
super(ValidationError, self).__init__(message)

```"
"Here is the problem:

I have a *requirements.txt* file that looks like:

```
BeautifulSoup==3.2.0
Django==1.3
Fabric==1.2.0
Jinja2==2.5.5
PyYAML==3.09
Pygments==1.4
SQLAlchemy==0.7.1
South==0.7.3
amqplib==0.6.1
anyjson==0.3
...

```

I have a local archive directory containing all the packages + others.

I have created a new [virtualenv](http://pypi.python.org/pypi/virtualenv) with

```
bin/virtualenv testing

```

Upon activating it, I tried to install the packages according to *requirements.txt* from the local archive directory.

```
source bin/activate
pip install -r /path/to/requirements.txt -f file:///path/to/archive/

```

I got some output that seems to indicate that the installation is fine:

```
Downloading/unpacking Fabric==1.2.0 (from -r ../testing/requirements.txt (line 3))
  Running setup.py egg_info for package Fabric
    warning: no previously-included files matching '*' found under directory 'docs/_build'
    warning: no files found matching 'fabfile.py'
Downloading/unpacking South==0.7.3 (from -r ../testing/requirements.txt (line 8))
  Running setup.py egg_info for package South
....

```

But a later check revealed that none of the packages are installed properly. I cannot import the packages, and none are found in the site-packages directory of my virtualenv. So what went wrong?","This works for everyone:

```
pip install -r /path/to/requirements.txt

```

**Explanation:**

> -r, --requirement < filename >

Install from the given requirements file. This option can be used multiple times."
"How can I generate random integers between `0` and `9` (inclusive) in Python?

For example, `0`, `1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`","Try [`random.randrange`](https://docs.python.org/3/library/random.html#random.randrange):

```
from random import randrange
print(randrange(10))

```"
"I am trying to build a shared library using a C extension file but first I have to generate the output file using the command below:

```
gcc -Wall utilsmodule.c -o Utilc

```

After executing the command, I get this error message:

```
> utilsmodule.c:1:20: fatal error: Python.h: No such file or directory
compilation terminated.

```

I have tried all the suggested solutions over the internet but the problem still exists. I have no problem with `Python.h`. I managed to locate the file on my machine.","Looks like you haven't properly installed the header files and static libraries for python dev. Use your package manager to install them system-wide.

For `apt` (**Ubuntu, Debian...**):

```
sudo apt-get install python-dev   # for python2.x installs
sudo apt-get install python3-dev  # for python3.x installs

```

For `yum` (**CentOS, RHEL...**):

```
sudo yum install python-devel    # for python2.x installs
sudo yum install python3-devel   # for python3.x installs

```

For `dnf` (**Fedora...**):

```
sudo dnf install python2-devel  # for python2.x installs
sudo dnf install python3-devel  # for python3.x installs

```

For `zypper` (**openSUSE...**):

```
sudo zypper in python-devel   # for python2.x installs
sudo zypper in python3-devel  # for python3.x installs

```

For `apk` (**Alpine...**):

```
# This is a departure from the normal Alpine naming
# scheme, which uses py2- and py3- prefixes
sudo apk add python2-dev  # for python2.x installs
sudo apk add python3-dev  # for python3.x installs

```

For `apt-cyg` (**Cygwin...**):

```
apt-cyg install python-devel   # for python2.x installs
apt-cyg install python3-devel  # for python3.x installs

```

***Important Note**:* python3-dev/devel does not automatically cover all minor versions of python3.  
E.g If you are using python 3.11 you may need to install `python3.11-dev` / `python3.11-devel`."
What do single and double leading underscores before an object's name represent in Python?,"Single Underscore
-----------------

In a class, names with a leading underscore indicate to other programmers that the attribute or method is intended to be be used inside that class. However, privacy is not *enforced* in any way.
Using leading underscores for functions in a module indicates it should not be imported from somewhere else.

From the [PEP-8](http://www.python.org/dev/peps/pep-0008/) style guide:

> `_single_leading_underscore`: weak ""internal use"" indicator. E.g. `from M import *` does not import objects whose name starts with an underscore.

Double Underscore (Name Mangling)
---------------------------------

From [the Python docs](https://docs.python.org/3/tutorial/classes.html#private-variables):

> Any identifier of the form `__spam` (at least two leading underscores, at most one trailing underscore) is textually replaced with `_classname__spam`, where `classname` is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, so it can be used to define class-private instance and class variables, methods, variables stored in globals, and even variables stored in instances. private to this class on instances of other classes.

And a warning from the same page:

> Name mangling is intended to give classes an easy way to define “private” instance variables and methods, without having to worry about instance variables defined by derived classes, or mucking with instance variables by code outside the class. Note that the mangling rules are designed mostly to avoid accidents; *it still is possible for a determined soul to access or modify a variable that is considered private.*

Example
-------

```
>>> class MyClass():
...     def __init__(self):
...             self.__superprivate = ""Hello""
...             self._semiprivate = "", world!""
...
>>> mc = MyClass()
>>> print mc.__superprivate
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: myClass instance has no attribute '__superprivate'
>>> print mc._semiprivate
, world!
>>> print mc.__dict__
{'_MyClass__superprivate': 'Hello', '_semiprivate': ', world!'}

```"
"I've been here:

* *[PEP 328 â€“ Imports: Multi-Line and Absolute/Relative](http://www.python.org/dev/peps/pep-0328/)*
* *[Modules, Packages](http://docs.python.org/2/tutorial/modules.html#packages)*
* [Python packages: relative imports](https://stackoverflow.com/questions/10059002/python-packages-relative-imports)
* [Python relative import example code does not work](https://stackoverflow.com/questions/9123062/python-relative-import-example-code-does-not-work)
* [Relative imports in Python 2.5](https://stackoverflow.com/questions/8299270/ultimate-answer-to-relative-python-imports)
* [Relative imports in Python](https://stackoverflow.com/questions/4175534/relative-imports-in-python)
* [Python: Disabling relative import](https://stackoverflow.com/questions/13233931/python-disabling-relative-import)

and plenty of URLs that I did not copy, some on SO, some on other sites, back when I thought I'd have the solution quickly.

The forever-recurring question is this: how do I solve this ""Attempted relative import in non-package"" message?

> ImportError: attempted relative import with no known parent package

I built an exact replica of the package on pep-0328:

```
package/
    __init__.py
    subpackage1/
        __init__.py
        moduleX.py
        moduleY.py
    subpackage2/
        __init__.py
        moduleZ.py
    moduleA.py

```

The imports were done from the console.

I did make functions named spam and eggs in their appropriate modules. Naturally, it didn't work. The answer is apparently in the 4th URL I listed, but it's all alumni to me. There was this response on one of the URLs I visited:

> Relative imports use a module's name attribute to determine that module's position in the package hierarchy. If the module's name does not contain any package information (e.g. it is set to 'main') then relative imports are resolved as if the module were a top level module, regardless of where the module is actually located on the file system.

The above response looks promising, but it's all hieroglyphs to me. How do I make Python not return to me ""Attempted relative import in non-package""? It has an answer that involves `-m`, supposedly.

Why does Python give that error message? What does by ""non-package"" mean? Why and how do you define a 'package'?","**Script vs. Module**

Here's an explanation. The short version is that there is a big difference between directly running a Python file, and importing that file from somewhere else. **Just knowing what directory a file is in does not determine what package Python thinks it is in.** That depends, additionally, on how you load the file into Python (by running or by importing).

There are two ways to load a Python file: as the top-level script, or as a
module. A file is loaded as the top-level script if you execute it directly, for instance by typing `python myfile.py` on the command line. It is loaded as a module when an `import` statement is encountered inside some other file. There can only be one top-level script at a time; the top-level script is the Python file you ran to start things off.

**Naming**

When a file is loaded, it is given a name (which is stored in its `__name__` attribute). If it was loaded as the top-level script, its name is `__main__`. If it was loaded as a module, its name is the filename, preceded by the names of any packages/subpackages of which it is a part, separated by dots.

So for instance in your example:

```
package/
    __init__.py
    subpackage1/
        __init__.py
        moduleX.py
    moduleA.py

```

if you imported `moduleX` (note: *imported*, not directly executed), its name would be `package.subpackage1.moduleX`. If you imported `moduleA`, its name would be `package.moduleA`. However, if you *directly run* `moduleX` from the command line, its name will instead be `__main__`, and if you directly run `moduleA` from the command line, its name will be `__main__`. When a module is run as the top-level script, it loses its normal name and its name is instead `__main__`.

**Accessing a module NOT through its containing package**

There is an additional wrinkle: the module's name depends on whether it was imported ""directly"" from the directory it is in or imported via a package. This only makes a difference if you run Python in a directory, and try to import a file in that same directory (or a subdirectory of it). For instance, if you start the Python interpreter in the directory `package/subpackage1` and then do `import moduleX`, the name of `moduleX` will just be `moduleX`, and not `package.subpackage1.moduleX`. This is because Python adds the current directory to its search path when the interpreter is entered interactively; if it finds the to-be-imported module in the current directory, it will not know that that directory is part of a package, and the package information will not become part of the module's name.

A special case is if you run the interpreter interactively (e.g., just type `python` and start entering Python code on the fly). In this case, the name of that interactive session is `__main__`.

Now here is the crucial thing for your error message: **if a module's name has no dots, it is not considered to be part of a package**. It doesn't matter where the file actually is on disk. All that matters is what its name is, and its name depends on how you loaded it.

Now look at the quote you included in your question:

> Relative imports use a module's name attribute to determine that module's position in the package hierarchy. If the module's name does not contain any package information (e.g. it is set to 'main') then relative imports are resolved as if the module were a top-level module, regardless of where the module is actually located on the file system.

**Relative imports...**

Relative imports use the module's *name* to determine where it is in a package. When you use a relative import like `from .. import foo`, the dots indicate to step up some number of levels in the package hierarchy. For instance, if your current module's name is `package.subpackage1.moduleX`, then `..moduleA` would mean `package.moduleA`. For a `from .. import` to work, the module's name must have at least as many dots as there are in the `import` statement.

**... are only relative in a package**

However, if your module's name is `__main__`, it is not considered to be in a package. Its name has no dots, and therefore you cannot use `from .. import` statements inside it. If you try to do so, you will get the ""relative-import in non-package"" error.

**Scripts can't import relative**

What you probably did is you tried to run `moduleX` or the like from the command line. When you did this, its name was set to `__main__`, which means that relative imports within it will fail, because its name does not reveal that it is in a package. Note that this will also happen if you run Python from the same directory where a module is, and then try to import that module, because, as described above, Python will find the module in the current directory ""too early"" without realizing it is part of a package.

Also remember that when you run the interactive interpreter, the ""name"" of that interactive session is always `__main__`. Thus **you cannot do relative imports directly from an interactive session**. Relative imports are only for use within module files.

**Two solutions:**

1. If you really do want to run `moduleX` directly, but you still want it to be considered part of a package, you can do `python -m package.subpackage1.moduleX`. The `-m` tells Python to load it as a module, not as the top-level script.
2. Or perhaps you don't actually want to *run* `moduleX`, you just want to run some other script, say `myfile.py`, that *uses* functions inside `moduleX`. If that is the case, put `myfile.py` *somewhere else* – *not* inside the `package` directory – and run it. If inside `myfile.py` you do things like `from package.moduleA import spam`, it will work fine.

**Notes**

* For either of these solutions, the package directory (`package` in your example) must be accessible from the Python module search path (`sys.path`). If it is not, you will not be able to use anything in the package reliably at all.
* Since Python 2.6, the module's ""name"" for package-resolution purposes is determined not just by its `__name__` attributes but also by the `__package__` attribute. That's why I'm avoiding using the explicit symbol `__name__` to refer to the module's ""name"". Since Python 2.6 a module's ""name"" is effectively `__package__ + '.' + __name__`, or just `__name__` if `__package__` is `None`.)"
Is there a function to extract the extension from a filename?,"Use [`os.path.splitext`](https://docs.python.org/3/library/os.path.html#os.path.splitext):

```
>>> import os
>>> filename, file_extension = os.path.splitext('/path/to/somefile.ext')
>>> filename
'/path/to/somefile'
>>> file_extension
'.ext'

```

Unlike most manual string-splitting attempts, `os.path.splitext` will correctly treat `/a/b.c/d` as having no extension instead of having extension `.c/d`, and it will treat `.bashrc` as having no extension instead of having extension `.bashrc`:

```
>>> os.path.splitext('/a/b.c/d')
('/a/b.c/d', '')
>>> os.path.splitext('.bashrc')
('.bashrc', '')

```"
"How do I write JSON data stored in the dictionary `data` to a file?

```
f = open('data.json', 'wb')
f.write(data)

```

This gives the error:

> TypeError: must be string or buffer, not dict","`data` is a Python dictionary. It needs to be encoded as JSON before writing.

Use this for maximum compatibility (Python 2 and 3):

```
import json
with open('data.json', 'w') as f:
    json.dump(data, f)

```

On a modern system (i.e. Python 3 and UTF-8 support), you can write a nicer file using:

```
import json
with open('data.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

```

See [`json`](https://docs.python.org/3/library/json.html) documentation."
"How do I load a Python module given its full path?

Note that the file can be anywhere in the filesystem where the user has access rights.

---

**See also:** [How to import a module given its name as string?](https://stackoverflow.com/questions/301134)","Let's have `MyClass` in `module.name` module defined at `/path/to/file.py`. Below is how we import `MyClass` from this module

For Python 3.5+ use ([docs](https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly)):

```
import importlib.util
import sys
spec = importlib.util.spec_from_file_location(""module.name"", ""/path/to/file.py"")
foo = importlib.util.module_from_spec(spec)
sys.modules[""module.name""] = foo
spec.loader.exec_module(foo)
foo.MyClass()

```

For Python 3.3 and 3.4 use:

```
from importlib.machinery import SourceFileLoader

foo = SourceFileLoader(""module.name"", ""/path/to/file.py"").load_module()
foo.MyClass()

```

(Although this has been deprecated in Python 3.4.)

For Python 2 use:

```
import imp

foo = imp.load_source('module.name', '/path/to/file.py')
foo.MyClass()

```

There are equivalent convenience functions for compiled Python files and DLLs.

See also <http://bugs.python.org/issue21436>."
How do I call an external command within Python as if I had typed it in a shell or command prompt?,"Use [`subprocess.run`](https://docs.python.org/library/subprocess.html#subprocess.run):

```
import subprocess

subprocess.run([""ls"", ""-l""]) 

```

Another common way is [`os.system`](https://docs.python.org/library/os.html#os.system) but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also `subprocess.run` is generally more flexible (you can get the [`stdout`](https://docs.python.org/library/subprocess.html#subprocess.CompletedProcess.stdout), [`stderr`](https://docs.python.org/library/subprocess.html#subprocess.CompletedProcess.stderr), the [""real"" status code](https://docs.python.org/library/subprocess.html#subprocess.CompletedProcess.returncode), better [error handling](https://docs.python.org/library/subprocess.html#subprocess.CalledProcessError), etc.). Even the [documentation for `os.system`](https://docs.python.org/library/os.html#os.system) recommends using `subprocess` instead.

On Python 3.4 and earlier, use `subprocess.call` instead of `.run`:

```
subprocess.call([""ls"", ""-l""])

```"
"Given the following code (that doesn't work):

```
while True:
    # Snip: print out current state
    while True:
        ok = get_input(""Is this ok? (y/n)"")
        if ok.lower() == ""y"": break 2 # This doesn't work :(
        if ok.lower() == ""n"": break

    # Do more processing with menus and stuff

```

Is there a way to make this work? Or do I have do one check to break out of the input loop, then another, more limited, check in the outside loop to break out all together if the user is satisfied?",My first instinct would be to refactor the nested loop into a function and use `return` to break out.
"I'm having troubles with installing packages in Python 3.

I have always installed packages with `setup.py install`. But now, when I try to install the ansicolors package I get:

```
importerror ""No Module named Setuptools""

```

I have no idea what to do because I didn't have setuptools installed in the past. Still, I was able to install many packages with `setup.py install` without setuptools. Why should I get setuptools now?

I can't even install setuptools because I have Python 3.3 and setuptools doesn't support Python 3.

Why doesn't my install command work anymore?","Your `setup.py` file needs `setuptools`. Some Python packages used to use `distutils` for distribution, but most now use `setuptools`, a more complete package. [Here](https://stackoverflow.com/questions/6344076/differences-between-distribute-distutils-and-setuptools) is a question about the differences between them.

To install `setuptools` on Debian:

```
sudo apt-get install python3-setuptools

```

For an older version of Python (Python 2.x):

```
sudo apt-get install python-setuptools

```"
"I am trying to make a scatter plot and annotate data points with different numbers from a list.
So, for example, I want to plot `y` vs `x` and annotate with corresponding numbers from `n`.

```
y = [2.56422, 3.77284, 3.52623, 3.51468, 3.02199]
x = [0.15, 0.3, 0.45, 0.6, 0.75]
n = [58, 651, 393, 203, 123]
ax = fig.add_subplot(111)
ax1.scatter(z, y, fmt='o')

```

Any ideas?","I'm not aware of any plotting method which takes arrays or lists but you could use `annotate()` while iterating over the values in `n`.

```
import matplotlib.pyplot as plt
x = [0.15, 0.3, 0.45, 0.6, 0.75]
y = [2.56422, 3.77284, 3.52623, 3.51468, 3.02199]
n = [58, 651, 393, 203, 123]

fig, ax = plt.subplots()
ax.scatter(x, y)

for i, txt in enumerate(n):
    ax.annotate(txt, (x[i], y[i]))

```

There are a lot of formatting options for `annotate()`, see the [matplotlib website:](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.annotate.html)

![enter image description here](https://i.sstatic.net/6g4Et.png)"
"```
import numpy as np
y = np.array(((1,2,3),(4,5,6),(7,8,9)))
OUTPUT:
print(y.flatten())
[1   2   3   4   5   6   7   8   9]
print(y.ravel())
[1   2   3   4   5   6   7   8   9]

```

Both function return the same list.
Then what is the need of two different functions performing same job.","The current API is that:

* [`flatten`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html) always returns a copy.
* [`ravel`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html) returns a contiguous view of the original array whenever possible. This isn't visible in the printed output, but if you modify the array returned by ravel, it may modify the entries in the original array. If you modify the entries in an array returned from flatten this will never happen. ravel will often be faster since no memory is copied, but you have to be more careful about modifying the array it returns.
* [`reshape((-1,))`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) gets a view whenever the strides of the array allow it even if that means you don't always get a contiguous array."
"I'm looking at using the **\*.ipynb** files as the source of truth and programmatically 'compiling' them into .py files for scheduled jobs/tasks.

The only way I understand to do this is via the GUI. Is there a way to do it via command line?","If you don't want to output a Python script every time you save, or you don't want to restart the IPython kernel:

On the **command line**, you can use [`nbconvert`](http://nbconvert.readthedocs.io/en/stable/usage.html):

```
$ jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb

```

As a bit of a hack, **you can even call the above command *in* an IPython notebook** by pre-pending `!` (used for any command line argument). Inside a notebook:

```
!jupyter nbconvert --to script config_template.ipynb

```

Before `--to script` was [added](https://github.com/ipython/ipython/commit/b28ec91e43c9ec34f8a09124706189a72be6652d), the option was `--to python` or `--to=python`, but it was [renamed](https://github.com/ipython/ipython/commit/b98d516ba65c) in the move toward a language-agnostic notebook system."
"In Python, how do I read in a binary file and loop over each byte of that file?","Python >= 3.8
-------------

Thanks to the [walrus operator (`:=`)](https://peps.python.org/pep-0572/) the solution is quite short. We read `bytes` objects from the file and assign them to the variable `byte`

```
with open(""myfile"", ""rb"") as f:
    while (byte := f.read(1)):
        # Do stuff with byte.

```

Python >= 3
-----------

In older Python 3 versions, we get have to use a slightly more verbose way:

```
with open(""myfile"", ""rb"") as f:
    byte = f.read(1)
    while byte != b"""":
        # Do stuff with byte.
        byte = f.read(1)

```

Or as benhoyt says, skip the not equal and take advantage of the fact that `b""""` evaluates to false. This makes the code compatible between 2.6 and 3.x without any changes. It would also save you from changing the condition if you go from byte mode to text or the reverse.

```
with open(""myfile"", ""rb"") as f:
    byte = f.read(1)
    while byte:
        # Do stuff with byte.
        byte = f.read(1)

```

Python >= 2.5
-------------

In Python 2, it's a bit different. Here we don't get bytes objects, but raw characters:

```
with open(""myfile"", ""rb"") as f:
    byte = f.read(1)
    while byte != """":
        # Do stuff with byte.
        byte = f.read(1)

```

Note that the with statement is not available in versions of Python below 2.5. To use it in v 2.5 you'll need to import it:

```
from __future__ import with_statement

```

In 2.6 this is not needed.

Python 2.4 and Earlier
----------------------

```
f = open(""myfile"", ""rb"")
try:
    byte = f.read(1)
    while byte != """":
        # Do stuff with byte.
        byte = f.read(1)
finally:
    f.close()

```"
"I'm wondering if there's any way to tell pip, specifically in a requirements file, to install a package with both a minimum version (`pip install package>=0.2`) and a maximum version which should never be installed (theoretical api: `pip install package<0.3`).

I ask because I am using a third party library that's in active development. I'd like my pip requirements file to specify that it should always install the most recent minor release of the 0.5.x branch, but I don't want pip to ever try to install any newer major versions (like 0.6.x) since the API is different. This is important because even though the 0.6.x branch is available, the devs are still releasing patches and bugfixes to the 0.5.x branch, so I don't want to use a static `package==0.5.9` line in my requirements file.

Is there any way to do that?","You can do:

```
$ pip install ""package>=0.2,<0.3""

```

And `pip` will look for the best match, assuming the version is at least 0.2, and less than 0.3.

This also applies to pip [requirements files](https://pip.readthedocs.io/en/stable/reference/pip_install/#requirements-file-format). See the full details on version specifiers in [PEP 440](https://www.python.org/dev/peps/pep-0440/#version-specifiers)."
"I want to create an empty array and append items to it, one at a time.

```
xs = []
for item in data:
    xs.append(item)

```

Can I use this list-style notation with [NumPy](http://en.wikipedia.org/wiki/NumPy) arrays?","That is the wrong mental model for using NumPy efficiently. NumPy arrays are stored in contiguous blocks of memory. To append rows or columns to an existing array, the entire array needs to be copied to a new block of memory, creating gaps for the new elements to be stored. This is very inefficient if done repeatedly.

Instead of appending rows, allocate a suitably sized array, and then assign to it row-by-row:

```
>>> import numpy as np

>>> a = np.zeros(shape=(3, 2))
>>> a
array([[ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.]])

>>> a[0] = [1, 2]
>>> a[1] = [3, 4]
>>> a[2] = [5, 6]

>>> a
array([[ 1.,  2.],
       [ 3.,  4.],
       [ 5.,  6.]])

```"
"I'm launching a subprocess with the following command:

```
p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)

```

However, when I try to kill using:

```
p.terminate()

```

or

```
p.kill()

```

The command keeps running in the background, so I was wondering how can I actually terminate the process.

Note that when I run the command with:

```
p = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)

```

It does terminate successfully when issuing the `p.terminate()`.","Use a [process group](http://en.wikipedia.org/wiki/Process_group) so as to enable sending a signal to all the process in the groups. For that, you should attach a [session id](http://pubs.opengroup.org/onlinepubs/009695399/functions/setsid.html) to the parent process of the spawned/child processes, which is a shell in your case. This will make it the group leader of the processes. So now, when a signal is sent to the process group leader, it's transmitted to all of the child processes of this group.

Here's the code:

```
import os
import signal
import subprocess

# The os.setsid() is passed in the argument preexec_fn so
# it's run after the fork() and before  exec() to run the shell.
pro = subprocess.Popen(cmd, stdout=subprocess.PIPE, 
                       shell=True, preexec_fn=os.setsid) 

os.killpg(os.getpgid(pro.pid), signal.SIGTERM)  # Send the signal to all the process groups

```"
"```
...
soup = BeautifulSoup(html, ""lxml"")
File ""/Library/Python/2.7/site-packages/bs4/__init__.py"", line 152, in __init__
% "","".join(features))
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

```

The above outputs on my Terminal. I am on Mac OS 10.7.x. I have Python 2.7.1, and followed [this tutorial](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/) to get Beautiful Soup and lxml, which both installed successfully and work with a separate test file [located here](http://web.engr.illinois.edu/%7Eshauk2/testWebScrape.py). In the Python script that causes this error, I have included this line:
`from pageCrawler import comparePages`
And in the pageCrawler file I have included the following two lines:
`from bs4 import BeautifulSoup`
`from urllib2 import urlopen`

How can this problem be solved?","I have a suspicion that this is related to the parser that BS will use to read the HTML. They [document is here](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser), but if you're like me (on OSX) you might be stuck with something that requires a bit of work:

You'll notice that in the BS4 documentation page above, they point out that by default BS4 will use the Python built-in HTML parser. Assuming you are in OSX, the Apple-bundled version of Python is 2.7.2 which is not lenient for character formatting. I hit this same problem, so I upgraded my version of Python to work around it. Doing this in a virtualenv will minimize disruption to other projects.

If doing that sounds like a pain, you can switch over to the LXML parser:

```
pip install lxml

```

And then try:

```
soup = BeautifulSoup(html, ""lxml"")

```

Depending on your scenario, that might be good enough. I found this annoying enough to warrant upgrading my version of Python. Using virtualenv, [you can migrate your packages](https://stackoverflow.com/questions/2170252/can-existing-virtualenv-be-upgraded-gracefully) fairly easily."
"I want to completely remove Python 2.7 from my Mac OS X 10.6.4. I managed to remove the entry from the `PATH` variable by reverting my `.bash_profile`. But I also want to remove all directories, files, symlinks, and entries that got installed by the Python 2.7 install package. I've got the install package from <http://www.python.org/>. What directories/files/configuration file entries do I need to remove? Is there a list somewhere?","### Do not attempt to remove any Apple-supplied system Python which are in `/System/Library` and `/usr/bin`, as this may break your whole operating system.

---

**NOTE:** *The steps listed below do **not** affect the Apple-supplied Python 2.7; they only remove a third-party Python framework, like those installed by [python.org installers](https://www.python.org/downloads/).*

---

The complete list is [documented here](http://bugs.python.org/issue7107). Basically, all you need to do is the following:

1. Remove the third-party Python 2.7 framework

   ```
    sudo rm -rf /Library/Frameworks/Python.framework/Versions/2.7

   ```
2. Remove the Python 2.7 applications directory

   ```
    sudo rm -rf ""/Applications/Python 2.7""

   ```
3. Remove the symbolic links, in `/usr/local/bin`, that point to this Python version. See them using

   ```
    ls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/2.7' 

   ```

and then run the following command to remove all the links:

```
    cd /usr/local/bin/
    ls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/2.7' | awk '{print $9}' | tr -d @ | xargs rm

```

4. If necessary, edit your shell profile file(s) to remove adding `/Library/Frameworks/Python.framework/Versions/2.7` to your `PATH` environment file. Depending on which shell you use, any of the following files may have been modified:
   `~/.bash_login`, `~/.bash_profile`, `~/.cshrc`, `~/.profile`, `~/.tcshrc`, `~/.zshrc`, and/or `~/.zprofile`."
"Suppose I have a function and a dataframe defined as below:

```
def get_sublist(sta, end):
    return mylist[sta:end+1]

df = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})
mylist = ['a','b','c','d','e','f']

```

Now I want to apply `get_sublist` to `df`'s two columns `'col_1', 'col_2'` to element-wise calculate a new column `'col_3'` to get an output that looks like:

```
  ID  col_1  col_2            col_3
0  1      0      1       ['a', 'b']
1  2      2      4  ['c', 'd', 'e']
2  3      3      5  ['d', 'e', 'f']

```

I tried:

```
df['col_3'] = df[['col_1','col_2']].apply(get_sublist, axis=1)

```

but this results in the following: error:

> TypeError: get\_sublist() missing 1 required positional argument:

How do I do this?","There is a clean, one-line way of doing this in Pandas:

```
df['col_3'] = df.apply(lambda x: f(x.col_1, x.col_2), axis=1)

```

This allows `f` to be a user-defined function with multiple input values, and uses (safe) column names rather than (unsafe) numeric indices to access the columns.

Example with data (based on original question):

```
import pandas as pd

df = pd.DataFrame({'ID':['1', '2', '3'], 'col_1': [0, 2, 3], 'col_2':[1, 4, 5]})
mylist = ['a', 'b', 'c', 'd', 'e', 'f']

def get_sublist(sta,end):
    return mylist[sta:end+1]

df['col_3'] = df.apply(lambda x: get_sublist(x.col_1, x.col_2), axis=1)

```

Output of `print(df)`:

```
  ID  col_1  col_2      col_3
0  1      0      1     [a, b]
1  2      2      4  [c, d, e]
2  3      3      5  [d, e, f]

```

If your column names contain spaces or share a name with an existing dataframe attribute, you can index with square brackets:

```
df['col_3'] = df.apply(lambda x: f(x['col 1'], x['col 2']), axis=1)

```"
"I'm having difficulty installing lxml with easy\_install on Ubuntu 11.

When I type `$ easy_install lxml` I get:

```
Searching for lxml
Reading http://pypi.python.org/simple/lxml/
Reading http://codespeak.net/lxml
Best match: lxml 2.3
Downloading http://lxml.de/files/lxml-2.3.tgz
Processing lxml-2.3.tgz
Running lxml-2.3/setup.py -q bdist_egg --dist-dir /tmp/easy_install-7UdQOZ/lxml-2.3/egg-dist-tmp-GacQGy
Building lxml version 2.3.
Building without Cython.
ERROR: /bin/sh: xslt-config: not found

** make sure the development packages of libxml2 and libxslt are installed **

Using build configuration of libxslt 
In file included from src/lxml/lxml.etree.c:227:0:
src/lxml/etree_defs.h:9:31: fatal error: libxml/xmlversion.h: No such file or directory
compilation terminated.

```

It seems that `libxslt` or `libxml2` is not installed. I've tried following the instructions at <http://www.techsww.com/tutorials/libraries/libxslt/installation/installing_libxslt_on_ubuntu_linux.php> and <http://www.techsww.com/tutorials/libraries/libxml/installation/installing_libxml_on_ubuntu_linux.php> with no success.

If I try `wget ftp://xmlsoft.org/libxml2/libxml2-sources-2.6.27.tar.gz` I get

```
<successful connection info>
==> SYST ... done.    ==> PWD ... done.
==> TYPE I ... done.  ==> CWD (1) /libxml2 ... done.
==> SIZE libxml2-sources-2.6.27.tar.gz ... done.
==> PASV ... done.    ==> RETR libxml2-sources-2.6.27.tar.gz ... 
No such file `libxml2-sources-2.6.27.tar.gz'.

```

If I try the other first, I'll get to `./configure --prefix=/usr/local/libxslt --with-libxml-prefix=/usr/local/libxml2` and that will fail eventually with:

```
checking for libxml libraries >= 2.6.27... configure: error: Could not find libxml2 anywhere, check ftp://xmlsoft.org/.

```

I've tried both versions `2.6.27` and `2.6.29` of `libxml2` with no difference.

Leaving no stone unturned, I have successfully done `sudo apt-get install libxml2-dev`, but this changes nothing.","Since you're on Ubuntu, don't bother with those source packages. Just install those development packages using apt-get.

```
apt-get install libxml2-dev libxslt1-dev python-dev

```

If you're happy with a possibly older version of lxml altogether though, you could try

```
apt-get install python-lxml

```

and be done with it. :)"
"By default, the [Requests](https://requests.readthedocs.io/) python library writes log messages to the console, along the lines of:

```
Starting new HTTP connection (1): example.com
http://example.com:80 ""GET / HTTP/1.1"" 200 606

```

I'm usually not interested in these messages, and would like to disable them. What would be the best way to silence those messages or decrease Requests' verbosity?","I found out how to configure *requests*'s logging level, it's done via the standard [logging](http://docs.python.org/library/logging.html) module. I decided to configure it to not log messages unless they are at least warnings:

```
import logging

logging.getLogger(""requests"").setLevel(logging.WARNING)

```

If you wish to apply this setting for the urllib3 library (typically used by requests) too, add the following:

```
logging.getLogger(""urllib3"").setLevel(logging.WARNING)

```"
"Which is better to use for timing in Python? time.clock() or time.time()? Which one provides more accuracy?

for example:

```
start = time.clock()
... do something
elapsed = (time.clock() - start)

```

vs.

```
start = time.time()
... do something
elapsed = (time.time() - start)

```","As of 3.3, [*time.clock()* is deprecated](https://docs.python.org/3/library/time.html#time.clock), and it's suggested to use **[time.process\_time()](https://docs.python.org/3/library/time.html#time.process_time)** or **[time.perf\_counter()](https://docs.python.org/3/library/time.html#time.perf_counter)** instead.

Previously in 2.7, according to the **[time module docs](https://docs.python.org/2.7/library/time.html#time.clock)**:

> **time.clock()**
>
> On Unix, return the current processor time as a floating point number
> expressed in seconds. The precision, and in fact the very definition
> of the meaning of “processor time”, depends on that of the C function
> of the same name, but in any case, **this is the function to use for
> benchmarking Python or timing algorithms.**
>
> On Windows, this function returns wall-clock seconds elapsed since the
> first call to this function, as a floating point number, based on the
> Win32 function QueryPerformanceCounter(). The resolution is typically
> better than one microsecond.

Additionally, there is the [timeit](https://docs.python.org/2/library/timeit.html) module for benchmarking code snippets."
"I am using Python 3.1 on a Windows 7 machine. Russian is the default system language, and utf-8 is the default encoding.

Looking at the answer to a [previous question](https://stackoverflow.com/questions/778096/problem-opening-a-text-document-unicode-error), I have attempting using the ""codecs"" module to give me a little luck. Here's a few examples:

```
>>> g = codecs.open(""C:\Users\Eric\Desktop\beeline.txt"", ""r"", encoding=""utf-8"")
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-4: truncated \UXXXXXXXX escape (<pyshell#39>, line 1)

```

```
>>> g = codecs.open(""C:\Users\Eric\Desktop\Site.txt"", ""r"", encoding=""utf-8"")
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-4: truncated \UXXXXXXXX escape (<pyshell#40>, line 1)

```

```
>>> g = codecs.open(""C:\Python31\Notes.txt"", ""r"", encoding=""utf-8"")
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 11-12: malformed \N character escape (<pyshell#41>, line 1)

```

```
>>> g = codecs.open(""C:\Users\Eric\Desktop\Site.txt"", ""r"", encoding=""utf-8"")
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-4: truncated \UXXXXXXXX escape (<pyshell#44>, line 1)

```

My last idea was, I thought it might have been the fact that Windows ""translates"" a few folders, such as the ""users"" folder, into Russian (though typing ""users"" is still the correct path), so I tried it in the Python31 folder. Still, no luck. Any ideas?","The problem is with the string

```
""C:\Users\Eric\Desktop\beeline.txt""

```

Here, `\U` in `""C:\Users`... starts an eight-character Unicode escape, such as `\U00014321`. In your code, the escape is followed by the character 's', which is invalid.

You either need to duplicate all backslashes:

```
""C:\\Users\\Eric\\Desktop\\beeline.txt""

```

Or prefix the string with `r` (to produce a raw string):

```
r""C:\Users\Eric\Desktop\beeline.txt""

```"
"I am walking a directory that contains eggs to add those eggs to the `sys.path`. If there are two versions of the same .egg in the directory, I want to add only the latest one.

I have a regular expression `r""^(?P<eggName>\w+)-(?P<eggVersion>[\d\.]+)-.+\.egg$` to extract the name and version from the filename. The problem is comparing the version number, which is a string like `2.3.1`.

Since I'm comparing strings, 2 sorts above 10, but that's not correct for versions.

```
>>> ""2.3.1"" > ""10.1.1""
True

```

I could do some splitting, parsing, casting to int, etc., and I would eventually get a workaround. But this is Python, [not Java](https://stackoverflow.com/questions/6701948/efficient-way-to-compare-version-strings-in-java). Is there an elegant way to compare version strings?","Use [`packaging.version.Version`](https://packaging.pypa.io/en/latest/version.html#packaging.version.Version) which supports [PEP 440](https://www.python.org/dev/peps/pep-0440/) style ordering of version strings.

```
>>> # pip install packaging
>>> from packaging.version import Version
>>> Version(""2.3.1"") < Version(""10.1.2"")
True
>>> Version(""1.3.a4"") < Version(""10.1.2"")
True

```

---

An ancient and [now deprecated](https://www.python.org/dev/peps/pep-0632/) method you might encounter is [`distutils.version`](https://docs.python.org/3.10/distutils/apiref.html#module-distutils.version), it's undocumented and conforms only to the superseded [PEP 386](https://www.python.org/dev/peps/pep-0386/);

```
>>> from distutils.version import LooseVersion, StrictVersion
>>> LooseVersion(""2.3.1"") < LooseVersion(""10.1.2"")
True
>>> StrictVersion(""2.3.1"") < StrictVersion(""10.1.2"")
True
>>> StrictVersion(""1.3.a4"")
Traceback (most recent call last):
...
ValueError: invalid version number '1.3.a4'

```

As you can see it sees valid PEP 440 versions as “not strict” and therefore doesn’t match modern Python’s notion of what a valid version is.

As `distutils.version` is undocumented, [here](https://github.com/python/cpython/blob/3.10/Lib/distutils/version.py) are the relevant docstrings."
"Two options in setup.py `develop` and `install` are confusing me. According to this [site](http://www.siafoo.net/article/77#install-vs-develop), using `develop` creates a special link to site-packages directory.

People have suggested that I use `python setup.py install` for a fresh installation and `python setup.py develop` after any changes have been made to the setup file.

Can anyone shed some light on the usage of these commands?","`python setup.py install` is used to install (typically third party) packages that you're not going to develop/modify/debug yourself.

For your own stuff, you want to first install your package and then be able to frequently edit the code *without* having to re-install the package every time — and that is exactly what `python setup.py develop` does: it installs the package (typically just a source folder) in a way that allows you to conveniently edit your code after it’s installed to the (virtual) environment, and have the changes take effect immediately.

---

**Note:** It is highly recommended to use `pip install .` (regular install) and `pip install -e .` (developer install) to install packages, as invoking `setup.py` directly will do the wrong things for many dependencies, such as pull prereleases and incompatible package versions, or make the package hard to uninstall with `pip`.

Update:

The `develop` counterpart for the latest `python -m build` approach is as follows ([as per](https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/#working-in-development-mode)):

[![enter image description here](https://i.sstatic.net/2Tl4q.jpg)](https://i.sstatic.net/2Tl4q.jpg)"
"Python has a singleton called [`NotImplemented`](https://docs.python.org/2/library/constants.html#NotImplemented).

Why would someone want to ever return `NotImplemented` instead of raising the [`NotImplementedError`](https://docs.python.org/2/library/exceptions.html#exceptions.NotImplementedError) exception? Won't it just make it harder to find bugs, such as code that executes invalid methods?","It's because `__lt__()` and related comparison methods are quite commonly used indirectly in list sorts and such. Sometimes the algorithm will choose to try another way or pick a default winner. Raising an exception would break out of the sort unless caught, whereas `NotImplemented` doesn't get raised and can be used in further tests.

<http://jcalderone.livejournal.com/32837.html>

To summarise that link:

> ""`NotImplemented` signals to the runtime that it should ask someone else to satisfy the operation. In the expression `a == b`, if `a.__eq__(b)` returns `NotImplemented`, then Python tries `b.__eq__(a)`. If `b` knows enough to return `True` or `False`, then the expression can succeed. If it doesn't, then the runtime will fall back to the built-in behavior (which is based on identity for `==` and `!=`)."""
"I have installed tensorflow in my ubuntu 16.04 using the second answer [here](https://devtalk.nvidia.com/default/topic/936429/-solved-tensorflow-with-gpu-in-anaconda-env-ubuntu-16-04-cuda-7-5-cudnn-/) with ubuntu's builtin apt cuda installation.

Now my question is how can I test if tensorflow is really using gpu? I have a gtx 960m gpu. When I `import tensorflow` this is the output

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally

```

Is this output enough to check if tensorflow is using gpu ?","No, I don't think ""open CUDA library"" is enough to tell, because different nodes of the graph may be on different devices.

When using tensorflow2:

```
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

```

For tensorflow1, to find out which device is used, you can enable log device placement like this:

```
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

```

Check your console for this type of output.

[![](https://i.sstatic.net/RtRiB.png)](https://i.sstatic.net/RtRiB.png)"
"I have a `requirements.txt` file that I'm using with Travis-CI. It seems silly to duplicate the requirements in both `requirements.txt` and `setup.py`, so I was hoping to pass a file handle to the `install_requires` kwarg in `setuptools.setup`.

Is this possible? If so, how should I go about doing it?

Here is my `requirements.txt` file:

```
guessit>=0.5.2
tvdb_api>=1.8.2
hachoir-metadata>=1.3.3
hachoir-core>=1.3.3
hachoir-parser>=1.3.4

```","On the face of it, it does seem that `requirements.txt` and `setup.py` are silly duplicates, but it's important to understand that while the form is similar, the intended function is very different.

The goal of a package author, when specifying dependencies, is to say ""wherever you install this package, these are the other packages you need, in order for this package to work.""

In contrast, the deployment author (which may be the same person at a different time) has a different job, in that they say ""here's the list of packages that we've gathered together and tested and that I now need to install"".

The package author writes for a wide variety of scenarios, because they're putting their work out there to be used in ways they may not know about, and have no way of knowing what packages will be installed alongside their package. In order to be a good neighbor and avoid dependency version conflicts with other packages, they need to specify as wide a range of dependency versions as can possibly work. This is what `install_requires` in `setup.py` does.

The deployment author writes for a very different, very specific goal: a single instance of an installed application or service, installed on a particular computer. In order to precisely control a deployment, and be sure that the right packages are tested and deployed, the deployment author must specify the exact version and source-location of every package to be installed, including dependencies and dependency's dependencies. With this spec, a deployment can be repeatably applied to several machines, or tested on a test machine, and the deployment author can be confident that the same packages are deployed every time. This is what a `requirements.txt` does.

So you can see that, while they both look like a big list of packages and versions, these two things have very different jobs. And it's definitely easy to mix this up and get it wrong! But the right way to think about this is that `requirements.txt` is an ""answer"" to the ""question"" posed by the requirements in all the various `setup.py` package files. Rather than write it by hand, it's often generated by telling pip to look at all the `setup.py` files in a set of desired packages, find a set of packages that it thinks fits all the requirements, and then, after they're installed, ""freeze"" that list of packages into a text file (this is where the `pip freeze` name comes from).

So the takeaway:

* `setup.py` should declare the loosest possible dependency versions that are still workable. Its job is to say what a particular package can work with.
* `requirements.txt` is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package. Its job is to declare an exhaustive list of all the necessary packages to make a deployment work.
* Because these two things have such different content and reasons for existing, it's not feasible to simply copy one into the other.

### References:

* [install\_requires vs Requirements files](https://packaging.python.org/requirements/) from the Python packaging user guide."
"I'm trying to disable warning C0321 (""more than one statement on a single line"" -- I often put `if` statements with short single-line results on the same line), in Pylint 0.21.1 (if it matters: astng 0.20.1, common 0.50.3, and Python 2.6.6 (r266:84292, Sep 15 2010, 16:22:56)).

I've tried adding `disable=C0321` in the Pylint configuration file, but Pylint insists on reporting it anyway. Variations on that line (like `disable=0321` or `disable=C321`) are flagged as errors, so Pylint *does* recognize the option properly. It's just ignoring it.

Is this a Pylint bug, or am I doing something wrong? Is there a way around this?

I'd really like to get rid of some of this noise.","Starting from Pylint v. 0.25.3, you can use the symbolic names for disabling warnings [instead of having to remember all those code numbers](https://docs.pylint.org/faq.html#do-i-have-to-remember-all-these-numbers). E.g.:

```
# pylint: disable=locally-disabled, multiple-statements, fixme, line-too-long

```

This style is more instructive than cryptic error codes, and also more practical since newer versions of Pylint only output the symbolic name, not the error code.

A disable comment can be inserted on its own line, applying the disable to everything that comes after in the same block. Alternatively, it can be inserted at the end of the line for which it is meant to apply.

If Pylint outputs ""`Locally disabling`"" messages, you can get rid of them by including the disable `locally-disabled` *first* as in the example above."
"What's the difference between:

```
class Child(SomeBaseClass):
    def __init__(self):
        super(Child, self).__init__()

```

and:

```
class Child(SomeBaseClass):
    def __init__(self):
        SomeBaseClass.__init__(self)

```

I've seen `super` being used quite a lot in classes with only single inheritance. I can see why you'd use it in multiple inheritance but am unclear as to what the advantages are of using it in this kind of situation.

---

This question is about technical implementation details and the distinction between different ways of accessing the base class `__init__` method. To close duplicate questions where OP is simply missing a `super` call and is asking why base class attributes aren't available, please use [Why don't my subclass instances contain the attributes from the base class (causing an AttributeError when I try to use them)?](https://stackoverflow.com/questions/10268603) instead.","What's the difference?
----------------------

```
SomeBaseClass.__init__(self) 

```

means to call `SomeBaseClass`'s `__init__`. while

```
super().__init__()

```

means to call a bound `__init__` from the parent class that follows `SomeBaseClass`'s child class (the one that defines this method) in the instance's Method Resolution Order (MRO).

If the instance is a subclass of *this* child class, there may be a different parent that comes next in the MRO.

### Explained simply

When you write a class, you want other classes to be able to use it. `super()` makes it easier for other classes to use the class you're writing.

As Bob Martin says, a good architecture allows you to postpone decision making as long as possible.

`super()` can enable that sort of architecture.

When another class subclasses the class you wrote, it could also be inheriting from other classes. And those classes could have an `__init__` that comes after this `__init__` based on the ordering of the classes for method resolution.

Without `super` you would likely hard-code the parent of the class you're writing (like the example does). This would mean that you would not call the next `__init__` in the MRO, and you would thus not get to reuse the code in it.

If you're writing your own code for personal use, you may not care about this distinction. But if you want others to use your code, using `super` is one thing that allows greater flexibility for users of the code.

### Python 2 versus 3

This works in Python 2 and 3:

```
super(Child, self).__init__()

```

This only works in Python 3:

```
super().__init__()

```

It works with no arguments by moving up in the stack frame and getting the first argument to the method (usually `self` for an instance method or `cls` for a class method - but could be other names) and finding the class (e.g. `Child`) in the free variables (it is looked up with the name `__class__` as a free closure variable in the method).

I used to prefer to demonstrate the cross-compatible way of using `super`, but now that Python 2 is largely deprecated, I will demonstrate the Python 3 way of doing things, that is, calling `super` with no arguments.

### Indirection with Forward Compatibility

What does it give you? For single inheritance, the examples from the question are practically identical from a static analysis point of view. However, using `super` gives you a layer of indirection with forward compatibility.

Forward compatibility is very important to seasoned developers. You want your code to keep working with minimal changes as you change it. When you look at your revision history, you want to see precisely what changed when.

You may start off with single inheritance, but if you decide to add another base class, you only have to change the line with the bases - if the bases change in a class you inherit from (say a mixin is added) you'd change nothing in this class.

In Python 2, getting the arguments to `super` and the correct method arguments right can be a little confusing, so I suggest using the Python 3 only method of calling it.

If you know you're using `super` correctly with single inheritance, that makes debugging less difficult going forward.

### Dependency Injection

Other people can use your code and inject parents into the method resolution:

```
class SomeBaseClass(object):
    def __init__(self):
        print('SomeBaseClass.__init__(self) called')
    
class UnsuperChild(SomeBaseClass):
    def __init__(self):
        print('UnsuperChild.__init__(self) called')
        SomeBaseClass.__init__(self)
    
class SuperChild(SomeBaseClass):
    def __init__(self):
        print('SuperChild.__init__(self) called')
        super().__init__()

```

Say you add another class to your object, and want to inject a class between Foo and Bar (for testing or some other reason):

```
class InjectMe(SomeBaseClass):
    def __init__(self):
        print('InjectMe.__init__(self) called')
        super().__init__()

class UnsuperInjector(UnsuperChild, InjectMe): pass

class SuperInjector(SuperChild, InjectMe): pass

```

Using the un-super child fails to inject the dependency because the child you're using has hard-coded the method to be called after its own:

```
>>> o = UnsuperInjector()
UnsuperChild.__init__(self) called
SomeBaseClass.__init__(self) called

```

However, the class with the child that uses `super` can correctly inject the dependency:

```
>>> o2 = SuperInjector()
SuperChild.__init__(self) called
InjectMe.__init__(self) called
SomeBaseClass.__init__(self) called

```

### Addressing a comment

> Why in the world would this be useful?

Python linearizes a complicated inheritance tree via the [C3 linearization algorithm](https://docs.python.org/3/howto/mro.html) to create a Method Resolution Order (MRO).

We want methods to be looked up *in that order*.

For a method defined in a parent to find the next one in that order without `super`, it would have to

1. get the mro from the instance's type
2. look for the type that defines the method
3. find the next type with the method
4. bind that method and call it with the expected arguments

> The `UnsuperChild` should not have access to `InjectMe`. Why isn't the conclusion ""Always avoid using `super`""? What am I missing here?

The `UnsuperChild` does *not* have access to `InjectMe`. It is the `UnsuperInjector` that has access to `InjectMe` - and yet cannot call that class's method from the method it inherits from `UnsuperChild`.

Both Child classes intend to call a method by the same name that comes next in the MRO, which might be *another* class it was not aware of when it was created.

The one without `super` hard-codes its parent's method - thus is has restricted the behavior of its method, and subclasses cannot inject functionality in the call chain.

The one *with* `super` has greater flexibility. The call chain for the methods can be intercepted and functionality injected.

You may not need that functionality, but subclassers of your code may.

Conclusion
----------

Always use `super` to reference the parent class instead of hard-coding it.

What you intend is to reference the parent class that is next-in-line, not specifically the one you see the child inheriting from.

Not using `super` can put unnecessary constraints on users of your code."
"I am using Python 3.5.1. I read the document and the package section here: <https://docs.python.org/3/tutorial/modules.html#packages>

Now, I have the following structure:

```
/home/wujek/Playground/a/b/module.py

```

`module.py`:

```
class Foo:
    def __init__(self):
        print('initializing Foo')

```

Now, while in `/home/wujek/Playground`:

```
~/Playground $ python3
>>> import a.b.module
>>> a.b.module.Foo()
initializing Foo
<a.b.module.Foo object at 0x100a8f0b8>

```

Similarly, now in `home`, superfolder of `Playground`:

```
~ $ PYTHONPATH=Playground python3
>>> import a.b.module
>>> a.b.module.Foo()
initializing Foo
<a.b.module.Foo object at 0x10a5fee10>

```

Actually, I can do all kinds of stuff:

```
~ $ PYTHONPATH=Playground python3
>>> import a
>>> import a.b
>>> import Playground.a.b

```

Why does this work? I though there needed to be `__init__.py` files (empty ones would work) in both `a` and `b` for `module.py` to be importable when the Python path points to the `Playground` folder?

This seems to have changed from Python 2.7:

```
~ $ PYTHONPATH=Playground python
>>> import a
ImportError: No module named a
>>> import a.b
ImportError: No module named a.b
>>> import a.b.module
ImportError: No module named a.b.module

```

With `__init__.py` in both `~/Playground/a` and `~/Playground/a/b` it works fine.","Overview
========

@Mike's answer is correct but **too imprecise**. It is true that Python 3.3+ supports *Implicit Namespace Packages* that allows it to create a package without an `__init__.py` file. This is called a **namespace package** in contrast to a **regular package** which does have an `__init__.py` file (empty or not empty).

However, creating a **namespace package** should ONLY be done if there is a need for it. For most use cases and developers out there, this doesn't apply so you should stick with **EMPTY `__init__.py`** files regardless.

Namespace package use case
==========================

To demonstrate the difference between the two types of python packages, lets look at the following example:

```
google_pubsub/              <- Package 1
    google/                 <- Namespace package (there is no __init__.py)
        cloud/              <- Namespace package (there is no __init__.py)
            pubsub/         <- Regular package (with __init__.py)
                __init__.py <- Required to make the package a regular package
                foo.py

google_storage/             <- Package 2
    google/                 <- Namespace package (there is no __init__.py)
        cloud/              <- Namespace package (there is no __init__.py)
            storage/        <- Regular package (with __init__.py)
                __init__.py <- Required to make the package a regular package
                bar.py

```

`google_pubsub` and `google_storage` are separate packages but they share the same namespace `google/cloud`. In order to share the same namespace, it is required to make each directory of the common path a namespace package, i.e. `google/` and `cloud/`. **This should be the only use case for creating namespace packages, otherwise, there is no need for it.**

It's crucial that there are no `__init__py` files in the `google` and `google/cloud` directories so that both directories can be interpreted as **namespace packages**. [In Python 3.3+ any directory on the `sys.path` with a name that matches the package name being looked for will be recognized as contributing modules and subpackages to that package](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-missing-init-py-trap). As a result, when you import both from `google_pubsub` and `google_storage`, the Python interpreter will be able to find them.

This is different from **regular packages** which are self-contained meaning all parts live in the same directory hierarchy. When importing a package and the Python interpreter encounters a subdirectory on the `sys.path` with an `__init__.py` file, then it will create a single directory package containing only modules from that directory, rather than finding all appropriately named subdirectories outside that directory. **This is perfectly fine for packages that don't want to share a namespace**. I highly recommend taking a look at [Traps for the Unwary in Python’s Import System](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html) to get a better understanding of how Python importing behaves with regular and namespace package and what `__init__.py` traps to watch out for.

Summary
=======

* Only skip `__init__.py` files if you want to create **namespace packages**. Only create namespace packages if you have different libraries that reside in different locations and you want them each to contribute a subpackage to the parent package, i.e. the namespace package.
* Keep on adding empty `__init__.py` to your directories because 99% of the time you just want to create **regular packages**. Also, Python tools out there such as `mypy` and `pytest` require empty `__init__.py` files to interpret the code structure accordingly. This can lead to weird errors if not done with care.

Resources
=========

My answer only touches the surface of how **regular packages** and **namespace packages** work, so take a look at the following resources for further information:

* [PEP 420 -- Implicit Namespace Packages](https://www.python.org/dev/peps/pep-0420/)
* [The import system - Regular packages](https://docs.python.org/3/reference/import.html#regular-packages)
* [The import system - Namespace packages](https://docs.python.org/3/reference/import.html#namespace-packages)
* [Traps for the Unwary in Python’s Import System](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html)"
"What it the difference between running two commands:

```
foo = FooModel()

```

and

```
bar = BarModel.objects.create()

```

Does the second one immediately create a `BarModel` in the database, while for `FooModel`, the `save()` method has to be called explicitly to add it to the database?","<https://docs.djangoproject.com/en/stable/topics/db/queries/#creating-objects>

> To create and save an object in a single step, use the `create()` method."
"I want to create a dynamic object in Python and then add attributes to it. This didn't work:

```
obj = object()
obj.somefield = ""somevalue""

```

> AttributeError: 'object' object has no attribute 'somefield'

---

For details on **why** it doesn't work, see [Can't set attributes on instance of ""object"" class](https://stackoverflow.com/questions/1529002/).","The built-in `object` can be instantiated but can't have any attributes set on it. (I wish it could, for this exact purpose.) This is because it doesn't have a `__dict__` to hold the attributes.

---

I generally just do this:

```
class Object(object):
    pass

obj = Object()
obj.somefield = ""somevalue""

```

But consider giving the `Object` class a more meaningful name, depending on what data it holds.

---

Another possibility is to use a sub-class of `dict` that allows attribute access to get at the keys:

```
class AttrDict(dict):
    def __getattr__(self, key):
        return self[key]

    def __setattr__(self, key, value):
        self[key] = value

obj = AttrDict()
obj.somefield = ""somevalue""

```

---

To instantiate the object attributes using a dictionary:

```
d = {""a"": 1, ""b"": 2, ""c"": 3}

for k, v in d.items():
    setattr(obj, k, v)

```"
"I have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs (sheets) in that large file. One of the tabs has a ton of data and the other is just a few square cells.

When I use [`pd.read_excel()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.read_excel.html) on *any* worksheet, it looks to me like the whole file is loaded (not just the worksheet I'm interested in). So when I use the method twice (once for each sheet), I effectively have to suffer the whole workbook being read in twice (even though we're only using the specified sheet).

How do I only load specific sheet(s) with `pd.read_excel()`?","Try [`pd.ExcelFile`](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excelfile-class):

```
xls = pd.ExcelFile('path_to_file.xls')
df1 = pd.read_excel(xls, 'Sheet1')
df2 = pd.read_excel(xls, 'Sheet2')

```

As noted by @HaPsantran, the entire Excel file is read in during the `ExcelFile()` call (there doesn't appear to be a way around this). This merely saves you from having to read the same file in each time you want to access a new sheet.

Note that the `sheet_name` argument to `pd.read_excel()` can be the name of the sheet (as above), an integer specifying the sheet number (eg 0, 1, etc), a list of sheet names or indices, or `None`. If a list is provided, it returns a dictionary where the keys are the sheet names/indices and the values are the data frames. The default is to simply return the first sheet (ie, `sheet_name=0`).

If `None` is specified, **all** sheets are returned, as a `{sheet_name:dataframe}` dictionary."
"I need the last 9 numbers of a list and I'm sure there is a way to do it with slicing, but I can't seem to get it. I can get the first 9 like this:

```
num_list[0:9]

```","You can use negative integers with the slicing operator for that. Here's an example using the python CLI interpreter:

```
>>> a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
>>> a[-9:]
[4, 5, 6, 7, 8, 9, 10, 11, 12]

```

the important line is `a[-9:]`"
"I am trying to get a list of files in a directory using Python, but I do not want a list of ALL the files.

What I essentially want is the ability to do something like the following but using Python and not executing ls.

```
ls 145592*.jpg

```

If there is no built-in method for this, I am currently thinking of writing a for loop to iterate through the results of an `os.listdir()` and to append all the matching files to a new list.

However, there are a lot of files in that directory and therefore I am hoping there is a more efficient method (or a built-in method).","```
import glob

jpgFilenamesList = glob.glob('145592*.jpg')

```

See [`glob`](http://docs.python.org/library/glob.html#glob.glob) in python documenttion"
"I have the following code

```
test = ""have it break.""
selectiveEscape = ""Print percent % in sentence and not %s"" % test

print(selectiveEscape)

```

I would like to get the output:

```
Print percent % in sentence and not have it break.

```

What actually happens:

```
    selectiveEscape = ""Use percent % in sentence and not %s"" % test
TypeError: %d format: a number is required, not str

```","```
>>> test = ""have it break.""
>>> selectiveEscape = ""Print percent %% in sentence and not %s"" % test
>>> print selectiveEscape
Print percent % in sentence and not have it break.

```"
"I believe that running an external command with a slightly modified environment is a very common case. That's how I tend to do it:

```
import subprocess, os
my_env = os.environ
my_env[""PATH""] = ""/usr/sbin:/sbin:"" + my_env[""PATH""]
subprocess.Popen(my_command, env=my_env)

```

I've got a gut feeling that there's a better way; does it look alright?","I think `os.environ.copy()` is better if you don't intend to modify the os.environ for the current process:

```
import subprocess, os
my_env = os.environ.copy()
my_env[""PATH""] = f""/usr/sbin:/sbin:{my_env['PATH']}""
subprocess.Popen(my_command, env=my_env)

```"
"What are the advantages and disadvantages of each?

From what I've seen, either one can work as a replacement for the other if need be, so should I bother using both or should I stick to just one of them?

Will the style of the program influence my choice? I am doing some machine learning using numpy, so there are indeed lots of matrices, but also lots of vectors (arrays).","Numpy **matrices** are strictly 2-dimensional, while numpy **arrays** (ndarrays) are
N-dimensional. Matrix objects are a subclass of ndarray, so they inherit all
the attributes and methods of ndarrays.

The main advantage of numpy matrices is that they provide a convenient notation
for matrix multiplication: if a and b are matrices, then `a*b` is their matrix
product.

```
import numpy as np

a = np.mat('4 3; 2 1')
b = np.mat('1 2; 3 4')
print(a)
# [[4 3]
#  [2 1]]
print(b)
# [[1 2]
#  [3 4]]
print(a*b)
# [[13 20]
#  [ 5  8]]

```

On the other hand, as of Python 3.5, NumPy supports infix matrix multiplication using the `@` operator, so you can achieve the same convenience of matrix multiplication with ndarrays in Python >= 3.5.

```
import numpy as np

a = np.array([[4, 3], [2, 1]])
b = np.array([[1, 2], [3, 4]])
print(a@b)
# [[13 20]
#  [ 5  8]]

```

Both matrix objects and ndarrays have `.T` to return the transpose, but matrix
objects also have `.H` for the conjugate transpose, and `.I` for the inverse.

In contrast, numpy arrays consistently abide by the rule that operations are
applied element-wise (except for the new `@` operator). Thus, if `a` and `b` are numpy arrays, then `a*b` is the array
formed by multiplying the components element-wise:

```
c = np.array([[4, 3], [2, 1]])
d = np.array([[1, 2], [3, 4]])
print(c*d)
# [[4 6]
#  [6 4]]

```

To obtain the result of matrix multiplication, you use `np.dot` (or `@` in Python >= 3.5, as shown above):

```
print(np.dot(c,d))
# [[13 20]
#  [ 5  8]]

```

The `**` operator also behaves differently:

```
print(a**2)
# [[22 15]
#  [10  7]]
print(c**2)
# [[16  9]
#  [ 4  1]]

```

Since `a` is a matrix, `a**2` returns the matrix product `a*a`.
Since `c` is an ndarray, `c**2` returns an ndarray with each component squared
element-wise.

There are other technical differences between matrix objects and ndarrays
(having to do with `np.ravel`, item selection and sequence behavior).

**The main advantage of numpy arrays is that they are more general than
2-dimensional matrices**. What happens when you want a 3-dimensional array? Then
you have to use an ndarray, not a matrix object. Thus, learning to use matrix
objects is more work -- you have to learn matrix object operations, and
ndarray operations.

Writing a program that mixes both matrices and arrays makes your life difficult
because you have to keep track of what type of object your variables are, lest
multiplication return something you don't expect.

**In contrast, if you stick solely with ndarrays, then you can do everything
matrix objects can do, and more, except with slightly different
functions/notation.**

If you are willing to give up the visual appeal of NumPy matrix product
notation (which can be achieved almost as elegantly with ndarrays in Python >= 3.5), then I think NumPy arrays are definitely the way to go.

PS. Of course, you really don't have to choose one at the expense of the other,
since `np.asmatrix` and `np.asarray` allow you to convert one to the other (as
long as the array is 2-dimensional).

---

There is a synopsis of the differences between NumPy `arrays` vs NumPy `matrix`es [here](https://www.numpy.org/devdocs/user/numpy-for-matlab-users.html#array-or-matrix-which-should-i-use)."
"I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be `keys` and the elements of other columns in the same row be `values`.

DataFrame:

```
    ID   A   B   C
0   p    1   3   2
1   q    4   3   2
2   r    4   0   9 

```

Output should be like this:

```
{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}

```","The [`to_dict()`](http://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.to_dict.html) method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.

`to_dict()` also accepts an 'orient' argument which you'll need in order to output a *list* of values for each column. Otherwise, a dictionary of the form `{index: value}` will be returned for each column.

These steps can be done with the following line:

```
>>> df.set_index('ID').T.to_dict('list')
{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}

```

---

In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame:

```
>>> df = pd.DataFrame({'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})
>>> df
        a      b
0     red  0.500
1  yellow  0.250
2    blue  0.125

```

Then the options are as follows.

**dict** - the default: column names are keys, values are dictionaries of index:data pairs

```
>>> df.to_dict('dict')
{'a': {0: 'red', 1: 'yellow', 2: 'blue'}, 
 'b': {0: 0.5, 1: 0.25, 2: 0.125}}

```

**list** - keys are column names, values are lists of column data

```
>>> df.to_dict('list')
{'a': ['red', 'yellow', 'blue'], 
 'b': [0.5, 0.25, 0.125]}

```

**series** - like 'list', but values are Series

```
>>> df.to_dict('series')
{'a': 0       red
      1    yellow
      2      blue
      Name: a, dtype: object, 

 'b': 0    0.500
      1    0.250
      2    0.125
      Name: b, dtype: float64}

```

**split** - splits columns/data/index as keys with values being column names, data values by row and index labels respectively

```
>>> df.to_dict('split')
{'columns': ['a', 'b'],
 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],
 'index': [0, 1, 2]}

```

**records** - each row becomes a dictionary where key is column name and value is the data in the cell

```
>>> df.to_dict('records')
[{'a': 'red', 'b': 0.5}, 
 {'a': 'yellow', 'b': 0.25}, 
 {'a': 'blue', 'b': 0.125}]

```

**index** - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list)

```
>>> df.to_dict('index')
{0: {'a': 'red', 'b': 0.5},
 1: {'a': 'yellow', 'b': 0.25},
 2: {'a': 'blue', 'b': 0.125}}

```"
"In [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29), is the following the only way to get the number of elements?

```
arr.__len__()

```

If so, why the strange syntax?","```
my_list = [1,2,3,4,5]
len(my_list)
# 5

```

The same works for tuples:

```
my_tuple = (1,2,3,4,5)
len(my_tuple)
# 5

```

And strings, which are really just arrays of characters:

```
my_string = 'hello world'
len(my_string)
# 11

```

It was [intentionally done this way](https://web.archive.org/web/20200627113910/http://effbot.org/pyfaq/why-does-python-use-methods-for-some-functionality-e-g-list-index-but-functions-for-other-e-g-len-list.htm) so that lists, tuples and other container types or iterables didn't all need to explicitly implement a public `.length()` method, instead you can just check the `len()` of anything that implements the 'magic' `__len__()` method.

Sure, this may seem redundant, but length checking implementations can vary considerably, even within the same language. It's not uncommon to see one collection type use a `.length()` method while another type uses a `.length` property, while yet another uses `.count()`. Having a language-level keyword unifies the entry point for all these types. So even objects you may not consider to be lists of elements could still be length-checked. This includes strings, queues, trees, etc.

The functional nature of `len()` also lends itself well to functional styles of programming.

```
lengths = map(len, list_of_containers)

```"
"I am trying to figure out how to append multiple values to a list in Python. I know there are few methods to do so, such as manually input the values, or put the append operation in a `for` loop, or the `append` and `extend` functions.

However, I wonder if there is neater way to do so? Maybe a certain package or function?","You can use the [sequence method `list.extend`](https://docs.python.org/3/library/stdtypes.html#mutable-sequence-types) to extend the list by multiple values from any kind of iterable, being it another list or any other thing that provides a sequence of values.

```
>>> lst = [1, 2]
>>> lst.append(3)
>>> lst.append(4)
>>> lst
[1, 2, 3, 4]

>>> lst.extend([5, 6, 7])
>>> lst.extend((8, 9, 10))
>>> lst
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

>>> lst.extend(range(11, 14))
>>> lst
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]

```

So you can use `list.append()` to append *a single* value, and `list.extend()` to append *multiple* values."
"I'm trying to understand how to use the `Optional` type hint. From [PEP-484](https://www.python.org/dev/peps/pep-0484/), I know I can use `Optional` for `def test(a: int = None)` either as `def test(a: Union[int, None])` or `def test(a: Optional[int])`.

But how about following examples?

```
def test(a : dict = None):
    #print(a) ==> {'a': 1234}
    #or
    #print(a) ==> None

def test(a : list = None):
    #print(a) ==> [1,2,3,4, 'a', 'b']
    #or
    #print(a) ==> None

```

If `Optional[type]` seems to mean the same thing as `Union[type, None]`, why should I use `Optional[]` at all?","If your code is designed to work with Python 3.10 or newer, you want to use the [PEP 604](https://www.python.org/dev/peps/pep-0604/) syntax, using `... | None` union syntax, and *not* use `typing.Optional`:

```
def test(a: dict[Any, Any] | None = None) -> None:
    #print(a) ==> {'a': 1234}
    #or
    #print(a) ==> None

def test(a: list[Any] | None = None) -> None:
    #print(a) ==> [1, 2, 3, 4, 'a', 'b']
    #or
    #print(a) ==> None

```

Code that still supports older Python versions can still stick to using `Optional`. `Optional[...]` is a shorthand notation for `Union[..., None]`, telling the type checker that either an object of the specific type is required, *or* `None` is required. `...` stands for *any valid type hint*, including complex compound types or a `Union[]` of more types. Whenever you have a keyword argument with default value `None`, you should use `Optional`.

So for your two examples, you have `dict` and `list` container types, but the default value for the `a` keyword argument shows that `None` is permitted too so use `Optional[...]`:

```
from typing import Optional

def test(a: Optional[dict] = None) -> None:
    #print(a) ==> {'a': 1234}
    #or
    #print(a) ==> None

def test(a: Optional[list] = None) -> None:
    #print(a) ==> [1, 2, 3, 4, 'a', 'b']
    #or
    #print(a) ==> None

```

There is technically no difference between using `Optional[]` on a `Union[]`, or just adding `None` to the `Union[]`. So `Optional[Union[str, int]]` and `Union[str, int, None]` are exactly the same thing.

Personally, I'd stick with *always* using `Optional[]` when setting the type for a keyword argument that uses `= None` to set a default value, this documents the reason why `None` is allowed better. Moreover, it makes it easier to move the `Union[...]` part into a separate type alias, or to later remove the `Optional[...]` part if an argument becomes mandatory.

For example, say you have

```
from typing import Optional, Union

def api_function(optional_argument: Optional[Union[str, int]] = None) -> None:
    """"""Frob the fooznar.

    If optional_argument is given, it must be an id of the fooznar subwidget
    to filter on. The id should be a string, or for backwards compatibility,
    an integer is also accepted.

    """"""

```

then documentation is improved by pulling out the `Union[str, int]` into a type alias:

```
from typing import Optional, Union

# subwidget ids used to be integers, now they are strings. Support both.
SubWidgetId = Union[str, int]


def api_function(optional_argument: Optional[SubWidgetId] = None) -> None:
    """"""Frob the fooznar.

    If optional_argument is given, it must be an id of the fooznar subwidget
    to filter on. The id should be a string, or for backwards compatibility,
    an integer is also accepted.

    """"""

```

The refactor to move the `Union[]` into an alias was made all the much easier because `Optional[...]` was used instead of `Union[str, int, None]`. The `None` value is not a 'subwidget id' after all, it's not part of the value, `None` is meant to flag the absence of a value.

Side note: Unless your code only has to support Python 3.9 or newer, you want to avoid using the standard library container types in type hinting, as you can't say anything about what types they must contain. So instead of `dict` and `list`, use `typing.Dict` and `typing.List`, respectively. And when only *reading* from a container type, you may just as well accept any immutable abstract container type; lists and tuples are `Sequence` objects, while `dict` is a `Mapping` type:

```
from typing import Mapping, Optional, Sequence, Union

def test(a: Optional[Mapping[str, int]] = None) -> None:
    """"""accepts an optional map with string keys and integer values""""""
    # print(a) ==> {'a': 1234}
    # or
    # print(a) ==> None

def test(a: Optional[Sequence[Union[int, str]]] = None) -> None:
    """"""accepts an optional sequence of integers and strings
    # print(a) ==> [1, 2, 3, 4, 'a', 'b']
    # or
    # print(a) ==> None

```

In Python 3.9 and up, the standard container types have all been updated to support using them in type hints, see [PEP 585](https://www.python.org/dev/peps/pep-0585/). *But*, while you now *can* use `dict[str, int]` or `list[Union[int, str]]`, you still may want to use the more expressive `Mapping` and `Sequence` annotations to indicate that a function won't be mutating the contents (they are treated as 'read only'), and that the functions would work with *any* object that works as a mapping or sequence, respectively.

Python 3.10 introduces the `|` union operator into type hinting, see [PEP 604](https://www.python.org/dev/peps/pep-0604/). Instead of `Union[str, int]` you can write `str | int`. In line with other type-hinted languages, the preferred (and more concise) way to denote an optional argument in Python 3.10 and up, is now `Type | None`, e.g. `str | None` or `list | None`."
"I want to write a Django query equivalent to this SQL query:

```
SELECT * from user where income >= 5000 or income is NULL.

```

How to construct the Django queryset filter?

```
User.objects.filter(income__gte=5000, income=0)

```

This doesn't work, because it `AND`s the filters. I want to `OR` the filters to get union of individual querysets.","```
from django.db.models import Q
User.objects.filter(Q(income__gte=5000) | Q(income__isnull=True))

```

[via Documentation](https://docs.djangoproject.com/en/dev/topics/db/queries/#complex-lookups-with-q-objects)"
"How would you say ""does not equal""?

```
if hi == hi:
    print ""hi""
elif hi (does not equal) bye:
    print ""no hi""

```

Is there something similar to `==` that means ""not equal""?","Use `!=`. See [comparison operators](https://docs.python.org/3/library/stdtypes.html#comparisons). For comparing object identities, you can use the keyword `is` and its negation `is not`.

e.g.

```
1 == 1 #  -> True
1 != 1 #  -> False
[] is [] #-> False (distinct objects)
a = b = []; a is b # -> True (same object)

```"
"Let's say I have a class that has a member called data which is a list.

I want to be able to initialize the class with, for example, a filename (which contains data to initialize the list) or with an actual list.

What's your technique for doing this?

Do you just check the type by looking at `__class__`?

Is there some trick I might be missing?

I'm used to C++ where overloading by argument type is easy.","A much neater way to get 'alternate constructors' is to use classmethods. For instance:

```
>>> class MyData:
...     def __init__(self, data):
...         ""Initialize MyData from a sequence""
...         self.data = data
...     
...     @classmethod
...     def fromfilename(cls, filename):
...         ""Initialize MyData from a file""
...         data = open(filename).readlines()
...         return cls(data)
...     
...     @classmethod
...     def fromdict(cls, datadict):
...         ""Initialize MyData from a dict's items""
...         return cls(datadict.items())
... 
>>> MyData([1, 2, 3]).data
[1, 2, 3]
>>> MyData.fromfilename(""/tmp/foobar"").data
['foo\n', 'bar\n', 'baz\n']
>>> MyData.fromdict({""spam"": ""ham""}).data
[('spam', 'ham')]

```

The reason it's neater is that there is no doubt about what type is expected, and you aren't forced to guess at what the caller intended for you to do with the datatype it gave you. The problem with `isinstance(x, basestring)` is that there is no way for the caller to tell you, for instance, that even though the type is not a basestring, you should treat it as a string (and not another sequence.) And perhaps the caller would like to use the same type for different purposes, sometimes as a single item, and sometimes as a sequence of items. Being explicit takes all doubt away and leads to more robust and clearer code."
"How do I get the dimensions of an array? For instance, this is 2x2:

```
a = np.array([[1, 2], [3, 4]])

```","Use [`.shape`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html#numpy.ndarray.shape) to obtain a tuple of array dimensions:

```
>>> a.shape
(2, 2)

```"
"What is the difference between 'SAME' and 'VALID' padding in `tf.nn.max_pool` of `tensorflow`?

In my opinion, 'VALID' means there will be no zero padding outside the edges when we do max pool.

According to [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285v1.pdf), it says that there will be no padding in pool operator, i.e. just use 'VALID' of `tensorflow`.
But what is 'SAME' padding of max pool in `tensorflow`?","If you like ascii art:

* `""VALID""` = without padding:

  ```
     inputs:         1  2  3  4  5  6  7  8  9  10 11 (12 13)
                    |________________|                dropped
                                   |_________________|

  ```
* `""SAME""` = with zero padding:

  ```
                 pad|                                      |pad
     inputs:      0 |1  2  3  4  5  6  7  8  9  10 11 12 13|0  0
                 |________________|
                                |_________________|
                                               |________________|

  ```

In this example:

* Input width = 13
* Filter width = 6
* Stride = 5

Notes:

* `""VALID""` only ever drops the right-most columns (or bottom-most rows).
* `""SAME""` tries to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right, as is the case in this example (the same logic applies vertically: there may be an extra row of zeros at the bottom).

**Edit**:

About the name:

* With `""SAME""` padding, if you use a stride of 1, the layer's outputs will have the **same** spatial dimensions as its inputs.
* With `""VALID""` padding, there's no ""made-up"" padding inputs. The layer only uses **valid** input data."
"How do I redirect stdout to an arbitrary file in Python?

When a long-running Python script (e.g, web application) is started from within the ssh session and backgounded, and the ssh session is closed, the application will raise IOError and fail the moment it tries to write to stdout. I needed to find a way to make the application and modules output to a file rather than stdout to prevent failure due to IOError. Currently, I employ nohup to redirect output to a file, and that gets the job done, but I was wondering if there was a way to do it without using nohup, out of curiosity.

I have already tried `sys.stdout = open('somefile', 'w')`, but this does not seem to prevent some external modules from still outputting to terminal (or maybe the `sys.stdout = ...` line did not fire at all). I know it should work from simpler scripts I've tested on, but I also didn't have time yet to test on a web application yet.","If you want to do the redirection within the Python script, setting `sys.stdout` to a file object does the trick:

```
# for python3
import sys
with open('file', 'w') as sys.stdout:
    print('test')

```

A far more common method is to use shell redirection when executing (same on Windows and Linux):

```
$ python3 foo.py > file

```"
"If I have the following Python code

```
>>> x = []
>>> x = x + [1]
>>> x = x + [2]
>>> x = x + [3]
>>> x
[1, 2, 3]

```

Will `x` be guaranteed to always be `[1,2,3]`, or are other orderings of the interim elements possible?","Yes, the order of elements in a python list is persistent."
"I'm playing with both learning Python and am trying to get GitHub issues into a readable form. Using the advice on *[How can I convert JSON to CSV?](https://stackoverflow.com/questions/1871524/convert-from-json-to-csv-using-python)*, I came up with this:

```
import json
import csv

f = open('issues.json')
data = json.load(f)
f.close()

f = open(""issues.csv"", ""wb+"")
csv_file = csv.writer(f)

csv_file.writerow([""gravatar_id"", ""position"", ""number""])

for item in data:
    csv_file.writerow([item[""gravatar_id""], item[""position""], item[""number""]])

```

Where ""issues.json"" is the JSON file containing my GitHub issues. When I try to run that, I get

```
TypeError: string indices must be integers

```

What am I missing here? Which are the ""string indices""?

Here's a bit of my JSON content:

```
{""issues"": [{""gravatar_id"": ""44230311a3dcd684b6c5f81bf2ec9f60"", ""position"": 2.0, ""number"": 263...

```","The variable `item` is a string. An index looks like this:

```
>>> mystring = 'helloworld'
>>> print mystring[0]
'h'

```

The above example uses the `0` index of the string to refer to the first character.

Strings can't have string indices (like dictionaries can). So this won't work:

```
>>> mystring = 'helloworld'
>>> print mystring['stringindex']
TypeError: string indices must be integers

```"
"I have a function that returns information in seconds, but I need to store that information in hours:minutes:seconds.

Is there an easy way to convert the seconds to this format in Python?","You can use [`datetime.timedelta`](https://docs.python.org/library/datetime.html#datetime.timedelta) function:

```
>>> import datetime
>>> str(datetime.timedelta(seconds=666))
'0:11:06'

```"
"I have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0.

```
points = [(10,5), (5,11), (24,13), (7,8)]    
x_arr = []
y_arr = []
for x,y in points:
    x_arr.append(x)
    y_arr.append(y)
plt.scatter(x_arr,y_arr)

```","There is a new API that makes this even simpler.

```
plt.gca().invert_xaxis()

```

and/or

```
plt.gca().invert_yaxis()

```"
"I have a small python project that has the following structure -

```
Project 
 -- pkg01
   -- test01.py
 -- pkg02
   -- test02.py
 -- logging.conf

```

I plan to use the default logging module to print messages to stdout and a log file.
To use the logging module, some initialization is required -

```
import logging.config

logging.config.fileConfig('logging.conf')
logger = logging.getLogger('pyApp')

logger.info('testing')

```

At present, I perform this initialization in every module before I start logging messages. Is it possible to perform this initialization only once in one place such that the same settings are reused by logging all over the project?","Best practice is, in each module, to have a logger defined like this:

```
import logging
logger = logging.getLogger(__name__)

```

near the top of the module, and then in other code in the module do e.g.

```
logger.debug('My message with %s', 'variable data')

```

If you need to subdivide logging activity inside a module, use e.g.

```
loggerA = logging.getLogger(__name__ + '.A')
loggerB = logging.getLogger(__name__ + '.B')

```

and log to `loggerA` and `loggerB` as appropriate.

In your main program or programs, do e.g.:

```
def main():
    ""your program code""

if __name__ == '__main__':
    import logging.config
    logging.config.fileConfig('/path/to/logging.conf')
    main()

```

or

```
def main():
    import logging.config
    logging.config.fileConfig('/path/to/logging.conf')
    # your program code

if __name__ == '__main__':
    main()

```

See [here](http://docs.python.org/howto/logging.html#logging-from-multiple-modules) for logging from multiple modules, and [here](http://docs.python.org/howto/logging.html#configuring-logging-for-a-library) for logging configuration for code which will be used as a library module by other code.

**Update:** When calling `fileConfig()`, you may want to specify `disable_existing_loggers=False` if you're using Python 2.6 or later (see [the docs](http://docs.python.org/2/library/logging.config.html#logging.config.fileConfig) for more information). The default value is `True` for backward compatibility, which causes all existing loggers to be disabled by `fileConfig()` unless they or their ancestor are explicitly named in the configuration. With the value set to `False`, existing loggers are left alone. If using Python 2.7/Python 3.2 or later, you may wish to consider the `dictConfig()` API which is better than `fileConfig()` as it gives more control over the configuration."
"I am in the process of learning Python and I have reached the section about the `pass` statement. The guide I'm using defines it as being a null statement that is commonly used as a placeholder.

I still don't fully understand what that means though. What would be a simple/basic situation where the `pass` statement would be used and why would it be needed?","Suppose you are designing a new class with some methods that you don't want to implement, yet.

```
class MyClass(object):
    def meth_a(self):
        pass

    def meth_b(self):
        print ""I'm meth_b""

```

If you were to leave out the `pass`, the code wouldn't run.

You would then get an:

```
IndentationError: expected an indented block

```

To summarize, the `pass` statement does nothing particular, but it can act as a placeholder, as demonstrated here."
"I want to assign the output of a command I run using `os.system` to a variable and prevent it from being output to the screen. But, in the below code ,the output is sent to the screen and the value printed for `var` is 0, which I guess signifies whether the command ran successfully or not. Is there any way to assign the command output to the variable and also stop it from being displayed on the screen?

```
var = os.system(""cat /etc/services"")
print var #Prints 0

```","From [this question](https://stackoverflow.com/questions/1410976/equivalent-of-backticks-in-python) which I asked a long time ago, what you may want to use is `popen`:

```
os.popen('cat /etc/services').read()

```

From the [docs for Python 3.6](https://docs.python.org/3.6/library/os.html#os.popen),

> This is implemented using subprocess.Popen; see that classâ€™s
> documentation for more powerful ways to manage and communicate with
> subprocesses.

---

Here's the corresponding code for `subprocess`:

```
import subprocess

proc = subprocess.Popen([""cat"", ""/etc/services""], stdout=subprocess.PIPE, shell=True)
(out, err) = proc.communicate()
print(""program output:"", out)

```"
"I'm trying to make a system call in Python and store the output to a string that I can manipulate in the Python program.

```
#!/usr/bin/python
import subprocess
p2 = subprocess.Popen(""ntpq -p"")

```

I've tried a few things including some of the suggestions here:

[Retrieving the output of subprocess.call()](https://stackoverflow.com/questions/1996518/retrieving-the-output-of-subprocess-call)

but without any luck.","**In Python 2.7 or Python 3**

Instead of making a `Popen` object directly, you can use the [`subprocess.check_output()` function](http://docs.python.org/2/library/subprocess.html#subprocess.check_output) to store output of a command in a string:

```
from subprocess import check_output
out = check_output([""ntpq"", ""-p""])

```

**In Python 2.4-2.6**

Use the `communicate` method.

```
import subprocess
p = subprocess.Popen([""ntpq"", ""-p""], stdout=subprocess.PIPE)
out, err = p.communicate()

```

`out` is what you want.

**Important note about the other answers**

Note how I passed in the command. The `""ntpq -p""` example brings up another matter. Since `Popen` does not invoke the shell, you would use a list of the command and optionsâ€”`[""ntpq"", ""-p""]`."
"I want to run a Python script from another Python script. I want to pass variables like I would using the command line.

For example, I would run my first script that would iterate through a list of values (0,1,2,3) and pass those to the 2nd script `script2.py 0` then `script2.py 1`, etc.

I found [StackÂ Overflow question 1186789](https://stackoverflow.com/questions/1186789/what-is-the-best-way-to-call-a-python-script-from-another-python-script) which is a similar question, but ars's answer calls a function, where as I want to run the whole script, not just a function, and balpha's answer calls the script but with no arguments. I changed this to something like the below as a test:

```
execfile(""script2.py 1"")

```

But it is not accepting variables properly. When I print out the `sys.argv` in script2.py it is the original command call to first script ""['C:\script1.py'].

I don't really want to change the original script (i.e. script2.py in my example) since I don't own it.

I figure there must be a way to do this; I am just confused how you do it.","Try using [`os.system`](http://docs.python.org/library/os.html#os.system):

```
os.system(""script2.py 1"")

```

`execfile` is different because it is designed to run a sequence of Python statements in the *current* execution context. That's why `sys.argv` didn't change for you."
"I am calling different processes with the `subprocess` module. However, I have a question.

In the following code:

```
callProcess = subprocess.Popen(['ls', '-l'], shell=True)

```

and

```
callProcess = subprocess.Popen(['ls', '-l']) # without shell

```

Both work. After reading the docs, I came to know that `shell=True` means executing the code through the shell. So that means in absence, the process is directly started.

So what should I prefer for my case - I need to run a process and get its output. What benefit do I have from calling it from within the shell or outside of it?","The benefit of not calling via the shell is that you are not invoking a 'mystery program.' On POSIX, the environment variable `SHELL` controls which binary is invoked as the ""shell."" On Windows, there is no bourne shell descendent, only cmd.exe.

So invoking the shell invokes a program of the user's choosing and is platform-dependent. Generally speaking, avoid invocations via the shell.

Invoking via the shell does allow you to expand environment variables and file globs according to the shell's usual mechanism. On POSIX systems, the shell expands file globs to a list of files. On Windows, a file glob (e.g., ""\*.\*"") is not expanded by the shell, anyway (but environment variables on a command line *are* expanded by cmd.exe).

If you think you want environment variable expansions and file globs, research the `ILS` attacks of 1992-ish on network services which performed subprogram invocations via the shell. Examples include the various `sendmail` backdoors involving `ILS`.

In summary, use `shell=False`."
"How would you check if a variable is a dictionary in Python?

For example, I'd like it to loop through the values in the dictionary until it finds a dictionary. Then, loop through the one it finds:

```
dict = {'abc': 'abc', 'def': {'ghi': 'ghi', 'jkl': 'jkl'}}

for k, v in dict.items():
    if ###check if v is a dictionary:
        for k, v in v.iteritems():
            print(k, ' ', v)
    else:
        print(k, ' ', v)

```","Use [`isinstance(ele, dict)`](https://docs.python.org/3/library/functions.html#isinstance), which will return true for `dict` objects as well as subclasses of `dict`, such as [`OrderedDict`](https://docs.python.org/3/library/collections.html#collections.OrderedDict) and [`defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict):

```
d = {'abc': 'abc', 'def': {'ghi': 'ghi', 'jkl': 'jkl'}}
for element in d.values():
    if isinstance(element, dict):
       for k, v in element.items():
           print(k, ' ', v)

```

You can do `if type(ele) is dict` if you want to check strictly for instances of `dict` and not subclasses of it, though this is generally [not recommended](https://peps.python.org/pep-0008/#programming-recommendations)."
"I have a matrix in the type of a Numpy array. How would I write it to disk it as an image? Any format works (png, jpeg, bmp...). One important constraint is that PIL is not present.","Using [`PIL`](https://pillow.readthedocs.io/en/stable/), save a NumPy array `arr` by doing:

```
from PIL import Image
im = Image.fromarray(arr)
im.save(""your_file.jpeg"")

```

See the docs for [available data formats](https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html), including JPEG, PNG, and so on."
"I've been making Python scripts for simple tasks at work and never really bothered packaging them for others to use. Now I have been assigned to make a Python wrapper for a REST API. I have absolutely no idea on how to start and I need help.

**What I have:**

(Just want to be specific as possible) I have the [virtualenv](http://www.pythonforbeginners.com/basics/how-to-use-python-virtualenv/) ready, it's also [up in github](https://github.com/yowmamasita/pyfirefly), the .gitignore file for python is there as well, plus, the [requests library](https://requests.readthedocs.io/) for interacting with the REST API. That's it.

Here's the current directory tree

```
.
├── bin
│   └── /the usual stuff/
├── include
│   └── /the usual stuff/
├── lib
│   └── python2.7
│       └── /the usual stuff/
├── local
│   └── /the usual stuff/
└── README.md

27 directories, 280 files

```

I don't even know where to put the .py files, if I ever make one.

**What I wanted to do:**

Make a python module install-able with ""pip install ...""

If possible, I want a general step by step process on writing Python modules.","A module is a file containing Python definitions and statements. The file name is the module name with the suffix `.py`

create `hello.py` then write the following function as its content:

```
def helloworld():
   print(""hello"")

```

Then you can import `hello`:

```
>>> import hello
>>> hello.helloworld()
'hello'

```

To group many `.py` files put them in a folder. Any folder with an `__init__.py` is considered a module by python and you can call them a package

```
|-HelloModule
  |_ __init__.py
  |_ hellomodule.py

```

You can go about with the import statement on your module the usual way.

For more information, see [6.4. Packages](http://docs.python.org/dev/tutorial/modules.html#packages)."
I want to change `a.txt` to `b.kml`.,"Use [`os.rename`](https://docs.python.org/3/library/os.html#os.rename):

```
import os

os.rename('a.txt', 'b.kml')

```

Usage:

```
os.rename('from.extension.whatever','to.another.extension')

```"
"I have a list of strings for which I would like to perform a [natural alphabetical sort](https://en.wikipedia.org/wiki/Natural_sort_order).

For instance, the following list is naturally sorted (what I want):

```
['elm0', 'elm1', 'Elm2', 'elm9', 'elm10', 'Elm11', 'Elm12', 'elm13']

```

And here's the ""sorted"" version of the above list (what I get using [`sorted()`](https://docs.python.org/library/functions.html#sorted)):

```
['Elm11', 'Elm12', 'Elm2', 'elm0', 'elm1', 'elm10', 'elm13', 'elm9']

```

I'm looking for a sort function which behaves like the first one.","There is a third party library for this on PyPI called [natsort](https://github.com/SethMMorton/natsort) (full disclosure, I am the package's author). For your case, you can do either of the following:

```
>>> from natsort import natsorted, ns
>>> x = ['Elm11', 'Elm12', 'Elm2', 'elm0', 'elm1', 'elm10', 'elm13', 'elm9']
>>> natsorted(x, key=lambda y: y.lower())
['elm0', 'elm1', 'Elm2', 'elm9', 'elm10', 'Elm11', 'Elm12', 'elm13']
>>> natsorted(x, alg=ns.IGNORECASE)  # or alg=ns.IC
['elm0', 'elm1', 'Elm2', 'elm9', 'elm10', 'Elm11', 'Elm12', 'elm13']

```

You should note that `natsort` uses a general algorithm so it should work for just about any input that you throw at it. If you want more details on why you might choose a library to do this rather than rolling your own function, check out the `natsort` documentation's [How It Works](http://natsort.readthedocs.io/en/stable/howitworks.html) page, in particular the [Special Cases Everywhere!](http://natsort.readthedocs.io/en/stable/howitworks.html#special-cases-everywhere) section.

---

If you need a sorting key instead of a sorting function, use either of the below formulas.

```
>>> from natsort import natsort_keygen, ns
>>> l1 = ['elm0', 'elm1', 'Elm2', 'elm9', 'elm10', 'Elm11', 'Elm12', 'elm13']
>>> l2 = l1[:]
>>> natsort_key1 = natsort_keygen(key=lambda y: y.lower())
>>> l1.sort(key=natsort_key1)
>>> l1
['elm0', 'elm1', 'Elm2', 'elm9', 'elm10', 'Elm11', 'Elm12', 'elm13']
>>> natsort_key2 = natsort_keygen(alg=ns.IGNORECASE)
>>> l2.sort(key=natsort_key2)
>>> l2
['elm0', 'elm1', 'Elm2', 'elm9', 'elm10', 'Elm11', 'Elm12', 'elm13']

```

---

**Update November 2020**

Given that a popular request/question is ""how to sort like Windows Explorer?"" (or whatever is your operating system's file system browser), as of `natsort` version 7.1.0 there is a function called [`os_sorted`](https://natsort.readthedocs.io/en/stable/api.html#natsort.os_sorted) to do exactly this. On Windows, it will sort in the same order as Windows Explorer, and on other operating systems it should sort like whatever is the local file system browser.

```
>>> from natsort import os_sorted
>>> os_sorted(list_of_paths)
# your paths sorted like your file system browser

```

For those needing a sort key, you can use `os_sort_keygen` (or `os_sort_key` if you just need the defaults).

*Caveat* - Please read the API documentation for this function before you use to understand the limitations and how to get best results."
I want to append a newline to my string every time I call `file.write()`. What's the easiest way to do this in Python?,"Use ""\n"":

```
file.write(""My String\n"")

```

See [the Python manual](http://docs.python.org/tutorial/inputoutput.html) for reference."
"For example, I have a string like this(return value of `subprocess.check_output`):

```
>>> b'a string'
b'a string'

```

Whatever I did to it, it is always printed with the annoying `b'` before the string:

```
>>> print(b'a string')
b'a string'
>>> print(str(b'a string'))
b'a string'

```

Does anyone have any ideas about how to use it as a normal string or convert it into a normal string?","Decode it.

```
>>> b'a string'.decode('ascii')
'a string'

```

To get bytes from string, encode it.

```
>>> 'a string'.encode('ascii')
b'a string'

```"
"I used *easy\_install* to install [pytest](https://docs.pytest.org/en/stable/) on a Mac and started writing tests for a project with a file structure likes so:

```
repo/
   |--app.py
   |--settings.py
   |--models.py
   |--tests/
          |--test_app.py

```

Run `py.test` while in the *repo* directory, and everything behaves as you would expect.

But when I try that same thing on either Linux or Windows (both have pytest 2.2.3 on them), it barks whenever it hits its first import of something from my application path. For instance, `from app import some_def_in_app`.

Do I need to be editing my [PATH](https://en.wikipedia.org/wiki/PATH_(variable)) to run *py.test* on these systems?","### Recommended approach for `pytest>=7`: use the `pythonpath` setting

Recently, `pytest` has added a new core plugin that supports `sys.path` modifications via the [`pythonpath` configuration value](https://docs.pytest.org/en/7.1.x/reference/reference.html#confval-pythonpath). The solution is thus much simpler now and doesn't require any workarounds anymore:

`pyproject.toml` example:

```
[tool.pytest.ini_options]
pythonpath = [
  "".""
]

```

`pytest.ini` example:

```
[pytest]
pythonpath = .

```

The path entries are calculated relative to the rootdir, thus `.` adds `repo` directory to `sys.path` in this case.

Multiple path entries are also allowed: for a layout

```
repo/
├── src/
|   └── lib.py
├── app.py
└── tests
     ├── test_app.py
     └── test_lib.py

```

the configuration

```
[tool.pytest.ini_options]
pythonpath = [
  ""."", ""src"",
]

```

or

```
[pytest]
pythonpath = . src

```

will add both `app` and `lib` modules to `sys.path`, so

```
import app
import lib

```

will both work.

### Original answer (not recommended for recent pytest versions; use for `pytest<7` only): `conftest` solution

The least invasive solution is adding an empty file named `conftest.py` in the `repo/` directory:

```
$ touch repo/conftest.py

```

That's it. No need to write custom code for mangling the `sys.path` or remember to drag `PYTHONPATH` along, or placing `__init__.py` into dirs where it doesn't belong (using `python -m pytest` as suggested in [Apteryx](https://stackoverflow.com/users/2896799/apteryx)'s [answer](https://stackoverflow.com/a/34140498/2650249) is a good solution though!).

The project directory afterwards:

```
repo
├── conftest.py
├── app.py
├── settings.py
├── models.py
└── tests
     └── test_app.py

```

### Explanation

`pytest` looks for the `conftest` modules on test collection to gather custom hooks and fixtures, and in order to import the custom objects from them, **`pytest` adds the parent directory of the `conftest.py` to the `sys.path`** (in this case the `repo` directory).

### Other project structures

If you have other project structure, place the `conftest.py` in the package root dir (the one that contains packages but is not a package itself, so does **not** contain an `__init__.py`), for example:

```
repo
├── conftest.py
├── spam
│   ├── __init__.py
│   ├── bacon.py
│   └── egg.py
├── eggs
│   ├── __init__.py
│   └── sausage.py
└── tests
     ├── test_bacon.py
     └── test_egg.py

```

### `src` layout

Although this approach can be used with the `src` layout (place `conftest.py` in the `src` dir):

```
repo
├── src
│   ├── conftest.py
│   ├── spam
│   │   ├── __init__.py
│   │   ├── bacon.py
│   │   └── egg.py
│   └── eggs 
│       ├── __init__.py
│       └── sausage.py
└── tests
     ├── test_bacon.py
     └── test_egg.py

```

beware that adding `src` to `PYTHONPATH` mitigates the meaning and benefits of the `src` layout! You will end up with testing the code from repository and not the installed package. If you need to do it, maybe you don't need the `src` dir at all.

### Where to go from here

Of course, `conftest` modules are not just some files to help the source code discovery; it's where all the project-specific enhancements of the `pytest` framework and the customization of your test suite happen. `pytest` has a lot of information on `conftest` modules scattered throughout [their docs](https://docs.pytest.org/); start with [`conftest.py`: local per-directory plugins](https://docs.pytest.org/en/3.6.0/writing_plugins.html#conftest-py-plugins)

Also, SO has an excellent question on `conftest` modules: [In py.test, what is the use of conftest.py files?](https://stackoverflow.com/q/34466027/2650249)"
"OS: [Mac OS X 10.7.5](https://en.wikipedia.org/wiki/OS_X_Lion) (Lion)   
Python ver: 2.7.5

I have installed setuptools 1.0 with ez\_setup.py from <https://pypi.python.org/pypi/setuptools>.   
Then I downloaded pip.1.4.1 pkg from <https://pypi.python.org/pypi/pip/1.4.1>.

I ran (sudo) `python setup.py install` in [iTerm](https://en.wikipedia.org/wiki/ITerm2), output:

```
running install
running bdist_egg running egg_info writing requirements to
pip.egg-info/requires.txt writing pip.egg-info/PKG-INFO writing
top-level names to pip.egg-info/top_level.txt writing dependency_links
to pip.egg-info/dependency_links.txt writing entry points to
pip.egg-info/entry_points.txt warning: manifest_maker: standard file
'setup.py' not found

reading manifest file 'pip.egg-info/SOURCES.txt' writing manifest file
'pip.egg-info/SOURCES.txt' installing library code to
build/bdist.macosx-10.6-intel/egg running install_lib warning:
install_lib: 'build/lib' does not exist -- no Python modules to
install

creating build/bdist.macosx-10.6-intel/egg creating
build/bdist.macosx-10.6-intel/egg/EGG-INFO copying
pip.egg-info/PKG-INFO -> build/bdist.macosx-10.6-intel/egg/EGG-INFO
copying pip.egg-info/SOURCES.txt ->
build/bdist.macosx-10.6-intel/egg/EGG-INFO copying
pip.egg-info/dependency_links.txt ->
build/bdist.macosx-10.6-intel/egg/EGG-INFO copying
pip.egg-info/entry_points.txt ->
build/bdist.macosx-10.6-intel/egg/EGG-INFO copying
pip.egg-info/not-zip-safe ->
build/bdist.macosx-10.6-intel/egg/EGG-INFO copying
pip.egg-info/requires.txt ->
build/bdist.macosx-10.6-intel/egg/EGG-INFO copying
pip.egg-info/top_level.txt ->
build/bdist.macosx-10.6-intel/egg/EGG-INFO creating
'dist/pip-1.4.1-py2.7.egg' and adding
'build/bdist.macosx-10.6-intel/egg' to it removing
'build/bdist.macosx-10.6-intel/egg' (and everything under it)
Processing pip-1.4.1-py2.7.egg removing
'/Users/dl/Library/Python/2.7/lib/python/site-packages/pip-1.4.1-py2.7.egg'
(and everything under it) creating
/Users/dl/Library/Python/2.7/lib/python/site-packages/pip-1.4.1-py2.7.egg
Extracting pip-1.4.1-py2.7.egg to
/Users/dl/Library/Python/2.7/lib/python/site-packages pip 1.4.1 is
already the active version in easy-install.pth Installing pip script
to /Users/dl/Library/Python/2.7/bin Installing pip-2.7 script to
/Users/dl/Library/Python/2.7/bin

Installed
/Users/dl/Library/Python/2.7/lib/python/site-packages/pip-1.4.1-py2.7.egg
Processing dependencies for pip==1.4.1 Finished processing
dependencies for pip==1.4.1

```

Then I ran `pip install` and got the following error message:

```
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/bin/pip"", line 9, in <module>
    load_entry_point('pip==1.4.1', 'console_scripts', 'pip')()
  File ""build/bdist.macosx-10.6-intel/egg/pkg_resources.py"", line 357, in load_entry_point
  File ""build/bdist.macosx-10.6-intel/egg/pkg_resources.py"", line 2394, in load_entry_point
  File ""build/bdist.macosx-10.6-intel/egg/pkg_resources.py"", line 2108, in load
ImportError: No module named pip

```

How can I solve it?","With [macOS v10.15](https://en.wikipedia.org/wiki/MacOS_Catalina) (Catalina) and Homebrew 2.1.6, I was getting this error with Python 3.7. I just needed to run:

```
python3 -m ensurepip

```

Now `python3 -m pip` works for me."
"I have both `python2.7` and `python3.2` installed in `Ubuntu 12.04`.  
The symbolic link `python` links to `python2.7`.

When I type:

```
sudo pip install package-name

```

It will default install `python2` version of `package-name`.

Some package supports both `python2` and `python3`.  
How to install `python3` version of `package-name` via `pip`?","**Ubuntu 12.10+ and Fedora 13+ have a package called `python3-pip` which will install `pip-3.2` (or `pip-3.3`, `pip-3.4` or `pip3` for newer versions) without needing this jumping through hoops.**

---

I came across this and fixed this without needing the likes of `wget` or virtualenvs (assuming Ubuntu 12.04):

1. Install package `python3-setuptools`: run `sudo aptitude install python3-setuptools`, this will give you the command `easy_install3`.
2. Install pip using Python 3's setuptools: run `sudo easy_install3 pip`, this will give you the command `pip-3.2` like kev's solution.
3. Install your PyPI packages: run `sudo pip-3.2 install <package>` (installing python packages into your base system requires root, of course).
4. â€¦
5. Profit!"
"How do I convert a datetime *string in local time* to a *string in UTC time*?

I'm sure I've done this before, but can't find it and SO will hopefully help me (and others) do that in future.

**Clarification**: For example, if I have `2008-09-17 14:02:00` in my local timezone (`+10`), I'd like to generate a string with the equivalent `UTC` time: `2008-09-17 04:02:00`.

Also, from <http://lucumr.pocoo.org/2011/7/15/eppur-si-muove/>, note that in general this isn't possible as with DST and other issues there is no unique conversion from local time to UTC time.","First, parse the string into a naive datetime object. This is an instance of `datetime.datetime` with no attached timezone information. See its [documentation](https://docs.python.org/3/library/datetime.html#available-types).

Use the [`pytz`](http://pytz.sourceforge.net/) module, which comes with a full list of time zones + UTC. Figure out what the local timezone is, construct a timezone object from it, and manipulate and attach it to the naive datetime.

Finally, use `datetime.astimezone()` method to convert the datetime to UTC.

Source code, using local timezone ""America/Los\_Angeles"", for the string ""2001-2-3 10:11:12"":

```
from datetime import datetime   
import pytz

local = pytz.timezone(""America/Los_Angeles"")
naive = datetime.strptime(""2001-2-3 10:11:12"", ""%Y-%m-%d %H:%M:%S"")
local_dt = local.localize(naive, is_dst=None)
utc_dt = local_dt.astimezone(pytz.utc)

```

From there, you can use the `strftime()` method to format the UTC datetime as needed:

```
utc_dt.strftime(""%Y-%m-%d %H:%M:%S"")

```"
"I've two pandas data frames that have some rows in common.

Suppose dataframe2 is a subset of dataframe1.

**How can I get the rows of dataframe1 which are not in dataframe2?**

```
df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) 
df2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})

```

df1

```
   col1  col2
0     1    10
1     2    11
2     3    12
3     4    13
4     5    14

```

df2

```
   col1  col2
0     1    10
1     2    11
2     3    12

```

Expected result:

```
   col1  col2
3     4    13
4     5    14

```","The currently selected solution produces incorrect results. To correctly solve this problem, we can perform a left-join from `df1` to `df2`, making sure to first get just the unique rows for `df2`.

First, we need to modify the original DataFrame to add the row with data [3, 10].

```
df1 = pd.DataFrame(data = {'col1' : [1, 2, 3, 4, 5, 3], 
                           'col2' : [10, 11, 12, 13, 14, 10]}) 
df2 = pd.DataFrame(data = {'col1' : [1, 2, 3],
                           'col2' : [10, 11, 12]})

df1

   col1  col2
0     1    10
1     2    11
2     3    12
3     4    13
4     5    14
5     3    10

df2

   col1  col2
0     1    10
1     2    11
2     3    12

```

Perform a left-join, eliminating duplicates in `df2` so that each row of `df1` joins with exactly 1 row of `df2`. Use the parameter `indicator` to return an extra column indicating which table the row was from.

```
df_all = df1.merge(df2.drop_duplicates(), on=['col1','col2'], 
                   how='left', indicator=True)
df_all

   col1  col2     _merge
0     1    10       both
1     2    11       both
2     3    12       both
3     4    13  left_only
4     5    14  left_only
5     3    10  left_only

```

Create a boolean condition:

```
df_all['_merge'] == 'left_only'

0    False
1    False
2    False
3     True
4     True
5     True
Name: _merge, dtype: bool

```

---

### Why other solutions are wrong

A few solutions make the same mistake - they only check that each value is independently in each column, not together in the same row. Adding the last row, which is unique but has the values from both columns from `df2` exposes the mistake:

```
common = df1.merge(df2,on=['col1','col2'])
(~df1.col1.isin(common.col1))&(~df1.col2.isin(common.col2))
0    False
1    False
2    False
3     True
4     True
5    False
dtype: bool

```

This solution gets the same wrong result:

```
df1.isin(df2.to_dict('l')).all(1)

```"
"I'm building a simple helper script for work that will copy a couple of template files in our code base to the current directory. I don't, however, have the absolute path to the directory where the templates are stored. I do have a relative path from the script but when I call the script it treats that as a path relative to the current working directory. Is there a way to specify that this relative url is from the location of the script instead?","In the file that has the script, you want to do something like this:

```
import os
dirname = os.path.dirname(__file__)
filename = os.path.join(dirname, 'relative/path/to/file/you/want')

```

This will give you the absolute path to the file you're looking for. Note that if you're using setuptools, you should probably use its [package resources API](http://peak.telecommunity.com/DevCenter/PythonEggs#accessing-package-resources) instead.

**UPDATE**: I'm responding to a comment here so I can paste a code sample. :-)

> Am I correct in thinking that `__file__` is not always available (e.g. when you run the file directly rather than importing it)?

I'm assuming you mean the `__main__` script when you mention running the file directly. If so, that doesn't appear to be the case on my system (python 2.5.1 on OS X 10.5.7):

```
#foo.py
import os
print os.getcwd()
print __file__

#in the interactive interpreter
>>> import foo
/Users/jason
foo.py

#and finally, at the shell:
~ % python foo.py
/Users/jason
foo.py

```

However, I do know that there are some quirks with `__file__` on C extensions. For example, I can do this on my Mac:

```
>>> import collections #note that collections is a C extension in Python 2.5
>>> collections.__file__
'/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/lib-
dynload/collections.so'

```

However, this raises an exception on my Windows machine."
"Most operations in `pandas` can be accomplished with operator chaining (`groupby`, `aggregate`, `apply`, etc), but the only way I've found to filter rows is via normal bracket indexing

```
df_filtered = df[df['column'] == value]

```

This is unappealing as it requires I assign `df` to a variable before being able to filter on its values. Is there something more like the following?

```
df_filtered = df.mask(lambda x: x['column'] == value)

```","I'm not entirely sure what you want, and your last line of code does not help either, but anyway:

""Chained"" filtering is done by ""chaining"" the criteria in the boolean index.

```
In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [99]: df[(df.A == 1) & (df.D == 6)]
Out[99]:
   A  B  C  D
d  1  3  9  6

```

If you want to chain methods, you can add your own mask method and use that one.

```
In [90]: def mask(df, key, value):
   ....:     return df[df[key] == value]
   ....:

In [92]: pandas.DataFrame.mask = mask

In [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))

In [95]: df.ix['d','A'] = df.ix['a', 'A']

In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [97]: df.mask('A', 1)
Out[97]:
   A  B  C  D
a  1  4  9  1
d  1  3  9  6

In [98]: df.mask('A', 1).mask('D', 6)
Out[98]:
   A  B  C  D
d  1  3  9  6

```"
"Is there any way to tell whether a **string** represents an integer (e.g., `'3'`, `'-17'` but not `'3.14'` or `'asfasfas'`) Without using a try/except mechanism?

```
is_int('3.14') == False
is_int('-7')   == True

```","with positive integers you could use [`.isdigit`](http://docs.python.org/library/stdtypes.html#str.isdigit):

```
>>> '16'.isdigit()
True

```

it doesn't work with negative integers though. suppose you could try the following:

```
>>> s = '-17'
>>> s.startswith('-') and s[1:].isdigit()
True

```

it won't work with `'16.0'` format, which is similar to `int` casting in this sense.

**edit**:

```
def check_int(s):
    if s[0] in ('-', '+'):
        return s[1:].isdigit()
    return s.isdigit()

```"
"I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:

```
data = np.array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])

```

I'd like the resulting DataFrame to have `Row1` and `Row2` as index values, and `Col1`, `Col2` as header values.

I can specify the index as follows:

```
df = pd.DataFrame(data, index=data[:,0])

```

However, I am unsure how to best assign column headers.","Specify `data`, `index` and `columns` to the [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) constructor, as follows:

```
>>> pd.DataFrame(data=data[1:,1:],    # values
...              index=data[1:,0],    # 1st column as index
...              columns=data[0,1:])  # 1st row as the column names

```

As @joris [mentions](https://stackoverflow.com/questions/20763012/creating-a-pandas-dataframe-from-a-numpy-array-how-do-i-specify-the-index-colum#comment31118225_20763012), you may need to change above to `np.int_(data[1:,1:])` to have the correct data type."
"I'm learning how to use AWS SDK for Python (Boto3) from the following resource:  
<https://boto3.readthedocs.io/en/latest/guide/quickstart.html#using-boto-3>.

My doubt is when to use `resource`, `client`, or `session`, and their respective functionality.

---

*I am using Python 2.7.12 in Ubuntu 16.04 LTS.*","**Client** and **Resource** are two different abstractions within the boto3 SDK for making AWS service requests. If you want to make API calls to an AWS service with boto3, then you do so via a Client or a Resource.

You would typically choose to use either the Client abstraction or the Resource abstraction, but you can use both, as needed. I've outlined the differences below to help readers decide which to use.

**Session** is largely orthogonal to the concepts of Client and Resource (but is used by both).

Here's some more detailed information on what [Client](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/clients.html), [Resource](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/resources.html), and [Session](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/session.html) are all about.

---

### Client

* this is the original boto3 API abstraction
* it provides low-level AWS service access
* all AWS service operations are supported by clients
* it exposes botocore client to the developer
* it typically maps 1:1 with the AWS service API
* it exposes snake-cased method names (e.g. [ListBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListBuckets.html) API => [list\_buckets](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.list_buckets) method)
* typically yields primitive, non-marshalled data (e.g. DynamoDB attributes are dicts representing primitive DynamoDB values)
* requires you to code result pagination
* it is generated from an AWS **service** description

Here's an example of client-level access to an S3 bucket's objects:

```
import boto3

client = boto3.client('s3')

response = client.list_objects_v2(Bucket='mybucket')

for content in response['Contents']:
    obj_dict = client.get_object(Bucket='mybucket', Key=content['Key'])
    print(content['Key'], obj_dict['LastModified'])

```

**Note:** this client-level code is limited to listing at most 1000 objects. You would have to use a [paginator](http://boto3.readthedocs.io/en/latest/guide/paginators.html), or implement your own loop, `calling list_objects_v2()` repeatedly with a continuation marker if there were more than 1000 objects.

OK, so that's the low-level Client interface. Now onto the higher-level (more abstract) Resource interface.

---

### Resource

* this is the newer boto3 API abstraction
* it provides a high-level, object-oriented API
* it does not provide 100% API coverage of AWS services
* it uses identifiers and attributes
* it has actions (operations on resources)
* it exposes sub-resources and collections of AWS resources
* typically yields marshalled data, not primitive AWS data (e.g. DynamoDB attributes are native Python values representing primitive DynamoDB values)
* does result pagination for you
* it is generated from an AWS **resource** description

Here's the equivalent example using *resource-level* access to an S3 bucket's objects:

```
import boto3

s3 = boto3.resource('s3')

bucket = s3.Bucket('mybucket')

for obj in bucket.objects.all():
    print(obj.key, obj.last_modified)

```

Note: in this case you do not have to make a second API call to get the objects; they're available to you as a collection on the bucket. These collections of sub-resources are lazily-loaded.

You can see that the `Resource` version of the code is much simpler, more compact, and has more capability (for example it does pagination for you and it exposes properties instead of a raw dictionary). The `Client` version of the code would actually be more complicated than shown above if you wanted to include pagination.

Finally, onto Session which is fundamental to both Client and Resource and how both get access to AWS credentials, for example.

---

### Session

* stores configuration information (primarily credentials and selected region)
* allows you to create service clients and resources
* boto3 creates a default session for you when needed

A useful resource to learn more about these boto3 concepts is the [introductory re:Invent video](https://www.youtube.com/watch?v=Cb2czfCV4Dg).

---

Update January 2023
-------------------

Per the [Resources](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/resources.html) page in the boto3 documentation:

> The AWS Python SDK team does not intend to add new features to the resources interface in boto3. Existing interfaces will continue to operate during boto3's lifecycle. Customers can find access to newer service features through the client interface.

You can read more about the plans to maintain but no longer enhance resources at [boto3/discussions/3563](https://github.com/boto/boto3/discussions/3563)."
"In a similar way to using varargs in C or C++:

```
fn(a, b)
fn(a, b, c, d, ...)

```","Yes. You can use `*args` as a *non-keyword* argument. You will then be able to pass any number of arguments.

```
def manyArgs(*arg):
  print ""I was called with"", len(arg), ""arguments:"", arg

>>> manyArgs(1)
I was called with 1 arguments: (1,)
>>> manyArgs(1, 2, 3)
I was called with 3 arguments: (1, 2, 3)

```

As you can see, Python will *unpack* the arguments as a single tuple with all the arguments.

For keyword arguments you need to accept those as a separate actual argument, as shown in [Skurmedel's answer](https://stackoverflow.com/a/919720/28169)."
"While searching through a Python project, I found a few lines commented with `# noqa`.

```
import sys
sys.path.append(r'C:\dev')
import some_module   # noqa

```

What does `noqa` mean in Python? Is it specific to Python only?","Adding `# noqa` to a line indicates that the linter (a program that automatically checks code quality) should not check this line. Any warnings that code may have generated will be ignored.

That line may have something that ""looks bad"" to the linter, but the developer understands and intends it to be there for some reason.

For more information, see the [Flake8 documentation for Selecting and Ignoring Violations](http://flake8.pycqa.org/en/latest/user/violations.html#in-line-ignoring-errors)."
"I want to get, given a character, its `ASCII` value.

For example, for the character `a`, I want to get `97`, and vice versa.","Use [`chr()`](http://docs.python.org/library/functions.html#chr) and [`ord()`](http://docs.python.org/library/functions.html#ord):

```
>>> chr(97)
'a'
>>> ord('a')
97

```"
"In Java [IoC](http://en.wikipedia.org/wiki/Inversion_of_Control) / [DI](http://en.wikipedia.org/wiki/Dependency_Injection) is a very common practice which is extensively used in web applications, nearly all available frameworks and Java EE. On the other hand, there are also lots of big Python web applications, but beside of Zope (which I've heard should be really horrible to code) IoC doesn't seem to be very common in the Python world. (Please name some examples if you think that I'm wrong).

There are of course several clones of popular Java IoC frameworks available for Python, [springpython](http://springpython.webfactional.com/) for example. But none of them seems to get used practically. At least, I've never stumpled upon a [Django](http://www.djangoproject.com/) or [sqlalchemy](http://www.sqlalchemy.org/)+`<insert your favorite wsgi toolkit here>` based web application which uses something like that.

In my opinion IoC has reasonable advantages and would make it easy to replace the django-default-user-model for example, but extensive usage of interface classes and IoC in Python looks a bit odd and not »pythonic«. But maybe someone has a better explanation, why IoC isn't widely used in Python.","I don't actually think that DI/IoC are *that* uncommon in Python. What *is* uncommon, however, are DI/IoC *frameworks/containers*.

Think about it: what does a DI container do? It allows you to

1. wire together independent components into a complete application ...
2. ... at runtime.

We have names for ""wiring together"" and ""at runtime"":

1. scripting
2. dynamic

So, a DI container is nothing but an interpreter for a dynamic scripting language. Actually, let me rephrase that: a typical Java/.NET DI container is nothing but a crappy interpreter for a really bad dynamic scripting language with butt-ugly, sometimes XML-based, syntax.

When you program in Python, why would you want to use an ugly, bad scripting language when you have a beautiful, brilliant scripting language at your disposal? Actually, that's a more general question: when you program in pretty much any language, why would you want to use an ugly, bad scripting language when you have Jython and IronPython at your disposal?

So, to recap: the *practice* of DI/IoC is just as important in Python as it is in Java, for exactly the same reasons. The *implementation* of DI/IoC however, is built into the language and often so lightweight that it completely vanishes.

(Here's a brief aside for an analogy: in assembly, a subroutine call is a pretty major deal - you have to save your local variables and registers to memory, save your return address somewhere, change the instruction pointer to the subroutine you are calling, arrange for it to somehow jump back into your subroutine when it is finished, put the arguments somewhere where the callee can find them, and so on. IOW: in assembly, ""subroutine call"" is a Design Pattern, and before there were languages like Fortran which had subroutine calls built in, people were building their own ""subroutine frameworks"". Would you say that subroutine calls are ""uncommon"" in Python, just because you don't use subroutine frameworks?)

BTW: for an example of what it looks like to take DI to its logical conclusion, take a look at [Gilad Bracha](http://GBracha.BlogSpot.Com/)'s [Newspeak Programming Language](http://NewspeakLanguage.Org/) and his writings on the subject:

* [Constructors Considered Harmful](http://GBracha.BlogSpot.Com/2007/06/constructors-considered-harmful.html)
* [Lethal Injection](http://GBracha.BlogSpot.Com/2007/12/some-months-ago-i-wrote-couple-of-posts.html)
* [A Ban on Imports](http://GBracha.BlogSpot.Com/2009/06/ban-on-imports.html) ([continued](http://GBracha.BlogSpot.Com/2009/07/ban-on-imports-continued.html))"
"In `numpy`, some of the operations return in shape `(R, 1)` but some return `(R,)`. This will make matrix multiplication more tedious since explicit `reshape` is required. For example, given a matrix `M`, if we want to do `numpy.dot(M[:,0], numpy.ones((1, R)))` where `R` is the number of rows (of course, the same issue also occurs column-wise). We will get `matrices are not aligned` error since `M[:,0]` is in shape `(R,)` but `numpy.ones((1, R))` is in shape `(1, R)`.

So my questions are:

1. What's the difference between shape `(R, 1)` and `(R,)`. I know literally it's list of numbers and list of lists where all list contains only a number. Just wondering why not design `numpy` so that it favors shape `(R, 1)` instead of `(R,)` for easier matrix multiplication.
2. Are there better ways for the above example? Without explicitly reshape like this: `numpy.dot(M[:,0].reshape(R, 1), numpy.ones((1, R)))`","### 1. The meaning of shapes in NumPy

You write, ""I know literally it's list of numbers and list of lists where all list contains only a number"" but that's a bit of an unhelpful way to think about it.

The best way to think about NumPy arrays is that they consist of two parts, a *data buffer* which is just a block of raw elements, and a *view* which describes how to interpret the data buffer.

For example, if we create an array of 12 integers:

```
>>> a = numpy.arange(12)
>>> a
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])

```

Then `a` consists of a data buffer, arranged something like this:

```
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘

```

and a view which describes how to interpret the data:

```
>>> a.flags
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  UPDATEIFCOPY : False
>>> a.dtype
dtype('int64')
>>> a.itemsize
8
>>> a.strides
(8,)
>>> a.shape
(12,)

```

Here the *shape* `(12,)` means the array is indexed by a single index which runs from 0 to 11. Conceptually, if we label this single index `i`, the array `a` looks like this:

```
i= 0    1    2    3    4    5    6    7    8    9   10   11
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘

```

If we [reshape](http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) an array, this doesn't change the data buffer. Instead, it creates a new view that describes a different way to interpret the data. So after:

```
>>> b = a.reshape((3, 4))

```

the array `b` has the same data buffer as `a`, but now it is indexed by *two* indices which run from 0 to 2 and 0 to 3 respectively. If we label the two indices `i` and `j`, the array `b` looks like this:

```
i= 0    0    0    0    1    1    1    1    2    2    2    2
j= 0    1    2    3    0    1    2    3    0    1    2    3
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘

```

which means that:

```
>>> b[2,1]
9

```

You can see that the second index changes quickly and the first index changes slowly. If you prefer this to be the other way round, you can specify the `order` parameter:

```
>>> c = a.reshape((3, 4), order='F')

```

which results in an array indexed like this:

```
i= 0    1    2    0    1    2    0    1    2    0    1    2
j= 0    0    0    1    1    1    2    2    2    3    3    3
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘

```

which means that:

```
>>> c[2,1]
5

```

It should now be clear what it means for an array to have a shape with one or more dimensions of size 1. After:

```
>>> d = a.reshape((12, 1))

```

the array `d` is indexed by two indices, the first of which runs from 0 to 11, and the second index is always 0:

```
i= 0    1    2    3    4    5    6    7    8    9   10   11
j= 0    0    0    0    0    0    0    0    0    0    0    0
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘

```

and so:

```
>>> d[10,0]
10

```

A dimension of length 1 is ""free"" (in some sense), so there's nothing stopping you from going to town:

```
>>> e = a.reshape((1, 2, 1, 6, 1))

```

giving an array indexed like this:

```
i= 0    0    0    0    0    0    0    0    0    0    0    0
j= 0    0    0    0    0    0    1    1    1    1    1    1
k= 0    0    0    0    0    0    0    0    0    0    0    0
l= 0    1    2    3    4    5    0    1    2    3    4    5
m= 0    0    0    0    0    0    0    0    0    0    0    0
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘

```

and so:

```
>>> e[0,1,0,0,0]
6

```

See the [NumPy internals documentation](http://docs.scipy.org/doc/numpy/reference/internals.html) for more details about how arrays are implemented.

### 2. What to do?

Since [`numpy.reshape`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) just creates a new view, you shouldn't be scared about using it whenever necessary. It's the right tool to use when you want to index an array in a different way.

However, in a long computation it's usually possible to arrange to construct arrays with the ""right"" shape in the first place, and so minimize the number of reshapes and transposes. But without seeing the actual context that led to the need for a reshape, it's hard to say what should be changed.

The example in your question is:

```
numpy.dot(M[:,0], numpy.ones((1, R)))

```

but this is not realistic. First, this expression:

```
M[:,0].sum()

```

computes the result more simply. Second, is there really something special about column 0? Perhaps what you actually need is:

```
M.sum(axis=0)

```"
"Why is `cls` sometimes used instead of `self` as an argument in Python classes?

For example:

```
class Person:
    def __init__(self, firstname, lastname):
        self.firstname = firstname
        self.lastname = lastname

    @classmethod
    def from_fullname(cls, fullname):
        cls.firstname, cls.lastname = fullname.split(' ', 1)

```","The distinction between `""self""` and `""cls""` is defined in [`PEP 8`](http://www.python.org/dev/peps/pep-0008/#function-and-method-arguments) . As Adrien said, this is not mandatory. It's a coding style. `PEP 8` says:

> *Function and method arguments*:
>
> Always use `self` for the first argument to instance methods.
>
> Always use `cls` for the first argument to class methods."
"When using Python `strftime`, is there a way to remove the first 0 of the date if it's before the 10th, ie. so `01` is `1`? Can't find a `%`thingy for that?

Thanks!","Actually I had the same problem and I realized that, if you add a hyphen between the `%` and the letter, you can remove the leading zero.

For example `%Y/%-m/%-d`:

```
>>> import datetime
>>> datetime.datetime(2023, 1, 2).strftime(""%Y/%-m/%-d"")
'2023/1/2'

```

This only works on Unix (Linux, OS X), not Windows (including Cygwin). On Windows, you would use `#`, e.g. `%Y/%#m/%#d`."
"I am using `Python-2.6 CGI` scripts but found this error in server log while doing `json.dumps()`,

```
Traceback (most recent call last):
  File ""/etc/mongodb/server/cgi-bin/getstats.py"", line 135, in <module>
    print json.dumps(​​__get​data())
  File ""/usr/lib/python2.7/json/__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python2.7/json/encoder.py"", line 201, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python2.7/json/encoder.py"", line 264, in iterencode
    return _iterencode(o, 0)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xa5 in position 0: invalid start byte

```

​Here, `​__get​data()` function returns a dictionary.

Before posting this question, I read [this](https://stackoverflow.com/questions/6180521/unicodedecodeerror-utf8-codec-cant-decode-bytes-in-position-3-6-invalid-dat) question on SO. How do I resolve this error?","If you get this error when trying to read a csv file, the `read_csv()` function from `pandas` lets you set the encoding:

```
import pandas as pd
data = pd.read_csv(filename, encoding='unicode_escape')

```"
How can I get the position of a character inside a string in Python?,"There are two string methods for this, `find()` and `index()`. The difference between the two is what happens when the search string isn't found. `find()` returns `-1` and `index()` raises a `ValueError`.

### Using [`find()`](https://docs.python.org/3/library/stdtypes.html#str.find)

```
>>> myString = 'Position of a character'
>>> myString.find('s')
2
>>> myString.find('x')
-1

```

---

### Using [`index()`](https://docs.python.org/3/library/stdtypes.html#str.index)

```
>>> myString = 'Position of a character'
>>> myString.index('s')
2
>>> myString.index('x')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: substring not found

```

---

From the [Python manual](https://docs.python.org/2/library/string.html)
-----------------------------------------------------------------------

> `string.find(s, sub[, start[, end]])`  
> Return the lowest index in *s* where the substring *sub* is found such that *sub* is wholly contained in `s[start:end]`. Return `-1` on failure. Defaults for *start* and *end* and interpretation of negative values is the same as for slices.

And:

> `string.index(s, sub[, start[, end]])`  
> Like `find()` but raise `ValueError` when the substring is not found."
What is the cost of [`len()`](https://docs.python.org/2/library/functions.html#len) function for Python built-ins? (list/tuple/string/dictionary),"It's **O(1)** (constant time, not depending of actual length of the element - very fast) on every type you've mentioned, plus `set` and others such as `array.array`."
"The [docs](http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once) show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:

```
In [563]: grouped['D'].agg({'result1' : np.sum,
   .....:                   'result2' : np.mean})
   .....:
Out[563]: 
      result2   result1
A                      
bar -0.579846 -1.739537
foo -0.280588 -1.402938

```

However, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column names that the function will be applied to.

What I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times). Also, *some functions will depend on other columns in the groupby object* (like sumif functions). My current solution is to go column by column, and doing something like the code above, using lambdas for functions that depend on other rows. But this is taking a long time, (I think it takes a long time to iterate through a groupby object). I'll have to change it so that I iterate through the whole groupby object in a single run, but I'm wondering if there's a built in way in pandas to do this somewhat cleanly.

For example, I've tried something like

```
grouped.agg({'C_sum' : lambda x: x['C'].sum(),
             'C_std': lambda x: x['C'].std(),
             'D_sum' : lambda x: x['D'].sum()},
             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)

```

but as expected I get a KeyError (since the keys have to be a column if `agg` is called from a DataFrame).

Is there any built in way to do what I'd like to do, or a possibility that this functionality may be added, or will I just need to iterate through the groupby manually?","The second half of the [currently accepted answer](https://stackoverflow.com/a/14530027) is outdated and has two deprecations. First and most important, you can no longer pass a dictionary of dictionaries to the `agg` groupby method. Second, never use `.ix`.

If you desire to work with two separate columns at the same time I would suggest using the `apply` method which implicitly passes a DataFrame to the applied function. Let's use a similar dataframe as the one from above

```
df = pd.DataFrame(np.random.rand(4,4), columns=list('abcd'))
df['group'] = [0, 0, 1, 1]
df

          a         b         c         d  group
0  0.418500  0.030955  0.874869  0.145641      0
1  0.446069  0.901153  0.095052  0.487040      0
2  0.843026  0.936169  0.926090  0.041722      1
3  0.635846  0.439175  0.828787  0.714123      1

```

A dictionary mapped from column names to aggregation functions is still a perfectly good way to perform an aggregation.

```
df.groupby('group').agg({'a':['sum', 'max'], 
                         'b':'mean', 
                         'c':'sum', 
                         'd': lambda x: x.max() - x.min()})

              a                   b         c         d
            sum       max      mean       sum  <lambda>
group                                                  
0      0.864569  0.446069  0.466054  0.969921  0.341399
1      1.478872  0.843026  0.687672  1.754877  0.672401

```

If you don't like that ugly lambda column name, you can use a normal function and supply a custom name to the special `__name__` attribute like this:

```
def max_min(x):
    return x.max() - x.min()

max_min.__name__ = 'Max minus Min'

df.groupby('group').agg({'a':['sum', 'max'], 
                         'b':'mean', 
                         'c':'sum', 
                         'd': max_min})

              a                   b         c             d
            sum       max      mean       sum Max minus Min
group                                                      
0      0.864569  0.446069  0.466054  0.969921      0.341399
1      1.478872  0.843026  0.687672  1.754877      0.672401

```

---

Using `apply` and returning a Series
------------------------------------

Now, if you had multiple columns that needed to interact together then you cannot use `agg`, which implicitly passes a Series to the aggregating function. When using `apply` the entire group as a DataFrame gets passed into the function.

I recommend making a single custom function that returns a Series of all the aggregations. Use the Series index as labels for the new columns:

```
def f(x):
    d = {}
    d['a_sum'] = x['a'].sum()
    d['a_max'] = x['a'].max()
    d['b_mean'] = x['b'].mean()
    d['c_d_prodsum'] = (x['c'] * x['d']).sum()
    return pd.Series(d, index=['a_sum', 'a_max', 'b_mean', 'c_d_prodsum'])

df.groupby('group').apply(f)

         a_sum     a_max    b_mean  c_d_prodsum
group                                           
0      0.864569  0.446069  0.466054     0.173711
1      1.478872  0.843026  0.687672     0.630494

```

If you are in love with MultiIndexes, you can still return a Series with one like this:

```
    def f_mi(x):
        d = []
        d.append(x['a'].sum())
        d.append(x['a'].max())
        d.append(x['b'].mean())
        d.append((x['c'] * x['d']).sum())
        return pd.Series(d, index=[['a', 'a', 'b', 'c_d'], 
                                   ['sum', 'max', 'mean', 'prodsum']])

df.groupby('group').apply(f_mi)

              a                   b       c_d
            sum       max      mean   prodsum
group                                        
0      0.864569  0.446069  0.466054  0.173711
1      1.478872  0.843026  0.687672  0.630494

```"
"The below code will not join, when debugged the command does not store the whole path but just the last entry.

```
os.path.join('/home/build/test/sandboxes/', todaystr, '/new_sandbox/')

```

When I test this it only stores the `/new_sandbox/` part of the code.","The latter strings shouldn't start with a slash. If they start with a slash, then they're considered an ""absolute path"" and everything before them is discarded.

Quoting the [Python docs for `os.path.join`](http://docs.python.org/library/os.path.html#os.path.join):

> If a component is an absolute path, all previous components are thrown away and joining continues from the absolute path component.

Note on Windows, the behaviour in relation to drive letters, which seems to have changed compared to earlier Python versions:

> On Windows, the drive letter is not reset when an absolute path component (e.g., `r'\foo'`) is encountered. If a component contains a drive letter, all previous components are thrown away and the drive letter is reset. Note that since there is a current directory for each drive, `os.path.join(""c:"", ""foo"")` represents a path relative to the current directory on drive `C:` (`c:foo`), not `c:\foo`."
"I have a function that takes the argument `NBins`. I want to make a call to this function with a scalar `50` or an array `[0, 10, 20, 30]`. How can I identify within the function, what the length of `NBins` is? or said differently, if it is a scalar or a vector?

I tried this:

```
>>> N=[2,3,5]
>>> P = 5
>>> len(N)
3
>>> len(P)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: object of type 'int' has no len()
>>> 

```

As you see, I can't apply `len` to `P`, since it's not an array.... Is there something like `isarray` or `isscalar` in python?

thanks","```
>>> import collections.abc
>>> isinstance([0, 10, 20, 30], collections.abc.Sequence) and not isinstance([0, 10, 20, 30], (str, unicode))
True
>>> isinstance(50, collections.abc.Sequence) and not isinstance(50, (str, unicode))
False

```

**note**: `isinstance` also supports a tuple of classes, check `type(x) in (..., ...)` should be avoided and is unnecessary.

You may also wanna check `not isinstance(x, (str, unicode))`

As noted by [@2080](https://stackoverflow.com/questions/16807011/python-how-to-identify-if-a-variable-is-an-array-or-a-scalar/16807050?noredirect=1#comment128960203_16807050) and also [here](https://stackoverflow.com/questions/2937114/python-check-if-an-object-is-a-sequence#comment88639515_2937122) this won't work for `numpy` arrays. eg.

```
>>> import collections.abc
>>> import numpy as np
>>> isinstance((1, 2, 3), collections.abc.Sequence)
True
>>> isinstance(np.array([1, 2, 3]), collections.abc.Sequence)
False

```

In which case you may try the answer from [@jpaddison3](https://stackoverflow.com/a/19773559/1219006):

```
>>> hasattr(np.array([1, 2, 3]), ""__len__"")
True
>>> hasattr([1, 2, 3], ""__len__"")
True
>>> hasattr((1, 2, 3), ""__len__"")
True

```

However as noted [here](https://stackoverflow.com/questions/2937114/python-check-if-an-object-is-a-sequence#comment127867518_63465553), this is not perfect either, and will incorrectly (at least according to me) classify dictionaries as sequences whereas `isinstance` with `collections.abc.Sequence` classifies correctly:

```
>>> hasattr({""a"": 1}, ""__len__"")
True
>>> from numpy.distutils.misc_util import is_sequence
>>> is_sequence({""a"": 1})
True
>>> isinstance({""a"": 1}, collections.abc.Sequence)
False

```

You could customise your solution to something like this, add more types to `isinstance` depending on your needs:

```
>>> isinstance(np.array([1, 2, 3]), (collections.abc.Sequence, np.ndarray))
True
>>> isinstance([1, 2, 3], (collections.abc.Sequence, np.ndarray))
True

```"
"This could be a simple scoping question. The following code in a Python file (module) is confusing me slightly:

```
if __name__ == '__main__':
    x = 1
    
print x

```

In other languages I've worked in, this code would throw an exception, as the `x` variable is local to the `if` statement and should not exist outside of it. But this code executes, and prints 1. Can anyone explain this behavior? Are all variables created in a module global/available to the entire module?","Python variables are scoped to the innermost function, class, or module in which they're assigned. Control blocks like `if` and `while` blocks don't count, so a variable assigned inside an `if` is still scoped to a function, class, or module.

(Implicit functions defined by a generator expression or list/set/dict comprehension *do* count, as do lambda expressions. You can't stuff an assignment statement into any of those, but lambda parameters and `for` clause targets are implicit assignment.)"
"I am trying to implement a ""Digit Recognition OCR"" in OpenCV-Python (cv2). It is just for learning purposes. I would like to learn both KNearest and SVM features in OpenCV.

I have 100 samples (i.e. images) of each digit. I would like to train with them.

There is a sample `letter_recog.py` that comes with OpenCV sample. But I still couldn't figure out on how to use it. I don't understand what are the samples, responses etc. Also, it loads a txt file at first, which I didn't understand first.

Later on searching a little bit, I could find a letter\_recognition.data in cpp samples. I used it and made a code for cv2.KNearest in the model of letter\_recog.py (just for testing):

```
import numpy as np
import cv2

fn = 'letter-recognition.data'
a = np.loadtxt(fn, np.float32, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })
samples, responses = a[:,1:], a[:,0]

model = cv2.KNearest()
retval = model.train(samples,responses)
retval, results, neigh_resp, dists = model.find_nearest(samples, k = 10)
print results.ravel()

```

It gave me an array of size 20000, I don't understand what it is.

**Questions:**

1) What is letter\_recognition.data file? How to build that file from my own data set?

2) What does `results.reval()` denote?

3) How we can write a simple digit recognition tool using letter\_recognition.data file (either KNearest or SVM)?","Well, I decided to workout myself on my question to solve the above problem. What I wanted is to implement a simple OCR using KNearest or SVM features in OpenCV. And below is what I did and how. (it is just for learning how to use KNearest for simple OCR purposes).

**1)** My first question was about `letter_recognition.data` file that comes with OpenCV samples. I wanted to know what is inside that file.

It contains a letter, along with 16 features of that letter.

And [`this SOF`](https://stackoverflow.com/questions/1270798/how-to-create-data-fom-image-like-letter-image-recognition-dataset-from-uci) helped me to find it. These 16 features are explained in the paper [**`Letter Recognition Using Holland-Style Adaptive Classifiers`**](http://cns-classes.bu.edu/cn550/Readings/frey-slate-91.pdf).
(Although I didn't understand some of the features at the end)

**2)** Since I knew, without understanding all those features, it is difficult to do that method. I tried some other papers, but all were a little difficult for a beginner.

So I just decided to take all the pixel values as my features. (I was not worried about accuracy or performance, I just wanted it to work, at least with the least accuracy)

I took the below image for my training data:

![enter image description here](https://i.sstatic.net/IwQY6.png)

(I know the amount of training data is less. But, since all letters are of the same font and size, I decided to try on this).

**To prepare the data for training, I made a small code in OpenCV. It does the following things:**

1. It loads the image.
2. Selects the digits (obviously by contour finding and applying constraints on area and height of letters to avoid false detections).
3. Draws the bounding rectangle around one letter and wait for `key press manually`. This time we **press the digit key ourselves** corresponding to the letter in the box.
4. Once the corresponding digit key is pressed, it resizes this box to 10x10 and saves all 100 pixel values in an array (here, samples) and corresponding manually entered digit in another array(here, responses).
5. Then save both the arrays in separate `.txt` files.

At the end of the manual classification of digits, all the digits in the training data (`train.png`) are labeled manually by ourselves, image will look like below:

![enter image description here](https://i.sstatic.net/jyAhT.png)

Below is the code I used for the above purpose (of course, not so clean):

```
import sys

import numpy as np
import cv2

im = cv2.imread('pitrain.png')
im3 = im.copy()

gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(gray,(5,5),0)
thresh = cv2.adaptiveThreshold(blur,255,1,1,11,2)

#################      Now finding Contours         ###################

contours,hierarchy = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)

samples =  np.empty((0,100))
responses = []
keys = [i for i in range(48,58)]

for cnt in contours:
    if cv2.contourArea(cnt)>50:
        [x,y,w,h] = cv2.boundingRect(cnt)
        
        if  h>28:
            cv2.rectangle(im,(x,y),(x+w,y+h),(0,0,255),2)
            roi = thresh[y:y+h,x:x+w]
            roismall = cv2.resize(roi,(10,10))
            cv2.imshow('norm',im)
            key = cv2.waitKey(0)

            if key == 27:  # (escape to quit)
                sys.exit()
            elif key in keys:
                responses.append(int(chr(key)))
                sample = roismall.reshape((1,100))
                samples = np.append(samples,sample,0)

responses = np.array(responses,np.float32)
responses = responses.reshape((responses.size,1))
print ""training complete""

np.savetxt('generalsamples.data',samples)
np.savetxt('generalresponses.data',responses)

```

---

**Now we enter in to training and testing part.**

For the testing part, I used the below image, which has the same type of letters I used for the training phase.

![enter image description here](https://i.sstatic.net/dPaE8.png)

**For training we do as follows**:

1. Load the `.txt` files we already saved earlier
2. create an instance of the classifier we are using (it is KNearest in this case)
3. Then we use KNearest.train function to train the data

**For testing purposes, we do as follows:**

1. We load the image used for testing
2. process the image as earlier and extract each digit using contour methods
3. Draw a bounding box for it, then resize it to 10x10, and store its pixel values in an array as done earlier.
4. Then we use KNearest.find\_nearest() function to find the nearest item to the one we gave. ( If lucky, it recognizes the correct digit.)

I included last two steps (training and testing) in single code below:

```
import cv2
import numpy as np

#######   training part    ############### 
samples = np.loadtxt('generalsamples.data',np.float32)
responses = np.loadtxt('generalresponses.data',np.float32)
responses = responses.reshape((responses.size,1))

model = cv2.KNearest()
model.train(samples,responses)

############################# testing part  #########################

im = cv2.imread('pi.png')
out = np.zeros(im.shape,np.uint8)
gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
thresh = cv2.adaptiveThreshold(gray,255,1,1,11,2)

contours,hierarchy = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)

for cnt in contours:
    if cv2.contourArea(cnt)>50:
        [x,y,w,h] = cv2.boundingRect(cnt)
        if  h>28:
            cv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),2)
            roi = thresh[y:y+h,x:x+w]
            roismall = cv2.resize(roi,(10,10))
            roismall = roismall.reshape((1,100))
            roismall = np.float32(roismall)
            retval, results, neigh_resp, dists = model.find_nearest(roismall, k = 1)
            string = str(int((results[0][0])))
            cv2.putText(out,string,(x,y+h),0,1,(0,255,0))

cv2.imshow('im',im)
cv2.imshow('out',out)
cv2.waitKey(0)

```

And it worked, below is the result I got:

![enter image description here](https://i.sstatic.net/xS3gF.png)

---

Here it worked with 100% accuracy. I assume this is because all the digits are of the same kind and the same size.

But anyway, this is a good start to go for beginners (I hope so)."
"I've seen plenty of examples of people extracting all of the classes from a module, usually something like:

```
# foo.py
class Foo:
    pass

# test.py
import inspect
import foo

for name, obj in inspect.getmembers(foo):
    if inspect.isclass(obj):
        print obj

```

Awesome.

But I can't find out how to get all of the classes from the *current* module.

```
# foo.py
import inspect

class Foo:
    pass

def print_classes():
    for name, obj in inspect.getmembers(???): # what do I do here?
        if inspect.isclass(obj):
            print obj

# test.py
import foo

foo.print_classes()

```

This is probably something really obvious, but I haven't been able to find anything. Can anyone help me out?","Try this:

```
import sys
current_module = sys.modules[__name__]

```

In your context:

```
import sys, inspect
def print_classes():
    for name, obj in inspect.getmembers(sys.modules[__name__]):
        if inspect.isclass(obj):
            print(obj)

```

And even better:

```
clsmembers = inspect.getmembers(sys.modules[__name__], inspect.isclass)

```

Because `inspect.getmembers()` takes a predicate."
"I'm trying to install a new Python environment on my shared hosting. I follow the steps written in [this post](https://stackoverflow.com/questions/1534210/use-different-python-version-with-virtualenv):

```
mkdir ~/src
wget http://www.python.org/ftp/python/2.7.1/Python-2.7.1.tgz
tar -zxvf Python-2.7.1.tar.gz
cd Python-2.7.1
mkdir ~/.localpython
./configure --prefix=/home/<user>/.localpython
make
make install

```

After coming to the `./configure --prefix=/home/<user>/.localpython` command, I get the following output:

```
checking for --enable-universalsdk... no
checking for --with-universal-archs... 32-bit
checking MACHDEP... linux3
checking EXTRAPLATDIR... 
checking machine type as reported by uname -m... x86_64
checking for --without-gcc... no
checking for gcc... no
checking for cc... no
checking for cl.exe... no
configure: error: in `/home3/mikos89/Python-2.7.1':
configure: error: no acceptable C compiler found in $PATH
See `config.log' for more details.

```

How can this problem be solved? I've been trying to find a solution for 3 hours, but I'm still stuck in one place.

**UPDATE**

Hostgator [does not allow gcc](http://support.hostgator.com/articles/pre-sales-questions/compatible-technologies) on their shared accounts:","The gcc compiler is not in your `$PATH`.
It means either you dont have gcc installed or it's not in your $PATH variable.

To install gcc use this: (run as root)

* Redhat base:

  ```
   yum groupinstall ""Development Tools""

  ```
* Debian base:

  ```
   apt-get install build-essential

  ```
* openSUSE base:

  ```
   zypper install --type pattern devel_basis

  ```
* Alpine:

  ```
   apk add build-base

  ```"
"How do I get the SQL that Django will use on the database from a QuerySet object? I'm trying to debug some strange behavior, but I'm not sure what queries are going to the database.","You print the queryset's `query` attribute.

```
>>> queryset = MyModel.objects.all()
>>> print(queryset.query)
SELECT ""myapp_mymodel"".""id"", ... FROM ""myapp_mymodel""

```"
"Could anyone explain the difference between `filter` and `filter_by` functions in SQLAlchemy?
Which one should I be using?","`filter_by` is used for simple queries on the column names using regular kwargs, like

`db.users.filter_by(name='Joe')`

The same can be accomplished with `filter`, not using kwargs, but instead using the '==' equality operator, which has been overloaded on the db.users.name object:

`db.users.filter(db.users.name=='Joe')`

You can also write more powerful queries using `filter`, such as expressions like:

`db.users.filter(or_(db.users.name=='Ryan', db.users.country=='England'))`"
"```
import csv

with open('thefile.csv', 'rb') as f:
  data = list(csv.reader(f))
  import collections
  counter = collections.defaultdict(int)

  for row in data:
        counter[row[10]] += 1


with open('/pythonwork/thefile_subset11.csv', 'w') as outfile:
    writer = csv.writer(outfile)
    for row in data:
        if counter[row[10]] >= 504:
           writer.writerow(row)

```

This code reads `thefile.csv`, makes changes, and writes results to `thefile_subset1`.

However, when I open the resulting csv in Microsoft Excel, there is an extra blank line after each record!

Is there a way to make it not put an extra blank line?","The `csv.writer` module directly controls line endings and writes `\r\n` into the file directly. In **Python 3** the file must be opened in untranslated text mode with the parameters `'w', newline=''` (empty string) or it will write `\r\r\n` on Windows, where the default text mode will translate each `\n` into `\r\n`.

```
#!python3
with open('/pythonwork/thefile_subset11.csv', 'w', newline='') as outfile:
    writer = csv.writer(outfile)

```

If using the `Path` module:

```
from pathlib import Path
import csv

with Path('/pythonwork/thefile_subset11.csv').open('w', newline='') as outfile:
    writer = csv.writer(outfile)

```

If using the `StringIO` module to build an in-memory result, the result string will contain the translated line terminator:

```
from io import StringIO
import csv

s = StringIO()
writer = csv.writer(s)
writer.writerow([1,2,3])
print(repr(s.getvalue()))  # '1,2,3\r\n'   (Windows result)

```

If writing that string to a file later, remember to use `newline=''`:

```
# built-in open()
with open('/pythonwork/thefile_subset11.csv', 'w', newline='') as f:
    f.write(s.getvalue())

# Path's open()
with Path('/pythonwork/thefile_subset11.csv').open('w', newline='') as f:
    f.write(s.getvalue())

# Path's write_text() added the newline parameter to Python 3.10.
Path('/pythonwork/thefile_subset11.csv').write_text(s.getvalue(), newline='')

```

In **Python 2**, use binary mode to open `outfile` with mode `'wb'` instead of `'w'` to prevent Windows newline translation. Python 2 also has problems with Unicode and requires other workarounds to write non-ASCII text. See the Python 2 link below and the `UnicodeReader` and `UnicodeWriter` examples at the end of the page if you have to deal with writing Unicode strings to CSVs on Python 2, or look into the 3rd party [unicodecsv](https://pypi.org/project/unicodecsv/) module:

```
#!python2
with open('/pythonwork/thefile_subset11.csv', 'wb') as outfile:
    writer = csv.writer(outfile)

```

### Documentation Links

* <https://docs.python.org/3/library/csv.html#csv.writer>
* <https://docs.python.org/2/library/csv.html#csv.writer>"
I'm thinking about putting the virtualenv for a Django web app I am making inside my git repository for the app. It seems like an easy way to keep deploy's simple and easy. Is there any reason why I shouldn't do this?,"I use [`pip freeze`](https://pip.pypa.io/en/stable/cli/pip_freeze/) to get the packages I need into a [`requirements.txt`](https://pip.pypa.io/en/stable/reference/requirements-file-format/#requirements-file-format) file and add that to my repository. I tried to think of a way of why you would want to store the entire virtualenv, but I could not."
"I've got a list of objects. I want to find one (first or whatever) object in this list that has an attribute (or method result - whatever) equal to `value`.

What's the best way to find it?

Here's a test case:

```
class Test:
    def __init__(self, value):
        self.value = value

import random

value = 5

test_list = [Test(random.randint(0,100)) for x in range(1000)]

# that I would do in Pascal, I don't believe it's anywhere near 'Pythonic'
for x in test_list:
    if x.value == value:
        print ""i found it!""
        break

```

I think using generators and `reduce()` won't make any difference because it still would be iterating through the list.

ps.: Equation to `value` is just an example. Of course, we want to get an element that meets any condition.","```
next((x for x in test_list if x.value == value), None)

```

This gets the first item from the list that matches the condition, and returns `None` if no item matches. It's my preferred single-expression form.

However,

```
for x in test_list:
    if x.value == value:
        print(""i found it!"")
        break

```

The naive loop-break version, is perfectly Pythonic -- it's concise, clear, and efficient. To make it match the behavior of the one-liner:

```
for x in test_list:
    if x.value == value:
        print(""i found it!"")
        break
else:
    x = None

```

This will assign `None` to `x` if you don't `break` out of the loop."
"You know how in Linux when you try some Sudo stuff it tells you to enter the password and, as you type, nothing is shown in the terminal window (the password is not shown)?

Is there a way to do that in Python? I'm working on a script that requires so sensitive info and would like for it to be hidden when I'm typing it.

In other words, I want to get the password from the user without showing the password.","Use [`getpass.getpass()`](http://docs.python.org/library/getpass.html#getpass.getpass):

```
from getpass import getpass
password = getpass()

```

An optional prompt can be passed as parameter; the default is `""Password: ""`.

Note that this function requires a proper terminal, so it can turn off echoing of typed characters – see [“GetPassWarning: Can not control echo on the terminal” when running from IDLE](https://stackoverflow.com/questions/38878741/getpasswarning-can-not-control-echo-on-the-terminal-when-running-from-idle) for further details."
"From `pip install --help`:

```
--user  Install to the Python user install directory for your platform.
        Typically ~/.local/, or %APPDATA%\Python on Windows.
        (See the Python documentation for site.USER_BASE for full details.)

```

The documentation for `site.USER_BASE` is a terrifying wormhole of interesting [Unix](https://en.wikipedia.org/wiki/Unix)-like subject matter that I don't understand.

What is the purpose of `--user` in plain English? Why would installing the package to `~/.local/` matter? Why not just put an executable somewhere in my `$PATH`?","pip defaults to installing Python packages to a system directory (such as `/usr/local/lib/python3.4`). This requires root access.

`--user` makes pip install packages in your home directory instead, which doesn't require any special privileges."
"Why doesn't list have a safe ""get"" method like dictionary?

```
>>> d = {'a':'b'}
>>> d['a']
'b'
>>> d['c']
KeyError: 'c'
>>> d.get('c', 'fail')
'fail'

>>> l = [1]
>>> l[10]
IndexError: list index out of range

```","Ultimately it probably doesn't have a safe `.get` method because a `dict` is an associative collection (values are associated with names) where it is inefficient to check if a key is present (and return its value) without throwing an exception, while it is super trivial to avoid exceptions accessing list elements (as the `len` method is very fast). The `.get` method allows you to query the value associated with a name, not directly access the 37th item in the dictionary (which would be more like what you're asking of your list).

Of course, you can easily implement this yourself:

```
def safe_list_get (l, idx, default):
  try:
    return l[idx]
  except IndexError:
    return default

```

You could even monkeypatch it onto the `__builtins__.list` constructor in `__main__`, but that would be a less pervasive change since most code doesn't use it. If you just wanted to use this with lists created by your own code you could simply subclass `list` and add the `get` method."
"I often see comments on other Stack Overflow questions about how the use of `except: pass` is discouraged. Why is this bad? Sometimes I just don't care what the errors are and I want to just continue with the code.

```
try:
    something
except:
    pass

```

Why is using an `except: pass` block bad? What makes it bad? Is it the fact that I `pass` on an error or that I `except` any error?","As you correctly guessed, there are two sides to it: Catching *any* error by specifying no exception type after `except`, and simply passing it without taking any action.

My explanation is “a bit” longer—so tl;dr it breaks down to this:

1. **Don’t catch *any* error**. Always specify which exceptions you are prepared to recover from and only catch those.
2. **Try to avoid passing in except blocks**. Unless explicitly desired, this is usually not a good sign.

But let’s go into detail:

### Don’t catch *any* error

When using a `try` block, you usually do this because you know that there is a chance of an exception being thrown. As such, you also already have an approximate idea of *what* can break and what exception can be thrown. In such cases, you catch an exception because you can *positively recover* from it. That means that you are prepared for the exception and have some alternative plan which you will follow in case of that exception.

For example, when you ask for the user to input a number, you can convert the input using `int()` which might raise a [`ValueError`](http://docs.python.org/3/library/exceptions.html#ValueError). You can easily recover that by simply asking the user to try it again, so catching the `ValueError` and prompting the user again would be an appropriate plan. A different example would be if you want to read some configuration from a file, and that file happens to not exist. Because it is a configuration file, you might have some default configuration as a fallback, so the file is not exactly necessary. So catching a [`FileNotFoundError`](http://docs.python.org/3/library/exceptions.html#FileNotFoundError) and simply applying the default configuration would be a good plan here. Now in both these cases, we have a very specific exception we expect and have an equally specific plan to recover from it. As such, in each case, we explicitly only `except` *that certain* exception.

However, if we were to catch *everything*, then—in addition to those exceptions we are prepared to recover from—there is also a chance that we get exceptions that we didn’t expect, and which we indeed cannot recover from; or shouldn’t recover from.

Let’s take the configuration file example from above. In case of a missing file, we just applied our default configuration and might decide at a later point to automatically save the configuration (so next time, the file exists). Now imagine we get a [`IsADirectoryError`](http://docs.python.org/3/library/exceptions.html#IsADirectoryError), or a [`PermissionError`](http://docs.python.org/3/library/exceptions.html#PermissionError) instead. In such cases, we probably do not want to continue; we could still apply our default configuration, but we later won’t be able to save the file. And it’s likely that the user meant to have a custom configuration too, so using the default values is likely not desired. So we would want to tell the user about it immediately, and probably abort the program execution too. But that’s not something we want to do somewhere deep within some small code part; this is something of application-level importance, so it should be handled at the top—so let the exception bubble up.

Another simple example is also mentioned in the [Python 2 idioms](http://docs.python.org/2/howto/doanddont.html#except) document. Here, a simple typo exists in the code which causes it to break. Because we are catching *every* exception, we also catch [`NameError`s](http://docs.python.org/3/library/exceptions.html#NameError) and [`SyntaxError`s](http://docs.python.org/3/library/exceptions.html#SyntaxError). Both are mistakes that happen to us all while programming and both are mistakes we absolutely don’t want to include when shipping the code. But because we also caught those, we won’t even know that they occurred there and lose any help to debug it correctly.

But there are also more dangerous exceptions which we are unlikely prepared for. For example, [SystemError](http://docs.python.org/3/library/exceptions.html#SystemError) is usually something that happens rarely and which we cannot really plan for; it means there is something more complicated going on, something that likely prevents us from continuing the current task.

In any case, it’s very unlikely that you are prepared for everything in a small-scale part of the code, so that’s really where you should only catch those exceptions you are prepared for. Some people suggest to at least catch [`Exception`](http://docs.python.org/3/library/exceptions.html#Exception) as it won’t include things like `SystemExit` and `KeyboardInterrupt` which *by design* are to terminate your application, but I would argue that this is still far too unspecific. There is only one place where I personally accept catching `Exception` or just *any* exception, and that is in a single global application-level exception handler which has the single purpose to log any exception we were not prepared for. That way, we can still retain as much information about unexpected exceptions, which we then can use to extend our code to handle those explicitly (if we can recover from them) or—in case of a bug—to create test cases to make sure it won’t happen again. But of course, that only works if we only ever caught those exceptions we were already expecting, so the ones we didn’t expect will naturally bubble up.

### Try to avoid passing in except blocks

When explicitly catching a small selection of specific exceptions, there are many situations in which we will be fine by simply doing nothing. In such cases, just having `except SomeSpecificException: pass` is just fine. Most of the time though, this is not the case as we likely need some code related to the recovery process (as mentioned above). This can be for example something that retries the action again, or to set up a default value instead.

If that’s not the case though, for example, because our code is already structured to repeat until it succeeds, then just passing is good enough. Taking our example from above, we might want to ask the user to enter a number. Because we know that users like to not do what we ask them for, we might just put it into a loop in the first place, so it could look like this:

```
def askForNumber ():
    while True:
        try:
            return int(input('Please enter a number: '))
        except ValueError:
            pass

```

Because we keep trying until no exception is thrown, we don’t need to do anything special in the except block, so this is fine. But of course, one might argue that we at least want to show the user some error message to tell him why he has to repeat the input.

In many other cases though, just passing in an `except` is a sign that we weren’t really prepared for the exception we are catching. Unless those exceptions are simple (like `ValueError` or `TypeError`), and the reason why we can pass is obvious, try to avoid just passing. If there’s really nothing to do (and you are absolutely sure about it), then consider adding a comment why that’s the case; otherwise, expand the except block to actually include some recovery code.

### `except: pass`

The worst offender though is the combination of both. This means that we are willingly catching *any* error although we are absolutely not prepared for it **and** we also don’t do anything about it. You *at least* want to log the error and also likely reraise it to still terminate the application (it’s unlikely you can continue like normal after a MemoryError). Just passing though will not only keep the application somewhat alive (depending on where you catch of course), but also throw away all the information, making it impossible to discover the error—which is especially true if you are not the one discovering it.

---

So the bottom line is: Catch only exceptions you really expect and are prepared to recover from; all others are likely either mistakes you should fix or something you are not prepared for anyway. Passing *specific* exceptions are fine if you really don’t need to do something about them. In all other cases, it’s just a sign of presumption and being lazy. And you definitely want to fix that."
"Is there a way to find all Python PyPI packages that were installed with easy\_install or pip? I mean, excluding everything that was/is installed with the distributions tools (in this case apt-get on Debian).","`pip freeze` will output a list of installed packages and their versions. It also allows you to write those packages to a file that can later be used to set up a new environment.

<https://pip.pypa.io/en/stable/reference/pip_freeze/#pip-freeze>"
"What's the recommended Python idiom for splitting a string on the *last* occurrence of the delimiter in the string? example:

```
# instead of regular split
>> s = ""a,b,c,d""
>> s.split("","")
>> ['a', 'b', 'c', 'd']

# ..split only on last occurrence of ',' in string:
>>> s.mysplit(s, -1)
>>> ['a,b,c', 'd']

```

`mysplit` takes a second argument that is the occurrence of the delimiter to be split. Like in regular list indexing, `-1` means the last from the end. How can this be done?","Use [`.rsplit()`](http://docs.python.org/library/stdtypes.html#str.rsplit) or [`.rpartition()`](http://docs.python.org/library/stdtypes.html#str.rpartition) instead:

```
s.rsplit(',', 1)
s.rpartition(',')

```

`str.rsplit()` lets you specify how many times to split, while `str.rpartition()` only splits once but always returns a fixed number of elements (prefix, delimiter & postfix) and is faster for the single split case.

Demo:

```
>>> s = ""a,b,c,d""
>>> s.rsplit(',', 1)
['a,b,c', 'd']
>>> s.rsplit(',', 2)
['a,b', 'c', 'd']
>>> s.rpartition(',')
('a,b,c', ',', 'd')

```

Both methods start splitting from the right-hand-side of the string; by giving `str.rsplit()` a maximum as the second argument, you get to split just the right-hand-most occurrences.

If you only need the last element, but there is a chance that the delimiter is not present in the input string or is the very last character in the input, use the following expressions:

```
# last element, or the original if no `,` is present or is the last character
s.rsplit(',', 1)[-1] or s
s.rpartition(',')[-1] or s

```

If you need the delimiter gone even when it is the last character, I'd use:

```
def last(string, delimiter):
    """"""Return the last element from string, after the delimiter

    If string ends in the delimiter or the delimiter is absent,
    returns the original string without the delimiter.

    """"""
    prefix, delim, last = string.rpartition(delimiter)
    return last if (delim and last) else prefix

```

This uses the fact that `string.rpartition()` returns the delimiter as the second argument only if it was present, and an empty string otherwise."
"I'm trying to do this query in sqlalchemy

```
SELECT id, name FROM user WHERE id IN (123, 456)

```

I would like to bind the list `[123, 456]` at execution time.","How about

```
session.query(MyUserClass).filter(MyUserClass.id.in_((123,456))).all()

```

**edit**: Without the ORM, it would be

```
session.execute(
    select(
        [MyUserTable.c.id, MyUserTable.c.name], 
        MyUserTable.c.id.in_((123, 456))
    )
).fetchall()

```

[`select()`](http://docs.sqlalchemy.org/en/rel_0_8/core/expression_api.html#sqlalchemy.sql.expression.select) takes two parameters, the first one is a list of fields to retrieve, the second one is the `where` condition. You can access all fields on a table object via the `c` (or `columns`) property."
I would like to know how to simply reverse the color order of a given colormap in order to use it with plot\_surface.,The standard colormaps also all have reversed versions. They have the same names with `_r` tacked on to the end. ([Documentation here.](https://matplotlib.org/stable/gallery/color/colormap_reference.html#reversed-colormaps))
"Can you tell me when to use these vectorization methods with basic examples?

I see that `map` is a `Series` method whereas the rest are `DataFrame` methods. I got confused about `apply` and `applymap` methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!","`apply` works on a row / column basis of a DataFrame  
`applymap` works element-wise on a DataFrame  
`map` works element-wise on a Series

---

Straight from Wes McKinney's [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do) book, pg. 132 (I highly recommended this book):

> Another frequent operation is applying a function on 1D arrays to each column or row. DataFrameâ€™s apply method does exactly this:

```
In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])

In [117]: frame
Out[117]: 
               b         d         e
Utah   -0.029638  1.081563  1.280300
Ohio    0.647747  0.831136 -1.549481
Texas   0.513416 -0.884417  0.195343
Oregon -0.485454 -0.477388 -0.309548

In [118]: f = lambda x: x.max() - x.min()

In [119]: frame.apply(f)
Out[119]: 
b    1.133201
d    1.965980
e    2.829781
dtype: float64

```

> Many of the most common array statistics (like sum and mean) are DataFrame methods,
> so using apply is not necessary.

> Element-wise Python functions can be used, too. Suppose you wanted to compute a formatted string from each floating point value in frame. You can do this with applymap:

```
In [120]: format = lambda x: '%.2f' % x

In [121]: frame.applymap(format)
Out[121]: 
            b      d      e
Utah    -0.03   1.08   1.28
Ohio     0.65   0.83  -1.55
Texas    0.51  -0.88   0.20
Oregon  -0.49  -0.48  -0.31

```

> The reason for the name applymap is that Series has a map method for applying an element-wise function:

```
In [122]: frame['e'].map(format)
Out[122]: 
Utah       1.28
Ohio      -1.55
Texas      0.20
Oregon    -0.31
Name: e, dtype: object

```"
"I am trying to use [Pythons mock package](http://www.voidspace.org.uk/python/mock) to mock Pythons `requests` module. What are the basic calls to get me working in below scenario?

In my views.py, I have a function that makes variety of requests.get() calls with different response each time

```
def myview(request):
  res1 = requests.get('aurl')
  res2 = request.get('burl')
  res3 = request.get('curl')

```

In my test class I want to do something like this but cannot figure out exact method calls

Step 1:

```
# Mock the requests module
# when mockedRequests.get('aurl') is called then return 'a response'
# when mockedRequests.get('burl') is called then return 'b response'
# when mockedRequests.get('curl') is called then return 'c response'

```

Step 2:

Call my view

Step 3:

verify response contains 'a response', 'b response' , 'c response'

How can I complete Step 1 (mocking the requests module)?","This is how you can do it (you can run this file as-is):

```
import requests
import unittest
from unittest import mock

# This is the class we want to test
class MyGreatClass:
    def fetch_json(self, url):
        response = requests.get(url)
        return response.json()

# This method will be used by the mock to replace requests.get
def mocked_requests_get(*args, **kwargs):
    class MockResponse:
        def __init__(self, json_data, status_code):
            self.json_data = json_data
            self.status_code = status_code

        def json(self):
            return self.json_data

    if args[0] == 'http://someurl.com/test.json':
        return MockResponse({""key1"": ""value1""}, 200)
    elif args[0] == 'http://someotherurl.com/anothertest.json':
        return MockResponse({""key2"": ""value2""}, 200)

    return MockResponse(None, 404)

# Our test case class
class MyGreatClassTestCase(unittest.TestCase):

    # We patch 'requests.get' with our own method. The mock object is passed in to our test case method.
    @mock.patch('requests.get', side_effect=mocked_requests_get)
    def test_fetch(self, mock_get):
        # Assert requests.get calls
        mgc = MyGreatClass()
        json_data = mgc.fetch_json('http://someurl.com/test.json')
        self.assertEqual(json_data, {""key1"": ""value1""})
        json_data = mgc.fetch_json('http://someotherurl.com/anothertest.json')
        self.assertEqual(json_data, {""key2"": ""value2""})
        json_data = mgc.fetch_json('http://nonexistenturl.com/cantfindme.json')
        self.assertIsNone(json_data)

        # We can even assert that our mocked method was called with the right parameters
        self.assertIn(mock.call('http://someurl.com/test.json'), mock_get.call_args_list)
        self.assertIn(mock.call('http://someotherurl.com/anothertest.json'), mock_get.call_args_list)

        self.assertEqual(len(mock_get.call_args_list), 3)

if __name__ == '__main__':
    unittest.main()

```

**Important Note:** If your `MyGreatClass` class lives in a different package, say `my.great.package`, you have to mock `my.great.package.requests.get` instead of just 'request.get'. In that case your test case would look like this:

```
import unittest
from unittest import mock
from my.great.package import MyGreatClass

# This method will be used by the mock to replace requests.get
def mocked_requests_get(*args, **kwargs):
    # Same as above


class MyGreatClassTestCase(unittest.TestCase):

    # Now we must patch 'my.great.package.requests.get'
    @mock.patch('my.great.package.requests.get', side_effect=mocked_requests_get)
    def test_fetch(self, mock_get):
        # Same as above

if __name__ == '__main__':
    unittest.main()

```

Enjoy!"
"Is there a Pool class for worker **threads**, similar to the multiprocessing module's [Pool class](http://docs.python.org/library/multiprocessing.html#module-multiprocessing.pool)?

I like for example the easy way to parallelize a map function

```
def long_running_func(p):
    c_func_no_gil(p)

p = multiprocessing.Pool(4)
xs = p.map(long_running_func, range(100))

```

however I would like to do it without the overhead of creating new processes.

I know about the GIL. However, in my usecase, the function will be an IO-bound C function for which the python wrapper will release the GIL before the actual function call.

Do I have to write my own threading pool?","I just found out that there actually *is* a thread-based Pool interface in the `multiprocessing` module, however it is hidden somewhat and not properly documented.

It can be imported via

```
from multiprocessing.pool import ThreadPool

```

It is implemented using a dummy Process class wrapping a python thread. This thread-based Process class can be found in [`multiprocessing.dummy`](http://docs.python.org/dev/py3k/library/multiprocessing.html#module-multiprocessing.dummy) which is mentioned briefly in the [docs](http://docs.python.org/dev/py3k/library/multiprocessing.html#module-multiprocessing.dummy). This dummy module supposedly provides the whole multiprocessing interface based on threads."
"I'm trying to rename some files in a directory using Python.

Say I have a file called `CHEESE_CHEESE_TYPE.***` and want to remove `CHEESE_` so my resulting filename would be `CHEESE_TYPE`

I'm trying to use the `os.path.split` but it's not working properly. I have also considered using string manipulations, but have not been successful with that either.","Use [`os.rename(src, dst)`](https://docs.python.org/3/library/os.html#os.rename) to rename or move a file or a directory.

```
$ ls
cheese_cheese_type.bar  cheese_cheese_type.foo
$ python
>>> import os
>>> for filename in os.listdir("".""):
...  if filename.startswith(""cheese_""):
...    os.rename(filename, filename[7:])
... 
>>> 
$ ls
cheese_type.bar  cheese_type.foo

```"
"How can I indicate a newline in a string in Python, so that I can write multiple lines to a text file?","It depends on how correct you want to be. `\n` will usually do the job. If you really want to get it right, you look up the newline character in the [`os` package](http://docs.python.org/library/os.html). (It's actually called `linesep`.)

Note: when writing to files using the Python API, do not use the `os.linesep`. Just use `\n`; Python automatically translates that to the proper newline character for your platform."
"Suppose I have a function:

```
def get_some_date(some_argument: int=None) -> %datetime_or_None%:
    if some_argument is not None and some_argument == 1:
        return datetime.utcnow()
    else:
        return None

```

How do I specify the return type for something that can be `None`?","You're looking for [`Optional`](https://docs.python.org/3/library/typing.html#typing.Optional).

Since your return type can either be `datetime` (as returned from `datetime.utcnow()`) or `None` you should use `Optional[datetime]`:

```
from typing import Optional

def get_some_date(some_argument: int=None) -> Optional[datetime]:
    # as defined

```

From the documentation on typing, `Optional` is shorthand for:

> `Optional[X]` is equivalent to `Union[X, None]`.

where `Union[X, Y]` means a value of type `X` or `Y`.

---

If you want to be explicit due to concerns that others might stumble on `Optional` and not realize it's meaning, you could always use `Union`:

```
from typing import Union

def get_some_date(some_argument: int=None) -> Union[datetime, None]:

```

But I doubt this is a good idea, `Optional` is an indicative name and it does save a couple of keystrokes.

As pointed out in the comments by @Michael0x2a `Union[T, None]` is tranformed to `Union[T, type(None)]` so no need to use `type` here.

Visually these might differ but programatically, in both cases, the result is *exactly the same*; `Union[datetime.datetime, NoneType]` will be the type stored in `get_some_date.__annotations__`\*:

```
>>> from typing import get_type_hints
>>> print(get_type_hints(get_some_date))
{'return': typing.Union[datetime.datetime, NoneType],
 'some_argument': typing.Union[int, NoneType]}

```

\*Use `typing.get_type_hints` to grab the objects' `__annotations__` attribute instead of accessing it directly."
"I can't figure out how to handle exception for python 'with' statement. If I have a code:

```
with open(""a.txt"") as f:
    print f.readlines()

```

I really want to handle 'file not found exception' in order to do something. But I can't write

```
with open(""a.txt"") as f:
    print f.readlines()
except:
    print 'oops'

```

and can't write

```
with open(""a.txt"") as f:
    print f.readlines()
else:
    print 'oops'

```

Enclosing `with` in a try/except statement doesn't work either, and an exception is not raised. What can I do in order to process failure inside `with` statement in a Pythonic way?","This solution will keep the with-block-code outside of the try-except-clause.

```
try:
    f = open('foo.txt')
except FileNotFoundError:
    print('error')
else:
    with f:
        print f.readlines()

```"
"How can I crop images, like I've done before in PIL, using OpenCV.

Working example on PIL

```
im = Image.open('0.png').convert('L')
im = im.crop((1, 1, 98, 33))
im.save('_0.png')

```

But how I can do it on OpenCV?

This is what I tried:

```
im = cv.imread('0.png', cv.CV_LOAD_IMAGE_GRAYSCALE)
(thresh, im_bw) = cv.threshold(im, 128, 255, cv.THRESH_OTSU)
im = cv.getRectSubPix(im_bw, (98, 33), (1, 1))
cv.imshow('Img', im)
cv.waitKey(0)

```

But it doesn't work.

I think I incorrectly used `getRectSubPix`. If this is the case, please explain how I can correctly use this function.","It's very simple. Use numpy slicing.

```
import cv2
img = cv2.imread(""lenna.png"")
crop_img = img[y:y+h, x:x+w]
cv2.imshow(""cropped"", crop_img)
cv2.waitKey(0)

```"
I have a text file. How can I check whether it's empty or not?,"```
>>> import os
>>> os.stat(""file"").st_size == 0
True

```"
"I have a list of sets:

```
setlist = [s1,s2,s3...]

```

I want s1 ∩ s2 ∩ s3 ...

I can write a function to do it by performing a series of pairwise `s1.intersection(s2)`, etc.

Is there a recommended, better, or built-in way?","From Python version 2.6 on you can use multiple arguments to [`set.intersection()`](https://docs.python.org/library/stdtypes.html#frozenset.intersection), like

```
u = set.intersection(s1, s2, s3)

```

If the sets are in a list, this translates to:

```
u = set.intersection(*setlist)

```

where `*a_list` is [list expansion](https://stackoverflow.com/questions/36901/what-does-double-star-and-star-do-for-python-parameters)

Note that `set.intersection` is *not* a static method, but this uses the functional notation to apply intersection of the first set with the rest of the list. So if the argument list is empty this will fail."
"I've noticed the following code is legal in Python. My question is why? Is there a specific reason?

```
n = 5
while n != 0:
    print n
    n -= 1
else:
    print ""what the...""

```

---

Many beginners accidentally stumble on this syntax when they try to put an `if`/`else` block inside of a `while` or `for` loop, and don't indent the `else` properly. The solution is to make sure the `else` block lines up with the `if`, assuming that it was your intent to pair them. This question explains *why it didn't cause a syntax error*, and *what the resulting code means.* See also [I'm getting an IndentationError. How do I fix it?](https://stackoverflow.com/questions/45621722), for the cases where there *is* a syntax error reported.

See also [Why does python use 'else' after for and while loops?](https://stackoverflow.com/questions/9979970/) for questions about *how to make good use of* the feature.","The `else` clause is only executed after the `while` condition is evaluated to be false.

Thus, if you `break` out of the loop, or if an exception is raised, the `else` won't be executed (since the `while` condition has not been evaluated to be false yet).

One way to think about it is as an if/else construct with respect to the condition:

```
if condition:
    handle_true()
else:
    handle_false()

```

is analogous to the looping construct:

```
while condition:
    handle_true()
else:
    # condition is false now, handle and go on with the rest of the program
    handle_false()

```

An example might be along the lines of:

```
while value < threshold:
    if not process_acceptable_value(value):
        # something went wrong, exit the loop; don't pass go, don't collect 200
        break
    value = update(value)
else:
    # value >= threshold; pass go, collect 200
    handle_threshold_reached()

```"
"How do I create a directory at a given path, and also create any missing parent directories along that path? For example, the Bash command `mkdir -p /path/to/nested/directory` does this.","On Python ≥ 3.5, use [`pathlib.Path.mkdir`](https://docs.python.org/library/pathlib.html#pathlib.Path.mkdir):

```
from pathlib import Path
Path(""/my/directory"").mkdir(parents=True, exist_ok=True)

```

For older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:

Try [`os.path.exists`](https://docs.python.org/library/os.path.html#os.path.exists), and consider [`os.makedirs`](https://docs.python.org/library/os.html#os.makedirs) for the creation.

```
import os
if not os.path.exists(directory):
    os.makedirs(directory)

```

As noted in comments and elsewhere, there's a race condition – if the directory is created between the `os.path.exists` and the `os.makedirs` calls, the `os.makedirs` will fail with an `OSError`. Unfortunately, blanket-catching `OSError` and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.

One option would be to trap the `OSError` and examine the embedded error code (see [Is there a cross-platform way of getting information from Python’s OSError](https://stackoverflow.com/questions/273698/is-there-a-cross-platform-way-of-getting-information-from-pythons-oserror)):

```
import os, errno

try:
    os.makedirs(directory)
except OSError as e:
    if e.errno != errno.EEXIST:
        raise

```

Alternatively, there could be a second `os.path.exists`, but suppose another created the directory after the first check, then removed it before the second one – we could still be fooled.

Depending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.

Modern versions of Python improve this code quite a bit, both by exposing [`FileExistsError`](https://docs.python.org/3.3/library/exceptions.html?#FileExistsError) (in 3.3+)...

```
try:
    os.makedirs(""path/to/directory"")
except FileExistsError:
    # directory already exists
    pass

```

...and by allowing [a keyword argument to `os.makedirs` called `exist_ok`](https://docs.python.org/3.2/library/os.html#os.makedirs) (in 3.2+).

```
os.makedirs(""path/to/directory"", exist_ok=True)  # succeeds even if directory exists.

```"
How can I find local IP addresses (i.e. 192.168.x.x or 10.0.x.x) in Python platform independently and using only the standard library?,"I just found this but it seems a bit hackish, however they say tried it on \*nix and I did on windows and it worked.

```
import socket
s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
s.connect((""8.8.8.8"", 80))
print(s.getsockname()[0])
s.close()

```

This assumes you have an internet access, and that there is no local proxy."
"I have made my plots inline on my Ipython Notebook with ""`%matplotlib inline`.""

Now, the plot appears. However, it is very small. Is there a way to make it appear larger using either notebook settings or plot settings?

[![enter image description here](https://i.sstatic.net/TiQum.png)](https://i.sstatic.net/TiQum.png)","The default figure size (in inches) is controlled by

```
matplotlib.rcParams['figure.figsize'] = [width, height]

```

For example:

```
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [10, 5]

```

creates a figure with 10 (width) x 5 (height) inches"
"I use `pandas.to_datetime` to parse the dates in my data. Pandas by default represents the dates with `datetime64[ns]` even though the dates are all daily only.
I wonder whether there is an elegant/clever way to convert the dates to `datetime.date` or `datetime64[D]` so that, when I write the data to CSV, the dates are not appended with `00:00:00`. I know I can convert the type manually element-by-element:

```
[dt.to_datetime().date() for dt in df.dates]

```

But this is really slow since I have many rows and it sort of defeats the purpose of using `pandas.to_datetime`. Is there a way to convert the `dtype` of the entire column at once? Or alternatively, does `pandas.to_datetime` support a precision specification so that I can get rid of the time part while working with daily data?","[Since version `0.15.0`](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.15.0.html) this can now be easily done using [`.dt`](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor) to access just the date component:

```
df['just_date'] = df['dates'].dt.date

```

The above returns `datetime.date`, so `object` dtype. If you want to keep the dtype as `datetime64` then you can just [`normalize`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.normalize.html#pandas.Series.dt.normalize):

```
df['normalised_date'] = df['dates'].dt.normalize()

```

This sets the time component to midnight, i.e. `00:00:00`, but the display shows just the date value.

* [`pandas.Series.dt`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html)"
"Can I insert a column at a specific column index in pandas?

```
import pandas as pd
df = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})
df['n'] = 0

```

This will put column `n` as the last column of `df`, but isn't there a way to tell `df` to put `n` at the beginning?","see docs: <http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html>

using loc = 0 will insert at the beginning

```
df.insert(loc, column, value)

```

---

```
df = pd.DataFrame({'B': [1, 2, 3], 'C': [4, 5, 6]})

df
Out: 
   B  C
0  1  4
1  2  5
2  3  6

idx = 0
new_col = [7, 8, 9]  # can be a list, a Series, an array or a scalar   
df.insert(loc=idx, column='A', value=new_col)

df
Out: 
   A  B  C
0  7  1  4
1  8  2  5
2  9  3  6

```"
"I'm trying to port a shell script to the much more readable python version. The original shell script starts several processes (utilities, monitors, etc.) in the background with ""&"". How can I achieve the same effect in python? I'd like these processes not to die when the python scripts complete. I am sure it's related to the concept of a daemon somehow, but I couldn't find how to do this easily.","While [jkp](https://stackoverflow.com/questions/1196074/starting-a-background-process-in-python/1196122#1196122)'s solution works, the newer way of doing things (and the way the documentation recommends) is to use the `subprocess` module. For simple commands its equivalent, but it offers more options if you want to do something complicated.

Example for your case:

```
import subprocess
subprocess.Popen([""rm"",""-r"",""some.file""])

```

This will run `rm -r some.file` in the background. Note that calling `.communicate()` on the object returned from `Popen` will block until it completes, so don't do that if you want it to run in the background:

```
import subprocess
ls_output=subprocess.Popen([""sleep"", ""30""])
ls_output.communicate()  # Will block for 30 seconds

```

See the documentation [here](http://docs.python.org/library/subprocess.html#module-subprocess).

Also, a point of clarification: ""Background"" as you use it here is purely a shell concept; technically, what you mean is that you want to spawn a process without blocking while you wait for it to complete. However, I've used ""background"" here to refer to shell-background-like behavior."
I have two Series `s1` and `s2` with the same (non-consecutive) indices. How do I combine `s1` and `s2` to being two columns in a DataFrame and keep one of the indices as a third column?,"I think [`concat`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.merge.concat.html) is a nice way to do this. If they are present it uses the name attributes of the Series as the columns (otherwise it simply numbers them):

```
In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')

In [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')

In [3]: pd.concat([s1, s2], axis=1)
Out[3]:
   s1  s2
A   1   3
B   2   4

In [4]: pd.concat([s1, s2], axis=1).reset_index()
Out[4]:
  index  s1  s2
0     A   1   3
1     B   2   4

```

*Note: This extends to more than 2 Series.*"
"How do I concatenate two one-dimensional arrays in [NumPy](http://en.wikipedia.org/wiki/NumPy)? I tried [`numpy.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html):

```
import numpy as np
a = np.array([1, 2, 3])
b = np.array([4, 5])
np.concatenate(a, b)

```

But I get an error:

> TypeError: only length-1 arrays can be converted to Python scalars","Use:

```
np.concatenate([a, b])

```

The arrays you want to concatenate need to be passed in as a sequence, not as separate arguments.

From the [NumPy documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html):

> `numpy.concatenate((a1, a2, ...), axis=0)`
>
> Join a sequence of arrays together.

It was trying to interpret your `b` as the axis parameter, which is why it complained it couldn't convert it into a scalar."
"In the `requirements.txt` for a Python library I am using, one of the requirements is specified like:

```
mock-django~=0.6.10

```

What does `~=` mean?","It means it will select the latest version of the package, greater than or equal to 0.6.10, but still in the 0.6.\* version, so it won't download 0.7.0 for example. It ensures you will get security fixes but keep backward-compatibility, if the package maintainer respects the semantic versioning (which states that breaking changes should occur only in major versions).

Or, as said by PEP 440:

> For a given release identifier V.N , the compatible release clause is approximately equivalent to the pair of comparison clauses:
>
> `>= V.N, == V.*`

* [Definition in PEP 440](https://www.python.org/dev/peps/pep-0440/#compatible-release)
* [Complete example here in the documentation](https://pip.pypa.io/en/stable/reference/requirement-specifiers/#requirement-specifiers)"
"Why do I get the following error when doing this in python:

```
>>> import locale
>>> print str( locale.getlocale() )
(None, None)
>>> locale.setlocale(locale.LC_ALL, 'de_DE')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/locale.py"", line 531, in setlocale
    return _setlocale(category, locale)
locale.Error: unsupported locale setting

```

This works with other locales like fr or nl as well. I'm using Ubuntu 11.04.

Update: Doing the following did not yield anything:

```
dpkg-reconfigure locales
perl: warning: Setting locale failed.
perl: warning: Please check that your locale settings:
    LANGUAGE = (unset),
    LC_ALL = (unset),
    LC_CTYPE = ""UTF-8"",
    LANG = (unset)
    are supported and installed on your system.
perl: warning: Falling back to the standard locale (""C"").
locale: Cannot set LC_CTYPE to default locale: No such file or directory
locale: Cannot set LC_ALL to default locale: No such file or directory

```","Run following commands

```
export LC_ALL=""en_US.UTF-8""
export LC_CTYPE=""en_US.UTF-8""
sudo dpkg-reconfigure locales

```

It will solve this.

Make sure to match the `.UTF-8` part to the actual syntax found in the output of `locale -a` e.g. `.utf8` on some systems."
"I am trying to raise a Warning in Python without making the program crash / stop / interrupt.

I use the following simple function to check if the user passed a non-zero number to it. If so, the program should warn them, but continue as per normal. It should work like the code below, but should use class `Warning()`, `Error()` or `Exception()` instead of printing the warning out manually.

```
def is_zero(i):
   if i != 0:
     print ""OK""
   else:
     print ""WARNING: the input is 0!""
   return i

```

If I use the code below and pass 0 to the function, the program crashes and the value is never returned. Instead, I want the program to continue normally and just inform the user that he passed 0 to the function.

```
def is_zero(i):
   if i != 0:
     print ""OK""
   else:
     raise Warning(""the input is 0!"")
   return i

```

I want to be able to test that a warning has been thrown testing it by unittest. If I simply print the message out, I am not able to test it with assertRaises in unittest.","```
import warnings
warnings.warn(""Warning...........Message"")

```

See the python documentation: [here](http://docs.python.org/library/warnings.html#warnings.warn)"
"What would be the most elegant and efficient way of finding/returning the first list item that matches a certain criterion?

For example, if I have a list of objects and I would like to get the first object of those with attribute `obj.val==5`. I could of course use list comprehension, but that would incur O(n) and if n is large, it's wasteful. I could also use a loop with `break` once the criterion was met, but I thought there could be a more pythonic/elegant solution.","If you don't have any other indexes or sorted information for your objects, then you will have to iterate until such an object is found:

```
next(obj for obj in objs if obj.val == 5)

```

This is however faster than a complete list comprehension. Compare these two:

```
[i for i in xrange(100000) if i == 1000][0]

next(i for i in xrange(100000) if i == 1000)

```

The first one needs 5.75ms, the second one 58.3Âµs (100 times faster because the loop 100 times shorter)."
"I have constructed a condition that extracts exactly one row from my dataframe:

```
d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]

```

Now I would like to take a value from a particular column:

```
val = d2['col_name']

```

But as a result, I get a dataframe that contains one row and one column (i.e., one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?","If you have a DataFrame with only one row, then access the first (only) row as a Series using *[iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)*, and then the value using the column name:

```
In [3]: sub_df
Out[3]:
          A         B
2 -0.133653 -0.030854

In [4]: sub_df.iloc[0]
Out[4]:
A   -0.133653
B   -0.030854
Name: 2, dtype: float64

In [5]: sub_df.iloc[0]['A']
Out[5]: -0.13365288513107493

```"
"I have seen many projects using **`simplejson`** module instead of **`json`** module from the Standard Library. Also, there are many different `simplejson` modules. Why would use these alternatives, instead of the one in the Standard Library?","`json` [is](http://docs.python.org/whatsnew/2.6.html#the-json-module-javascript-object-notation) `simplejson`, added to the stdlib. But since `json` was added in 2.6, `simplejson` has the advantage of working on more Python versions (2.4+).

`simplejson` is also updated more frequently than Python, so if you need (or want) the latest version, it's best to use `simplejson` itself, if possible.

A good practice, in my opinion, is to use one or the other as a fallback.

```
try:
    import simplejson as json
except ImportError:
    import json

```"
Is there any python module to convert PDF files into text? I tried [one piece of code](http://code.activestate.com/recipes/511465/) found in Activestate which uses pypdf but the text generated had no space between and was of no use.,"Try [PDFMiner](http://www.unixuser.org/~euske/python/pdfminer/index.html). It can extract text from PDF files as HTML, SGML or ""Tagged PDF"" format.

The Tagged PDF format seems to be the cleanest, and stripping out the XML tags leaves just the bare text.

A Python 3 version is available under:

* <https://github.com/pdfminer/pdfminer.six>"
"In my case, I'm using the `requests` library to call PayPal's API over HTTPS. Unfortunately, I'm getting an error from PayPal, and PayPal support cannot figure out what the error is or what's causing it. They want me to ""Please provide the entire request, headers included"".

How can I do that?","A simple method: enable logging in recent versions of Requests (1.x and higher.)

Requests uses the `http.client` and `logging` module configuration to control logging verbosity, as described [here](https://requests.readthedocs.io/en/master/api/#api-changes).

Demonstration
-------------

Code excerpted from the linked documentation:

```
import requests
import logging

# These two lines enable debugging at httplib level (requests->urllib3->http.client)
# You will see the REQUEST, including HEADERS and DATA, and RESPONSE with HEADERS but without DATA.
# The only thing missing will be the response.body which is not logged.
try:
    import http.client as http_client
except ImportError:
    # Python 2
    import httplib as http_client
http_client.HTTPConnection.debuglevel = 1

# You must initialize logging, otherwise you'll not see debug output.
logging.basicConfig()
logging.getLogger().setLevel(logging.DEBUG)
requests_log = logging.getLogger(""requests.packages.urllib3"")
requests_log.setLevel(logging.DEBUG)
requests_log.propagate = True

requests.get('https://httpbin.org/headers')

```

Example Output
--------------

```
$ python requests-logging.py 
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): httpbin.org
send: 'GET /headers HTTP/1.1\r\nHost: httpbin.org\r\nAccept-Encoding: gzip, deflate, compress\r\nAccept: */*\r\nUser-Agent: python-requests/1.2.0 CPython/2.7.3 Linux/3.2.0-48-generic\r\n\r\n'
reply: 'HTTP/1.1 200 OK\r\n'
header: Content-Type: application/json
header: Date: Sat, 29 Jun 2013 11:19:34 GMT
header: Server: gunicorn/0.17.4
header: Content-Length: 226
header: Connection: keep-alive
DEBUG:requests.packages.urllib3.connectionpool:""GET /headers HTTP/1.1"" 200 226

```"
"In our team, we define most test cases like this:

One ""framework"" class `ourtcfw.py`:

```
import unittest

class OurTcFw(unittest.TestCase):
    def setUp:
        # Something

    # Other stuff that we want to use everywhere

```

And a lot of test cases like testMyCase.py:

```
import localweather

class MyCase(OurTcFw):

    def testItIsSunny(self):
        self.assertTrue(localweather.sunny)

    def testItIsHot(self):
        self.assertTrue(localweather.temperature > 20)

if __name__ == ""__main__"":
    unittest.main()

```

When I'm writing new test code and want to run it often, and save time, I do put ""\_\_"" in front of all other tests. But it's cumbersome, distracts me from the code I'm writing, and the commit noise this creates is plain annoying.

So, for example, when making changes to `testItIsHot()`, I want to be able to do this:

```
$ python testMyCase.py testItIsHot

```

and have `unittest` run *only* `testItIsHot()`

How can I achieve that?

I tried to rewrite the `if __name__ == ""__main__"":` part, but since I'm new to Python, I'm feeling lost and keep bashing into everything else than the methods.","This works as you suggest - you just have to specify the class name as well:

```
python testMyCase.py MyCase.testItIsHot

```"
"I need to add two subplots to a figure. One subplot needs to be about three times as wide as the second (same height). I accomplished this using `GridSpec` and the `colspan` argument but I would like to do this using `figure` so I can save to PDF. I can adjust the first figure using the `figsize` argument in the constructor, but how do I change the size of the second plot?","* As of `matplotlib 3.6.0`, `width_ratios` and `height_ratios` can now be passed directly as keyword arguments to [`plt.subplots`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.subplots) and [`subplot_mosaic`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.subplot_mosaic), as per [What's new in Matplotlib 3.6.0 (Sep 15, 2022)](https://matplotlib.org/stable/users/prev_whats_new/whats_new_3.6.0.html#subplots-subplot-mosaic-accept-height-ratios-and-width-ratios-arguments).

`f, (a0, a1) = plt.subplots(1, 2, width_ratios=[3, 1])`

`f, (a0, a1, a2) = plt.subplots(3, 1, height_ratios=[1, 1, 3])`

---

* Another way is to use the `subplots` function and pass the width ratio with `gridspec_kw`
  + [matplotlib Tutorial: Customizing Figure Layouts Using GridSpec and Other Functions](https://matplotlib.org/stable/tutorials/intermediate/gridspec.html)
  + [`matplotlib.gridspec.GridSpec`](https://matplotlib.org/stable/api/_as_gen/matplotlib.gridspec.GridSpec.html#matplotlib.gridspec.GridSpec) has available `gridspect_kw` options

```
import numpy as np
import matplotlib.pyplot as plt 

# generate some data
x = np.arange(0, 10, 0.2)
y = np.sin(x)

# plot it
f, (a0, a1) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 1]})
a0.plot(x, y)
a1.plot(y, x)

f.tight_layout()
f.savefig('grid_figure.pdf')

```

[![enter image description here](https://i.sstatic.net/aBJVa.png)](https://i.sstatic.net/aBJVa.png)

* Because the question is canonical, here is an example with vertical subplots.

```
# plot it
f, (a0, a1, a2) = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1, 1, 3]})

a0.plot(x, y)
a1.plot(x, y)
a2.plot(x, y)

f.tight_layout()

```

[![enter image description here](https://i.sstatic.net/a2djk.png)](https://i.sstatic.net/a2djk.png)"
"I am using Pandas as a database substitute as I have multiple databases ([Oracle](https://en.wikipedia.org/wiki/Oracle_Database), [SQLÂ Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server), etc.), and I am unable to make a sequence of commands to a SQL equivalent.

I have a table loaded in a DataFrame with some columns:

```
YEARMONTH, CLIENTCODE, SIZE, etc., etc.

```

In SQL, to count the amount of different clients per year would be:

```
SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;

```

And the result would be

```
201301    5000
201302    13245

```

How can I do that in Pandas?","I believe this is what you want:

```
table.groupby('YEARMONTH').CLIENTCODE.nunique()

```

Example:

```
In [2]: table
Out[2]: 
   CLIENTCODE  YEARMONTH
0           1     201301
1           1     201301
2           2     201301
3           1     201302
4           2     201302
5           2     201302
6           3     201302

In [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()
Out[3]: 
YEARMONTH
201301       2
201302       3

```"
"If I want to use the results of `argparse.ArgumentParser()`, which is a `Namespace` object, with a method that expects a dictionary or mapping-like object (see [collections.Mapping](http://docs.python.org/2/library/collections.html#collections.Mapping)), what is the right way to do it?

```
C:\>python
Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win
32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import argparse
>>> args = argparse.Namespace()
>>> args.foo = 1
>>> args.bar = [1,2,3]
>>> args.baz = 'yippee'
>>> args['baz']
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'Namespace' object has no attribute '__getitem__'
>>> dir(args)
['__class__', '__contains__', '__delattr__', '__dict__', '__doc__', '__eq__', '_
_format__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__',
'__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__
', '__str__', '__subclasshook__', '__weakref__', '_get_args', '_get_kwargs', 'ba
r', 'baz', 'foo']

```

**Is it proper to ""reach into"" an object and use its `__dict__` property?**

I would think the answer is no: `__dict__` smells like a convention for implementation, but not for an interface, the way `__getattribute__` or `__setattr__` or `__contains__` seem to be.","You can access the namespace's dictionary with [*vars()*](http://docs.python.org/3/library/functions.html#vars):

```
>>> import argparse
>>> args = argparse.Namespace()
>>> args.foo = 1
>>> args.bar = [1,2,3]
>>> d = vars(args)
>>> d
{'foo': 1, 'bar': [1, 2, 3]}

```

You can modify the dictionary directly if you wish:

```
>>> d['baz'] = 'store me'
>>> args.baz
'store me'

```

Yes, it is okay to access the \_\_dict\_\_ attribute. It is a well-defined, tested, and guaranteed behavior."
"I have a Pandas Dataframe as shown below:

```
    1    2       3
 0  a  NaN    read
 1  b    l  unread
 2  c  NaN    read

```

I want to remove the NaN values with an empty string so that it looks like so:

```
    1    2       3
 0  a   """"    read
 1  b    l  unread
 2  c   """"    read

```","```
df = df.fillna('')

```

This will fill na's (e.g. NaN's) with `''`.

`inplace` is possible but should be avoided as [it makes a copy internally anyway, and it will be deprecated](https://github.com/pandas-dev/pandas/issues/16529):

```
df.fillna('', inplace=True)

```

To fill only a single column:

```
df.column1 = df.column1.fillna('')

```

One can use `df['column1']` instead of `df.column1`."
"### Background

I was about to try Python package downloaded from GitHub, and realized that it did not have a `setup.py`, so I could not install it with

```
pip install -e <folder>

```

Instead, the package had a **`pyproject.toml`** file which seems to have very similar entries as the `setup.py` usually has.

### What I found

Googling lead me into [PEP-518](https://www.python.org/dev/peps/pep-0518/) and it gives some critique to `setup.py` in [Rationale](https://www.python.org/dev/peps/pep-0518/#rationale) section. However, it does not clearly tell that usage of `setup.py` should be avoided, or that `pyproject.toml` would as such completely replace `setup.py`.

### Questions

Is the `pyproject.toml` something that is used to replace `setup.py`? Or should a package come with both, a `pyproject.toml` and a `setup.py`?  
How would one install a project with `pyproject.toml` in an editable state?","Yes, `pyproject.toml` is the specified file format of [PEP 518](https://www.python.org/dev/peps/pep-0518/) which contains the build system requirements of Python projects.

This solves the build-tool dependency chicken and egg problem, i.e. `pip` can read `pyproject.toml` and what version of `setuptools` or `wheel` one may need.

If you need a `setup.py` for an editable install, you could use a shim in `setup.py`:

```
#!/usr/bin/env python

import setuptools

if __name__ == ""__main__"":
    setuptools.setup()

```"
"Let `x` be a NumPy array. The following:

```
(x > 1) and (x < 3)

```

Gives the error message:

> ValueError: The truth value of an array with more than one element is
> ambiguous. Use a.any() or a.all()

How do I fix this?","If `a` and `b` are Boolean NumPy arrays, the `&` operation returns the elementwise-and of them:

```
a & b

```

That returns a Boolean *array*. To reduce this to a single Boolean *value*, use either

```
(a & b).any()

```

or

```
(a & b).all()

```

Note: if `a` and `b` are *non-Boolean* arrays, consider `(a - b).any()` or `(a - b).all()` instead.

---

#### Rationale

The NumPy developers felt there was no one commonly understood way to evaluate an array in Boolean context: it could mean `True` if *any* element is `True`, or it could mean `True` if *all* elements are `True`, or `True` if the array has non-zero length, just to name three possibilities.

Since different users might have different needs and different assumptions, the
NumPy developers refused to guess and instead decided to raise a `ValueError` whenever one tries to evaluate an array in Boolean context. Applying `and` to two numpy arrays causes the two arrays to be evaluated in Boolean context (by calling `__bool__` in Python3 or `__nonzero__` in Python2)."
"How do I check whether a pandas DataFrame has NaN values?

I know about `pd.isnan` but it returns a DataFrame of booleans. I also found [this post](https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas) but it doesn't exactly answer my question either.","[jwilner](https://stackoverflow.com/users/1567452/jwilner)'s response is spot on. I was exploring to see if there's a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:

```
df.isnull().values.any()

```

[![enter image description here](https://i.sstatic.net/7l80g.png)](https://i.sstatic.net/7l80g.png)

```
import numpy as np
import pandas as pd
import perfplot


def setup(n):
    df = pd.DataFrame(np.random.randn(n))
    df[df > 0.9] = np.nan
    return df


def isnull_any(df):
    return df.isnull().any()


def isnull_values_sum(df):
    return df.isnull().values.sum() > 0


def isnull_sum(df):
    return df.isnull().sum() > 0


def isnull_values_any(df):
    return df.isnull().values.any()


perfplot.save(
    ""out.png"",
    setup=setup,
    kernels=[isnull_any, isnull_values_sum, isnull_sum, isnull_values_any],
    n_range=[2 ** k for k in range(25)],
)

```

`df.isnull().sum().sum()` is a bit slower, but of course, has additional information -- the number of `NaNs`."
"In R when you need to retrieve a column index based on the name of the column you could do

```
idx <- which(names(my_data)==my_colum_name)

```

Is there a way to do the same with pandas dataframes?","Sure, you can use `.get_loc()`:

```
In [45]: df = DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})

In [46]: df.columns
Out[46]: Index([apple, orange, pear], dtype=object)

In [47]: df.columns.get_loc(""pear"")
Out[47]: 2

```

although to be honest I don't often need this myself. Usually access by name does what I want it to (`df[""pear""]`, `df[[""apple"", ""orange""]]`, or maybe `df.columns.isin([""orange"", ""pear""])`), although I can definitely see cases where you'd want the index number."
"Is there a standard way to associate version string with a Python package in such way that I could do the following?

```
import foo
print(foo.version)

```

I would imagine there's some way to retrieve that data without any extra hardcoding, since minor/major strings are specified in `setup.py` already. Alternative solution that I found was to have `import __version__` in my `foo/__init__.py` and then have `__version__.py` generated by `setup.py`.","Not directly an answer to your question, but you should consider naming it `__version__`, not `version`.

This is almost a quasi-standard. Many modules in the standard library use `__version__`, and this is also used in [lots](http://www.google.com/codesearch?as_q=__version__&btnG=Search+Code&hl=en&as_lang=python&as_license_restrict=i&as_license=&as_package=&as_filename=&as_case=) of 3rd-party modules, so it's the quasi-standard.

Usually, `__version__` is a string, but sometimes it's also a float or tuple.

As mentioned by S.Lott (Thank you!), [PEP 8](https://www.python.org/dev/peps/pep-0008/#module-level-dunder-names) says it explicitly:

> Module Level Dunder Names
> -------------------------
>
> Module level ""dunders"" (i.e. names with two leading and two trailing
> underscores) such as `__all__`, `__author__`, `__version__`, etc.
> should be placed after the module docstring but before any import
> statements except from `__future__` imports.

You should also make sure that the version number conforms to the format described in [PEP 440](http://www.python.org/dev/peps/pep-0440/) ([PEP 386](http://www.python.org/dev/peps/pep-0386/) a previous version of this standard)."
"When I ask the model manager to get an object, it raises `DoesNotExist` when there is no matching object.

```
go = Content.objects.get(name=""baby"")

```

Instead of `DoesNotExist`, how can I have `go` be `None` instead?","There is no 'built in' way to do this as of version 1.2. Django will raise the `DoesNotExist` exception every time.

The idiomatic way to handle this in Python is to wrap it in a try catch:

```
try:
    go = SomeModel.objects.get(foo='bar')
except SomeModel.DoesNotExist:
    go = None

```

What I did was to subclass `models.Manager`, create a `safe_get` like the code above, and use that manager for my models. That way you can write one line to achieve this:

```
SomeModel.objects.safe_get(foo='bar')

```"
"What's the correct way to convert bytes to a hex string in Python 3?

I see claims of a `bytes.hex` method, `bytes.decode` codecs, and have tried [other](http://docs.python.org/py3k/library/functions.html#hex) possible functions of least astonishment without avail. I just want my bytes as hex!","Since Python 3.5 this is finally no longer awkward:

```
>>> b'\xde\xad\xbe\xef'.hex()
'deadbeef'

```

and reverse:

```
>>> bytes.fromhex('deadbeef')
b'\xde\xad\xbe\xef'

```

works also with the mutable `bytearray` type.

Reference: <https://docs.python.org/3/library/stdtypes.html#bytes.hex>"
"I have a dictionary that I declared in a particular order and want to keep it in that order all the time. The keys/values can't really be kept in order based on their value, I just want it in the order that I declared it.

So if I have the dictionary:

```
d = {'ac': 33, 'gw': 20, 'ap': 102, 'za': 321, 'bs': 10}

```

It isn't in that order if I view it or iterate through it. Is there any way to make sure Python will keep the explicit order that I declared the keys/values in?","From Python 3.6 onwards, the standard `dict` type maintains insertion order by default.

Defining

```
d = {'ac':33, 'gw':20, 'ap':102, 'za':321, 'bs':10}

```

will result in a dictionary with the keys in the order listed in the source code.

This was achieved by using a simple array with integers for the sparse hash table, where those integers index into another array that stores the key-value pairs (plus the calculated hash). That latter array just happens to store the items in insertion order, and the whole combination actually uses less memory than the implementation used in Python 3.5 and before. See the [original idea post by Raymond Hettinger](https://mail.python.org/pipermail/python-dev/2012-December/123028.html) for details.

In 3.6 this was still considered an implementation detail; see the [*What's New in Python 3.6* documentation](https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-compactdict):

> The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5).

Python 3.7 elevates this implementation detail to a *language specification*, so it is now mandatory that `dict` preserves order in all Python implementations compatible with that version or newer. See the [pronouncement by the BDFL](https://mail.python.org/pipermail/python-dev/2017-December/151283.html). As of Python 3.8, dictionaries also support [iteration in reverse](https://docs.python.org/3/library/functions.html#reversed).

You may still want to use the [`collections.OrderedDict()` class](https://docs.python.org/3/library/collections.html#collections.OrderedDict) in certain cases, as it offers some additional functionality on top of the standard `dict` type. Such as as being [reversible](https://docs.python.org/3/library/collections.abc.html#collections.abc.Reversible) (this extends to the [view objects](https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects)), and supporting reordering (via the [`move_to_end()` method](https://docs.python.org/3/library/collections.html#collections.OrderedDict.move_to_end))."
"I am using PyCharm on Windows and want to change the settings to limit the maximum line length to `79` characters, as opposed to the default limit of `120` characters.

Where can I change the maximum amount of characters per line in PyCharm?","Here is screenshot of my Pycharm. Required settings is in following path: `File -> Settings -> Editor -> Code Style -> General: Right margin (columns)`

[![Pycharm 4 Settings Screenshot](https://i.sstatic.net/V3BLg.png)](https://i.sstatic.net/V3BLg.png)"
"I am getting the `too many values to unpack` error. Any idea how I can fix this?

```
first_names = ['foo', 'bar']
last_names = ['gravy', 'snowman']

fields = {
    'first_names': first_names,
    'last_name': last_names,
}        

for field, possible_values in fields:  # error happens on this line

```","Python 3
--------

Use [`items()`](http://docs.python.org/library/stdtypes.html#dict.items).

```
for field, possible_values in fields.items():
    print(field, possible_values)

```

Python 2
--------

Use [`iteritems()`](http://docs.python.org/library/stdtypes.html#dict.iteritems).

```
for field, possible_values in fields.iteritems():
    print field, possible_values

```

---

See [this answer](https://stackoverflow.com/a/3294899/1489538) for more information on iterating through dictionaries, such as using `items()`, across Python versions.

For reference, [`iteritems()` was removed in Python 3](https://docs.python.org/3/whatsnew/3.0.html#views-and-iterators-instead-of-lists)."
"Say I have a Python function that returns multiple values in a tuple:

```
def func():
    return 1, 2

```

Is there a nice way to ignore one of the results rather than just assigning to a temporary variable? Say if I was only interested in the first value, is there a better way than this:

```
x, temp = func()

```","You can use `x = func()[0]` to return the first value, `x = func()[1]` to return the second, and so on.

If you want to get multiple values at a time, use something like `x, y = func()[2:4]`."
"I have a .txt file with values in it.

The values are listed like so:

```
Value1
Value2
Value3
Value4

```

My goal is to put the values in a list. When I do so, the list looks like this:

`['Value1\n', 'Value2\n', ...]`

The `\n` is not needed.

Here is my code:

```
t = open('filename.txt')
contents = t.readlines()

```","This should do what you want (file contents in a list, by line, without \n)

```
with open(filename) as f:
    mylist = f.read().splitlines() 

```"
"If I've got a multi-level column index:

```
>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
>>> pd.DataFrame([[1,2], [3,4]], columns=cols)

```

```
    a
   ---+--
    b | c
--+---+--
0 | 1 | 2
1 | 3 | 4

```

How can I drop the ""a"" level of that index, so I end up with:

```
    b | c
--+---+--
0 | 1 | 2
1 | 3 | 4

```","You can use [`MultiIndex.droplevel`](https://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.MultiIndex.droplevel.html):

```
>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
>>> df = pd.DataFrame([[1,2], [3,4]], columns=cols)
>>> df
   a   
   b  c
0  1  2
1  3  4

[2 rows x 2 columns]
>>> df.columns = df.columns.droplevel()
>>> df
   b  c
0  1  2
1  3  4

[2 rows x 2 columns]

```"
"Is there a way to make `pip` play well with multiple versions of Python? For example, I want to use `pip` to explicitly install things to either my site 2.5 installation or my site 2.6 installation.

For example, with `easy_install`, I use `easy_install-2.{5,6}`.

And, yes — I know about virtualenv, and no — it's not a solution to this particular problem.","The [current recommendation](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) is to use `python -m pip`, where `python` is the version of Python you would like to use. This is the recommendation because it works across all versions of Python, and in all forms of virtualenv. For example:

```
# The system default Python installation:
python -m pip install fish

# A virtualenv's Python installation:
.env/bin/python -m pip install fish

# A specific version of python:
python-3.6 -m pip install fish

```

Previous answer, left for posterity:
------------------------------------

Since version 0.8, Pip supports `pip-{version}`. You can use it the same as `easy_install-{version}`:

```
pip-2.5 install myfoopackage
pip-2.6 install otherpackage
pip-2.7 install mybarpackage

```

---

pip changed its schema to use `pipVERSION` instead of `pip-VERSION` in version 1.5. You should use the following if you have `pip >= 1.5`:

```
pip2.6 install otherpackage
pip2.7 install mybarpackage

```

Check *[Versioned commands consistent with Python. #1053](https://github.com/pypa/pip/pull/1053)* for more details

---

References:

* *[Need pip-x.y scripts #200](https://github.com/pypa/pip/issues/200)*
* *[v0.8 changelog](https://pip.pypa.io/en/stable/news/#v0-8)* or *[News for pip, v0.8](https://web.archive.org/web/20140310013920/http://www.pip-installer.org:80/docs/pip/en/0.8.3/news.html#id4)*"
"I would like to see what is the best way to determine the current script directory in Python.

I discovered that, due to the many ways of calling Python code, it is hard to find a good solution.

Here are some problems:

* `__file__` is not defined if the script is executed with `exec`, `execfile`
* `__module__` is defined only in modules

Use cases:

* `./myfile.py`
* `python myfile.py`
* `./somedir/myfile.py`
* `python somedir/myfile.py`
* `execfile('myfile.py')` (from another script, that can be located in another directory and that can have another current directory.

I know that there is no perfect solution, but I'm looking for the best approach that solves most of the cases.

The most used approach is `os.path.dirname(os.path.abspath(__file__))` but this really doesn't work if you execute the script from another one with `exec()`.

Warning
-------

Any solution that uses current directory will fail, this can be different based on the way the script is called or it can be changed inside the running script.","```
os.path.dirname(os.path.abspath(__file__))

```

is indeed the best you're going to get.

It's unusual to be executing a script with `exec`/`execfile`; normally you should be using the module infrastructure to load scripts. If you must use these methods, I suggest setting `__file__` in the `globals` you pass to the script so it can read that filename.

There's no other way to get the filename in execed code: as you note, the CWD may be in a completely different place."
I want to send a datetime.datetime object in serialized form from Python using [JSON](http://en.wikipedia.org/wiki/JSON) and de-serialize in JavaScript using JSON. What is the best way to do this?,"You can add the 'default' parameter to json.dumps to handle this:

```
date_handler = lambda obj: (
    obj.isoformat()
    if isinstance(obj, (datetime.datetime, datetime.date))
    else None
)
json.dumps(datetime.datetime.now(), default=date_handler)
'""2010-04-20T20:08:21.634121""'

```

Which is [ISO 8601](http://en.wikipedia.org/wiki/ISO_8601) format.

A more comprehensive default handler function:

```
def handler(obj):
    if hasattr(obj, 'isoformat'):
        return obj.isoformat()
    elif isinstance(obj, ...):
        return ...
    else:
        raise TypeError, 'Object of type %s with value of %s is not JSON serializable' % (type(obj), repr(obj))

```

Update: Added output of type as well as value.  
Update: Also handle date"
"Is there any difference at all between both approaches?

```
>>> os.getenv('TERM')
'xterm'
>>> os.environ.get('TERM')
'xterm'

>>> os.getenv('FOOBAR', ""not found"") == ""not found""
True
>>> os.environ.get('FOOBAR', ""not found"") == ""not found""
True

```

They seem to have the exact same functionality.","See [this related thread](https://stackoverflow.com/questions/10952507/when-would-os-environfoo-not-match-os-getenvfoo). Basically, `os.environ` is found on import, and `os.getenv` is a wrapper to `os.environ.get`, at least in CPython.

EDIT: To respond to a comment, in CPython, `os.getenv` is basically a shortcut to `os.environ.get` ; since `os.environ` is loaded at import of `os`, and only then, the same holds for
`os.getenv`."
"How do you express an integer as a binary number with Python literals?

I was easily able to find the answer for hex:

```
>>> 0x12AF
4783
>>> 0x100
256

```

and octal:

```
>>> 01267
695
>>> 0100
64

```

**How do you use literals to express binary in Python?**

---

***Summary of Answers***

* Python 2.5 and earlier: can express binary using `int('01010101111',2)` but not with a literal.
* Python 2.5 and earlier: there is *no way* to express binary literals.
* Python 2.6 beta: You can do like so: `0b1100111` or `0B1100111`.
* Python 2.6 beta: will also allow `0o27` or `0O27` (second character is the letter O) to represent an octal.
* Python 3.0 beta: Same as 2.6, but will no longer allow the older `027` syntax for octals.","For referenceâ€”*future* Python possibilities:  
Starting with Python 2.6 you can express binary literals using the prefix **0b** or **0B**:

```
>>> 0b101111
47

```

You can also use the new **bin** function to get the binary representation of a number:

```
>>> bin(173)
'0b10101101'

```

Development version of the documentation: [What's New in Python 2.6](http://docs.python.org/dev/whatsnew/2.6.html#pep-3127-integer-literal-support-and-syntax)"
"I need to remove all special characters, punctuation and spaces from a string so that I only have letters and numbers.","This can be done without regex:

```
>>> string = ""Special $#! characters   spaces 888323""
>>> ''.join(e for e in string if e.isalnum())
'Specialcharactersspaces888323'

```

You can use [`str.isalnum`](https://docs.python.org/library/stdtypes.html#str.isalnum):

> ```
> S.isalnum() -> bool
>
> Return True if all characters in S are alphanumeric
> and there is at least one character in S, False otherwise.
>
> ```

If you insist on using regex, other solutions will do fine. However note that if it can be done without using a regular expression, that's the best way to go about it."
"How do I find a string between two substrings (`'123STRINGabc' -> 'STRING'`)?

My current method is like this:

```
>>> start = 'asdf=5;'
>>> end = '123jasd'
>>> s = 'asdf=5;iwantthis123jasd'
>>> print((s.split(start))[1].split(end)[0])
iwantthis

```

However, this seems very inefficient and un-pythonic. What is a better way to do something like this?

Forgot to mention:
The string might not start and end with `start` and `end`. They may have more characters before and after.","```
import re

s = 'asdf=5;iwantthis123jasd'
result = re.search('asdf=5;(.*)123jasd', s)
print(result.group(1))

# returns 'iwantthis'

```"
"I'm trying to create an application in Python 3.2 and I use tabs all the time for indentation, but even the editor changes some of them into spaces and then print out ""inconsistent use of tabs and spaces in indentation"" when I try to run the program.

How can I change the spaces into tabs? It's driving me crazy.

```
import random

attraktioner = [""frittfall"",""bergodalbana"",""spökhuset""]


class Nojesfalt:
    def __init__(self, attraktion):
        self.val = attraktion
        self.langd = 0
        self.alder = 0


#längdgräns för fritt fall
    def langdgrans(self):
        print("""")
        self.langd = int(input(""Hur lång är du i cm? ""))
        if self.langd < 140:
            print(""tyvärr, du är för kort, prova något annat"")
            return 0
        elif self.langd >= 140:
            print(""håll dig hatten, nu åker vi!"")
            print("" "")
            return 1

#åldersgräns för spökhuset
    def aldersgrans(self):
        print("""")
        self.alder = int(input(""Hur gammal är du? ""))
        if self.alder < 10:
            print(""tyvärr, du är för ung, prova något annat"")
            return 0
        elif self.alder >= 10:
            print(""Gå in om du törs!"")
            print("" "")
            return 1


#åker attraktion frittfall lr bergodalbana
        def aka(self):
                print("""")
        print(self.val)
        tal = random.randint(0,100)
        if tal < 20:
            print(""åkturen gick åt skogen, bättre lycka nästa gång"")
        elif tal >= 20:
            print(""jabbadabbbadoooooooo"")
            return 1

#går i spökhuset
        def aka1(self):
                print("""")
        print(self.val)
        tal = random.randint(0,100)
        if tal < 20:
            print(""du är omringad av spöken och kan inte fortsätta"")            return 0
        elif tal >= 20:
            print(""Buhuuuuuu, buuuhuuuu"")
            return 1

#programkod
print(""Välkommen till nöjesfältet, vad vill du göra?"")
print("" "")

while 1:
    vald_attr = input(""Vad vill du göra?\n1. frittfall\n2. bergodalbana\n3. spökhuset\n4. Avsluta\n"")
    if vald_attr == ""1"":
        val = Nojesfalt(attraktioner[0])
        if val.langdgrans() == 1:
            val.aka()
    elif vald_attr == ""2"":
        val = Nojesfalt(attraktioner[1])
        val.aka()
    elif vald_attr == ""3"":
        val = Nojesfalt(attraktioner[2])
        if val.aldersgrans() == 1:
            val.aka1()
    elif vald_attr == ""4"":
        break

```","Don't use tabs.

1. Set your editor to use 4 **spaces** for indentation.
2. Make a search and replace to replace all tabs with 4 spaces.
3. Make sure your editor is set to **display** tabs as 8 spaces.

Note: The reason for 8 spaces for tabs is so that you immediately notice when tabs have been inserted unintentionally - such as when copying and pasting from example code that uses tabs instead of spaces."
"I'm currently working on a computation in python shell. What I want to have is Matlab style listout where you can see all the variables that have been defined up to a point (so I know which names I've used, their values and such).

Is there a way, and how can I do that?","A few things you could use:

* [`dir()`](https://docs.python.org/3/library/functions.html#dir) will give you the list of in-scope variables
* [`globals()`](https://docs.python.org/3/library/functions.html#globals) will give you a dictionary of global variables
* [`locals()`](https://docs.python.org/3/library/functions.html#locals) will give you a dictionary of local variables"
"I have this code which calculates the distance between two coordinates. The two functions are both within the same class.

However, how do I call the function `distToPoint` in the function `isNear`?

```
class Coordinates:
    def distToPoint(self, p):
        """"""
        Use pythagoras to find distance
        (a^2 = b^2 + c^2)
        """"""
        ...

    def isNear(self, p):
        distToPoint(self, p)
        ...

```","Since these are member functions, call it as a member function on the instance, `self`.

```
def isNear(self, p):
    self.distToPoint(p)
    ...

```"
"How do I use type hints to annotate a function that returns an `Iterable` that always yields two values: a `bool` and a `str`? The hint `Tuple[bool, str]` is close, except that it limits the return value type to a tuple, not a generator or other type of iterable.

I'm mostly curious because I would like to annotate a function `foo()` that is used to return multiple values like this:

```
always_a_bool, always_a_str = foo()

```

Usually functions like `foo()` do something like `return a, b` (which returns a tuple), but I would like the type hint to be flexible enough to replace the returned tuple with a generator or list or something else.","You are always returning *one* object; using `return one, two` simply returns a tuple.

So yes, `-> Tuple[bool, str]` is entirely correct.

*Only* the `Tuple` type lets you specify a *fixed number* of elements, each with a distinct type. You really should be returning a tuple, always, if your function produces a *fixed* number of return values, especially when those values are specific, distinct types.

Other sequence types are expected to have *one* type specification for a variable number of elements, so `typing.Sequence` is not suitable here. Also see [What's the difference between lists and tuples?](https://stackoverflow.com/questions/626759/whats-the-difference-between-list-and-tuples)

> Tuples are heterogeneous data structures (i.e., their entries have different meanings), while lists are homogeneous sequences. **Tuples have structure, lists have order.**

Python's type hint system adheres to that philosophy, there is currently no syntax to specify an iterable of fixed length and containing specific types at specific positions.

If you *must* specify that any iterable will do, then the best you can do is:

```
-> Iterable[Union[bool, str]]

```

at which point the caller can expect booleans and strings *in any order*, and of unknown length (anywhere between 0 and infinity).

Last but not least, as of Python 3.9, you can use

```
-> tuple[bool, str]

```

instead of `-> Tuple[bool, str]`; support for type hinting notation [has been added to most standard-library container types](https://docs.python.org/3/whatsnew/3.9.html#type-hinting-generics-in-standard-collections) (see [PEP 585](https://www.python.org/dev/peps/pep-0585/#implementation) for the complete list). In fact, you can use this as of Python 3.7 too provided you use the `from __future__ import annotations` compiler switch for your modules and a type checker that supports the syntax."
"Is there a way to make Python logging using the `logging` module automatically output things to stdout *in addition* to the log file where they are supposed to go? For example, I'd like all calls to `logger.warning`, `logger.critical`, `logger.error` to go to their intended places but in addition always be copied to `stdout`. This is to avoid duplicating messages like:

```
mylogger.critical(""something failed"")
print(""something failed"")

```","All logging output is handled by the handlers; just add a [`logging.StreamHandler()`](http://docs.python.org/library/logging.handlers.html#streamhandler) to the root logger.

Here's an example configuring a stream handler (using `stdout` instead of the default `stderr`) and adding it to the root logger:

```
import logging
import sys

root = logging.getLogger()
root.setLevel(logging.DEBUG)

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
root.addHandler(handler)

```"
"I would like to get some feedback on these tools on:

* features;
* adaptability;
* ease of use and learning curve.","Well, I am a bit curious, so I just tested the three myself right after asking the question ;-)

Ok, this is not a very serious review, but here is what I can say:

I tried the tools **with the default settings** (it's important because you can pretty much choose your check rules) on the following script:

```
#!/usr/local/bin/python
# by Daniel Rosengren modified by e-satis

import sys, time
stdout = sys.stdout

BAILOUT = 16
MAX_ITERATIONS = 1000

class Iterator(object) :

    def __init__(self):

        print 'Rendering...'
        for y in xrange(-39, 39):
            stdout.write('\n')
            for x in xrange(-39, 39):
                if self.mandelbrot(x/40.0, y/40.0) :
                    stdout.write(' ')
                else:
                    stdout.write('*')


    def mandelbrot(self, x, y):
        cr = y - 0.5
        ci = x
        zi = 0.0
        zr = 0.0

        for i in xrange(MAX_ITERATIONS) :
            temp = zr * zi
            zr2 = zr * zr
            zi2 = zi * zi
            zr = zr2 - zi2 + cr
            zi = temp + temp + ci

            if zi2 + zr2 > BAILOUT:
                return i

        return 0

t = time.time()
Iterator()
print '\nPython Elapsed %.02f' % (time.time() - t)

```

**As a result:**

* `PyChecker` is troublesome because it compiles the module to analyze it. If you don't want your code to run (e.g, it performs a SQL query), that's bad.
* `PyFlakes` is supposed to be light. Indeed, it decided that the code was perfect. I am looking for something quite severe so I don't think I'll go for it.
* `PyLint` has been very talkative and rated the code 3/10 (OMG, I'm a dirty coder !).

**Strong points of `PyLint`:**

* Very descriptive and accurate report.
* Detect some code smells. Here it told me to drop my class to write something with functions because the OO approach was useless in this specific case. Something I knew, but never expected a computer to tell me :-p
* The fully corrected code run faster (no class, no reference binding...).
* Made by a French team. OK, it's not a plus for everybody, but I like it ;-)

**Cons of Pylint:**

* Some rules are really strict. I know that you can change it and that the default is to match PEP8, but is it such a crime to write 'for x in seq'? Apparently yes because you can't write a variable name with less than 3 letters. I will change that.
* Very very talkative. Be ready to use your eyes.

Corrected script (with lazy doc strings and variable names):

```
#!/usr/local/bin/python
# by Daniel Rosengren, modified by e-satis
""""""
Module doctring
""""""


import time
from sys import stdout

BAILOUT = 16
MAX_ITERATIONS = 1000

def mandelbrot(dim_1, dim_2):
    """"""
    function doc string
    """"""
    cr1 = dim_1 - 0.5
    ci1 = dim_2
    zi1 = 0.0
    zr1 = 0.0

    for i in xrange(MAX_ITERATIONS) :
        temp = zr1 * zi1
        zr2 = zr1 * zr1
        zi2 = zi1 * zi1
        zr1 = zr2 - zi2 + cr1
        zi1 = temp + temp + ci1

        if zi2 + zr2 > BAILOUT:
            return i

    return 0

def execute() :
    """"""
    func doc string
    """"""
    print 'Rendering...'
    for dim_1 in xrange(-39, 39):
        stdout.write('\n')
        for dim_2 in xrange(-39, 39):
            if mandelbrot(dim_1/40.0, dim_2/40.0) :
                stdout.write(' ')
            else:
                stdout.write('*')


START_TIME = time.time()
execute()
print '\nPython Elapsed %.02f' % (time.time() - START_TIME)

```

[Thanks to Rudiger Wolf](https://stackoverflow.com/questions/1428872/pylint-pychecker-or-pyflakes/1431749#1431749), I discovered `pep8` that does exactly what its name suggests: matching PEP8. It has found several syntax no-nos that Pylint did not. But Pylint found stuff that was not specifically linked to PEP8 but interesting. Both tools are interesting and complementary.

Eventually I will use both since there are really easy to install (via packages or setuptools) and the output text is so easy to chain.

To give you a little idea of their output:

**pep8**:

```
./python_mandelbrot.py:4:11: E401 multiple imports on one line
./python_mandelbrot.py:10:1: E302 expected 2 blank lines, found 1
./python_mandelbrot.py:10:23: E203 whitespace before ':'
./python_mandelbrot.py:15:80: E501 line too long (108 characters)
./python_mandelbrot.py:23:1: W291 trailing whitespace
./python_mandelbrot.py:41:5: E301 expected 1 blank line, found 3

```

**Pylint**:

```
************* Module python_mandelbrot
C: 15: Line too long (108/80)
C: 61: Line too long (85/80)
C:  1: Missing docstring
C:  5: Invalid name ""stdout"" (should match (([A-Z_][A-Z0-9_]*)|(__.*__))$)
C: 10:Iterator: Missing docstring
C: 15:Iterator.__init__: Invalid name ""y"" (should match [a-z_][a-z0-9_]{2,30}$)
C: 17:Iterator.__init__: Invalid name ""x"" (should match [a-z_][a-z0-9_]{2,30}$)

[...] and a very long report with useful stats like :

Duplication
-----------

+-------------------------+------+---------+-----------+
|                         |now   |previous |difference |
+=========================+======+=========+===========+
|nb duplicated lines      |0     |0        |=          |
+-------------------------+------+---------+-----------+
|percent duplicated lines |0.000 |0.000    |=          |
+-------------------------+------+---------+-----------+

```"
"I have a Dataframe, `df`, with the following column:

```
     ArrivalDate
936   2012-12-31
938   2012-12-29
965   2012-12-31
966   2012-12-31
967   2012-12-31
968   2012-12-31
969   2012-12-31
970   2012-12-29
971   2012-12-31
972   2012-12-29
973   2012-12-29

```

The elements of the column are `pandas.tslib.Timestamp` type. I want to extract the year and month.

Here's what I've tried:

```
df['ArrivalDate'].resample('M', how = 'mean')

```

which throws the following error:

```
Only valid with DatetimeIndex or PeriodIndex 

```

Then I tried:

```
df['ArrivalDate'].apply(lambda(x):x[:-2])

```

which throws the following error:

```
'Timestamp' object has no attribute '__getitem__' 

```

My current solution is

```
df.index = df['ArrivalDate']

```

Then, I can resample another column using the index.

But I'd still like a method for reconfiguring the entire column. Any ideas?","If you want new columns showing year and month separately you can do this:

```
df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year
df['month'] = pd.DatetimeIndex(df['ArrivalDate']).month

```

or...

```
df['year'] = df['ArrivalDate'].dt.year
df['month'] = df['ArrivalDate'].dt.month

```

Then you can combine them or work with them just as they are."
"I needed to write a weighted version of random.choice (each element in the list has a different probability for being selected). This is what I came up with:

```
def weightedChoice(choices):
    """"""Like random.choice, but each element can have a different chance of
    being selected.

    choices can be any iterable containing iterables with two items each.
    Technically, they can have more than two items, the rest will just be
    ignored.  The first item is the thing being chosen, the second item is
    its weight.  The weights can be any numeric values, what matters is the
    relative differences between them.
    """"""
    space = {}
    current = 0
    for choice, weight in choices:
        if weight > 0:
            space[current] = choice
            current += weight
    rand = random.uniform(0, current)
    for key in sorted(space.keys() + [current]):
        if rand < key:
            return choice
        choice = space[key]
    return None

```

This function seems overly complex to me, and ugly. I'm hoping everyone here can offer some suggestions on improving it or alternate ways of doing this. Efficiency isn't as important to me as code cleanliness and readability.","Since version 1.7.0, NumPy has a [`choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) function that supports probability distributions.

```
from numpy.random import choice
draw = choice(list_of_candidates, number_of_items_to_pick,
              p=probability_distribution)

```

Note that `probability_distribution` is a sequence in the same order of `list_of_candidates`. You can also use the keyword `replace=False` to change the behavior so that drawn items are not replaced."
"In [python](/questions/tagged/python ""show questions tagged 'python'""), suppose I have a path like this:

```
/folderA/folderB/folderC/folderD/

```

How can I get just the `folderD` part?","Use [`os.path.normpath`](https://docs.python.org/library/os.path.html#os.path.normpath) to strip off any trailing slashes, then [`os.path.basename`](https://docs.python.org/library/os.path.html#os.path.basename) gives you the last part of the path:

```
>>> os.path.basename(os.path.normpath('/folderA/folderB/folderC/folderD/'))
'folderD'

```

Using only `basename` gives everything after the last slash, which in this case is `''`."
"I have not seen clear examples with use-cases for [Pool.apply](https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.multiprocessing.Pool.apply), [Pool.apply\_async](https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.multiprocessing.Pool.apply_async) and [Pool.map](https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.multiprocessing.Pool.map). I am mainly using `Pool.map`; what are the advantages of others?","Back in the old days of Python, to call a function with arbitrary arguments, you would use `apply`:

```
apply(f,args,kwargs)

```

`apply` still exists in Python2.7 though not in Python3, and is generally not used anymore. Nowadays,

```
f(*args,**kwargs)

```

is preferred. The `multiprocessing.Pool` modules tries to provide a similar interface.

`Pool.apply` is like Python `apply`, except that the function call is performed in a separate process. `Pool.apply` blocks until the function is completed.

`Pool.apply_async` is also like Python's built-in `apply`, except that the call returns immediately instead of waiting for the result. An `AsyncResult` object is returned. You call its `get()` method to retrieve the result of the function call. The `get()` method blocks until the function is completed. Thus, `pool.apply(func, args, kwargs)` is equivalent to `pool.apply_async(func, args, kwargs).get()`.

In contrast to `Pool.apply`, the `Pool.apply_async` method also has a callback which, if supplied, is called when the function is complete. This can be used instead of calling `get()`.

For example:

```
import multiprocessing as mp
import time

def foo_pool(x):
    time.sleep(2)
    return x*x

result_list = []
def log_result(result):
    # This is called whenever foo_pool(i) returns a result.
    # result_list is modified only by the main process, not the pool workers.
    result_list.append(result)

def apply_async_with_callback():
    pool = mp.Pool()
    for i in range(10):
        pool.apply_async(foo_pool, args = (i, ), callback = log_result)
    pool.close()
    pool.join()
    print(result_list)

if __name__ == '__main__':
    apply_async_with_callback()

```

may yield a result such as

```
[1, 0, 4, 9, 25, 16, 49, 36, 81, 64]

```

Notice, unlike `pool.map`, the order of the results may not correspond to the order in which the `pool.apply_async` calls were made.

---

So, if you need to run a function in a separate process, but want the current process to **block** until that function returns, use `Pool.apply`. Like `Pool.apply`, `Pool.map` blocks until the complete result is returned.

If you want the Pool of worker processes to perform many function calls asynchronously, use `Pool.apply_async`. The **order** of the results is not guaranteed to be the same as the order of the calls to `Pool.apply_async`.

Notice also that you could call a number of **different** functions with `Pool.apply_async` (not all calls need to use the same function).

In contrast, `Pool.map` applies the same function to many arguments.
However, unlike `Pool.apply_async`, the results are returned in an order corresponding to the order of the arguments."
"I know how to use both for loops and if statements on separate lines, such as:

```
>>> a = [2,3,4,5,6,7,8,9,0]
... xyz = [0,12,4,6,242,7,9]
... for x in xyz:
...     if x in a:
...         print(x)
0,4,6,7,9

```

And I know I can use a list comprehension to combine these when the statements are simple, such as:

```
print([x for x in xyz if x in a])

```

But what I can't find is a good example anywhere (to copy and learn from) demonstrating a complex set of commands (not just ""print x"") that occur following a combination of a for loop and some if statements. Something that I would expect looks like:

```
for x in xyz if x not in a:
    print(x...)

```

Is this just not the way python is supposed to work?","You can use [generator expressions](https://www.python.org/dev/peps/pep-0289) like this:

```
gen = (x for x in xyz if x not in a)

for x in gen:
    print(x)

```"
"Is there an easy way to be inside a python function and get a list of the parameter names?

For example:

```
def func(a,b,c):
    print magic_that_does_what_I_want()

>>> func()
['a','b','c']

```

Thanks","Well we don't actually need `inspect` here.

```
>>> func = lambda x, y: (x, y)
>>> 
>>> func.__code__.co_argcount
2
>>> func.__code__.co_varnames
('x', 'y')
>>>
>>> def func2(x,y=3):
...  print(func2.__code__.co_varnames)
...  pass # Other things
... 
>>> func2(3,3)
('x', 'y')
>>> 
>>> func2.__defaults__
(3,)

```"
"I have a socket server that is supposed to receive UTF-8 valid characters from clients.

The problem is some clients (mainly hackers) are sending all the wrong kind of data over it.

I can easily distinguish the genuine client, but I am logging to files all the data sent so I can analyze it later.

Sometimes I get characters like this `Å“` that cause the `UnicodeDecodeError` error.

I need to be able to make the string UTF-8 with or without those characters.

---

**Update:**

For my particular case the socket service was an MTA and thus I only expect to receive ASCII commands such as:

```
EHLO example.com
MAIL FROM: <john.doe@example.com>
...

```

I was logging all of this in JSON.

Then some folks out there without good intentions decided to send all kind of junk.

That is why for my specific case it is perfectly OK to strip the non ASCII characters.","<http://docs.python.org/howto/unicode.html#the-unicode-type>

```
str = unicode(str, errors='replace')

```

or

```
str = unicode(str, errors='ignore')

```

**Note:** *This will strip out (ignore) the characters in question returning the string without them.*

*For me this is ideal case since I'm using it as protection against non-ASCII input which is not allowed by my application.*

**Alternatively:** Use the open method from the [`codecs`](https://docs.python.org/2/library/codecs.html#codecs.open) module to read in the file:

```
import codecs
with codecs.open(file_name, 'r', encoding='utf-8',
                 errors='ignore') as fdata:

```"
"I have a pandas dataframe with few columns.
Now I know that certain rows are outliers based on a certain column value.
For instance
column `Vol` has all values around 12xx and one value is 4000 (outlier).
I would like to exclude those rows that have `Vol` column like this.

So, essentially I need to put a filter on the data frame such that we select all rows where the values of a certain column are within, say, 3 standard deviations from mean.

What is an elegant way to achieve this?","Use [`scipy.stats.zscore`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html)
--------------------------------------------------------------------------------------------------------

### Remove all rows that have outliers in at least one column

If you have multiple columns in your dataframe and would like to remove all rows that have outliers in at least one column, the following expression would do that in one shot:

```
import pandas as pd
import numpy as np
from scipy import stats

df = pd.DataFrame(np.random.randn(100, 3))

df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]

```

#### Description:

* For each column, it first computes the Z-score of each value in the
  column, relative to the column mean and standard deviation.
* It then takes the absolute Z-score because the direction does not
  matter, only if it is below the threshold.
* `( < 3).all(axis=1)` checks if, for each row, all column values are within 3 standard deviations from the mean
* Finally, the result of this condition is used to index the dataframe.

### Filter other columns based on a single column

The same as above, but specify a column for the `zscore`, `df[0]` for example, and remove `.all(axis=1)`.

```
df[np.abs(stats.zscore(df[0])) < 3]

```"
"I need to create a NumPy array of length `n`, each element of which is `v`.

Is there anything better than:

```
a = empty(n)
for i in range(n):
    a[i] = v

```

I know `zeros` and `ones` would work for v = 0, 1. I could use `v * ones(n)`, but it ~~won't work when `v` is `None`, and also~~ would be much slower.","NumPyÂ 1.8 introduced [`np.full()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html), which is a more direct method than `empty()` followed by `fill()` for creating an array filled with a certain value:

```
>>> np.full((3, 5), 7)
array([[ 7.,  7.,  7.,  7.,  7.],
       [ 7.,  7.,  7.,  7.,  7.],
       [ 7.,  7.,  7.,  7.,  7.]])

>>> np.full((3, 5), 7, dtype=int)
array([[7, 7, 7, 7, 7],
       [7, 7, 7, 7, 7],
       [7, 7, 7, 7, 7]])

```

This is arguably *the* way of creating an array filled with certain values, because it explicitly describes what is being achieved (and it can in principle be very efficient since it performs a very specific task)."
"I have taken [Problem #12](http://projecteuler.net/index.php?section=problems&id=12) from [Project Euler](http://projecteuler.net/) as a programming exercise and to compare my (surely not optimal) implementations in C, Python, Erlang and Haskell. In order to get some higher execution times, I search for the first triangle number with more than 1000 divisors instead of 500 as stated in the original problem.

The result is the following:

**C:**

```
lorenzo@enzo:~/erlang$ gcc -lm -o euler12.bin euler12.c
lorenzo@enzo:~/erlang$ time ./euler12.bin
842161320

real    0m11.074s
user    0m11.070s
sys 0m0.000s

```

**Python:**

```
lorenzo@enzo:~/erlang$ time ./euler12.py 
842161320

real    1m16.632s
user    1m16.370s
sys 0m0.250s

```

**Python with PyPy:**

```
lorenzo@enzo:~/Downloads/pypy-c-jit-43780-b590cf6de419-linux64/bin$ time ./pypy /home/lorenzo/erlang/euler12.py 
842161320

real    0m13.082s
user    0m13.050s
sys 0m0.020s

```

**Erlang:**

```
lorenzo@enzo:~/erlang$ erlc euler12.erl 
lorenzo@enzo:~/erlang$ time erl -s euler12 solve
Erlang R13B03 (erts-5.7.4) [source] [64-bit] [smp:4:4] [rq:4] [async-threads:0] [hipe] [kernel-poll:false]

Eshell V5.7.4  (abort with ^G)
1> 842161320

real    0m48.259s
user    0m48.070s
sys 0m0.020s

```

**Haskell:**

```
lorenzo@enzo:~/erlang$ ghc euler12.hs -o euler12.hsx
[1 of 1] Compiling Main             ( euler12.hs, euler12.o )
Linking euler12.hsx ...
lorenzo@enzo:~/erlang$ time ./euler12.hsx 
842161320

real    2m37.326s
user    2m37.240s
sys 0m0.080s

```

**Summary:**

* C: 100%
* Python: 692% (118% with PyPy)
* Erlang: 436% (135% thanks to RichardC)
* Haskell: 1421%

I suppose that C has a big advantage as it uses long for the calculations and not arbitrary length integers as the other three. Also it doesn't need to load a runtime first (Do the others?).

**Question 1:**
Do Erlang, Python and Haskell lose speed due to using arbitrary length integers or don't they as long as the values are less than `MAXINT`?

**Question 2:**
Why is Haskell so slow? Is there a compiler flag that turns off the brakes or is it my implementation? (The latter is quite probable as Haskell is a book with seven seals to me.)

**Question 3:**
Can you offer me some hints how to optimize these implementations without changing the way I determine the factors? Optimization in any way: nicer, faster, more ""native"" to the language.

### **EDIT:**

**Question 4:**
Do my functional implementations permit LCO (last call optimization, a.k.a tail recursion elimination) and hence avoid adding unnecessary frames onto the call stack?

I really tried to implement the same algorithm as similar as possible in the four languages, although I have to admit that my Haskell and Erlang knowledge is very limited.

---

Source codes used:

```
#include <stdio.h>
#include <math.h>

int factorCount (long n)
{
    double square = sqrt (n);
    int isquare = (int) square;
    int count = isquare == square ? -1 : 0;
    long candidate;
    for (candidate = 1; candidate <= isquare; candidate ++)
        if (0 == n % candidate) count += 2;
    return count;
}

int main ()
{
    long triangle = 1;
    int index = 1;
    while (factorCount (triangle) < 1001)
    {
        index ++;
        triangle += index;
    }
    printf (""%ld\n"", triangle);
}

```

---

```
#! /usr/bin/env python3.2

import math

def factorCount (n):
    square = math.sqrt (n)
    isquare = int (square)
    count = -1 if isquare == square else 0
    for candidate in range (1, isquare + 1):
        if not n % candidate: count += 2
    return count

triangle = 1
index = 1
while factorCount (triangle) < 1001:
    index += 1
    triangle += index

print (triangle)

```

---

```
-module (euler12).
-compile (export_all).

factorCount (Number) -> factorCount (Number, math:sqrt (Number), 1, 0).

factorCount (_, Sqrt, Candidate, Count) when Candidate > Sqrt -> Count;

factorCount (_, Sqrt, Candidate, Count) when Candidate == Sqrt -> Count + 1;

factorCount (Number, Sqrt, Candidate, Count) ->
    case Number rem Candidate of
        0 -> factorCount (Number, Sqrt, Candidate + 1, Count + 2);
        _ -> factorCount (Number, Sqrt, Candidate + 1, Count)
    end.

nextTriangle (Index, Triangle) ->
    Count = factorCount (Triangle),
    if
        Count > 1000 -> Triangle;
        true -> nextTriangle (Index + 1, Triangle + Index + 1)  
    end.

solve () ->
    io:format (""~p~n"", [nextTriangle (1, 1) ] ),
    halt (0).

```

---

```
factorCount number = factorCount' number isquare 1 0 - (fromEnum $ square == fromIntegral isquare)
    where square = sqrt $ fromIntegral number
          isquare = floor square

factorCount' number sqrt candidate count
    | fromIntegral candidate > sqrt = count
    | number `mod` candidate == 0 = factorCount' number sqrt (candidate + 1) (count + 2)
    | otherwise = factorCount' number sqrt (candidate + 1) count

nextTriangle index triangle
    | factorCount triangle > 1000 = triangle
    | otherwise = nextTriangle (index + 1) (triangle + index + 1)

main = print $ nextTriangle 1 1

```","Using `GHC 7.0.3`, `gcc 4.4.6`, `Linux 2.6.29` on an x86\_64 Core2 Duo (2.5GHz) machine, compiling using `ghc -O2 -fllvm -fforce-recomp` for Haskell and `gcc -O3 -lm` for C.

* Your C routine runs in 8.4 seconds (faster than your run probably because of `-O3`)
* The Haskell solution runs in 36 seconds (due to the `-O2` flag)
* Your `factorCount'` code isn't explicitly typed and defaulting to `Integer` (thanks to Daniel for correcting my misdiagnosis here!). Giving an explicit type signature (which is standard practice anyway) using `Int` and the time changes to **11.1 seconds**
* in `factorCount'` you have needlessly called `fromIntegral`. A fix results in no change though (the compiler is smart, lucky for you).
* You used `mod` where `rem` is faster and sufficient. This changes the time to **8.5 seconds**.
* `factorCount'` is constantly applying two extra arguments that never change (`number`, `sqrt`). A worker/wrapper transformation gives us:

```
 $ time ./so
 842161320  

 real    0m7.954s  
 user    0m7.944s  
 sys     0m0.004s  

```

That's right, **7.95 seconds**. Consistently **half a second faster than the C solution**. Without the `-fllvm` flag I'm still getting `8.182 seconds`, so the NCG backend is doing well in this case too.

Conclusion: Haskell is awesome.

**Resulting Code**

```
factorCount number = factorCount' number isquare 1 0 - (fromEnum $ square == fromIntegral isquare)
    where square = sqrt $ fromIntegral number
          isquare = floor square

factorCount' :: Int -> Int -> Int -> Int -> Int
factorCount' number sqrt candidate0 count0 = go candidate0 count0
  where
  go candidate count
    | candidate > sqrt = count
    | number `rem` candidate == 0 = go (candidate + 1) (count + 2)
    | otherwise = go (candidate + 1) count

nextTriangle index triangle
    | factorCount triangle > 1000 = triangle
    | otherwise = nextTriangle (index + 1) (triangle + index + 1)

main = print $ nextTriangle 1 1

```

EDIT: So now that we've explored that, lets address the questions

> Question 1: Do erlang, python and haskell lose speed due to using
> arbitrary length integers or don't they as long as the values are less
> than MAXINT?

In Haskell, using `Integer` is slower than `Int` but how much slower depends on the computations performed. Luckily (for 64 bit machines) `Int` is sufficient. For portability sake you should probably rewrite my code to use `Int64` or `Word64` (C isn't the only language with a `long`).

> Question 2: Why is haskell so slow? Is there a compiler flag that
> turns off the brakes or is it my implementation? (The latter is quite
> probable as haskell is a book with seven seals to me.)
>
> Question 3: Can you offer me some hints how to optimize these
> implementations without changing the way I determine the factors?
> Optimization in any way: nicer, faster, more ""native"" to the language.

That was what I answered above. The answer was

* 0) Use optimization via `-O2`
* 1) Use fast (notably: unbox-able) types when possible
* 2) `rem` not `mod` (a frequently forgotten optimization) and
* 3) worker/wrapper transformation (perhaps the most common optimization).

> Question 4: Do my functional implementations permit LCO and hence
> avoid adding unnecessary frames onto the call stack?

Yes, that wasn't the issue. Good work and glad you considered this."
"I read an [article](http://diveintopython.net/power_of_introspection/getattr.html) about the `getattr` function, but I still can't understand what it's for.

The only thing I understand about `getattr()` is that `getattr(li, ""pop"")` is the same as calling `li.pop`.

When and how do I use this exactly? The book said something about using it to get a reference to a function whose name isn't known until runtime, but when and why would I use this?","Objects in Python can have attributes -- data attributes and functions to work with those (methods). Actually, every object has built-in attributes (try `dir(None)`, `dir(True)`, `dir(...)`, `dir(dir)` in Python console).

For example you have an object `person`, that has several attributes: `name`, `gender`, etc.

You access these attributes (be it methods or data objects) usually writing: `person.name`, `person.gender`, `person.the_method()`, etc.

But what if you don't know the attribute's name at the time you write the program? For example you have attribute's name stored in a variable called `attr_name`.

if

```
attr_name = 'gender'

```

then, instead of writing

```
gender = person.gender

```

you can write

```
gender = getattr(person, attr_name)

```

Some practice:

```
Python 3.4.0 (default, Apr 11 2014, 13:05:11)

>>> class Person():
...     name = 'Victor'
...     def say(self, what):
...         print(self.name, what)
... 
>>> getattr(Person, 'name')
'Victor'
>>> attr_name = 'name'
>>> person = Person()
>>> getattr(person, attr_name)
'Victor'
>>> getattr(person, 'say')('Hello')
Victor Hello

```

[`getattr`](https://docs.python.org/3/library/functions.html#getattr) will raise `AttributeError` if attribute with the given name does not exist in the object:

```
>>> getattr(person, 'age')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'Person' object has no attribute 'age'

```

But you can pass a default value as the third argument, which will be returned if such attribute does not exist:

```
>>> getattr(person, 'age', 0)
0

```

You can use `getattr` along with [`dir`](https://docs.python.org/3/library/functions.html#dir) to iterate over all attribute names and get their values:

```
>>> dir(1000)
['__abs__', '__add__', ..., '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']

>>> obj = 1000
>>> for attr_name in dir(obj):
...     attr_value = getattr(obj, attr_name)
...     print(attr_name, attr_value, callable(attr_value))
... 
__abs__ <method-wrapper '__abs__' of int object at 0x7f4e927c2f90> True
...
bit_length <built-in method bit_length of int object at 0x7f4e927c2f90> True
...

>>> getattr(1000, 'bit_length')()
10

```

A practical use for this would be to find all methods whose names start with `test` and [call them](https://docs.python.org/3/library/unittest.html#organizing-test-code).

Similar to [`getattr`](https://docs.python.org/3/library/functions.html#getattr) there is [`setattr`](https://docs.python.org/3/library/functions.html#setattr) which allows you to set an attribute of an object having its name:

```
>>> setattr(person, 'name', 'Andrew')
>>> person.name  # accessing instance attribute
'Andrew'
>>> Person.name  # accessing class attribute
'Victor'
>>>

```"
"I'm trying to install some packages with `pip`.

But `pip install unroll` gives me

> Command ""python setup.py egg\_info"" failed with error code 1 in
> C:\Users\MARKAN~1\AppData\Local\Temp\pip-build-wa7uco0k\unroll\

How can I solve this?","**About the error code**

According to [the Python documentation](https://docs.python.org/2/library/errno.html):

> This module makes available standard errno system symbols. The value of each symbol is the corresponding integer value. **The names and descriptions are borrowed from linux/include/errno.h**, which should be pretty all-inclusive.

Error code 1 is defined in `errno.h` and means `Operation not permitted`.

**About your error**

Your [setuptools](https://pypi.python.org/pypi/setuptools) do not appear to be installed. Just follow the [`Installation Instructions`](https://packaging.python.org/tutorials/installing-packages/) from the PyPI website.

If it's already installed, try

```
pip install --upgrade setuptools

```

If it's already up to date, check that the module ez\_setup is not missing. If it is, then

```
pip install ez_setup

```

Then try again

```
pip install unroll

```

If it's still not working, maybe pip didn't install/upgrade setup\_tools properly so you might want to try

```
easy_install -U setuptools

```

And again

```
pip install unroll

```"
"While trying to reconcile my understanding of LSTMs pointed out here in [this post by Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) implemented in Keras and following the [blog written by Jason Brownlee](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) for the Keras tutorial, I am confused about the following:

1. The reshaping of the data series into `[samples, time steps, features]` and,
2. The stateful LSTMs

Considering the above two questions that are referenced by the code below:

```
# reshape into X=t and Y=t+1
look_back = 3
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)

# reshape input to be [samples, time steps, features]
trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, 1))
testX = numpy.reshape(testX, (testX.shape[0], look_back, 1))
########################
# The IMPORTANT BIT
##########################
# create and fit the LSTM network
batch_size = 1
model = Sequential()
model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
for i in range(100):
    model.fit(trainX, trainY, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)
    model.reset_states()

```

Note: create\_dataset takes a sequence of length N and returns a `N-look_back` array of which each element is a `look_back` length sequence.

What are the Time Steps and Features?
=====================================

As it can be seen, TrainX is a 3-D array with Time\_steps and Feature being the last two dimensions respectively (3 and 1 in this particular code). Looking at the image below, does this mean that we are considering the `many to one` case, where the number of pink boxes is 3? Or does it mean the chain length is 3 (?. [![enter image description here](https://i.sstatic.net/kwhAP.jpg)](https://i.sstatic.net/kwhAP.jpg)

Does the features argument become relevant when we consider multivariate series? e.g. Modelling two financial stocks simultaneously?

Stateful LSTMs
==============

Does stateful LSTMs mean that we save the cell memory values between runs of batches? If this is the case, `batch_size` is one, and the memory is reset between the training runs, so what was the point of saying that it was stateful? I am guessing this is related to the fact that training data is not shuffled, but am not sure how.

Any thoughts?
Image reference: <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>

Edit 1:
-------

A bit confused about @van's comment about the red and green boxes being equal. Does the following API calls correspond to the unrolled diagrams? Especially noting the second diagram (`batch_size` was arbitrarily chosen.):
[![enter image description here](https://i.sstatic.net/sW207.jpg)](https://i.sstatic.net/sW207.jpg)
[![enter image description here](https://i.sstatic.net/15V2C.jpg)](https://i.sstatic.net/15V2C.jpg)

Edit 2:
-------

For people who have done Udacity's deep learning course and confused about the time\_step argument, look at the following discussion: <https://discussions.udacity.com/t/rnn-lstm-use-implementation/163169>

Update:
-------

It turns out `model.add(TimeDistributed(Dense(vocab_len)))` was what I was looking for. Here is an example: <https://github.com/sachinruk/ShakespeareBot>

Update2:
--------

I have summarised most of my understanding of LSTMs here: <https://www.youtube.com/watch?v=ywinX5wgdEU>","As a complement to the accepted answer, this answer shows keras behaviors and how to achieve each picture.

General Keras behavior
----------------------

The standard keras internal processing is always a many to many as in the following picture (where I used `features=2`, pressure and temperature, just as an example):

[![ManyToMany](https://i.sstatic.net/vTYa5.jpg)](https://i.sstatic.net/vTYa5.jpg)

In this image, I increased the number of steps to 5, to avoid confusion with the other dimensions.

For this example:

* We have N oil tanks
* We spent 5 hours taking measures hourly (time steps)
* We measured two features:
  + Pressure P
  + Temperature T

Our input array should then be something shaped as `(N,5,2)`:

```
        [     Step1      Step2      Step3      Step4      Step5
Tank A:    [[Pa1,Ta1], [Pa2,Ta2], [Pa3,Ta3], [Pa4,Ta4], [Pa5,Ta5]],
Tank B:    [[Pb1,Tb1], [Pb2,Tb2], [Pb3,Tb3], [Pb4,Tb4], [Pb5,Tb5]],
  ....
Tank N:    [[Pn1,Tn1], [Pn2,Tn2], [Pn3,Tn3], [Pn4,Tn4], [Pn5,Tn5]],
        ]

```

Inputs for sliding windows
--------------------------

Often, LSTM layers are supposed to process the entire sequences. Dividing windows may not be the best idea. The layer has internal states about how a sequence is evolving as it steps forward. Windows eliminate the possibility of learning long sequences, limiting all sequences to the window size.

In windows, each window is part of a long original sequence, but by Keras they will be seen each as an independent sequence:

```
        [     Step1    Step2    Step3    Step4    Step5
Window  A:  [[P1,T1], [P2,T2], [P3,T3], [P4,T4], [P5,T5]],
Window  B:  [[P2,T2], [P3,T3], [P4,T4], [P5,T5], [P6,T6]],
Window  C:  [[P3,T3], [P4,T4], [P5,T5], [P6,T6], [P7,T7]],
  ....
        ]

```

Notice that in this case, you have initially only one sequence, but you're dividing it in many sequences to create windows.

The concept of ""what is a sequence"" is abstract. The important parts are:

* you can have batches with many individual sequences
* what makes the sequences be sequences is that they evolve in steps (usually time steps)

Achieving each case with ""single layers""
========================================

### Achieving standard many to many:

[![StandardManyToMany](https://i.sstatic.net/RXYW2.jpg)](https://i.sstatic.net/RXYW2.jpg)

You can achieve many to many with a simple LSTM layer, using `return_sequences=True`:

```
outputs = LSTM(units, return_sequences=True)(inputs)

#output_shape -> (batch_size, steps, units)

```

### Achieving many to one:

Using the exact same layer, keras will do the exact same internal preprocessing, but when you use `return_sequences=False` (or simply ignore this argument), keras will automatically discard the steps previous to the last:

[![ManyToOne](https://i.sstatic.net/GMe8r.jpg)](https://i.sstatic.net/GMe8r.jpg)

```
outputs = LSTM(units)(inputs)

#output_shape -> (batch_size, units) --> steps were discarded, only the last was returned

```

Achieving one to many
---------------------

Now, this is not supported by keras LSTM layers alone. You will have to create your own strategy to multiplicate the steps. There are two good approaches:

* Create a constant multi-step input by repeating a tensor
* Use a `stateful=True` to recurrently take the output of one step and serve it as the input of the next step (needs `output_features == input_features`)

### One to many with repeat vector

In order to fit to keras standard behavior, we need inputs in steps, so, we simply repeat the inputs for the length we want:

[![OneToManyRepeat](https://i.sstatic.net/ZADR6.jpg)](https://i.sstatic.net/ZADR6.jpg)

```
outputs = RepeatVector(steps)(inputs) #where inputs is (batch,features)
outputs = LSTM(units,return_sequences=True)(outputs)

#output_shape -> (batch_size, steps, units)

```

Understanding stateful = True
-----------------------------

Now comes one of the possible usages of `stateful=True` (besides avoiding loading data that can't fit your computer's memory at once)

Stateful allows us to input ""parts"" of the sequences in stages. The difference is:

* In `stateful=False`, the second batch contains whole new sequences, independent from the first batch
* In `stateful=True`, the second batch continues the first batch, extending the same sequences.

It's like dividing the sequences in windows too, with these two main differences:

* these windows do not superpose!!
* `stateful=True` will see these windows connected as a single long sequence

In `stateful=True`, every new batch will be interpreted as continuing the previous batch (until you call `model.reset_states()`).

* Sequence 1 in batch 2 will continue sequence 1 in batch 1.
* Sequence 2 in batch 2 will continue sequence 2 in batch 1.
* Sequence n in batch 2 will continue sequence n in batch 1.

Example of inputs, batch 1 contains steps 1 and 2, batch 2 contains steps 3 to 5:

```
                   BATCH 1                           BATCH 2
        [     Step1      Step2        |    [    Step3      Step4      Step5
Tank A:    [[Pa1,Ta1], [Pa2,Ta2],     |       [Pa3,Ta3], [Pa4,Ta4], [Pa5,Ta5]],
Tank B:    [[Pb1,Tb1], [Pb2,Tb2],     |       [Pb3,Tb3], [Pb4,Tb4], [Pb5,Tb5]],
  ....                                |
Tank N:    [[Pn1,Tn1], [Pn2,Tn2],     |       [Pn3,Tn3], [Pn4,Tn4], [Pn5,Tn5]],
        ]                                  ]

```

Notice the alignment of tanks in batch 1 and batch 2! That's why we need `shuffle=False` (unless we are using only one sequence, of course).

You can have any number of batches, indefinitely. (For having variable lengths in each batch, use `input_shape=(None,features)`.

One to many with stateful=True
------------------------------

For our case here, we are going to use only 1 step per batch, because we want to get one output step and make it be an input.

Please notice that the behavior in the picture is not ""caused by"" `stateful=True`. We will force that behavior in a manual loop below. In this example, `stateful=True` is what ""allows"" us to stop the sequence, manipulate what we want, and continue from where we stopped.

[![OneToManyStateful](https://i.sstatic.net/ihAFT.jpg)](https://i.sstatic.net/ihAFT.jpg)

Honestly, the repeat approach is probably a better choice for this case. But since we're looking into `stateful=True`, this is a good example. The best way to use this is the next ""many to many"" case.

Layer:

```
outputs = LSTM(units=features, 
               stateful=True, 
               return_sequences=True, #just to keep a nice output shape even with length 1
               input_shape=(None,features))(inputs) 
    #units = features because we want to use the outputs as inputs
    #None because we want variable length

#output_shape -> (batch_size, steps, units) 

```

Now, we're going to need a manual loop for predictions:

```
input_data = someDataWithShape((batch, 1, features))

#important, we're starting new sequences, not continuing old ones:
model.reset_states()

output_sequence = []
last_step = input_data
for i in steps_to_predict:

    new_step = model.predict(last_step)
    output_sequence.append(new_step)
    last_step = new_step

 #end of the sequences
 model.reset_states()

```

### Many to many with stateful=True

Now, here, we get a very nice application: given an input sequence, try to predict its future unknown steps.

We're using the same method as in the ""one to many"" above, with the difference that:

* we will use the sequence itself to be the target data, one step ahead
* we know part of the sequence (so we discard this part of the results).

[![ManyToManyStateful](https://i.sstatic.net/4HPZB.jpg)](https://i.sstatic.net/4HPZB.jpg)

Layer (same as above):

```
outputs = LSTM(units=features, 
               stateful=True, 
               return_sequences=True, 
               input_shape=(None,features))(inputs) 
    #units = features because we want to use the outputs as inputs
    #None because we want variable length

#output_shape -> (batch_size, steps, units) 

```

**Training:**

We are going to train our model to predict the next step of the sequences:

```
totalSequences = someSequencesShaped((batch, steps, features))
    #batch size is usually 1 in these cases (often you have only one Tank in the example)

X = totalSequences[:,:-1] #the entire known sequence, except the last step
Y = totalSequences[:,1:] #one step ahead of X

#loop for resetting states at the start/end of the sequences:
for epoch in range(epochs):
    model.reset_states()
    model.train_on_batch(X,Y)

```

**Predicting:**

The first stage of our predicting involves ""ajusting the states"". That's why we're going to predict the entire sequence again, even if we already know this part of it:

```
model.reset_states() #starting a new sequence
predicted = model.predict(totalSequences)
firstNewStep = predicted[:,-1:] #the last step of the predictions is the first future step

```

Now we go to the loop as in the one to many case. But **don't reset states here!**. We want the model to know in which step of the sequence it is (and it knows it's at the first new step because of the prediction we just made above)

```
output_sequence = [firstNewStep]
last_step = firstNewStep
for i in steps_to_predict:

    new_step = model.predict(last_step)
    output_sequence.append(new_step)
    last_step = new_step

 #end of the sequences
 model.reset_states()

```

This approach was used in these answers and file:

* [Predicting a multiple forward time step of a time series using LSTM](https://stackoverflow.com/questions/47594861/predicting-a-multiple-time-step-forward-of-a-time-series-using-lstm/47719094#47719094)
* [how to use the Keras model to forecast for future dates or events?](https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events/48807811#48807811)
* <https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb>

Achieving complex configurations
================================

In all examples above, I showed the behavior of ""one layer"".

You can, of course, stack many layers on top of each other, not necessarly all following the same pattern, and create your own models.

One interesting example that has been appearing is the ""autoencoder"" that has a ""many to one encoder"" followed by a ""one to many"" decoder:

**Encoder:**

```
inputs = Input((steps,features))

#a few many to many layers:
outputs = LSTM(hidden1,return_sequences=True)(inputs)
outputs = LSTM(hidden2,return_sequences=True)(outputs)    

#many to one layer:
outputs = LSTM(hidden3)(outputs)

encoder = Model(inputs,outputs)

```

**Decoder:**

Using the ""repeat"" method;

```
inputs = Input((hidden3,))

#repeat to make one to many:
outputs = RepeatVector(steps)(inputs)

#a few many to many layers:
outputs = LSTM(hidden4,return_sequences=True)(outputs)

#last layer
outputs = LSTM(features,return_sequences=True)(outputs)

decoder = Model(inputs,outputs)

```

**Autoencoder:**

```
inputs = Input((steps,features))
outputs = encoder(inputs)
outputs = decoder(outputs)

autoencoder = Model(inputs,outputs)

```

Train with `fit(X,X)`

Additional explanations
=======================

If you want details about how steps are calculated in LSTMs, or details about the `stateful=True` cases above, you can read more in this answer: [Doubts regarding `Understanding Keras LSTMs`](https://stackoverflow.com/questions/53955093/doubts-regarding-understanding-keras-lstms)"
"I want to display:

`49` as `49.00`

and:

`54.9` as `54.90`

Regardless of the length of the decimal or whether there are are any decimal places, I would like to display a `Decimal` with 2 decimal places, and I'd like to do it in an efficient way. The purpose is to display money values.

eg, `4898489.00`

---

For the analogous issue with the built-in `float` type, see [Limiting floats to two decimal points](https://stackoverflow.com/questions/455612).","You should use the [new format specifications](https://docs.python.org/3/library/string.html#format-specification-mini-language ""Format Specification Mini-Language"") to define how your value should be represented:

```
>>> from math import pi  # pi ~ 3.141592653589793
>>> '{0:.2f}'.format(pi)
'3.14'

```

The documentation can be a bit obtuse at times, so I recommend the following, easier readable references:

* the [Python String Format Cookbook](https://mkaz.blog/code/python-string-format-cookbook/ ""Python String Format Cookbook""): shows examples of the new-style [`.format()`](https://docs.python.org/2/library/string.html#format-string-syntax) string formatting
* [pyformat.info](https://pyformat.info/): compares the old-style `%` string formatting with the new-style [`.format()`](https://docs.python.org/2/library/string.html#format-string-syntax) string formatting

Python 3.6 introduced [literal string interpolation](https://www.python.org/dev/peps/pep-0498/) (also known as f-strings) so now you can write the above even more succinct as:

```
>>> f'{pi:.2f}'
'3.14'

```"
"I want to perform my own complex operations on financial data in dataframes in a sequential manner.

For example I am using the following MSFT CSV file taken from [Yahoo Finance](http://finance.yahoo.com/q/hp?s=MSFT):

```
Date,Open,High,Low,Close,Volume,Adj Close
2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13
2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31
2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98
2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27

....

```

I then do the following:

```
#!/usr/bin/env python
from pandas import *

df = read_csv('table.csv')

for i, row in enumerate(df.values):
    date = df.index[i]
    open, high, low, close, adjclose = row
    #now perform analysis on open/close based on date, etc..

```

Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? `df.iteritems` unfortunately only iterates column by column.","The newest versions of pandas now include a built-in function for iterating over rows.

```
for index, row in df.iterrows():

    # do some logic here

```

Or, if you want it faster use `itertuples()`

But, unutbu's suggestion to use numpy functions to avoid iterating over rows will produce the fastest code."
"In Python you can have multiple iterators in a list comprehension, like

```
[(x,y) for x in a for y in b]

```

for some suitable sequences a and b. I'm aware of the nested loop semantics of Python's list comprehensions.

My question is: Can one iterator in the comprehension refer to the other? In other words: Could I have something like this:

```
[x for x in a for a in b]

```

where the current value of the outer loop is the iterator of the inner?

As an example, if I have a nested list:

```
a=[[1,2],[3,4]]

```

what would the list comprehension expression be to achieve this result:

```
[1,2,3,4]

```

?? (Please only list comprehension answers, since this is what I want to find out).","Suppose you have a text full of sentences and you want an array of words.

```
# Without list comprehension
list_of_words = []
for sentence in text:
    for word in sentence:
       list_of_words.append(word)
return list_of_words

```

I like to think of list comprehension as stretching code horizontally.

Try breaking it up into:

```
# List Comprehension 
[word for sentence in text for word in sentence]

```

**Example:**

```
>>> text = ((""Hi"", ""Steve!""), (""What's"", ""up?""))
>>> [word for sentence in text for word in sentence]
['Hi', 'Steve!', ""What's"", 'up?']

```

This also works for generators

```
>>> text = ((""Hi"", ""Steve!""), (""What's"", ""up?""))
>>> gen = (word for sentence in text for word in sentence)
>>> for word in gen: print(word)
Hi
Steve!
What's
up?

```"
"In the [Anaconda repository](https://repo.continuum.io/), there are two types of installers:

""**Anaconda installers**"" and ""**Miniconda installers**"".

What are their differences?

Besides, for an installer file, `Anaconda2-4.4.0.1-Linux-ppc64le.sh`, what does `2-4.4.0.1` stand for?","Per the [original docs](https://conda.io/projects/conda/en/latest/user-guide/install/download.html#anaconda-or-miniconda):

Choose Anaconda if you:

* Are new to conda or Python
* Like the convenience of having Python and over 1500 scientific packages automatically installed at once
* Have the time and disk space (a few minutes and 3 GB), and/or
* Donâ€™t want to install each of the packages you want to use individually.

Choose Miniconda if you:

* Do not mind installing each of the packages you want to use
  individually.
* Do not have time or disk space to install over 1500
  packages at once, and/or
* Just want fast access to Python and the conda commands, and wish to sort out the other programs later.

I use Miniconda myself. Anaconda is bloated. Many of the packages are never used and could still be easily installed if and when needed.

Note that [Conda](https://conda.io/projects/conda/en/latest/) is the package manager (e.g. `conda list` displays all installed packages in the environment), whereas Anaconda and Miniconda are distributions. A software distribution is a collection of packages, pre-built and pre-configured, that can be installed and used on a system. A package manager is a tool that automates the process of installing, updating, and removing packages.

> Anaconda is a full distribution of the central software in the PyData ecosystem, and includes Python itself along with the binaries for several hundred third-party open-source projects. Miniconda is essentially an installer for an empty conda environment, containing only Conda, its dependencies, and Python. [Source](https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/).

Once Conda is installed, you can then install whatever package you need from scratch along with any desired version of Python.

`2-4.4.0.1` is the version number for your Anaconda installation package. Strangely, it is not listed in their [Old Package Lists](https://docs.continuum.io/anaconda/packages/oldpkglists).

In April 2016, the Anaconda versioning jumped from 2.5 to 4.0 in order to avoid confusion with Python versions 2 & 3. Version 4.0 included the Anaconda Navigator.

Release notes for subsequent versions can be found [here](https://docs.anaconda.com/anaconda/reference/release-notes/).

**LICENSE NOTE: The company behind Anaconda updated their Terms of Service in 2020 to prohibit commercial usage for most uses.** You are NOT permitted to use Anaconda or Miniconda in a business with more than 200 employees, unless you acquire licenses. Please review the current license terms [here](https://legal.anaconda.com/policies/en/?name=end-user-license-agreements)."
"I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.

Existing df:

```
Date, Open, High, Low, Close
01-01-2015, 565, 600, 400, 450

```

New df:

```
Name, Date, Open, High, Low, Close
abc, 01-01-2015, 565, 600, 400, 450

```

I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.","`df['Name']='abc'` will add the new column and set all rows to that value:

```
In [79]:

df
Out[79]:
         Date, Open, High,  Low,  Close
0  01-01-2015,  565,  600,  400,    450
In [80]:

df['Name'] = 'abc'
df
Out[80]:
         Date, Open, High,  Low,  Close Name
0  01-01-2015,  565,  600,  400,    450  abc

```"
"Let's say I have an associative array like so: `{'key1': 22, 'key2': 42}`.

How can I check if `key1` exists in the dictionary?","```
if key in array:
  # do something

```

Associative arrays are called dictionaries in Python and you can learn more about them in [the stdtypes documentation](http://docs.python.org/library/stdtypes.html#dict)."
"I am creating a DataFrame from a csv as follows:

```
stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)

```

The DataFrame has a date column. Is there a way to create a new DataFrame (or just overwrite the existing one) which only contains rows with date values that fall within a specified date range or between two specified date values?","There are two possible solutions:

* Use a boolean mask, then use `df.loc[mask]`
* Set the date column as a DatetimeIndex, then use `df[start_date : end_date]`

---

**Using a boolean mask**:

Ensure `df['date']` is a Series with dtype `datetime64[ns]`:

```
df['date'] = pd.to_datetime(df['date'])  

```

Make a boolean mask. `start_date` and `end_date` can be `datetime.datetime`s,
`np.datetime64`s, `pd.Timestamp`s, or even datetime strings:

```
#greater than the start date and smaller than the end date
mask = (df['date'] > start_date) & (df['date'] <= end_date)

```

Select the sub-DataFrame:

```
df.loc[mask]

```

or re-assign to `df`

```
df = df.loc[mask]

```

---

For example,

```
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random((200,3)))
df['date'] = pd.date_range('2000-1-1', periods=200, freq='D')
mask = (df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')
print(df.loc[mask])

```

yields

```
            0         1         2       date
153  0.208875  0.727656  0.037787 2000-06-02
154  0.750800  0.776498  0.237716 2000-06-03
155  0.812008  0.127338  0.397240 2000-06-04
156  0.639937  0.207359  0.533527 2000-06-05
157  0.416998  0.845658  0.872826 2000-06-06
158  0.440069  0.338690  0.847545 2000-06-07
159  0.202354  0.624833  0.740254 2000-06-08
160  0.465746  0.080888  0.155452 2000-06-09
161  0.858232  0.190321  0.432574 2000-06-10

```

---

**Using a [DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#partial-string-indexing)**:

If you are going to do a lot of selections by date, it may be quicker to set the
`date` column as the index first. Then you can select rows by date using
`df.loc[start_date:end_date]`.

```
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random((200,3)))
df['date'] = pd.date_range('2000-1-1', periods=200, freq='D')
df = df.set_index(['date'])
print(df.loc['2000-6-1':'2000-6-10'])

```

yields

```
                   0         1         2
date                                    
2000-06-01  0.040457  0.326594  0.492136    # <- includes start_date
2000-06-02  0.279323  0.877446  0.464523
2000-06-03  0.328068  0.837669  0.608559
2000-06-04  0.107959  0.678297  0.517435
2000-06-05  0.131555  0.418380  0.025725
2000-06-06  0.999961  0.619517  0.206108
2000-06-07  0.129270  0.024533  0.154769
2000-06-08  0.441010  0.741781  0.470402
2000-06-09  0.682101  0.375660  0.009916
2000-06-10  0.754488  0.352293  0.339337

```

While Python list indexing, e.g. `seq[start:end]` includes `start` but not `end`, in contrast, Pandas `df.loc[start_date : end_date]` includes *both* end-points in the result if they are in the index. Neither `start_date` nor `end_date` has to be in the index however.

---

Also note that [`pd.read_csv` has a `parse_dates` parameter](https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html) which you could use to parse the `date` column as `datetime64`s. Thus, if you use `parse_dates`, you would not need to use `df['date'] = pd.to_datetime(df['date'])`."
"I'm having a hard time wrapping my brain around [PEP 380](http://www.python.org/dev/peps/pep-0380/).

1. What are the situations where `yield from` is useful?
2. What is the classic use case?
3. Why is it compared to micro-threads?

So far I have used generators, but never really used coroutines (introduced by [PEP-342](http://www.python.org/dev/peps/pep-0342/)). Despite some similarities, generators and coroutines are basically two different concepts. Understanding coroutines (not only generators) is the key to understanding the new syntax.

IMHO **coroutines are the most obscure Python feature**, most books make it look useless and uninteresting.

---

Thanks for the great answers, but special thanks to [agf](https://stackoverflow.com/users/500584/agf) and his comment linking to [David Beazley presentations](http://www.dabeaz.com/coroutines/).","Let's get one thing out of the way first. The explanation that `yield from g` is equivalent to `for v in g: yield v` **does not even begin to do justice** to what `yield from` is all about. Because, let's face it, if all `yield from` does is expand the `for` loop, then it does not warrant adding `yield from` to the language and preclude a whole bunch of new features from being implemented in Python 2.x.

What `yield from` does is it ***establishes a transparent, bidirectional connection between the caller and the sub-generator***:

* The connection is ""transparent"" in the sense that it will propagate everything correctly, not just the elements being generated (e.g. exceptions are propagated).
* The connection is ""bidirectional"" in the sense that data can be both sent *from* and *to* a generator.

(*If we were talking about TCP, `yield from g` might mean ""now temporarily disconnect my client's socket and reconnect it to this other server socket"".*)

BTW, if you are not sure what *sending data to a generator* even means, you need to drop everything and read about *coroutines* firstâ€”they're very useful (contrast them with *subroutines*), but unfortunately lesser-known in Python. [Dave Beazley's Curious Course on Coroutines](https://dabeaz.com/coroutines/) is an excellent start. [Read slides 24-33](https://dabeaz.com/coroutines/Coroutines.pdf) for a quick primer.

Reading data from a generator using yield from
----------------------------------------------

```
def reader():
    """"""A generator that fakes a read from a file, socket, etc.""""""
    for i in range(4):
        yield '<< %s' % i

def reader_wrapper(g):
    # Manually iterate over data produced by reader
    for v in g:
        yield v

wrap = reader_wrapper(reader())
for i in wrap:
    print(i)

# Result
<< 0
<< 1
<< 2
<< 3

```

Instead of manually iterating over `reader()`, we can just `yield from` it.

```
def reader_wrapper(g):
    yield from g

```

That works, and we eliminated one line of code. And probably the intent is a little bit clearer (or not). But nothing life changing.

Sending data to a generator (coroutine) using yield from - Part 1
-----------------------------------------------------------------

Now let's do something more interesting. Let's create a coroutine called `writer` that accepts data sent to it and writes to a socket, fd, etc.

```
def writer():
    """"""A coroutine that writes data *sent* to it to fd, socket, etc.""""""
    while True:
        w = (yield)
        print('>> ', w)

```

Now the question is, how should the wrapper function handle sending data to the writer, so that any data that is sent to the wrapper is *transparently* sent to the `writer()`?

```
def writer_wrapper(coro):
    # TBD
    pass

w = writer()
wrap = writer_wrapper(w)
wrap.send(None)  # ""prime"" the coroutine
for i in range(4):
    wrap.send(i)

# Expected result
>>  0
>>  1
>>  2
>>  3

```

The wrapper needs to *accept* the data that is sent to it (obviously) and should also handle the `StopIteration` when the for loop is exhausted. Evidently just doing `for x in coro: yield x` won't do. Here is a version that works.

```
def writer_wrapper(coro):
    coro.send(None)  # prime the coro
    while True:
        try:
            x = (yield)  # Capture the value that's sent
            coro.send(x)  # and pass it to the writer
        except StopIteration:
            pass

```

Or, we could do this.

```
def writer_wrapper(coro):
    yield from coro

```

That saves 6 lines of code, make it much much more readable and it just works. Magic!

Sending data to a generator yield from - Part 2 - Exception handling
--------------------------------------------------------------------

Let's make it more complicated. What if our writer needs to handle exceptions? Let's say the `writer` handles a `SpamException` and it prints `***` if it encounters one.

```
class SpamException(Exception):
    pass

def writer():
    while True:
        try:
            w = (yield)
        except SpamException:
            print('***')
        else:
            print('>> ', w)

```

What if we don't change `writer_wrapper`? Does it work? Let's try

```
# writer_wrapper same as above

w = writer()
wrap = writer_wrapper(w)
wrap.send(None)  # ""prime"" the coroutine
for i in [0, 1, 2, 'spam', 4]:
    if i == 'spam':
        wrap.throw(SpamException)
    else:
        wrap.send(i)

# Expected Result
>>  0
>>  1
>>  2
***
>>  4

# Actual Result
>>  0
>>  1
>>  2
Traceback (most recent call last):
  ... redacted ...
  File ... in writer_wrapper
    x = (yield)
__main__.SpamException

```

Um, it's not working because `x = (yield)` just raises the exception and everything comes to a crashing halt. Let's make it work, but manually handling exceptions and sending them or throwing them into the sub-generator (`writer`)

```
def writer_wrapper(coro):
    """"""Works. Manually catches exceptions and throws them""""""
    coro.send(None)  # prime the coro
    while True:
        try:
            try:
                x = (yield)
            except Exception as e:   # This catches the SpamException
                coro.throw(e)
            else:
                coro.send(x)
        except StopIteration:
            pass

```

This works.

```
# Result
>>  0
>>  1
>>  2
***
>>  4

```

But so does this!

```
def writer_wrapper(coro):
    yield from coro

```

The `yield from` transparently handles sending the values or throwing values into the sub-generator.

This still does not cover all the corner cases though. What happens if the outer generator is closed? What about the case when the sub-generator returns a value (yes, in Python 3.3+, generators can return values), how should the return value be propagated? [That `yield from` transparently handles all the corner cases is really impressive](https://www.python.org/dev/peps/pep-0380/#formal-semantics). `yield from` just magically works and handles all those cases.

I personally feel `yield from` is a poor keyword choice because it does not make the *two-way* nature apparent. There were other keywords proposed (like `delegate` but were rejected because adding a new keyword to the language is much more difficult than combining existing ones.

In summary, it's best to think of `yield from` as a **`transparent two way channel`** between the caller and the sub-generator.

References:

1. [PEP 380](http://www.python.org/dev/peps/pep-0380/) - Syntax for delegating to a sub-generator (Ewing) [v3.3, 2009-02-13]
2. [PEP 342](http://www.python.org/dev/peps/pep-0342/) -
   Coroutines via Enhanced Generators (GvR, Eby) [v2.5, 2005-05-10]"
"I use this command in the shell to install PIL:

```
easy_install PIL

```

then I run `python` and type this: `import PIL`. But I get this error:

```
Traceback (most recent call last):
  File ""<console>"", line 1, in <module>
ImportError: No module named PIL

```

I've never had such problem, what do you think?","In shell, run:

```
pip install Pillow

```

Attention: PIL is deprecated, and [pillow](https://pypi.org/project/pillow/) is the successor."
"How to send a `multipart/form-data` with `requests` in python? How to send a file, I understand, but how to send the form data by this method can not understand.","Basically, if you specify a `files` parameter (a dictionary), then `requests` will send a `multipart/form-data` POST instead of a `application/x-www-form-urlencoded` POST. You are not limited to using actual files in that dictionary, however:

```
>>> import requests
>>> response = requests.post('http://httpbin.org/post', files=dict(foo='bar'))
>>> response.status_code
200

```

and httpbin.org lets you know what headers you posted with; in `response.json()` we have:

```
>>> from pprint import pprint
>>> pprint(response.json()['headers'])
{'Accept': '*/*',
 'Accept-Encoding': 'gzip, deflate',
 'Connection': 'close',
 'Content-Length': '141',
 'Content-Type': 'multipart/form-data; '
                 'boundary=c7cbfdd911b4e720f1dd8f479c50bc7f',
 'Host': 'httpbin.org',
 'User-Agent': 'python-requests/2.21.0'}

```

And just to be explicit: you should **not** set the `Content-Type` header when you use the `files` parameter, leave this to `requests` because it needs to specify a (unique) boundary value in the header that matches the value used in the request body.

Better still, you can further control the filename, content type and additional headers for each part by using a tuple instead of a single string or bytes object. The tuple is expected to contain between 2 and 4 elements; the filename, the content, optionally a content type, and an optional dictionary of further headers.

I'd use the tuple form with `None` as the filename, so that the `filename=""...""` parameter is dropped from the request for those parts:

```
>>> files = {'foo': 'bar'}
>>> print(requests.Request('POST', 'http://httpbin.org/post', files=files).prepare().body.decode('utf8'))
--bb3f05a247b43eede27a124ef8b968c5
Content-Disposition: form-data; name=""foo""; filename=""foo""

bar
--bb3f05a247b43eede27a124ef8b968c5--
>>> files = {'foo': (None, 'bar')}
>>> print(requests.Request('POST', 'http://httpbin.org/post', files=files).prepare().body.decode('utf8'))
--d5ca8c90a869c5ae31f70fa3ddb23c76
Content-Disposition: form-data; name=""foo""

bar
--d5ca8c90a869c5ae31f70fa3ddb23c76--

```

`files` can also be a list of two-value tuples, if you need ordering and/or multiple fields with the same name:

```
requests.post(
    'http://requestb.in/xucj9exu',
    files=(
        ('foo', (None, 'bar')),
        ('foo', (None, 'baz')),
        ('spam', (None, 'eggs')),
    )
)

```

If you specify both `files` and `data`, then it depends on the *value* of `data` what will be used to create the POST body. If `data` is a string, only it willl be used; otherwise both `data` and `files` are used, with the elements in `data` listed first.

There is also the excellent `requests-toolbelt` project, which includes [advanced Multipart support](https://toolbelt.readthedocs.io/en/latest/uploading-data.html). It takes field definitions in the same format as the `files` parameter, but unlike `requests`, it defaults to not setting a filename parameter. In addition, it can stream the request from open file objects, where `requests` will first construct the request body in memory:

```
from requests_toolbelt.multipart.encoder import MultipartEncoder

mp_encoder = MultipartEncoder(
    fields={
        'foo': 'bar',
        # plain file object, no filename or mime type produces a
        # Content-Disposition header with just the part name
        'spam': ('spam.txt', open('spam.txt', 'rb'), 'text/plain'),
    }
)
r = requests.post(
    'http://httpbin.org/post',
    data=mp_encoder,  # The MultipartEncoder is posted as data, don't use files=...!
    # The MultipartEncoder provides the content-type header with the boundary:
    headers={'Content-Type': mp_encoder.content_type}
)

```

Fields follow the same conventions; use a tuple with between 2 and 4 elements to add a filename, part mime-type or extra headers. Unlike the `files` parameter, no attempt is made to find a default `filename` value if you don't use a tuple."
"I am *trying* to understand what Python's descriptors are and what they are useful for. I understand how they work, but here are my doubts. Consider the following code:

```
class Celsius(object):
    def __init__(self, value=0.0):
        self.value = float(value)
    def __get__(self, instance, owner):
        return self.value
    def __set__(self, instance, value):
        self.value = float(value)


class Temperature(object):
    celsius = Celsius()

```

1. Why do I need the descriptor class?
2. What is `instance` and `owner` here? (in `__get__`). What is the purpose of these parameters?
3. How would I call/use this example?","The descriptor is how Python's `property` type is implemented. A descriptor simply implements `__get__`, `__set__`, etc. and is then added to another class in its definition (as you did above with the Temperature class). For example:

```
temp=Temperature()
temp.celsius #calls celsius.__get__

```

Accessing the property you assigned the descriptor to (`celsius` in the above example) calls the appropriate descriptor method.

`instance` in `__get__` is the instance of the class (so above, `__get__` would receive `temp`, while `owner` is the class with the descriptor (so it would be `Temperature`).

You need to use a descriptor class to encapsulate the logic that powers it. That way, if the descriptor is used to cache some expensive operation (for example), it could store the value on itself and not its class.

The official Python documentation includes an [article about descriptors](https://docs.python.org/3/howto/descriptor.html) that walks through how they work in more detail, including several examples.

EDIT: As jchl pointed out in the comments, if you simply try `Temperature.celsius`, `instance` will be `None`."
"How do I efficiently obtain the frequency count for each unique value in a NumPy array?

```
>>> x = np.array([1,1,1,2,2,2,5,25,1,1])
>>> freq_count(x)
[(1, 5), (2, 3), (5, 1), (25, 1)]

```","Use [`numpy.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) with `return_counts=True` (for NumPy 1.9+):

```
import numpy as np

x = np.array([1,1,1,2,2,2,5,25,1,1])
unique, counts = np.unique(x, return_counts=True)

>>> print(np.asarray((unique, counts)).T)
 [[ 1  5]
  [ 2  3]
  [ 5  1]
  [25  1]]

```

In comparison with [`scipy.stats.itemfreq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.itemfreq.html):

```
In [4]: x = np.random.random_integers(0,100,1e6)

In [5]: %timeit unique, counts = np.unique(x, return_counts=True)
10 loops, best of 3: 31.5 ms per loop

In [6]: %timeit scipy.stats.itemfreq(x)
10 loops, best of 3: 170 ms per loop

```"
"I am working with a library which returns a ""byte string"" (`bytes`) and I need to convert this to a string.

Is there actually a difference between those two things? How are they related, and how can I do the conversion?","The only thing that a computer can store is bytes.

To store anything in a computer, you must first *encode* it, i.e. convert it to bytes. For example:

* If you want to store music, you must first *encode* it using [MP3](https://en.wikipedia.org/wiki/MP3), [WAV](https://en.wikipedia.org/wiki/WAV), etc.
* If you want to store a picture, you must first *encode* it using [PNG](https://en.wikipedia.org/wiki/Portable_Network_Graphics), [JPEG](https://en.wikipedia.org/wiki/JPEG), etc.
* If you want to store text, you must first *encode* it using [ASCII](https://en.wikipedia.org/wiki/ASCII), [UTF-8](https://en.wikipedia.org/wiki/UTF-8), etc.

MP3, WAV, PNG, JPEG, ASCII and UTF-8 are examples of *encodings*. An encoding is a format to represent audio, images, text, etc. in bytes.

In Python, a byte string is just that: a sequence of bytes. It isn't human-readable. Under the hood, everything must be converted to a byte string before it can be stored in a computer.

On the other hand, a character string, often just called a ""string"", is a sequence of characters. It is human-readable. A character string can't be directly stored in a computer, it has to be *encoded* first (converted into a byte string). There are multiple encodings through which a character string can be converted into a byte string, such as ASCII and UTF-8.

```
'I am a string'.encode('ASCII')

```

The above Python code will encode the string *'I am a string'* using the encoding ASCII. The result of the above code will be a byte string. If you print it, Python will represent it as `b'I am a string'`. Remember, however, that byte strings *aren't human-readable*, it's just that Python decodes them from ASCII when you print them. In Python, a byte string is represented by a `b`, followed by the byte string's ASCII representation.

A byte string can be *decoded* back into a character string, if you know the encoding that was used to encode it.

```
b'I am a string'.decode('ASCII')

```

The above code will return the original string `'I am a string'`.

Encoding and decoding are inverse operations. Everything must be encoded before it can be written to disk, and it must be decoded before it can be read by a human."
"Today when I tried to run simple code on Sublime Text 3, the following message appeared:

> Python was not found but can be installed from the Microsoft Store: <https://go.microsoft.com/fwlink?linkID=2082640>

And when I type Python in CMD, it opens the Windows Store for me to download Python 3.7. This problem started today for no good reason. I didn't change or download anything about Python and already tried reinstalling Python, and the Path environment variable is correct.","Use the Windows search bar to find ""Manage app execution aliases"". There should be two aliases for Python. Unselect them, and this will allow the usual Python aliases ""python"" and ""python3"". See the image below.

[![Enter image description here](https://i.sstatic.net/eDPFY.png)](https://i.sstatic.net/eDPFY.png)

I think we have this problem when installing Python because in a new Windows installation the aliases are in the ON position as in image below. When turned on, Windows puts an empty or fake file named *python.exe* and *python3.exe* in the directory named %USERPROFILE%\AppData\Local\Microsoft\WindowsApps. This is the alias.

[![Enter image description here](https://i.sstatic.net/UCwkU.png)](https://i.sstatic.net/UCwkU.png)

Then Microsoft put that directory at the top of the list in the ""Path"" environment variables.

[![Enter image description here](https://i.sstatic.net/iTeHL.png)](https://i.sstatic.net/iTeHL.png)

When you enter ""python"" in cmd, it searches the directories listed in your ""Path"" environment variables page from top to bottom. So if you installed Python after a new Windows 10 install then get redirected to the Windows Store, it's because there are two python.exe's: The alias in the App Execution Alias page, and the real one wherever you installed Python. But cmd finds the App execution, alias python.exe, first because that directory is at the top of the Path.

I think the easiest solution is to just check the *python.exe* and *python3.exe* to OFF as I suggested before, which deletes the fake EXE file files.

The first time I ran into this problem, I manually deleted the *python.exe* and *python3.exe* files but when I restarted the files regenerated. That prompted me to search for the App Execution Aliases page and uncheck the box, which solved it for me, by not allowing the files to regenerate.

Based on [this Microsoft Devblog](https://devblogs.microsoft.com/python/python-in-the-windows-10-may-2019-update/), they stated they created this system partially for new Python users, specifically kids learning Python in school that had trouble installing it, and focus on learning to code. I think Windows probably deletes those aliases if you install Python from the Windows App Store. We are noticing that they do not get deleted if you manually install from another source.

(Also, the empty/fake python.exe is not really empty. It says 0 KB in the screenshot, but entering ""start ms-windows-store:"" in cmd opens the Windows App Store, so it probably just has a line with that and a way to direct it to the Python page.)

One alternative, as Chipjust suggested, you can create a new alias for Python using something like DOSKEY as explained in this article for example:
*[How to set aliases for the command prompt in Windows](https://winaero.com/how-to-set-aliases-for-the-command-prompt-in-windows/)*

Another alternative is to delete the user path environment variable that points to the alias files, %USERPROFILE%\AppData\Local\Microsoft\WindowsApps, but the App Execution Aliases handle more apps than just python, and deleting the path from environment variables breaks all the other apps that have execution aliases in that directory; which on my PC includes notepad, xbox game bar, spotify, monitoring software for my motherboard, paint, windows subsystem for android, to name a few. Also if you think about it, the average Windows user is unfamiliar editing environment variables and on school and business owned computers requires administrative access. So deleting the path to ...\WindowsApps, from the path environment variable, is not ideal."
"I have some code like:

```
good = [x for x in mylist if x in goodvals]
bad = [x for x in mylist if x not in goodvals]

```

The goal is to split up the contents of `mylist` into two other lists, based on whether or not they meet a condition.

How can I do this more elegantly? Can I avoid doing two separate iterations over `mylist`? Can I improve performance by doing so?","Iterate manually, using the condition to select a list to which each element will be appended:

```
good, bad = [], []
for x in mylist:
    (bad, good)[x in goodvals].append(x)

```"
"Is it possible to declare a variable in Python, like so?:

```
var

```

so that it initialized to None? It seems like Python allows this, but as soon as you access it, it crashes. Is this possible? If not, why?

EDIT: I want to do this for cases like this:

```
value

for index in sequence:

   if value == None and conditionMet:
       value = index
       break

```

### Related Questions

* [Why can a function modify some arguments as perceived by the caller, but not others?](https://stackoverflow.com/questions/575196/)
* [Python Variable Declaration](https://stackoverflow.com/questions/11007627/)

### See Also

* [Python Names and Values](https://nedbatchelder.com/text/names1.html)
* [Other languages have ""variables""](http://python.net/%7Egoodger/projects/pycon/2007/idiomatic/handout.html#other-languages-have-variables)","Why not just do this:

```
var = None

```

Python is dynamic, so you don't need to declare things; they exist automatically in the first scope where they're assigned. So, all you need is a regular old assignment statement as above.

This is nice, because you'll never end up with an uninitialized variable. But be careful -- this doesn't mean that you won't end up with *incorrectly* initialized variables. If you init something to `None`, make sure that's what you really want, and assign something more meaningful if you can."
"Could you explain to me what the difference is between calling

```
python -m mymod1 mymod2.py args

```

and

```
python mymod1.py mymod2.py args

```

It seems in both cases `mymod1.py` is called and `sys.argv` is

```
['mymod1.py', 'mymod2.py', 'args']

```

So what is the `-m` switch for?","Despite this question having been asked and answered several times (e.g., [here](https://stackoverflow.com/q/52441280/1066291), [here](https://stackoverflow.com/q/22241420/1066291), [here](https://stackoverflow.com/q/50821312/1066291), and [here](https://stackoverflow.com/q/46319694/1066291)), in my opinion no existing answer fully or concisely captures all the implications of the `-m` flag. Therefore, the following will attempt to improve on what has come before.

---

Introduction (TLDR)
===================

The `-m` flag does a lot of things, not all of which will be needed all the time. In short, it can be used to: (1) execute Python code from the command line via *modulename* rather than filename (2) add a directory to `sys.path` for use in `import` resolution and (3) execute Python code that contains relative imports from the command line.

Preliminaries
=============

To explain the `-m` flag we first need to explain a little terminology.

Python's primary organizational unit is known as a [module](https://docs.python.org/3/glossary.html#term-module). Modules come in one of two flavors: code modules and package modules. A code module is any file that contains Python executable code. A package module is any directory that contains other modules (either code modules or package modules). The most common type of code module is a `*.py` file while the most common type of package module is a directory containing an `__init__.py` file.

Python allows modules to be uniquely identified in two ways: modulename and filename. In general, modules are identified by modulename in Python code (e.g., `import <modulename>`) and by filename on the command line (e.g., `python <filename>`). All Python interpreters are able to convert modulenames to filenames by following the same few, well-defined rules. These rules hinge on the `sys.path` variable. By altering this variable one can change how Python resolves modulenames into filenames (for more on how this is done see [PEP 302](http://python.org/dev/peps/pep-0302/)).

All modules (both code and package) can be executed (i.e., code associated with the module will be evaluated by the Python interpreter). Depending on the execution method (and module type) what code gets evaluated, and when, can change quite a bit. For example, if one executes a package module via `python <filename>` then `<filename>/__main__.py` will be executed. On the other hand, if one executes that same package module via `import <modulename>` then only the package's `__init__.py` will be executed.

Historical Development of `-m`
==============================

The `-m` flag was first introduced in [Python 2.4.1](https://www.python.org/download/releases/2.4.1/notes/). Initially its only purpose was to provide an alternative means of identifying the Python module to execute from the command line. That is, if we knew both the `<filename>` and `<modulename>` for a module then the following two commands were equivalent: `python <filename> <args>` and `python -m <modulename> <args>`. One constraint with this iteration, according to [PEP 338](https://www.python.org/dev/peps/pep-0338/#current-behaviour), was that `-m` only worked with top level modulenames (i.e., modules that could be found directly on `sys.path` without any intervening package modules).

With the completion of [PEP 338](https://www.python.org/dev/peps/pep-0338/) the `-m` feature was extended to support `<modulename>` representations beyond the top level. This meant names such as `http.server` were now fully supported. This extension also meant that each parent package in modulename was now evaluated (i.e., all parent package `__init__.py` files were evaluated) in addition to the module referenced by the modulename itself.

The final major feature enhancement for `-m` came with [PEP 366](https://www.python.org/dev/peps/pep-0366/). With this upgrade `-m` gained the ability to support not only absolute imports but also explicit relative imports when executing modules. This was achieved by changing `-m` so that it set the `__package__` variable to the parent module of the given modulename (in addition to everything else it already did).

Use Cases
=========

There are two notable use cases for the `-m` flag:

1. To execute *modules* from the command line for which one may not know their filename. This use case takes advantage of the fact that the Python interpreter knows how to convert modulenames to filenames. This is particularly advantageous when one wants to run [*stdlib*](https://docs.python.org/3/library/index.html) modules or 3rd-party module from the command line. For example, very few people know the filename for the `http.server` module but most people do know its modulename so we can execute it from the command line using `python -m http.server`.
2. To execute a local *package* containing absolute or relative imports without needing to install it. This use case is detailed in [PEP 338](https://www.python.org/dev/peps/pep-0338/#import-statements-and-the-main-module) and leverages the fact that the current working directory is added to `sys.path` rather than the module's directory. This use case is very similar to using `pip install -e .` to install a package in develop/edit mode.

Shortcomings
============

With all the enhancements made to `-m` over the years it still has one major shortcoming -- it can only execute modules written in Python (i.e., `*.py`). For example, if `-m` is used to execute a C compiled code module the following error will be produced, `No code object available for <modulename>` (see [here](https://stackoverflow.com/q/6165824/1066291) for more details).

Detailed Comparisons
====================

**Module execution via import statement (i.e., `import <modulename>`):**

* `sys.path` is *not* modified in any way
* `__name__` is set to the absolute form of `<modulename>`
* `__package__` is set to the immediate parent package in `<modulename>`
* `__init__.py` is evaluated for all packages (including its own for package modules)
* `__main__.py` is *not* evaluated for package modules; the code is evaluated for code modules

**Module execution via command line with filename (i.e., `python <filename>`):**

* `sys.path` is modified to include the final directory in `<filename>`
* `__name__` is set to `'__main__'`
* `__package__` is set to `None`
* `__init__.py` is not evaluated for any package (including its own for package modules)
* `__main__.py` is evaluated for package modules; the code is evaluated for code modules.

**Module execution via command line with modulename (i.e., `python -m <modulename>`):**

* `sys.path` is modified to include the current directory
* `__name__` is set to `'__main__'`
* `__package__` is set to the immediate parent package in `<modulename>`
* `__init__.py` is evaluated for all packages (including its own for package modules)
* `__main__.py` is evaluated for package modules; the code is evaluated for code modules

---

Conclusion
----------

The `-m` flag is, at its simplest, a means to execute python scripts from the command line by using modulenames rather than filenames. The real power of `-m`, however, is in its ability to combine the power of `import` statements (e.g., support for explicit relative imports and automatic package `__init__` evaluation) with the convenience of the command line."
"A colleague is looking to generate UML class diagrams from heaps of Python source code.
He's primarily interested in the inheritance relationships, and mildly interested in compositional relationships, and doesn't care much about class attributes that are just Python primitives.

The source code is pretty straightforward and not tremendously evil--it doesn't do any fancy metaclass magic, for example. (It's mostly from the days of Python 1.5.2, with some sprinklings of ""modern"" 2.3ish stuff.)

What's the best existing solution to recommend?","You may have heard of [Pylint](http://www.pylint.org/) that helps statically checking Python code. Few people know that it comes with a tool named [Pyreverse](https://pylint.pycqa.org/en/latest/pyreverse.html) that draws UML diagrams from the Python code it reads. Pyreverse uses Graphviz as a backend.

It is used like this:

```
pyreverse -o png -p yourpackage .

```

where the `.` can also be a single file."
"I have a list of lists:

```
[[12, 'tall', 'blue', 1],
[2, 'short', 'red', 9],
[4, 'tall', 'blue', 13]]

```

If I wanted to sort by one element, say the tall/short element, I could do it via `s = sorted(s, key = itemgetter(1))`.

If I wanted to sort by *both* tall/short and colour, I could do the sort twice, once for each element, but is there a quicker way?","A key can be a function that returns a tuple:

```
s = sorted(s, key = lambda x: (x[1], x[2]))

```

Or you can achieve the same using `itemgetter` (which is faster and avoids a Python function call):

```
import operator
s = sorted(s, key = operator.itemgetter(1, 2))

```

And notice that here you can use `sort` instead of using `sorted` and then reassigning:

```
s.sort(key = operator.itemgetter(1, 2))

```"
"I'm trying to convert a longish hollow ""data"" class into a named tuple. My class currently looks like this:

```
class Node(object):
    def __init__(self, val, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right

```

After conversion to `namedtuple` it looks like:

```
from collections import namedtuple
Node = namedtuple('Node', 'val left right')

```

But there is a problem here. My original class allowed me to pass in just a value and took care of the default by using default values for the named/keyword arguments. Something like:

```
class BinaryTree(object):
    def __init__(self, val):
        self.root = Node(val)

```

But this doesn't work in the case of my refactored named tuple since it expects me to pass all the fields. I can of course replace the occurrences of `Node(val)` to `Node(val, None, None)` but it isn't to my liking.

So does there exist a good trick which can make my re-write successful without adding a lot of code complexity (metaprogramming) or should I just swallow the pill and go ahead with the ""search and replace""? :)","Python 3.7
----------

Use the *defaults* parameter.

```
>>> from collections import namedtuple
>>> fields = ('val', 'left', 'right')
>>> Node = namedtuple('Node', fields, defaults=(None,) * len(fields))
>>> Node()
Node(val=None, left=None, right=None)

```

Or better yet, use the new [dataclasses](https://docs.python.org/3/library/dataclasses.html) library, which is much nicer than namedtuple.

```
>>> from dataclasses import dataclass
>>> from typing import Any
>>> @dataclass
... class Node:
...     val: Any = None
...     left: 'Node' = None
...     right: 'Node' = None
>>> Node()
Node(val=None, left=None, right=None)

```

Before Python 3.7
-----------------

Set `Node.__new__.__defaults__` to the default values.

```
>>> from collections import namedtuple
>>> Node = namedtuple('Node', 'val left right')
>>> Node.__new__.__defaults__ = (None,) * len(Node._fields)
>>> Node()
Node(val=None, left=None, right=None)

```

Before Python 2.6
-----------------

Set `Node.__new__.func_defaults` to the default values.

```
>>> from collections import namedtuple
>>> Node = namedtuple('Node', 'val left right')
>>> Node.__new__.func_defaults = (None,) * len(Node._fields)
>>> Node()
Node(val=None, left=None, right=None)

```

Order
-----

In all versions of Python, if you set fewer default values than exist in the namedtuple, the defaults are applied to the rightmost parameters. This allows you to keep some arguments as required arguments.

```
>>> Node.__new__.__defaults__ = (1,2)
>>> Node()
Traceback (most recent call last):
  ...
TypeError: __new__() missing 1 required positional argument: 'val'
>>> Node(3)
Node(val=3, left=1, right=2)

```

Wrapper for Python 2.6 to 3.6
-----------------------------

Here's a wrapper for you, which even lets you (optionally) set the default values to something other than `None`. This does not support required arguments.

```
import collections
def namedtuple_with_defaults(typename, field_names, default_values=()):
    T = collections.namedtuple(typename, field_names)
    T.__new__.__defaults__ = (None,) * len(T._fields)
    if isinstance(default_values, collections.Mapping):
        prototype = T(**default_values)
    else:
        prototype = T(*default_values)
    T.__new__.__defaults__ = tuple(prototype)
    return T

```

Example:

```
>>> Node = namedtuple_with_defaults('Node', 'val left right')
>>> Node()
Node(val=None, left=None, right=None)
>>> Node = namedtuple_with_defaults('Node', 'val left right', [1, 2, 3])
>>> Node()
Node(val=1, left=2, right=3)
>>> Node = namedtuple_with_defaults('Node', 'val left right', {'right':7})
>>> Node()
Node(val=None, left=None, right=7)
>>> Node(4)
Node(val=4, left=None, right=7)

```"
"How do I get user's IP in Django?

I have a view like this:

```
# Create your views
from django.contrib.gis.utils import GeoIP
from django.template import  RequestContext
from django.shortcuts import render_to_response

def home(request):
  g = GeoIP()
  client_ip = request.META['REMOTE_ADDR']
  lat,long = g.lat_lon(client_ip)
  return render_to_response('home_page_tmp.html',locals())

```

But I get this error:

```
KeyError at /mypage/
    'REMOTE_ADDR'
    Request Method: GET
    Request URL:    http://mywebsite.example/mypage/
    Django Version: 1.2.4
    Exception Type: KeyError
    Exception Value:
    'REMOTE_ADDR'
    Exception Location: /mysite/homepage/views.py in home, line 9
    Python Executable:  /usr/bin/python
    Python Version: 2.6.6
    Python Path:    ['/mysite', '/usr/local/lib/python2.6/dist-packages/flup-1.0.2-py2.6.egg', '/usr/lib/python2.6', '/usr/lib/python2.6/plat-linux2', '/usr/lib/python2.6/lib-tk', '/usr/lib/python2.6/lib-old', '/usr/lib/python2.6/lib-dynload', '/usr/local/lib/python2.6/dist-packages', '/usr/lib/python2.6/dist-packages', '/usr/lib/pymodules/python2.6']
    Server time:    Sun, 2 Jan 2011 20:42:50 -0600

```","```
def get_client_ip(request):
    x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
    if x_forwarded_for:
        ip = x_forwarded_for.split(',')[0]
    else:
        ip = request.META.get('REMOTE_ADDR')
    return ip

```

Make sure you have reverse proxy (if any) configured correctly (e.g. `mod_rpaf` installed for Apache).

Note: the above uses the **first** item in `X-Forwarded-For`, but you might want to use the **last** item (e.g., in the case of Heroku: [Get client's real IP address on Heroku](https://stackoverflow.com/questions/18264304/get-clients-real-ip-address-on-heroku#answer-18517550))

And then just pass the request as argument to it;

```
get_client_ip(request)

```

[Django documentation for HttpRequest.META](https://docs.djangoproject.com/en/4.0/ref/request-response/#django.http.HttpRequest.META)"
"I'm trying to split my huge class into two; well, basically into the ""main"" class and a mixin with additional functions, like so:

`main.py` file:

```
import mymixin.py

class Main(object, MyMixin):
    def func1(self, xxx):
        ...

```

`mymixin.py` file:

```
class MyMixin(object):
    def func2(self: Main, xxx):  # <--- note the type hint
        ...

```

Now, while this works just fine, the type hint in `MyMixin.func2` of course can't work. I can't import `main.py`, because I'd get a cyclic import and without the hint, my editor (PyCharm) can't tell what `self` is.

I'm using Python 3.4, but I'm willing to move to 3.5 if a solution is available there.

Is there any way I can split my class into two files and keep all the ""connections"" so that my IDE still offers me auto-completion and all the other goodies that come from it knowing the types?","There isn't a hugely elegant way to handle import cycles in general, I'm afraid. Your choices are to either redesign your code to remove the cyclic dependency, or if it isn't feasible, do something like this:

```
# some_file.py

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from main import Main

class MyObject(object):
    def func2(self, some_param: 'Main'):
        ...

```

The `TYPE_CHECKING` constant is always `False` at runtime, so the import won't be evaluated, but mypy (and other type-checking tools) will evaluate the contents of that block.

We also need to make the `Main` type annotation into a string, effectively forward declaring it since the `Main` symbol isn't available at runtime.

If you are using Python 3.7+, we can at least skip having to provide an explicit string annotation by taking advantage of [PEP 563](https://www.python.org/dev/peps/pep-0563/):

```
# some_file.py

from __future__ import annotations
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from main import Main

class MyObject(object):
    # Hooray, cleaner annotations!
    def func2(self, some_param: Main):
        ...

```

The `from __future__ import annotations` import will make *all* type hints be strings and skip evaluating them. This can help make our code here mildly more ergonomic.

All that said, using mixins with mypy will likely require a bit more structure then you currently have. Mypy [recommends an approach](https://github.com/python/mypy/issues/1996) that's basically what `deceze` is describing -- to create an ABC that both your `Main` and `MyMixin` classes inherit. I wouldn't be surprised if you ended up needing to do something similar in order to make Pycharm's checker happy."
"I installed the latest version of Python `(3.6.4 64-bit)` and the latest version of `PyCharm (2017.3.3 64-bit)`. Then I installed some modules in PyCharm (Numpy, Pandas, etc), but when I tried installing Tensorflow it didn't install, and I got the error message:

> Could not find a version that satisfies the requirement TensorFlow (from versions: )
> No matching distribution found for TensorFlow.

Then I tried installing TensorFlow from the command prompt and I got the same error message.
I did however successfully install tflearn.

I also installed Python 2.7, but I got the same error message again. I googled the error and tried some of the things which were suggested to other people, but nothing worked (this included installing Flask).

How can I install Tensorflow? Thanks.","The latest requirements for running TensorFlow are documented in the [installation documentation](https://www.tensorflow.org/install/pip).

* TensorFlow only supports 64-bit Python
* TensorFlow only supports certain versions of Python (for example, Python 3.6 is not supported)

So, if you're using an out-of-range version of Python (older or newer) or a 32-bit version, then you'll need to use a different version."
"I have data saved in a `postgreSQL` database. I am querying this data using Python2.7 and turning it into a Pandas DataFrame. However, the last column of this dataframe has a dictionary of values inside it. The DataFrame `df` looks like this:

```
Station ID     Pollutants
8809           {""a"": ""46"", ""b"": ""3"", ""c"": ""12""}
8810           {""a"": ""36"", ""b"": ""5"", ""c"": ""8""}
8811           {""b"": ""2"", ""c"": ""7""}
8812           {""c"": ""11""}
8813           {""a"": ""82"", ""c"": ""15""}

```

I need to split this column into separate columns, so that the DataFrame `df2 looks like this:

```
Station ID     a      b       c
8809           46     3       12
8810           36     5       8
8811           NaN    2       7
8812           NaN    NaN     11
8813           82     NaN     15

```

The major issue I'm having is that the lists are not the same lengths. But **all of the lists only contain up to the same 3 values: 'a', 'b', and 'c'**. And they always appear in the same order ('a' first, 'b' second, 'c' third).

The following code USED to work and return exactly what I wanted (df2).

```
objs = [df, pandas.DataFrame(df['Pollutant Levels'].tolist()).iloc[:, :3]]
df2 = pandas.concat(objs, axis=1).drop('Pollutant Levels', axis=1)
print(df2)

```

I was running this code just last week and it was working fine. But now my code is broken and I get this error from line [4]:

```
IndexError: out-of-bounds on slice (end) 

```

I made no changes to the code but am now getting the error. I feel this is due to my method not being robust or proper.

Any suggestions or guidance on how to split this column of lists into separate columns would be super appreciated!

EDIT: I think the `.tolist()` and .apply methods are not working on my code because it is one Unicode string, i.e.:

```
#My data format 
u{'a': '1', 'b': '2', 'c': '3'}

#and not
{u'a': '1', u'b': '2', u'c': '3'}

```

The data is imported from the `postgreSQL` database in this format. Any help or ideas with this issue? is there a way to convert the Unicode?","I know the question is quite old, but I got here searching for answers. There is actually a better (and faster) way now of doing this using [`json_normalize`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html):

```
import pandas as pd

df2 = pd.json_normalize(df['Pollutant Levels'])

```

This avoids costly apply functions..."
"I have this Python application that gets stuck from time to time and I can't find out where.

Is there any way to signal Python interpreter to show you the exact code that's running?

Some kind of on-the-fly stacktrace?

***Related questions:***

* [Print current call stack from a method in Python code](https://stackoverflow.com/questions/1156023/print-current-call-stack-from-a-method-in-python-code)
* [Check what a running process is doing: print stack trace of an uninstrumented Python program](https://stackoverflow.com/questions/6849138/check-what-a-running-process-is-doing-print-stack-trace-of-an-uninstrumented-py)","I have module I use for situations like this - where a process will be running for a long time but gets stuck sometimes for unknown and irreproducible reasons. Its a bit hacky, and only works on unix (requires signals):

```
import code, traceback, signal

def debug(sig, frame):
    """"""Interrupt running process, and provide a python prompt for
    interactive debugging.""""""
    d={'_frame':frame}         # Allow access to frame object.
    d.update(frame.f_globals)  # Unless shadowed by global
    d.update(frame.f_locals)

    i = code.InteractiveConsole(d)
    message  = ""Signal received : entering python shell.\nTraceback:\n""
    message += ''.join(traceback.format_stack(frame))
    i.interact(message)

def listen():
    signal.signal(signal.SIGUSR1, debug)  # Register handler

```

To use, just call the listen() function at some point when your program starts up (You could even stick it in site.py to have all python programs use it), and let it run. At any point, send the process a SIGUSR1 signal, using kill, or in python:

```
    os.kill(pid, signal.SIGUSR1)

```

This will cause the program to break to a python console at the point it is currently at, showing you the stack trace, and letting you manipulate the variables. Use control-d (EOF) to continue running (though note that you will probably interrupt any I/O etc at the point you signal, so it isn't fully non-intrusive.

I've another script that does the same thing, except it communicates with the running process through a pipe (to allow for debugging backgrounded processes etc). Its a bit large to post here, but I've added it as a [python cookbook recipe](http://code.activestate.com/recipes/576515/)."
"If you read an entire file with `content = open('Path/to/file', 'r').read()` is the file handle left open until the script exits? Is there a more concise method to read a whole file?","The answer to that question depends somewhat on the particular Python implementation.

To understand what this is all about, pay particular attention to the actual `file` object. In your code, that object is mentioned only once, in an expression, and becomes inaccessible immediately after the `read()` call returns.

This means that the file object is garbage. The only remaining question is ""When will the garbage collector collect the file object?"".

in CPython, which uses a reference counter, this kind of garbage is noticed immediately, and so it will be collected immediately. This is not generally true of other python implementations.

A better solution, to make sure that the file is closed, is this pattern:

```
with open('Path/to/file', 'r') as content_file:
    content = content_file.read()

```

which will always close the file immediately after the block ends; even if an exception occurs.

Edit: To put a finer point on it:

Other than `file.__exit__()`, which is ""automatically"" called in a `with` context manager setting, the only other way that `file.close()` is automatically called (that is, other than explicitly calling it yourself,) is via `file.__del__()`. This leads us to the question of when does `__del__()` get called?

> A correctly-written program cannot assume that finalizers will ever run at any point prior to program termination.

-- <https://devblogs.microsoft.com/oldnewthing/20100809-00/?p=13203>

In particular:

> Objects are never explicitly destroyed; however, when they become unreachable they may be garbage-collected. **An implementation is allowed to postpone garbage collection or omit it altogether** â€” it is a matter of implementation quality how garbage collection is implemented, as long as no objects are collected that are still reachable.
>
> [...]
>
> CPython currently uses a reference-counting scheme with (optional) delayed detection of cyclically linked garbage, which collects most objects as soon as they become unreachable, but is not guaranteed to collect garbage containing circular references.

-- <https://docs.python.org/3.5/reference/datamodel.html#objects-values-and-types>

(Emphasis mine)

but as it suggests, other implementations may have other behavior. As an example, PyPy [has *6* different garbage collection implementations](https://pypy.readthedocs.org/en/release-2.4.x/garbage_collection.html)!"
"I need to join a list of items. Many of the items in the list are integer values returned from a function; i.e.,

```
myList.append(munfunc()) 

```

How should I convert the returned result to a string in order to join it with the list?

Do I need to do the following for every integer value:

```
myList.append(str(myfunc()))

```

Is there a more Pythonic way to solve casting problems?","Calling `str(...)` is the Pythonic way to convert something to a string.

You might want to consider why you want a list of strings. You could instead keep it as a list of integers and only convert the integers to strings when you need to display them. For example, if you have a list of integers then you can convert them one by one in a for-loop and join them with `,`:

```
print(','.join(str(x) for x in list_of_ints))

```"
"I am trying to `pip install` the `MySQL-python` package, but I get an `ImportError`.

```
Jans-MacBook-Pro:~ jan$ /Library/Frameworks/Python.framework/Versions/3.3/bin/pip-3.3 install MySQL-python
Downloading/unpacking MySQL-python
  Running setup.py egg_info for package MySQL-python
    Traceback (most recent call last):
      File ""<string>"", line 16, in <module>
      File ""/var/folders/lf/myf7bjr57_jg7_5c4014bh640000gn/T/pip-build/MySQL-python/setup.py"", line 14, in <module>
        from setup_posix import get_config
      File ""./setup_posix.py"", line 2, in <module>
        from ConfigParser import SafeConfigParser
    ImportError: No module named 'ConfigParser'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""<string>"", line 16, in <module>

  File ""/var/folders/lf/myf7bjr57_jg7_5c4014bh640000gn/T/pip-build/MySQL-python/setup.py"", line 14, in <module>

    from setup_posix import get_config

  File ""./setup_posix.py"", line 2, in <module>

    from ConfigParser import SafeConfigParser

ImportError: No module named 'ConfigParser'

----------------------------------------
Command python setup.py egg_info failed with error code 1 in /var/folders/lf/myf7bjr57_jg7_5c4014bh640000gn/T/pip-build/MySQL-python
Storing complete log in /Users/jan/.pip/pip.log
Jans-MacBook-Pro:~ jan$ 

```

Any ideas?","In Python 3, [`ConfigParser`](https://docs.python.org/2/library/configparser.html) has been renamed to [`configparser`](https://docs.python.org/3/library/configparser.html) for PEP 8 compliance. It looks like the package you are installing does not support Python 3."
"I'm trying to execute a file with Python commands from within the interpreter.

I'm trying to use variables and settings from that file, not to invoke a separate process.","Several ways.

* From the shell

  ```
  python someFile.py

  ```
* From inside IDLE, hit **F5**.
* If you're typing interactively, try this (**Python3**):

  ```
  >>> exec(open(""filename.py"").read())

  ```
* For Python 2:

  ```
  >>> variables= {}
  >>> execfile( ""someFile.py"", variables )
  >>> print variables # globals from the someFile module

  ```"
"Is there a way to rename a dictionary key, without reassigning its value to a new name and removing the old name key; and without iterating through dict key/value?

In case of [`OrderedDict`](https://docs.python.org/3/library/collections.html#collections.OrderedDict) do the same, while keeping that key's position.","For a regular dict, you can use:

```
mydict[k_new] = mydict.pop(k_old)

```

This will move the item to the end of the dict, unless `k_new` was already existing in which case it will overwrite the value in-place.

For a Python 3.7+ dict where you additionally want to preserve the ordering, the simplest is to rebuild an entirely new instance. For example, renaming key `2` to `'two'`:

```
>>> d = {0:0, 1:1, 2:2, 3:3}
>>> {""two"" if k == 2 else k:v for k,v in d.items()}
{0: 0, 1: 1, 'two': 2, 3: 3}

```

The same is true for an `OrderedDict`, where you can't use dict comprehension syntax, but you can use a generator expression:

```
OrderedDict((k_new if k == k_old else k, v) for k, v in od.items())

```

Modifying the key *itself*, as the question asks for, is impractical because keys are [hashable](https://docs.python.org/3/glossary.html#term-hashable) which usually implies they're [immutable](https://docs.python.org/3/glossary.html#term-immutable) and can't be modified."
What is the difference between `raw_input()` and `input()` in Python 3?,"The difference is that `raw_input()` does not exist in Python 3.x, while `input()` does. Actually, the old `raw_input()` has been renamed to `input()`, and the old `input()` is gone, but can easily be simulated by using `eval(input())`. (Remember that `eval()` is evil. Try to use safer ways of parsing your input if possible.)"
"This is the best algorithm I could come up.

```
def get_primes(n):
    numbers = set(range(n, 1, -1))
    primes = []
    while numbers:
        p = numbers.pop()
        primes.append(p)
        numbers.difference_update(set(range(p*2, n+1, p)))
    return primes

>>> timeit.Timer(stmt='get_primes.get_primes(1000000)', setup='import   get_primes').timeit(1)
1.1499958793645562

```

Can it be made even faster?

This code has a flaw: Since `numbers` is an unordered set, there is no guarantee that `numbers.pop()` will remove the lowest number from the set. Nevertheless, it works (at least for me) for some input numbers:

```
>>> sum(get_primes(2000000))
142913828922L
#That's the correct sum of all numbers below 2 million
>>> 529 in get_primes(1000)
False
>>> 529 in get_primes(530)
True

```","**Warning:** `timeit` results may vary due to differences in hardware or
version of Python.

Below is a script which compares a number of implementations:

* ambi\_sieve\_plain,
* [rwh\_primes](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188),
* [rwh\_primes1](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188),
* [rwh\_primes2](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188),
* [sieveOfAtkin](https://web.archive.org/web/20080324064651/http://krenzel.info/?p=83),
* [sieveOfEratosthenes](http://groups.google.com/group/comp.lang.python/msg/f1f10ced88c68c2d),
* [sundaram3](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/2073279#2073279),
* [sieve\_wheel\_30](http://zerovolt.com/?p=88),
* [ambi\_sieve](http://tommih.blogspot.com/2009/04/fast-prime-number-generator.html) (requires numpy)
* [primesfrom3to](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188) (requires numpy)
* [primesfrom2to](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188) (requires numpy)

Many thanks to [stephan](https://stackoverflow.com/users/92092/stephan) for bringing sieve\_wheel\_30 to my attention.
Credit goes to [Robert William Hanks](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188) for primesfrom2to, primesfrom3to, rwh\_primes, rwh\_primes1, and rwh\_primes2.

Of the plain Python methods tested, **with psyco**, for n=1000000,
**rwh\_primes1** was the fastest tested.

```
+---------------------+-------+
| Method              | ms    |
+---------------------+-------+
| rwh_primes1         | 43.0  |
| sieveOfAtkin        | 46.4  |
| rwh_primes          | 57.4  |
| sieve_wheel_30      | 63.0  |
| rwh_primes2         | 67.8  |    
| sieveOfEratosthenes | 147.0 |
| ambi_sieve_plain    | 152.0 |
| sundaram3           | 194.0 |
+---------------------+-------+

```

Of the plain Python methods tested, **without psyco**, for n=1000000,
**rwh\_primes2** was the fastest.

```
+---------------------+-------+
| Method              | ms    |
+---------------------+-------+
| rwh_primes2         | 68.1  |
| rwh_primes1         | 93.7  |
| rwh_primes          | 94.6  |
| sieve_wheel_30      | 97.4  |
| sieveOfEratosthenes | 178.0 |
| ambi_sieve_plain    | 286.0 |
| sieveOfAtkin        | 314.0 |
| sundaram3           | 416.0 |
+---------------------+-------+

```

Of all the methods tested, *allowing numpy*, for n=1000000,
**primesfrom2to** was the fastest tested.

```
+---------------------+-------+
| Method              | ms    |
+---------------------+-------+
| primesfrom2to       | 15.9  |
| primesfrom3to       | 18.4  |
| ambi_sieve          | 29.3  |
+---------------------+-------+

```

Timings were measured using the command:

```
python -mtimeit -s""import primes"" ""primes.{method}(1000000)""

```

with `{method}` replaced by each of the method names.

primes.py:

```
#!/usr/bin/env python
import psyco; psyco.full()
from math import sqrt, ceil
import numpy as np

def rwh_primes(n):
    # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188
    """""" Returns  a list of primes < n """"""
    sieve = [True] * n
    for i in xrange(3,int(n**0.5)+1,2):
        if sieve[i]:
            sieve[i*i::2*i]=[False]*((n-i*i-1)/(2*i)+1)
    return [2] + [i for i in xrange(3,n,2) if sieve[i]]

def rwh_primes1(n):
    # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188
    """""" Returns  a list of primes < n """"""
    sieve = [True] * (n/2)
    for i in xrange(3,int(n**0.5)+1,2):
        if sieve[i/2]:
            sieve[i*i/2::i] = [False] * ((n-i*i-1)/(2*i)+1)
    return [2] + [2*i+1 for i in xrange(1,n/2) if sieve[i]]

def rwh_primes2(n):
    # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188
    """""" Input n>=6, Returns a list of primes, 2 <= p < n """"""
    correction = (n%6>1)
    n = {0:n,1:n-1,2:n+4,3:n+3,4:n+2,5:n+1}[n%6]
    sieve = [True] * (n/3)
    sieve[0] = False
    for i in xrange(int(n**0.5)/3+1):
      if sieve[i]:
        k=3*i+1|1
        sieve[      ((k*k)/3)      ::2*k]=[False]*((n/6-(k*k)/6-1)/k+1)
        sieve[(k*k+4*k-2*k*(i&1))/3::2*k]=[False]*((n/6-(k*k+4*k-2*k*(i&1))/6-1)/k+1)
    return [2,3] + [3*i+1|1 for i in xrange(1,n/3-correction) if sieve[i]]

def sieve_wheel_30(N):
    # http://zerovolt.com/?p=88
    ''' Returns a list of primes <= N using wheel criterion 2*3*5 = 30

Copyright 2009 by zerovolt.com
This code is free for non-commercial purposes, in which case you can just leave this comment as a credit for my work.
If you need this code for commercial purposes, please contact me by sending an email to: info [at] zerovolt [dot] com.'''
    __smallp = ( 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59,
    61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139,
    149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227,
    229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311,
    313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401,
    409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491,
    499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599,
    601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683,
    691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797,
    809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887,
    907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997)

    wheel = (2, 3, 5)
    const = 30
    if N < 2:
        return []
    if N <= const:
        pos = 0
        while __smallp[pos] <= N:
            pos += 1
        return list(__smallp[:pos])
    # make the offsets list
    offsets = (7, 11, 13, 17, 19, 23, 29, 1)
    # prepare the list
    p = [2, 3, 5]
    dim = 2 + N // const
    tk1  = [True] * dim
    tk7  = [True] * dim
    tk11 = [True] * dim
    tk13 = [True] * dim
    tk17 = [True] * dim
    tk19 = [True] * dim
    tk23 = [True] * dim
    tk29 = [True] * dim
    tk1[0] = False
    # help dictionary d
    # d[a , b] = c  ==> if I want to find the smallest useful multiple of (30*pos)+a
    # on tkc, then I need the index given by the product of [(30*pos)+a][(30*pos)+b]
    # in general. If b < a, I need [(30*pos)+a][(30*(pos+1))+b]
    d = {}
    for x in offsets:
        for y in offsets:
            res = (x*y) % const
            if res in offsets:
                d[(x, res)] = y
    # another help dictionary: gives tkx calling tmptk[x]
    tmptk = {1:tk1, 7:tk7, 11:tk11, 13:tk13, 17:tk17, 19:tk19, 23:tk23, 29:tk29}
    pos, prime, lastadded, stop = 0, 0, 0, int(ceil(sqrt(N)))
    # inner functions definition
    def del_mult(tk, start, step):
        for k in xrange(start, len(tk), step):
            tk[k] = False
    # end of inner functions definition
    cpos = const * pos
    while prime < stop:
        # 30k + 7
        if tk7[pos]:
            prime = cpos + 7
            p.append(prime)
            lastadded = 7
            for off in offsets:
                tmp = d[(7, off)]
                start = (pos + prime) if off == 7 else (prime * (const * (pos + 1 if tmp < 7 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # 30k + 11
        if tk11[pos]:
            prime = cpos + 11
            p.append(prime)
            lastadded = 11
            for off in offsets:
                tmp = d[(11, off)]
                start = (pos + prime) if off == 11 else (prime * (const * (pos + 1 if tmp < 11 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # 30k + 13
        if tk13[pos]:
            prime = cpos + 13
            p.append(prime)
            lastadded = 13
            for off in offsets:
                tmp = d[(13, off)]
                start = (pos + prime) if off == 13 else (prime * (const * (pos + 1 if tmp < 13 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # 30k + 17
        if tk17[pos]:
            prime = cpos + 17
            p.append(prime)
            lastadded = 17
            for off in offsets:
                tmp = d[(17, off)]
                start = (pos + prime) if off == 17 else (prime * (const * (pos + 1 if tmp < 17 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # 30k + 19
        if tk19[pos]:
            prime = cpos + 19
            p.append(prime)
            lastadded = 19
            for off in offsets:
                tmp = d[(19, off)]
                start = (pos + prime) if off == 19 else (prime * (const * (pos + 1 if tmp < 19 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # 30k + 23
        if tk23[pos]:
            prime = cpos + 23
            p.append(prime)
            lastadded = 23
            for off in offsets:
                tmp = d[(23, off)]
                start = (pos + prime) if off == 23 else (prime * (const * (pos + 1 if tmp < 23 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # 30k + 29
        if tk29[pos]:
            prime = cpos + 29
            p.append(prime)
            lastadded = 29
            for off in offsets:
                tmp = d[(29, off)]
                start = (pos + prime) if off == 29 else (prime * (const * (pos + 1 if tmp < 29 else 0) + tmp) )//const
                del_mult(tmptk[off], start, prime)
        # now we go back to top tk1, so we need to increase pos by 1
        pos += 1
        cpos = const * pos
        # 30k + 1
        if tk1[pos]:
            prime = cpos + 1
            p.append(prime)
            lastadded = 1
            for off in offsets:
                tmp = d[(1, off)]
                start = (pos + prime) if off == 1 else (prime * (const * pos + tmp) )//const
                del_mult(tmptk[off], start, prime)
    # time to add remaining primes
    # if lastadded == 1, remove last element and start adding them from tk1
    # this way we don't need an ""if"" within the last while
    if lastadded == 1:
        p.pop()
    # now complete for every other possible prime
    while pos < len(tk1):
        cpos = const * pos
        if tk1[pos]: p.append(cpos + 1)
        if tk7[pos]: p.append(cpos + 7)
        if tk11[pos]: p.append(cpos + 11)
        if tk13[pos]: p.append(cpos + 13)
        if tk17[pos]: p.append(cpos + 17)
        if tk19[pos]: p.append(cpos + 19)
        if tk23[pos]: p.append(cpos + 23)
        if tk29[pos]: p.append(cpos + 29)
        pos += 1
    # remove exceeding if present
    pos = len(p) - 1
    while p[pos] > N:
        pos -= 1
    if pos < len(p) - 1:
        del p[pos+1:]
    # return p list
    return p

def sieveOfEratosthenes(n):
    """"""sieveOfEratosthenes(n): return the list of the primes < n.""""""
    # Code from: <dickinsm@gmail.com>, Nov 30 2006
    # http://groups.google.com/group/comp.lang.python/msg/f1f10ced88c68c2d
    if n <= 2:
        return []
    sieve = range(3, n, 2)
    top = len(sieve)
    for si in sieve:
        if si:
            bottom = (si*si - 3) // 2
            if bottom >= top:
                break
            sieve[bottom::si] = [0] * -((bottom - top) // si)
    return [2] + [el for el in sieve if el]

def sieveOfAtkin(end):
    """"""sieveOfAtkin(end): return a list of all the prime numbers <end
    using the Sieve of Atkin.""""""
    # Code by Steve Krenzel, <Sgk284@gmail.com>, improved
    # Code: https://web.archive.org/web/20080324064651/http://krenzel.info/?p=83
    # Info: http://en.wikipedia.org/wiki/Sieve_of_Atkin
    assert end > 0
    lng = ((end-1) // 2)
    sieve = [False] * (lng + 1)

    x_max, x2, xd = int(sqrt((end-1)/4.0)), 0, 4
    for xd in xrange(4, 8*x_max + 2, 8):
        x2 += xd
        y_max = int(sqrt(end-x2))
        n, n_diff = x2 + y_max*y_max, (y_max << 1) - 1
        if not (n & 1):
            n -= n_diff
            n_diff -= 2
        for d in xrange((n_diff - 1) << 1, -1, -8):
            m = n % 12
            if m == 1 or m == 5:
                m = n >> 1
                sieve[m] = not sieve[m]
            n -= d

    x_max, x2, xd = int(sqrt((end-1) / 3.0)), 0, 3
    for xd in xrange(3, 6 * x_max + 2, 6):
        x2 += xd
        y_max = int(sqrt(end-x2))
        n, n_diff = x2 + y_max*y_max, (y_max << 1) - 1
        if not(n & 1):
            n -= n_diff
            n_diff -= 2
        for d in xrange((n_diff - 1) << 1, -1, -8):
            if n % 12 == 7:
                m = n >> 1
                sieve[m] = not sieve[m]
            n -= d

    x_max, y_min, x2, xd = int((2 + sqrt(4-8*(1-end)))/4), -1, 0, 3
    for x in xrange(1, x_max + 1):
        x2 += xd
        xd += 6
        if x2 >= end: y_min = (((int(ceil(sqrt(x2 - end))) - 1) << 1) - 2) << 1
        n, n_diff = ((x*x + x) << 1) - 1, (((x-1) << 1) - 2) << 1
        for d in xrange(n_diff, y_min, -8):
            if n % 12 == 11:
                m = n >> 1
                sieve[m] = not sieve[m]
            n += d

    primes = [2, 3]
    if end <= 3:
        return primes[:max(0,end-2)]

    for n in xrange(5 >> 1, (int(sqrt(end))+1) >> 1):
        if sieve[n]:
            primes.append((n << 1) + 1)
            aux = (n << 1) + 1
            aux *= aux
            for k in xrange(aux, end, 2 * aux):
                sieve[k >> 1] = False

    s  = int(sqrt(end)) + 1
    if s  % 2 == 0:
        s += 1
    primes.extend([i for i in xrange(s, end, 2) if sieve[i >> 1]])

    return primes

def ambi_sieve_plain(n):
    s = range(3, n, 2)
    for m in xrange(3, int(n**0.5)+1, 2): 
        if s[(m-3)/2]: 
            for t in xrange((m*m-3)/2,(n>>1)-1,m):
                s[t]=0
    return [2]+[t for t in s if t>0]

def sundaram3(max_n):
    # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/2073279#2073279
    numbers = range(3, max_n+1, 2)
    half = (max_n)//2
    initial = 4

    for step in xrange(3, max_n+1, 2):
        for i in xrange(initial, half, step):
            numbers[i-1] = 0
        initial += 2*(step+1)

        if initial > half:
            return [2] + filter(None, numbers)

################################################################################
# Using Numpy:
def ambi_sieve(n):
    # http://tommih.blogspot.com/2009/04/fast-prime-number-generator.html
    s = np.arange(3, n, 2)
    for m in xrange(3, int(n ** 0.5)+1, 2): 
        if s[(m-3)/2]: 
            s[(m*m-3)/2::m]=0
    return np.r_[2, s[s>0]]

def primesfrom3to(n):
    # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188
    """""" Returns a array of primes, p < n """"""
    assert n>=2
    sieve = np.ones(n/2, dtype=np.bool)
    for i in xrange(3,int(n**0.5)+1,2):
        if sieve[i/2]:
            sieve[i*i/2::i] = False
    return np.r_[2, 2*np.nonzero(sieve)[0][1::]+1]    

def primesfrom2to(n):
    # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188
    """""" Input n>=6, Returns a array of primes, 2 <= p < n """"""
    sieve = np.ones(n/3 + (n%6==2), dtype=np.bool)
    sieve[0] = False
    for i in xrange(int(n**0.5)/3+1):
        if sieve[i]:
            k=3*i+1|1
            sieve[      ((k*k)/3)      ::2*k] = False
            sieve[(k*k+4*k-2*k*(i&1))/3::2*k] = False
    return np.r_[2,3,((3*np.nonzero(sieve)[0]+1)|1)]

if __name__=='__main__':
    import itertools
    import sys

    def test(f1,f2,num):
        print('Testing {f1} and {f2} return same results'.format(
            f1=f1.func_name,
            f2=f2.func_name))
        if not all([a==b for a,b in itertools.izip_longest(f1(num),f2(num))]):
            sys.exit(""Error: %s(%s) != %s(%s)""%(f1.func_name,num,f2.func_name,num))

    n=1000000
    test(sieveOfAtkin,sieveOfEratosthenes,n)
    test(sieveOfAtkin,ambi_sieve,n)
    test(sieveOfAtkin,ambi_sieve_plain,n) 
    test(sieveOfAtkin,sundaram3,n)
    test(sieveOfAtkin,sieve_wheel_30,n)
    test(sieveOfAtkin,primesfrom3to,n)
    test(sieveOfAtkin,primesfrom2to,n)
    test(sieveOfAtkin,rwh_primes,n)
    test(sieveOfAtkin,rwh_primes1,n)         
    test(sieveOfAtkin,rwh_primes2,n)

```

Running the script tests that all implementations give the same result."
"Using pandas, how do I read in only a subset of the columns (say 4th and 7th columns) of a .csv file with no headers? I cannot seem to be able to do so using `usecols`.","In order to read a csv in that doesn't have a header and for only certain columns you need to pass params `header=None` and `usecols=[3,6]` for the 4th and 7th columns:

```
df = pd.read_csv(file_path, header=None, usecols=[3,6])

```

See the [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"
"I'm beginning python and I'm trying to use a two-dimensional list, that I initially fill up with the same variable in every place. I came up with this:

```
def initialize_twodlist(foo):
    twod_list = []
    new = []
    for i in range (0, 10):
        for j in range (0, 10):
            new.append(foo)
        twod_list.append(new)
        new = []

```

It gives the desired result, but feels like a workaround. Is there an easier/shorter/more elegant way to do this?","To initialize a two-dimensional list in Python, use

```
t = [ [0]*3 for i in range(3)]

```

But don't use `[[v]*n]*n`, it is a trap!

```
>>> a = [[0]*3]*3
>>> a
[[0, 0, 0], [0, 0, 0], [0, 0, 0]]
>>> a[0][0]=1
>>> a
[[1, 0, 0], [1, 0, 0], [1, 0, 0]]

```"
"I have a data frame with one (string) column and I'd like to split it into two (string) columns, with one column header as '`fips'` and the other `'row'`

My dataframe `df` looks like this:

```
          row
0    00000 UNITED STATES
1    01000 ALABAMA
2    01001 Autauga County, AL
3    01003 Baldwin County, AL
4    01005 Barbour County, AL

```

I do not know how to use `df.row.str[:]` to achieve my goal of splitting the row cell. I can use `df['fips'] = hello` to add a new column and populate it with `hello`. Any ideas?

```
         fips       row
0    00000 UNITED STATES
1    01000 ALABAMA 
2    01001 Autauga County, AL
3    01003 Baldwin County, AL
4    01005 Barbour County, AL

```","TL;DR version:
==============

For the simple case of:

* I have a text column with a delimiter and I want two columns

The simplest solution is:

```
df[['A', 'B']] = df['AB'].str.split(' ', n=1, expand=True)

```

You must use `expand=True` if your strings have a non-uniform number of splits and you want `None` to replace the missing values.

Notice how, in either case, the `.tolist()` method is not necessary. Neither is `zip()`.

In detail:
==========

[Andy Hayden's solution](https://stackoverflow.com/a/21296915/1273938) is most excellent in demonstrating the power of the [`str.extract()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html) method.

But for a simple split over a known separator (like, splitting by dashes, or splitting by whitespace), the [`.str.split()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html) method is enough1. It operates on a column (Series) of strings, and returns a column (Series) of lists:

```
>>> import pandas as pd
>>> df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2']})
>>> df

      AB
0  A1-B1
1  A2-B2
>>> df['AB_split'] = df['AB'].str.split('-')
>>> df

      AB  AB_split
0  A1-B1  [A1, B1]
1  A2-B2  [A2, B2]

```

1: If you're unsure what the first two parameters of `.str.split()` do,
I recommend the docs for the [plain Python version of the method](https://docs.python.org/3.6/library/stdtypes.html#str.split).

But how do you go from:

* a column containing two-element lists

to:

* two columns, each containing the respective element of the lists?

Well, we need to take a closer look at the `.str` attribute of a column.

It's a magical object that is used to collect methods that treat each element in a column as a string, and then apply the respective method in each element as efficient as possible:

```
>>> upper_lower_df = pd.DataFrame({""U"": [""A"", ""B"", ""C""]})
>>> upper_lower_df

   U
0  A
1  B
2  C
>>> upper_lower_df[""L""] = upper_lower_df[""U""].str.lower()
>>> upper_lower_df

   U  L
0  A  a
1  B  b
2  C  c

```

But it also has an ""indexing"" interface for getting each element of a string by its index:

```
>>> df['AB'].str[0]

0    A
1    A
Name: AB, dtype: object

>>> df['AB'].str[1]

0    1
1    2
Name: AB, dtype: object

```

Of course, this indexing interface of `.str` doesn't really care if each element it's indexing is actually a string, as long as it can be indexed, so:

```
>>> df['AB'].str.split('-', 1).str[0]

0    A1
1    A2
Name: AB, dtype: object

>>> df['AB'].str.split('-', 1).str[1]

0    B1
1    B2
Name: AB, dtype: object

```

Then, it's a simple matter of taking advantage of the Python tuple unpacking of iterables to do

```
>>> df['A'], df['B'] = df['AB'].str.split('-', n=1).str
>>> df

      AB  AB_split   A   B
0  A1-B1  [A1, B1]  A1  B1
1  A2-B2  [A2, B2]  A2  B2

```

Of course, getting a DataFrame out of splitting a column of strings is so useful that the `.str.split()` method can do it for you with the `expand=True` parameter:

```
>>> df['AB'].str.split('-', n=1, expand=True)

    0   1
0  A1  B1
1  A2  B2

```

So, another way of accomplishing what we wanted is to do:

```
>>> df = df[['AB']]
>>> df

      AB
0  A1-B1
1  A2-B2

>>> df.join(df['AB'].str.split('-', n=1, expand=True).rename(columns={0:'A', 1:'B'}))

      AB   A   B
0  A1-B1  A1  B1
1  A2-B2  A2  B2

```

The `expand=True` version, although longer, has a distinct advantage over the tuple unpacking method. Tuple unpacking doesn't deal well with splits of different lengths:

```
>>> df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2', 'A3-B3-C3']})
>>> df
         AB
0     A1-B1
1     A2-B2
2  A3-B3-C3
>>> df['A'], df['B'], df['C'] = df['AB'].str.split('-')
Traceback (most recent call last):
  [...]    
ValueError: Length of values does not match length of index
>>> 

```

But `expand=True` handles it nicely by placing `None` in the columns for which there aren't enough ""splits"":

```
>>> df.join(
...     df['AB'].str.split('-', expand=True).rename(
...         columns={0:'A', 1:'B', 2:'C'}
...     )
... )
         AB   A   B     C
0     A1-B1  A1  B1  None
1     A2-B2  A2  B2  None
2  A3-B3-C3  A3  B3    C3

```"
"Here's the Python code to run an arbitrary command returning its `stdout` data, or raise an exception on non-zero exit codes:

```
proc = subprocess.Popen(
    cmd,
    stderr=subprocess.STDOUT,  # Merge stdout and stderr
    stdout=subprocess.PIPE,
    shell=True)

```

`communicate` is used to wait for the process to exit:

```
stdoutdata, stderrdata = proc.communicate()

```

The `subprocess` module does not support timeout--ability to kill a process running for more than X number of seconds--therefore, `communicate` may take forever to run.

What is the **simplest** way to implement timeouts in a Python program meant to run on Windows and Linux?","In Python 3.3+:

```
from subprocess import STDOUT, check_output

output = check_output(cmd, stderr=STDOUT, timeout=seconds)

```

`output` is a byte string that contains command's merged stdout, stderr data.

[`check_output`](https://docs.python.org/3/library/subprocess.html#subprocess.check_output) raises `CalledProcessError` on non-zero exit status as specified in the question's text unlike `proc.communicate()` method.

I've removed `shell=True` because it is often used unnecessarily. You can always add it back if `cmd` indeed requires it. If you add `shell=True` i.e., if the child process spawns its own descendants; `check_output()` can return much later than the timeout indicates, see [Subprocess timeout failure](https://stackoverflow.com/q/36952245/4279).

The timeout feature is available on Python 2.x via the [`subprocess32`](http://pypi.python.org/pypi/subprocess32/) backport of the 3.2+ subprocess module."
What is the difference between [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) and [`array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) in NumPy? Where is their implementation in the NumPy source code?,"`numpy.array` is just a convenience function to create an `ndarray`; it is not a class itself.

You can also create an array using `numpy.ndarray`, but it is not the recommended way. From the docstring of `numpy.ndarray`:

> Arrays should be constructed using `array`, `zeros` or `empty` ... The parameters given here refer to a
> low-level method (`ndarray(...)`) for instantiating an array.

Most of the meat of the implementation is in C code, [here in multiarray](https://github.com/numpy/numpy/tree/master/numpy/core/src/multiarray), but you can start looking at the ndarray interfaces here:

<https://github.com/numpy/numpy/blob/master/numpy/core/numeric.py>"
"So which is better and why?

```
def my_function():

```

or

```
def myFunction():

```","for everything related to Python's style guide: i'd recommend you read [PEP8](http://www.python.org/dev/peps/pep-0008/).

To answer your question:

> Function names should be lowercase, with words separated by
> underscores as necessary to improve readability."
"I know that I can use something like `string[3:4]` to get a substring in Python, but what does the 3 mean in `somesequence[::3]`?","it means 'nothing for the first argument, nothing for the second, and jump by three'. It gets every third item of the sequence sliced.
[Extended slices](http://docs.python.org/release/2.3.5/whatsnew/section-slices.html) is what you want. New in Python 2.3"
"I have some kind of test data and want to create a unit test for each item. My first idea was to do it like this:

```
import unittest

l = [[""foo"", ""a"", ""a"",], [""bar"", ""a"", ""b""], [""lee"", ""b"", ""b""]]

class TestSequence(unittest.TestCase):
    def testsample(self):
        for name, a,b in l:
            print ""test"", name
            self.assertEqual(a,b)

if __name__ == '__main__':
    unittest.main()

```

The downside of this is that it handles all data in one test. I would like to generate one test for each item on the fly. Any suggestions?","This is called ""parametrization"".

There are several tools that support this approach. E.g.:

* [pytest's decorator](https://docs.pytest.org/en/latest/parametrize.html)
* [parameterized](https://github.com/wolever/parameterized)

The resulting code looks like this:

```
from parameterized import parameterized

class TestSequence(unittest.TestCase):
    @parameterized.expand([
        [""foo"", ""a"", ""a"",],
        [""bar"", ""a"", ""b""],
        [""lee"", ""b"", ""b""],
    ])
    def test_sequence(self, name, a, b):
        self.assertEqual(a,b)

```

Which will generate the tests:

```
test_sequence_0_foo (__main__.TestSequence) ... ok
test_sequence_1_bar (__main__.TestSequence) ... FAIL
test_sequence_2_lee (__main__.TestSequence) ... ok

======================================================================
FAIL: test_sequence_1_bar (__main__.TestSequence)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/parameterized/parameterized.py"", line 233, in <lambda>
    standalone_func = lambda *a: func(*(a + p.args), **p.kwargs)
  File ""x.py"", line 12, in test_sequence
    self.assertEqual(a,b)
AssertionError: 'a' != 'b'

```

For historical reasons I'll leave the original answer circa 2008):

I use something like this:

```
import unittest

l = [[""foo"", ""a"", ""a"",], [""bar"", ""a"", ""b""], [""lee"", ""b"", ""b""]]

class TestSequense(unittest.TestCase):
    pass

def test_generator(a, b):
    def test(self):
        self.assertEqual(a,b)
    return test

if __name__ == '__main__':
    for t in l:
        test_name = 'test_%s' % t[0]
        test = test_generator(t[1], t[2])
        setattr(TestSequense, test_name, test)
    unittest.main()

```"
"I have this tail recursive function here:

```
def recursive_function(n, sum):
    if n < 1:
        return sum
    else:
        return recursive_function(n-1, sum+n)

c = 998
print(recursive_function(c, 0))

```

It works up to `n=997`, then it just breaks and spits out a `RecursionError: maximum recursion depth exceeded in comparison`. Is this just a stack overflow? Is there a way to get around it?","It is a guard against a stack overflow, yes. Python (or rather, the CPython implementation) doesn't optimize tail recursion, and unbridled recursion causes stack overflows. You can check the recursion limit with [`sys.getrecursionlimit`](https://docs.python.org/library/sys.html#sys.getrecursionlimit):

```
import sys
print(sys.getrecursionlimit())

```

and change the recursion limit with [`sys.setrecursionlimit`](https://docs.python.org/library/sys.html#sys.setrecursionlimit):

```
sys.setrecursionlimit(1500)

```

but doing so is dangerous -- the standard limit is a little conservative, but Python stackframes can be quite big.

Python isn't a functional language and tail recursion is not a particularly efficient technique. Rewriting the algorithm iteratively, if possible, is generally a better idea."
"I've tried reading through questions about sibling imports and even the
[package documentation](http://docs.python.org/tutorial/modules.html#intra-package-references), but I've yet to find an answer.

With the following structure:

```
├── LICENSE.md
├── README.md
├── api
│   ├── __init__.py
│   ├── api.py
│   └── api_key.py
├── examples
│   ├── __init__.py
│   ├── example_one.py
│   └── example_two.py
└── tests
│   ├── __init__.py
│   └── test_one.py

```

How can the scripts in the `examples` and `tests` directories import from the
`api` module and be run from the commandline?

Also, I'd like to avoid the ugly `sys.path.insert` hack for every file. Surely
this can be done in Python, right?","Tired of sys.path hacks?
========================

There are plenty of `sys.path.append` -hacks available, but I found an alternative way of solving the problem in hand.

### Summary

* Wrap the code into one folder (e.g. `packaged_stuff`)
* Create `pyproject.toml` file to describe your package (see minimal `pyproject.toml` below)
* Pip install the package in editable state with `pip install -e <myproject_folder>`
* Import using `from packaged_stuff.modulename import function_name`

---

Setup
=====

The starting point is the file structure you have provided, wrapped in a folder called `myproject`.

```
.
└── myproject
    ├── api
    │   ├── api_key.py
    │   ├── api.py
    │   └── __init__.py
    ├── examples
    │   ├── example_one.py
    │   ├── example_two.py
    │   └── __init__.py
    ├── LICENCE.md
    ├── README.md
    └── tests
        ├── __init__.py
        └── test_one.py

```

I will call the `.` the root folder, and in my example case it is located at `C:\tmp\test_imports\`.

api.py
------

As a test case, let's use the following ./api/api.py

```
def function_from_api():
    return 'I am the return value from api.api!'

```

test\_one.py
------------

```
from api.api import function_from_api

def test_function():
    print(function_from_api())

if __name__ == '__main__':
    test_function()

```

Try to run test\_one:
---------------------

```
PS C:\tmp\test_imports> python .\myproject\tests\test_one.py
Traceback (most recent call last):
  File "".\myproject\tests\test_one.py"", line 1, in <module>
    from api.api import function_from_api
ModuleNotFoundError: No module named 'api'

```

Also trying relative imports wont work:
---------------------------------------

Using `from ..api.api import function_from_api` would result into

```
PS C:\tmp\test_imports> python .\myproject\tests\test_one.py
Traceback (most recent call last):
  File "".\tests\test_one.py"", line 1, in <module>
    from ..api.api import function_from_api
ValueError: attempted relative import beyond top-level package

```

---

Steps
=====

##### 1) Make a pyproject.toml file to the root level directory

(previously people used a setup.py file)

The contents for a minimal `pyproject.toml` would be\*

```
[project]
name = ""myproject""
version = ""0.1.0""
description = ""My small project""

[build-system]
build-backend = ""flit_core.buildapi""
requires = [""flit_core >=3.2,<4""]

```

---

##### 2) Use a virtual environment

*If you are familiar with virtual environments, activate one, and skip to the next step.* Usage of virtual environments are not *absolutely* required, but they will *really* help you out in the long run (when you have more than 1 project ongoing..). The most basic steps are (run in the root folder)

* Create virtual env
  + `python -m venv venv`
* Activate virtual env
  + `source ./venv/bin/activate` (Linux, macOS) or `./venv/Scripts/activate` (Win)

To learn more about this, just Google out ""python virtual env tutorial"" or similar. You probably never need any other commands than creating, activating and deactivating.

Once you have made and activated a virtual environment, your console should give the name of the virtual environment in parenthesis

```
PS C:\tmp\test_imports> python -m venv venv
PS C:\tmp\test_imports> .\venv\Scripts\activate
(venv) PS C:\tmp\test_imports>

```

and your folder tree should look like this\*\*

```
.
├── myproject
│   ├── api
│   │   ├── api_key.py
│   │   ├── api.py
│   │   └── __init__.py
│   ├── examples
│   │   ├── example_one.py
│   │   ├── example_two.py
│   │   └── __init__.py
│   ├── LICENCE.md
│   ├── README.md
│   └── tests
│       ├── __init__.py
│       └── test_one.py
├── pyproject.toml
└── venv
    ├── Include
    ├── Lib
    ├── pyvenv.cfg
    └── Scripts [87 entries exceeds filelimit, not opening dir]

```

---

##### 3) pip install your project in editable state

Install your top level package `myproject` using `pip`. The trick is to use the `-e` flag when doing the install. This way it is installed in an editable state, and all the edits made to the .py files will be automatically included in the installed package. *Using pyproject.toml and -e flag requires pip >= 21.3*

In the root directory, run

`pip install -e .` (note the dot, it stands for ""current directory"")

You can also see that it is installed by using `pip freeze`

```
Obtaining file:///home/user/projects/myproject
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Building wheels for collected packages: myproj
  Building editable for myproj (pyproject.toml) ... done
  Created wheel for myproj: filename=myproj-0.1.0-py2.py3-none-any.whl size=903 sha256=f19858b080d4e770c2a172b9a73afcad5f33f4c43c86e8eb9bdacbe50a627064
  Stored in directory: /tmp/pip-ephem-wheel-cache-qohzx1u0/wheels/55/5f/e4/507fdeb40cdef333e3e0a8c50c740a430b8ce84cbe17ae5875
Successfully built myproject
Installing collected packages: myproject
Successfully installed myproject-0.1.0
(venv) PS C:\tmp\test_imports> pip freeze
myproject==0.1.0

```

---

##### 4) Add `myproject.` into your imports

Note that you will have to add `myproject.` only into imports that would not work otherwise. Imports that worked without the `pyproject.toml` & `pip install` will work still work fine. See an example below.

---

Test the solution
=================

Now, let's test the solution using `api.py` defined above, and `test_one.py` defined below.

test\_one.py
------------

```
from myproject.api.api import function_from_api

def test_function():
    print(function_from_api())

if __name__ == '__main__':
    test_function()

```

running the test
----------------

```
(venv) PS C:\tmp\test_imports> python .\myproject\tests\test_one.py
I am the return value from api.api!

```

---

\* here using flit as build backend. Other alternatives exist.

\*\* In reality, you could put your virtual environment anywhere on your hard disk."
"I'm trying to convert one range of numbers to another, maintaining ratio. Maths is not my strong point.

I have an image file where point values may range from -16000.00 to 16000.00 though the typical range may be much less. What I want to do is compress these values into the integer range 0-100, where 0 is the value of the smallest point, and 100 is the value of the largest. All points in between should keep a relative ratio even though some precision is being lost I'd like to do this in python but even a general algorithm should suffice. I'd prefer an algorithm where the min/max or either range can be adjusted (ie, the second range could be -50 to 800 instead of 0 to 100).","```
NewValue = (((OldValue - OldMin) * (NewMax - NewMin)) / (OldMax - OldMin)) + NewMin

```

Or a little more readable:

```
OldRange = (OldMax - OldMin)  
NewRange = (NewMax - NewMin)  
NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin

```

Or if you want to protect for the case where the old range is 0 (*OldMin = OldMax*):

```
OldRange = (OldMax - OldMin)
if (OldRange == 0)
    NewValue = NewMin
else
{
    NewRange = (NewMax - NewMin)  
    NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin
}

```

Note that in this case we're forced to pick one of the possible new range values arbitrarily. Depending on context, sensible choices could be: `NewMin` (*see sample*), `NewMax` or `(NewMin + NewMax) / 2`"
I need to find which version of TensorFlow I have installed. I'm using Ubuntu 16.04 Long Term Support.,"This depends on how you installed TensorFlow. I am going to use the same headings used by [TensorFlow's installation instructions](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#download-and-setup) to structure this answer.

---

Pip installation
----------------

Run:

```
python -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 2
python3 -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 3

```

Note that `python` is symlinked to `/usr/bin/python3` in some Linux distributions, so use `python` instead of `python3` in these cases.

`pip list | grep tensorflow` for Python 2 or `pip3 list | grep tensorflow` for Python 3 will also show the version of Tensorflow installed.

---

Virtualenv installation
-----------------------

Run:

```
python -c 'import tensorflow as tf; print(tf.__version__)'  # for both Python 2 and Python 3

```

`pip list | grep tensorflow` will also show the version of Tensorflow installed.

For example, I have installed TensorFlow 0.9.0 in a `virtualenv` for Python 3. So, I get:

```
$ python -c 'import tensorflow as tf; print(tf.__version__)'
0.9.0

$ pip list | grep tensorflow
tensorflow (0.9.0)

```"
"I have a Python list variable that contains strings. Is there a function that can convert all the strings in one pass to lowercase and vice versa, uppercase?","It can be done with [list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)

```
>>> [x.lower() for x in [""A"", ""B"", ""C""]]
['a', 'b', 'c']
>>> [x.upper() for x in [""a"", ""b"", ""c""]]
['A', 'B', 'C']

```

or with the [`map` function](https://docs.python.org/3/library/functions.html#map)

```
>>> list(map(lambda x: x.lower(), [""A"", ""B"", ""C""]))
['a', 'b', 'c']
>>> list(map(lambda x: x.upper(), [""a"", ""b"", ""c""]))
['A', 'B', 'C']

```"
"How do I save a trained model in PyTorch? I have read that:

1. [`torch.save()`](https://github.com/torch/torch7/blob/master/doc/serialization.md#torchsavefilename-object--format-referenced)/[`torch.load()`](https://github.com/torch/torch7/blob/master/doc/serialization.md#object-torchloadfilename--format-referenced) is for saving/loading a serializable object.
2. [`model.state_dict()`](http://pytorch.org/docs/nn.html#torch.nn.Module.state_dict)/[`model.load_state_dict()`](http://pytorch.org/docs/nn.html#torch.nn.Module.load_state_dict) is for saving/loading model state.","Found [this page](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) on their github repo:

> #### Recommended approach for saving a model
>
> There are two main approaches for serializing and restoring a model.
>
> The first (recommended) saves and loads only the model parameters:
>
> ```
> torch.save(the_model.state_dict(), PATH)
>
> ```
>
> Then later:
>
> ```
> the_model = TheModelClass(*args, **kwargs)
> the_model.load_state_dict(torch.load(PATH))
>
> ```
>
> ---
>
> The second saves and loads the entire model:
>
> ```
> torch.save(the_model, PATH)
>
> ```
>
> Then later:
>
> ```
> the_model = torch.load(PATH)
>
> ```
>
> However in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.

---

See also: [Save and Load the Model](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#save-and-load-the-model) section from the official PyTorch tutorials."
"I'm trying to test a complicated JavaScript interface with Selenium (using the Python interface, and across multiple browsers). I have a number of buttons of the form:

```
<div>My Button</div>

```

I'd like to be able to search for buttons based on ""My Button"" (or non-case-sensitive, partial matches such as ""my button"" or ""button"").

I'm finding this amazingly difficult, to the extent to which I feel like I'm missing something obvious. The best thing I have so far is:

```
driver.find_elements_by_xpath('//div[contains(text(), ""' + text + '"")]')

```

This is case-sensitive, however. The other thing I've tried is iterating through all the divs on the page, and checking the element.text property. However, every time you get a situation of the form:

```
<div class=""outer""><div class=""inner"">My Button</div></div>

```

div.outer also has ""My Button"" as the text. To fix *that*, I've tried looking to see if div.outer is the parent of div.inner, but I couldn't figure out how to do that (element.get\_element\_by\_xpath('..') returns an element's parent, but it tests not equal to div.outer).

Also, iterating through all the elements on the page seems to be really slow, at least using the Chrome webdriver.

Ideas?

---

I asked (and answered) a more specific version here: *[How can I get text of an element in Selenium WebDriver, without including child element text?](https://stackoverflow.com/questions/12325454/how-to-get-text-of-an-element-in-selenium-webdriver-via-the-python-api-without/12325461#12325461)*","Try the following:

```
driver.find_elements_by_xpath(""//*[contains(text(), 'My Button')]"")

```"
"In the [Flickr API docs](http://www.flickr.com/services/api/auth.howto.web.html), you need to find the MD5 sum of a string to generate the `[api_sig]` value.

How does one go about generating an MD5 sum from a string?

Flickr's example:

string: `000005fab4534d05api_key9a0554259914a86fb9e7eb014e4e5d52permswrite`

MD5 sum: `a02506b31c1cd46c2e0b6380fb94eb3d`","You can do the following:

**Python 2.x**

```
import hashlib
print hashlib.md5(""whatever your string is"").hexdigest()

```

---

**Python 3.x**

```
import hashlib
print(hashlib.md5(""whatever your string is"".encode('utf-8')).hexdigest())

```

---

However in this case you're probably better off using this helpful Python module for interacting with the Flickr API:

* <http://stuvel.eu/flickrapi>

... which will deal with the authentication for you.

Official documentation of [hashlib](https://docs.python.org/2/library/hashlib.html#module-hashlib)"
"I am using this dataframe:

```
Fruit   Date      Name  Number
Apples  10/6/2016 Bob    7
Apples  10/6/2016 Bob    8
Apples  10/6/2016 Mike   9
Apples  10/7/2016 Steve 10
Apples  10/7/2016 Bob    1
Oranges 10/7/2016 Bob    2
Oranges 10/6/2016 Tom   15
Oranges 10/6/2016 Mike  57
Oranges 10/6/2016 Bob   65
Oranges 10/7/2016 Tony   1
Grapes  10/7/2016 Bob    1
Grapes  10/7/2016 Tom   87
Grapes  10/7/2016 Bob   22
Grapes  10/7/2016 Bob   12
Grapes  10/7/2016 Tony  15

```

I would like to aggregate this by `Name` and then by `Fruit` to get a total number of `Fruit` per `Name`. For example:

```
Bob,Apples,16

```

I tried grouping by `Name` and `Fruit` but how do I get the total number of `Fruit`?","Use [`GroupBy.sum`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.sum.html):

```
df.groupby(['Fruit','Name']).sum()

Out[31]: 
               Number
Fruit   Name         
Apples  Bob        16
        Mike        9
        Steve      10
Grapes  Bob        35
        Tom        87
        Tony       15
Oranges Bob        67
        Mike       57
        Tom        15
        Tony        1

```

To specify the column to sum, use this: `df.groupby(['Name', 'Fruit'])['Number'].sum()`"
"Run the following code from a directory that contains a directory named `bar` (containing one or more files) and a directory named `baz` (also containing one or more files). Make sure there is not a directory named `foo`.

```
import shutil
shutil.copytree('bar', 'foo')
shutil.copytree('baz', 'foo')

```

It will fail with:

```
$ python copytree_test.py 
Traceback (most recent call last):
  File ""copytree_test.py"", line 5, in <module>
    shutil.copytree('baz', 'foo')
  File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/shutil.py"", line 110, in copytree
  File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/os.py"", line 172, in makedirs
OSError: [Errno 17] File exists: 'foo'

```

I want this to work the same way as if I had typed:

```
$ mkdir foo
$ cp bar/* foo/
$ cp baz/* foo/

```

Do I need to use `shutil.copy()` to copy each file in `baz` into `foo`? (After I've already copied the contents of 'bar' into 'foo' with `shutil.copytree()`?) Or is there an easier/better way?","Here's a solution that's part of the standard library:

For Python 3.7 and earlier:

```
from distutils.dir_util import copy_tree
copy_tree(""/a/b/c"", ""/x/y/z"")

```

For Python 3.8+, they added the `dirs_exist_ok` parameter:

```
import shutil
shutil.copytree(""/a/b/c"", ""/x/y/z"", dirs_exist_ok=True)

```"
"Is there a way to open a file for both reading and writing?

As a workaround, I open the file for writing, close it, then open it again for reading. But is there a way to open a file for *both* reading and writing?","Here's how you read a file, and then write to it (overwriting any existing data), without closing and reopening:

```
with open(filename, ""r+"") as f:
    data = f.read()
    f.seek(0)
    f.write(output)
    f.truncate()

```"
"How do I access the index while iterating over a sequence with a `for` loop?

```
xs = [8, 23, 45]

for x in xs:
    print(""item #{} = {}"".format(index, x))

```

Desired output:

```
item #1 = 8
item #2 = 23
item #3 = 45

```","Use the built-in function [`enumerate()`](https://docs.python.org/3/library/functions.html#enumerate ""enumerate""):

```
for idx, x in enumerate(xs):
    print(idx, x)

```

It is *[non-Pythonic](https://stackoverflow.com/questions/25011078/what-does-pythonic-mean)* to manually index via `for i in range(len(xs)): x = xs[i]` or manually manage an additional state variable.

Check out [PEP 279](https://www.python.org/dev/peps/pep-0279/ ""PEP 279"") for more."
"I'm using *`virtualenv`* and I need to install ""psycopg2"".

I have done the following:

```
pip install http://pypi.python.org/packages/source/p/psycopg2/psycopg2-2.4.tar.gz#md5=24f4368e2cfdc1a2b03282ddda814160

```

And I have the following messages:

```
Downloading/unpacking http://pypi.python.org/packages/source/p/psycopg2/psycopg2
-2.4.tar.gz#md5=24f4368e2cfdc1a2b03282ddda814160
  Downloading psycopg2-2.4.tar.gz (607Kb): 607Kb downloaded
  Running setup.py egg_info for package from http://pypi.python.org/packages/sou
rce/p/psycopg2/psycopg2-2.4.tar.gz#md5=24f4368e2cfdc1a2b03282ddda814160
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.
    Complete output from command python setup.py egg_info:
    running egg_info

creating pip-egg-info\psycopg2.egg-info

writing pip-egg-info\psycopg2.egg-info\PKG-INFO

writing top-level names to pip-egg-info\psycopg2.egg-info\top_level.txt

writing dependency_links to pip-egg-info\psycopg2.egg-info\dependency_links.txt

writing manifest file 'pip-egg-info\psycopg2.egg-info\SOURCES.txt'

warning: manifest_maker: standard file '-c' not found

Error: pg_config executable not found.



Please add the directory containing pg_config to the PATH

or specify the full executable path with the option:



    python setup.py build_ext --pg-config /path/to/pg_config build ...



or with the pg_config option in 'setup.cfg'.

----------------------------------------
Command python setup.py egg_info failed with error code 1
Storing complete log in C:\Documents and Settings\anlopes\Application Data\pip\p
ip.log

```

My question, I only need to do this to get the psycopg2 working?

```
python setup.py build_ext --pg-config /path/to/pg_config build ...

```","***Note**: Since a while back, there are binary wheels for Windows in PyPI, so this should no longer be an issue for Windows users. Below are solutions for Linux, Mac users, since lots of them find this post through web searches.*

---

Option 1
========

Install the `psycopg2-binary` PyPI package instead, it has Python wheels for Linux and Mac OS.

```
pip install psycopg2-binary

```

---

Option 2
========

Install the prerequsisites for building the `psycopg2` package from source:

Debian/Ubuntu
-------------

| Python version | Command | Note |
| --- | --- | --- |
| Default Python 3 | `sudo apt install libpq-dev python3-dev` |  |
| Python 3.x | `sudo apt install libpq-dev python3.x-dev` | substitute `x` in command |
| Python 2 | `sudo apt install libpq-dev python-dev` |  |

If that's not enough, you might additionally need to install

```
sudo apt install build-essential

```

or

```
sudo apt install postgresql-server-dev-all

```

as well before installing psycopg2 again.

CentOS 6
--------

See [Banjer's answer](https://stackoverflow.com/a/13158616/202522)

macOS
-----

See [nichochar's answer](https://stackoverflow.com/a/19711831/202522)"
"I am trying to loop from 100 to 0. How do I do this in Python?

`for i in range (100,0)` doesn't work.

---

For discussion of **why** `range` works the way it does, see [Why are slice and range upper-bound exclusive?](https://stackoverflow.com/questions/11364533).","Try `range(100,-1,-1)`, the 3rd argument being the increment to use (documented [here](https://docs.python.org/library/functions.html#range)).

(""range"" options, start, stop, step are documented [here](https://docs.python.org/3/library/stdtypes.html?highlight=range#range))"
"How do I drop `nan`, `inf`, and `-inf` values from a `DataFrame` without resetting `mode.use_inf_as_null`?

Can I tell `dropna` to include `inf` in its definition of missing values so that the following works?

```
df.dropna(subset=[""col1"", ""col2""], how=""all"")

```","First [`replace()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html) infs with NaN:

```
df.replace([np.inf, -np.inf], np.nan, inplace=True)

```

and then drop NaNs via [`dropna()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html):

```
df.dropna(subset=[""col1"", ""col2""], how=""all"", inplace=True)

```

---

For example:

```
>>> df = pd.DataFrame({""col1"": [1, np.inf, -np.inf], ""col2"": [2, 3, np.nan]})
>>> df
   col1  col2
0   1.0   2.0
1   inf   3.0
2  -inf   NaN

>>> df.replace([np.inf, -np.inf], np.nan, inplace=True)
>>> df
   col1  col2
0   1.0   2.0
1   NaN   3.0
2   NaN   NaN

>>> df.dropna(subset=[""col1"", ""col2""], how=""all"", inplace=True)
>>> df
   col1  col2
0   1.0   2.0
1   NaN   3.0

```

---

*The same method also works for `Series`.*"
"I am getting an error when running a python program:

```
Traceback (most recent call last):
  File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\_sandbox.py"", line 110, in <module>
  File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\_sandbox.py"", line 27, in __init__
  File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\class\inventory.py"", line 17, in __init__
builtins.NameError: global name 'xrange' is not defined

```

The game is from [here](https://github.com/linkey11/Necromonster).

What causes this error?","You are trying to run a Python 2 codebase with Python 3. [`xrange()`](https://docs.python.org/2/library/functions.html#xrange) was renamed to [`range()`](https://docs.python.org/3/library/functions.html#func-range) in Python 3.

Run the game with Python 2 instead. Don't try to port it unless you know what you are doing, most likely there will be more problems beyond `xrange()` vs. `range()`.

For the record, what you are seeing is not a syntax error but a runtime exception instead.

---

If you do know what your are doing and are actively making a Python 2 codebase compatible with Python 3, you can bridge the code by adding the global name to your module as an alias for `range`. (Take into account that you *may* have to update any existing `range()` use in the Python 2 codebase with `list(range(...))` to ensure you still get a list object in Python 3):

```
try:
    # Python 2
    xrange
except NameError:
    # Python 3, xrange is now named range
    xrange = range

# Python 2 code that uses xrange(...) unchanged, and any
# range(...) replaced with list(range(...))

```

or replace all uses of `xrange(...)` with `range(...)` in the codebase and then use a different shim to make the Python 3 syntax compatible with Python 2:

```
try:
    # Python 2 forward compatibility
    range = xrange
except NameError:
    pass

# Python 2 code transformed from range(...) -> list(range(...)) and
# xrange(...) -> range(...).

```

The latter is preferable for codebases that want to aim to be Python 3 compatible *only* in the long run, it is easier to then just use Python 3 syntax whenever possible."
"Does \* have a special meaning in Python as it does in C? I saw a function like this in the Python Cookbook:

```
def get(self, *a, **kw)

```

Would you please explain it to me or point out where I can find an answer (Google interprets the \* as wild card character and thus I cannot find a satisfactory answer).","See [Function Definitions](https://docs.python.org/2.7/reference/compound_stmts.html#function-definitions) in the Language Reference.

> If the form `*identifier` is
> present, it is initialized to a tuple
> receiving any excess positional
> parameters, defaulting to the empty
> tuple. If the form `**identifier` is
> present, it is initialized to a new
> dictionary receiving any excess
> keyword arguments, defaulting to a new
> empty dictionary.

Also, see [Function Calls](https://docs.python.org/2.7/reference/expressions.html#calls).

Assuming that one knows what positional and keyword arguments are, here are some examples:

Example 1:

```
# Excess keyword argument (python 3) example:
def foo(a, b, c, **args):
    print(""a = %s"" % (a,))
    print(""b = %s"" % (b,))
    print(""c = %s"" % (c,))
    print(args)
    
foo(a=""testa"", d=""excess"", c=""testc"", b=""testb"", k=""another_excess"")

```

As you can see in the above example, we only have parameters `a, b, c` in the signature of the `foo` function. Since `d` and `k` are not present, they are put into the args dictionary. The output of the program is:

```
a = testa
b = testb
c = testc
{'k': 'another_excess', 'd': 'excess'}

```

Example 2:

```
# Excess positional argument (python 3) example:
def foo(a, b, c, *args):
    print(""a = %s"" % (a,))
    print(""b = %s"" % (b,))
    print(""c = %s"" % (c,))
    print(args)
        
foo(""testa"", ""testb"", ""testc"", ""excess"", ""another_excess"")

```

Here, since we're testing positional arguments, the excess ones have to be on the end, and `*args` packs them into a tuple, so the output of this program is:

```
a = testa
b = testb
c = testc
('excess', 'another_excess')

```

You can also unpack a dictionary or a tuple into arguments of a function:

```
def foo(a,b,c,**args):
    print(""a=%s"" % (a,))
    print(""b=%s"" % (b,))
    print(""c=%s"" % (c,))
    print(""args=%s"" % (args,))

argdict = dict(a=""testa"", b=""testb"", c=""testc"", excessarg=""string"")
foo(**argdict)

```

Prints:

```
a=testa
b=testb
c=testc
args={'excessarg': 'string'}

```

And

```
def foo(a,b,c,*args):
    print(""a=%s"" % (a,))
    print(""b=%s"" % (b,))
    print(""c=%s"" % (c,))
    print(""args=%s"" % (args,))

argtuple = (""testa"",""testb"",""testc"",""excess"")
foo(*argtuple)

```

Prints:

```
a=testa
b=testb
c=testc
args=('excess',)

```"
"I'm using the [Requests: HTTP for Humans](http://docs.python-requests.org/en/latest/) library and I got this error:

```
No connection adapters were found for '192.168.1.61:8080/api/call'

```

What does this mean and how can I fix it?","You need to include the protocol scheme:

```
'http://192.168.1.61:8080/api/call'

```

Without the `http://` part, `requests` has no idea how to connect to the remote server.

Note that the protocol scheme must be all lowercase; if your URL starts with `HTTP://` for example, it wonâ€™t find the `http://` connection adapter either."
"I want to create a string using an integer appended to it, in a *for* loop. Like this:

```
for i in range(1, 11):
  string = ""string"" + i

```

But it returns an error:

> TypeError: unsupported operand type(s) for +: 'int' and 'str'

What's the best way to concatenate the string and integer?","NOTE:
-----

The method used in this answer (backticks) is deprecated in later versions of Python 2, and removed in Python 3. Use the [`str()`](https://docs.python.org/3/library/stdtypes.html#str) function instead.

---

You can use:

```
string = 'string'
for i in range(11):
    string +=`i`
print string

```

It will print `string012345678910`.

To get `string0, string1 ..... string10` you can use this as [YOU suggested](https://stackoverflow.com/questions/2847386/python-strings-and-integer-concatenation/2847394#2847394):

```
>>> string = ""string""
>>> [string+`i` for i in range(11)]

```

---

For Python 3
============

You can use:

```
string = 'string'
for i in range(11):
    string += str(i)
print string

```

It will print `string012345678910`.

To get `string0, string1 ..... string10`, you can use this as [YOU suggested](https://stackoverflow.com/questions/2847386/python-strings-and-integer-concatenation/2847394#2847394):

```
>>> string = ""string""
>>> [string+str(i) for i in range(11)]

```"
"I want to use `subprocess.check_output()` with `ps -A | grep 'process_name'`.
I tried various solutions but so far nothing worked. Can someone guide me how to do it?","To use a pipe with the `subprocess` module, you can pass `shell=True` but be aware of the [Security Considerations](https://docs.python.org/3/library/subprocess.html#security-considerations). It is discouraged using `shell=True`. In most cases there are better solutions for the same problem.

However, this isn't really advisable for various reasons, not least of which is security. Instead, create the `ps` and `grep` processes separately, and pipe the output from one into the other, like so:

```
ps = subprocess.Popen(('ps', '-A'), stdout=subprocess.PIPE)
output = subprocess.check_output(('grep', 'process_name'), stdin=ps.stdout)
ps.wait()

```

In your particular case, however, the simple solution is to call `subprocess.check_output(('ps', '-A'))` and then `str.find` on the output."
"How can I get the month name from the month number?

For instance, if I have `3`, I want to return `march`

```
date.tm_month()

```

How to get the string `march`?","```
import datetime
mydate = datetime.datetime.now()
mydate.strftime(""%B"")

```

Returns: *December*

Some more info on the [Python doc website](http://docs.python.org/library/datetime.html#strftime-strptime-behavior)

---

[EDIT : great comment from @GiriB] You can also use `%b` which returns the short notation for month name.

```
mydate.strftime(""%b"")

```

For the example above, it would return `Dec`."
How can I format a [`datetime`](https://docs.python.org/3/library/datetime.html#datetime-objects) object as a string with milliseconds?,"To get a date string with milliseconds, use `[:-3]` to trim the last three digits of `%f` (microseconds):

```
>>> from datetime import datetime
>>> datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
'2022-09-24 10:18:32.926'

```

Or shorter:

```
>>> from datetime import datetime
>>> datetime.utcnow().strftime('%F %T.%f')[:-3]
'2022-09-24 10:18:32.926'

```

See the [Python docs](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes) for more ""`%`"" format codes and the [`strftime(3)` man page](https://man.archlinux.org/man/strftime.3#DESCRIPTION) for the full list."
"I've got a timedelta. I want the days, hours and minutes from that - either as a tuple or a dictionary... I'm not fussed.

I must have done this a dozen times in a dozen languages over the years but Python usually has a simple answer to everything so I thought I'd ask here before busting out some nauseatingly simple (yet verbose) mathematics.

Mr Fooz raises a good point.

I'm dealing with ""listings"" (a bit like ebay listings) where each one has a duration. I'm trying to find the time left by doing `when_added + duration - now`

Am I right in saying that wouldn't account for DST? If not, what's the simplest way to add/subtract an hour?","If you have a `datetime.timedelta` value `td`, `td.days` already gives you the ""days"" you want. `timedelta` values keep fraction-of-day as seconds (not directly hours or minutes) so you'll indeed have to perform ""nauseatingly simple mathematics"", e.g.:

```
def days_hours_minutes(td):
    return td.days, td.seconds//3600, (td.seconds//60)%60

```"
How to set the current working directory in Python?,"Try [`os.chdir`](http://docs.python.org/library/os.html#os.chdir)

> ```
> import os
> os.chdir(path)
>
> ```
>
>         Change the current working directory to path. Availability: Unix, Windows."
"How can a pre-existing conda environment be updated with another .yml file. This is extremely helpful when working on projects that have multiple requirement files, i.e. `base.yml, local.yml, production.yml`, etc.

For example, below is a `base.yml` file has conda-forge, conda, and pip packages:

base.yml

```
name: myenv
channels:
  - conda-forge
dependencies:
  - django=1.10.5
  - pip:
    - django-crispy-forms==1.6.1

```

The actual environment is created with:
`conda env create -f base.yml`.

Later on, additional packages need to be added to `base.yml`. Another file, say `local.yml`, needs to import those updates.

Previous attempts to accomplish this include:

creating a `local.yml` file with an import definition:

```
channels:

dependencies:
  - pip:
    - boto3==1.4.4
imports:
  - requirements/base. 

```

And then run the command:
`conda install -f local.yml`.

This does not work. Any thoughts?","Try using [conda env update](https://docs.conda.io/projects/conda/en/latest/commands/update.html):

```
conda activate myenv
conda env update --file local.yml --prune

```

`--prune` uninstalls dependencies which were removed from `local.yml`, as pointed out in [this answer](https://stackoverflow.com/a/54825300/2261298) by @Blink.

*Attention:* if there is a `name` tag with a name other than that of your environment in `local.yml`, the command above will create a new environment with that name. To avoid this, use (thanks @NumesSanguis):

```
conda env update --name myenv --file local.yml --prune

```

See [Updating an environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html?highlight=prune#updating-an-environment) in Conda User Guide."
"Is there a cross-platform way of getting the path to the *`temp`* directory in Python 2.6?

For example, under Linux that would be `/tmp`, while under XP `C:\Documents and settings\[user]\Application settings\Temp`.","That would be the [tempfile](http://docs.python.org/library/tempfile.html) module.

It has functions to get the temporary directory, and also has some shortcuts to create temporary files and directories in it, either named or unnamed.

Example:

```
import tempfile

print tempfile.gettempdir() # prints the current temporary directory

f = tempfile.TemporaryFile()
f.write('something on temporaryfile')
f.seek(0) # return to beginning of file
print f.read() # reads data back from the file
f.close() # temporary file is automatically deleted here

```

For completeness, here's how it searches for the temporary directory, according to the [documentation](https://docs.python.org/3/library/tempfile.html#tempfile.gettempdir):

> 1. The directory named by the `TMPDIR` environment variable.
> 2. The directory named by the `TEMP` environment variable.
> 3. The directory named by the `TMP` environment variable.
> 4. A platform-specific location:
>    * On *RiscOS*, the directory named by the `Wimp$ScrapDir` environment variable.
>    * On *Windows*, the directories `C:\TEMP`, `C:\TMP`, `\TEMP`, and `\TMP`, in that order.
>    * On all other platforms, the directories `/tmp`, `/var/tmp`, and `/usr/tmp`, in that order.
> 5. As a last resort, the current working directory."
"Which image processing techniques could be used to implement an application that detects the Christmas trees displayed in the following images?

![](https://i.sstatic.net/nmzwj.png)
![](https://i.sstatic.net/aVZhC.png)
![](https://i.sstatic.net/2K9Ef.png)

![](https://i.sstatic.net/YowlH.png)
![](https://i.sstatic.net/2y4o5.png)
![](https://i.sstatic.net/FWhSP.png)

I'm searching for solutions that are going to work on all these images. Therefore, approaches that require training *haar cascade classifiers* or *template matching* are not very interesting.

I'm looking for something that can be written in *any* programming language, *as long as* it uses only *Open Source* technologies. The solution must be tested with the images that are shared on this question. There are *6 input images* and the answer should display the results of processing each of them. Finally, for each *output image* there must be *red lines* draw to surround the detected tree.

How would you go about programmatically detecting the trees in these images?","I have an approach which I think is interesting and a bit different from the rest. The main difference in my approach, compared to some of the others, is in how the image segmentation step is performed--I used the [DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithm from Python's scikit-learn; it's optimized for finding somewhat amorphous shapes that may not necessarily have a single clear centroid.

At the top level, my approach is fairly simple and can be broken down into about 3 steps. First I apply a threshold (or actually, the logical ""or"" of two separate and distinct thresholds). As with many of the other answers, I assumed that the Christmas tree would be one of the brighter objects in the scene, so the first threshold is just a simple monochrome brightness test; any pixels with values above 220 on a 0-255 scale (where black is 0 and white is 255) are saved to a binary black-and-white image. The second threshold tries to look for red and yellow lights, which are particularly prominent in the trees in the upper left and lower right of the six images, and stand out well against the blue-green background which is prevalent in most of the photos. I convert the rgb image to hsv space, and require that the hue is either less than 0.2 on a 0.0-1.0 scale (corresponding roughly to the border between yellow and green) or greater than 0.95 (corresponding to the border between purple and red) and additionally I require bright, saturated colors: saturation and value must both be above 0.7. The results of the two threshold procedures are logically ""or""-ed together, and the resulting matrix of black-and-white binary images is shown below:

![Christmas trees, after thresholding on HSV as well as monochrome brightness](https://i.sstatic.net/iIkWV.png)

You can clearly see that each image has one large cluster of pixels roughly corresponding to the location of each tree, plus a few of the images also have some other small clusters corresponding either to lights in the windows of some of the buildings, or to a background scene on the horizon. The next step is to get the computer to recognize that these are separate clusters, and label each pixel correctly with a cluster membership ID number.

For this task I chose [DBSCAN](http://en.wikipedia.org/wiki/DBSCAN). There is a pretty good visual comparison of how DBSCAN typically behaves, relative to other clustering algorithms, available [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html). As I said earlier, it does well with amorphous shapes. The output of DBSCAN, with each cluster plotted in a different color, is shown here:

![DBSCAN clustering output](https://i.sstatic.net/rWSqn.png)

There are a few things to be aware of when looking at this result. First is that DBSCAN requires the user to set a ""proximity"" parameter in order to regulate its behavior, which effectively controls how separated a pair of points must be in order for the algorithm to declare a new separate cluster rather than agglomerating a test point onto an already pre-existing cluster. I set this value to be 0.04 times the size along the diagonal of each image. Since the images vary in size from roughly VGA up to about HD 1080, this type of scale-relative definition is critical.

Another point worth noting is that the DBSCAN algorithm as it is implemented in scikit-learn has memory limits which are fairly challenging for some of the larger images in this sample. Therefore, for a few of the larger images, I actually had to ""decimate"" (i.e., retain only every 3rd or 4th pixel and drop the others) each cluster in order to stay within this limit. As a result of this culling process, the remaining individual sparse pixels are difficult to see on some of the larger images. Therefore, for display purposes only, the color-coded pixels in the above images have been effectively ""dilated"" just slightly so that they stand out better. It's purely a cosmetic operation for the sake of the narrative; although there are comments mentioning this dilation in my code, rest assured that it has nothing to do with any calculations that actually matter.

Once the clusters are identified and labeled, the third and final step is easy: I simply take the largest cluster in each image (in this case, I chose to measure ""size"" in terms of the total number of member pixels, although one could have just as easily instead used some type of metric that gauges physical extent) and compute the convex hull for that cluster. The convex hull then becomes the tree border. The six convex hulls computed via this method are shown below in red:

![Christmas trees with their calculated borders](https://i.sstatic.net/sl6Ar.jpg)

The source code is written for Python 2.7.6 and it depends on [numpy](http://www.numpy.org/), [scipy](http://www.scipy.org/), [matplotlib](http://matplotlib.org/) and [scikit-learn](http://scikit-learn.org/stable/). I've divided it into two parts. The first part is responsible for the actual image processing:

```
from PIL import Image
import numpy as np
import scipy as sp
import matplotlib.colors as colors
from sklearn.cluster import DBSCAN
from math import ceil, sqrt

""""""
Inputs:

    rgbimg:         [M,N,3] numpy array containing (uint, 0-255) color image

    hueleftthr:     Scalar constant to select maximum allowed hue in the
                    yellow-green region

    huerightthr:    Scalar constant to select minimum allowed hue in the
                    blue-purple region

    satthr:         Scalar constant to select minimum allowed saturation

    valthr:         Scalar constant to select minimum allowed value

    monothr:        Scalar constant to select minimum allowed monochrome
                    brightness

    maxpoints:      Scalar constant maximum number of pixels to forward to
                    the DBSCAN clustering algorithm

    proxthresh:     Proximity threshold to use for DBSCAN, as a fraction of
                    the diagonal size of the image

Outputs:

    borderseg:      [K,2,2] Nested list containing K pairs of x- and y- pixel
                    values for drawing the tree border

    X:              [P,2] List of pixels that passed the threshold step

    labels:         [Q,2] List of cluster labels for points in Xslice (see
                    below)

    Xslice:         [Q,2] Reduced list of pixels to be passed to DBSCAN

""""""

def findtree(rgbimg, hueleftthr=0.2, huerightthr=0.95, satthr=0.7, 
             valthr=0.7, monothr=220, maxpoints=5000, proxthresh=0.04):

    # Convert rgb image to monochrome for
    gryimg = np.asarray(Image.fromarray(rgbimg).convert('L'))
    # Convert rgb image (uint, 0-255) to hsv (float, 0.0-1.0)
    hsvimg = colors.rgb_to_hsv(rgbimg.astype(float)/255)

    # Initialize binary thresholded image
    binimg = np.zeros((rgbimg.shape[0], rgbimg.shape[1]))
    # Find pixels with hue<0.2 or hue>0.95 (red or yellow) and saturation/value
    # both greater than 0.7 (saturated and bright)--tends to coincide with
    # ornamental lights on trees in some of the images
    boolidx = np.logical_and(
                np.logical_and(
                  np.logical_or((hsvimg[:,:,0] < hueleftthr),
                                (hsvimg[:,:,0] > huerightthr)),
                                (hsvimg[:,:,1] > satthr)),
                                (hsvimg[:,:,2] > valthr))
    # Find pixels that meet hsv criterion
    binimg[np.where(boolidx)] = 255
    # Add pixels that meet grayscale brightness criterion
    binimg[np.where(gryimg > monothr)] = 255

    # Prepare thresholded points for DBSCAN clustering algorithm
    X = np.transpose(np.where(binimg == 255))
    Xslice = X
    nsample = len(Xslice)
    if nsample > maxpoints:
        # Make sure number of points does not exceed DBSCAN maximum capacity
        Xslice = X[range(0,nsample,int(ceil(float(nsample)/maxpoints)))]

    # Translate DBSCAN proximity threshold to units of pixels and run DBSCAN
    pixproxthr = proxthresh * sqrt(binimg.shape[0]**2 + binimg.shape[1]**2)
    db = DBSCAN(eps=pixproxthr, min_samples=10).fit(Xslice)
    labels = db.labels_.astype(int)

    # Find the largest cluster (i.e., with most points) and obtain convex hull   
    unique_labels = set(labels)
    maxclustpt = 0
    for k in unique_labels:
        class_members = [index[0] for index in np.argwhere(labels == k)]
        if len(class_members) > maxclustpt:
            points = Xslice[class_members]
            hull = sp.spatial.ConvexHull(points)
            maxclustpt = len(class_members)
            borderseg = [[points[simplex,0], points[simplex,1]] for simplex
                          in hull.simplices]

    return borderseg, X, labels, Xslice

```

and the second part is a user-level script which calls the first file and generates all of the plots above:

```
#!/usr/bin/env python

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from findtree import findtree

# Image files to process
fname = ['nmzwj.png', 'aVZhC.png', '2K9EF.png',
         'YowlH.png', '2y4o5.png', 'FWhSP.png']

# Initialize figures
fgsz = (16,7)        
figthresh = plt.figure(figsize=fgsz, facecolor='w')
figclust  = plt.figure(figsize=fgsz, facecolor='w')
figcltwo  = plt.figure(figsize=fgsz, facecolor='w')
figborder = plt.figure(figsize=fgsz, facecolor='w')
figthresh.canvas.set_window_title('Thresholded HSV and Monochrome Brightness')
figclust.canvas.set_window_title('DBSCAN Clusters (Raw Pixel Output)')
figcltwo.canvas.set_window_title('DBSCAN Clusters (Slightly Dilated for Display)')
figborder.canvas.set_window_title('Trees with Borders')

for ii, name in zip(range(len(fname)), fname):
    # Open the file and convert to rgb image
    rgbimg = np.asarray(Image.open(name))

    # Get the tree borders as well as a bunch of other intermediate values
    # that will be used to illustrate how the algorithm works
    borderseg, X, labels, Xslice = findtree(rgbimg)

    # Display thresholded images
    axthresh = figthresh.add_subplot(2,3,ii+1)
    axthresh.set_xticks([])
    axthresh.set_yticks([])
    binimg = np.zeros((rgbimg.shape[0], rgbimg.shape[1]))
    for v, h in X:
        binimg[v,h] = 255
    axthresh.imshow(binimg, interpolation='nearest', cmap='Greys')

    # Display color-coded clusters
    axclust = figclust.add_subplot(2,3,ii+1) # Raw version
    axclust.set_xticks([])
    axclust.set_yticks([])
    axcltwo = figcltwo.add_subplot(2,3,ii+1) # Dilated slightly for display only
    axcltwo.set_xticks([])
    axcltwo.set_yticks([])
    axcltwo.imshow(binimg, interpolation='nearest', cmap='Greys')
    clustimg = np.ones(rgbimg.shape)    
    unique_labels = set(labels)
    # Generate a unique color for each cluster 
    plcol = cm.rainbow_r(np.linspace(0, 1, len(unique_labels)))
    for lbl, pix in zip(labels, Xslice):
        for col, unqlbl in zip(plcol, unique_labels):
            if lbl == unqlbl:
                # Cluster label of -1 indicates no cluster membership;
                # override default color with black
                if lbl == -1:
                    col = [0.0, 0.0, 0.0, 1.0]
                # Raw version
                for ij in range(3):
                    clustimg[pix[0],pix[1],ij] = col[ij]
                # Dilated just for display
                axcltwo.plot(pix[1], pix[0], 'o', markerfacecolor=col, 
                    markersize=1, markeredgecolor=col)
    axclust.imshow(clustimg)
    axcltwo.set_xlim(0, binimg.shape[1]-1)
    axcltwo.set_ylim(binimg.shape[0], -1)

    # Plot original images with read borders around the trees
    axborder = figborder.add_subplot(2,3,ii+1)
    axborder.set_axis_off()
    axborder.imshow(rgbimg, interpolation='nearest')
    for vseg, hseg in borderseg:
        axborder.plot(hseg, vseg, 'r-', lw=3)
    axborder.set_xlim(0, binimg.shape[1]-1)
    axborder.set_ylim(binimg.shape[0], -1)

plt.show()

```"
"I am looking for the best way (fast and elegant) to get a random boolean in python (flip a coin).

For the moment I am using `random.randint(0, 1)` or `random.getrandbits(1)`.

Are there better choices that I am not aware of?","Adam's answer is quite fast, but I found that `random.getrandbits(1)` to be quite a lot faster. If you really want a boolean instead of a long then

```
import random
bool(random.getrandbits(1))

```

is still about twice as fast as `random.choice([True, False])`

Both solutions need to `import random`

If utmost speed isn't to priority then `random.choice` definitely reads better.

Note that `random.choice()` is slower than just `choice()` (after `from random import choice`) due to the attribute lookup.

```
$ python3 --version
Python 3.9.7
$ python3 -m timeit -s ""from random import choice"" ""choice([True, False])""
1000000 loops, best of 5: 376 nsec per loop
$ python3 -m timeit -s ""from random import choice"" ""choice((True, False))""
1000000 loops, best of 5: 352 nsec per loop
$ python3 -m timeit -s ""from random import getrandbits"" ""getrandbits(1)""
10000000 loops, best of 5: 33.7 nsec per loop
$ python3 -m timeit -s ""from random import getrandbits"" ""bool(getrandbits(1))""
5000000 loops, best of 5: 89.5 nsec per loop
$ python3 -m timeit -s ""from random import getrandbits"" ""not getrandbits(1)""
5000000 loops, best of 5: 46.3 nsec per loop
$ python3 -m timeit -s ""from random import random"" ""random() < 0.5""
5000000 loops, best of 5: 46.4 nsec per loop

```"
"I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely `id`, I want to specify the column type as `int`. The problem is the `id` series has missing/empty values.

When I try to cast the `id` column to integer while reading the .csv, I get:

```
df= pd.read_csv(""data.csv"", dtype={'id': int}) 
error: Integer column has NA values

```

Alternatively, I tried to convert the column type after reading as below, but this time I get:

```
df= pd.read_csv(""data.csv"") 
df[['id']] = df[['id']].astype(int)
error: Cannot convert NA to integer

```

How can I tackle this?","In version 0.24.+ pandas has gained the ability to hold integer dtypes with missing values.

[Nullable Integer Data Type](http://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html).

Pandas can represent integer data with possibly missing values using [`arrays.IntegerArray`](http://pandas.pydata.org/pandas-docs/stable/reference/pandas.arrays.IntegerArray.html). This is an extension types implemented within pandas. It is not the default dtype for integers, and will not be inferred; you must explicitly pass the dtype into [`array()`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.array.html#pandas.array) or `Series`:

```
arr = pd.array([1, 2, np.nan], dtype=pd.Int64Dtype())
pd.Series(arr)

0      1
1      2
2    NaN
dtype: Int64

```

For convert column to nullable integers use:

```
df['myCol'] = df['myCol'].astype('Int64')

```"
"I am sorry that I can't reproduce the error with a simpler example, and my code is too complicated to post. If I run the program in IPython shell instead of the regular Python, things work out well.

I looked up some previous notes on this problem. They were all caused by using pool to call function defined within a class function. But this is not the case for me.

```
Exception in thread Thread-3:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 552, in __bootstrap_inner
    self.run()
  File ""/usr/lib64/python2.7/threading.py"", line 505, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 313, in _handle_tasks
    put(task)
PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed

```

I would appreciate any help.

**Update**: The function I pickle is defined at the top level of the module. Though it calls a function that contains a nested function. i.e, `f()` calls `g()` calls `h()` which has a nested function `i()`, and I am calling `pool.apply_async(f)`. `f()`, `g()`, `h()` are all defined at the top level. I tried simpler example with this pattern and it works though.","Here is a [list of what can be pickled](http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled). In particular, functions are only picklable if they are defined at the top-level of a module.

This piece of code:

```
import multiprocessing as mp

class Foo():
    @staticmethod
    def work(self):
        pass

if __name__ == '__main__':   
    pool = mp.Pool()
    foo = Foo()
    pool.apply_async(foo.work)
    pool.close()
    pool.join()

```

yields an error almost identical to the one you posted:

```
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 552, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 505, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 315, in _handle_tasks
    put(task)
PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed

```

The problem is that the `pool` methods all use a `mp.SimpleQueue` to pass tasks to the worker processes. Everything that goes through the `mp.SimpleQueue` must be pickable, and `foo.work` is not picklable since it is not defined at the top level of the module.

It can be fixed by defining a function at the top level, which calls `foo.work()`:

```
def work(foo):
    foo.work()

pool.apply_async(work,args=(foo,))

```

Notice that `foo` is pickable, since `Foo` is defined at the top level and `foo.__dict__` is picklable."
"I'm basically looking for a python version of [Combination of `List<List<int>>`](https://stackoverflow.com/questions/545703/combination-of-listlistint)

Given a list of lists, I need a new list that gives all the possible combinations of items between the lists.

```
[[1,2,3],[4,5,6],[7,8,9,10]] -> [[1,4,7],[1,4,8],...,[3,6,10]]

```

The number of lists is unknown, so I need something that works for all cases. Bonus points for elegance!","you need [`itertools.product`](https://docs.python.org/2/library/itertools.html#itertools.product):

```
>>> import itertools
>>> a = [[1,2,3],[4,5,6],[7,8,9,10]]
>>> list(itertools.product(*a))
[(1, 4, 7), (1, 4, 8), (1, 4, 9), (1, 4, 10), (1, 5, 7), (1, 5, 8), (1, 5, 9), (1, 5, 10), (1, 6, 7), (1, 6, 8), (1, 6, 9), (1, 6, 10), (2, 4, 7), (2, 4, 8), (2, 4, 9), (2, 4, 10), (2, 5, 7), (2, 5, 8), (2, 5, 9), (2, 5, 10), (2, 6, 7), (2, 6, 8), (2, 6, 9), (2, 6, 10), (3, 4, 7), (3, 4, 8), (3, 4, 9), (3, 4, 10), (3, 5, 7), (3, 5, 8), (3, 5, 9), (3, 5, 10), (3, 6, 7), (3, 6, 8), (3, 6, 9), (3, 6, 10)]

```"
"Suppose I have

```
l = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

```

How can I get a result like so?

```
r = [[1, 4, 7], [2, 5, 8], [3, 6, 9]]

```

I know [how to get](https://stackoverflow.com/questions/19339)

```
r = [(1, 4, 7), (2, 5, 8), (3, 6, 9)]

```

but I must have lists as elements of the result.","For rectangular data
--------------------

(or to limit each ""column"" to the length of the shortest input ""row"")

In Python 3.x, use:

```
# short circuits at shortest nested list if table is jagged:
list(map(list, zip(*l)))

```

In Python 2.x, use:

```
# short circuits at shortest nested list if table is jagged:
map(list, zip(*l))

```

There are two important things to understand here:

1. The signature of [zip](https://docs.python.org/library/functions.html#zip): `zip(*iterables)` This means `zip` expects an arbitrary number of arguments each of which must be iterable. E.g. `zip([1, 2], [3, 4], [5, 6])`.
2. [Unpacked argument lists](https://docs.python.org/tutorial/controlflow.html#unpacking-argument-lists): Given a sequence of arguments `args`, `f(*args)` will call `f` such that each element in `args` is a separate positional argument of `f`. Given `l = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]`, `zip(*l)` would be equivalent to `zip([1, 2, 3], [4, 5, 6], [7, 8, 9])`. See also: [Expanding tuples into arguments](https://stackoverflow.com/questions/1993727)

The rest is just making sure the result is a list of lists instead of a list of tuples, by using `map` to create a list from each tuple.

For jagged data
---------------

To pad shorter rows with `None` values in the output, `import itertools` (this is in the standard library), and then:

In Python 3.x, use:

```
list(map(list, itertools.zip_longest(*l, fillvalue=None)))

```

In Python 2.x, use:

```
list(map(list, itertools.izip_longest(*l, fillvalue=None)))

```"
"I've been working with data imported from a CSV. Pandas changed some columns to float, so now the numbers in these columns get displayed as floating points! However, I need them to be displayed as integers or without comma. Is there a way to convert them to integers or not display the comma?","To modify the float output do this:

```
df= pd.DataFrame(range(5), columns=['a'])
df.a = df.a.astype(float)
df

Out[33]:

          a
0 0.0000000
1 1.0000000
2 2.0000000
3 3.0000000
4 4.0000000

pd.options.display.float_format = '{:,.0f}'.format
df

Out[35]:

   a
0  0
1  1
2  2
3  3
4  4

```"
"I want to take the [difference](https://en.wikipedia.org/wiki/Complement_(set_theory)#Relative_complement) between lists `x` and `y`:

```
>>> x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> y = [1, 3, 5, 7, 9]  
>>> x - y
# should return [0, 2, 4, 6, 8]

```","Use a list comprehension to compute the difference while maintaining the original [order](https://en.wikipedia.org/wiki/Sequence) from `x`:

```
[item for item in x if item not in y]

```

---

If you don't need list properties (e.g. ordering), use a [set difference](https://docs.python.org/3/library/stdtypes.html#frozenset.difference), as the [other answers](https://stackoverflow.com/a/3428547/365102) suggest:

```
list(set(x) - set(y))

```

---

To allow `x - y` infix syntax, override `__sub__` on a class inheriting from `list`:

```
class MyList(list):
    def __init__(self, *args):
        super(MyList, self).__init__(args)

    def __sub__(self, other):
        return self.__class__(*[item for item in self if item not in other])

```

Usage:

```
x = MyList(1, 2, 3, 4)
y = MyList(2, 5, 2)
z = x - y   

```"
"I am trying to check if a dictionary is empty but it doesn't behave properly. It just skips it and displays **ONLINE** without anything aside from the display the message. Any ideas why ?

```
def isEmpty(self, dictionary):
    for element in dictionary:
        if element:
            return True
        return False

def onMessage(self, socket, message):
    if self.isEmpty(self.users) == False:
        socket.send(""Nobody is online, please use REGISTER command"" \
                 "" in order to register into the server"")
    else:
        socket.send(""ONLINE "" + ' ' .join(self.users.keys()))    

```","Empty dictionaries [evaluate to `False`](https://docs.python.org/2/library/stdtypes.html#truth-value-testing) in Python:

```
>>> dct = {}
>>> bool(dct)
False
>>> not dct
True
>>>

```

Thus, your `isEmpty` function is unnecessary. All you need to do is:

```
def onMessage(self, socket, message):
    if not self.users:
        socket.send(""Nobody is online, please use REGISTER command"" \
                    "" in order to register into the server"")
    else:
        socket.send(""ONLINE "" + ' ' .join(self.users.keys()))

```"
"I have a dataframe that consist of hundreds of columns, and I need to see all column names.

What I did:

```
In[37]:
data_all2.columns

```

The output is:

```
Out[37]:
Index(['customer_id', 'incoming', 'outgoing', 'awan', 'bank', 'family', 'food',
       'government', 'internet', 'isipulsa',
       ...
       'overdue_3months_feature78', 'overdue_3months_feature79',
       'overdue_3months_feature80', 'overdue_3months_feature81',
       'overdue_3months_feature82', 'overdue_3months_feature83',
       'overdue_3months_feature84', 'overdue_3months_feature85',
       'overdue_3months_feature86', 'loan_overdue_3months_total_y'],
      dtype='object', length=102)

```

How do I show *all* columns, instead of a truncated list?","You can globally set printing options. I think this should work:

**Method 1:**

```
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

```

**Method 2:**

```
pd.options.display.max_columns = None
pd.options.display.max_rows = None

```

This will allow you to see all column names & rows when you are doing `.head()`. None of the column name will be truncated.

---

If you just want to see the column names you can do:

```
print(df.columns.tolist())

```"
"I have Anaconda installed on my computer and I'd like to update it. In Navigator I can see that there are several individual packages that can be updated, but also an `anaconda` package that sometimes has a version number and sometimes says `custom`. How do I proceed?

[![enter image description here](https://i.sstatic.net/YZraz.png)](https://i.sstatic.net/YZraz.png)","> `root` is the old (pre-conda 4.4) name for the main environment; after conda 4.4, it was renamed to be `base`. [source](https://stackoverflow.com/a/52592177/8828460)

What 95% of people actually want
--------------------------------

In most cases what you want to do when you say that you want to update Anaconda is to execute the command:

```
conda update --all

```

(But this should be **preceded** by `conda update -n base conda` or simply `conda update conda` so you have the latest `conda` version installed)

This will update all packages in the current environment to the latest version -- with the small print being that it may use an older version of some packages in order to satisfy dependency constraints (often this won't be necessary and when it is necessary the package plan solver will do its best to minimize the impact).

This needs to be executed from the command line, and the best way to get there is from Anaconda Navigator, then the ""Environments"" tab, then click on the triangle beside the `base` environment, selecting ""Open Terminal"":

[![Open terminal from Navigator](https://i.sstatic.net/6JjBC.png)](https://i.sstatic.net/6JjBC.png)

This operation will only update the one selected environment (in this case, the `base` environment). If you have other environments you'd like to update you can repeat the process above, but first click on the environment. When it is selected there is a triangular marker on the right (see image above, step 3). Or from the command line you can provide the environment name (`-n envname`) or path (`-p /path/to/env`), for example to update your `dspyr` environment from the screenshot above:

```
conda update -n dspyr --all

```

Update individual packages
--------------------------

If you are only interested in updating an individual package then simply click on the blue arrow or blue version number in Navigator, e.g. for `astroid` or `astropy` in the screenshot above, and this will tag those packages for an upgrade. When you are done you need to click the ""Apply"" button:

[![Apply to update individual packages](https://i.sstatic.net/wB5bx.png)](https://i.sstatic.net/wB5bx.png)

Or from the command line:

```
conda update astroid astropy

```

Updating just the packages in the standard Anaconda Distribution
----------------------------------------------------------------

If you don't care about package versions and just want *""the latest set of all packages in the standard Anaconda Distribution, so long as they work together""*, then you should [take a look at this gist](https://gist.github.com/ijstokes/45605149213e1630e928d8ff4cbdbe5f).

Why updating the Anaconda package is almost always a bad idea
-------------------------------------------------------------

In most cases updating the Anaconda package in the package list will have a surprising result: you may actually **downgrade** many packages (in fact, this is likely if it indicates the version as `custom`). The gist above provides details.

Leverage conda environments
---------------------------

Your `base` environment is probably not a good place to try and manage an exact set of packages: it is going to be a dynamic working space with new packages installed and packages randomly updated. If you need an exact set of packages then create a conda environment to hold them. Thanks to the conda package cache and the way file linking is used doing this is typically i) fast and ii) consumes very little additional disk space. E.g.

```
conda create -n myspecialenv -c bioconda -c conda-forge python=3.5 pandas beautifulsoup seaborn nltk

```

The [conda documentation](https://conda.io/docs/using/envs.html) has more details and examples.

pip, PyPI, and setuptools?
--------------------------

None of this is going to help with updating packages that have been installed from PyPI via `pip` or any packages installed using `python setup.py install`. `conda list` will give you some hints about the pip-based Python packages you have in an environment, but it won't do anything special to update them.

Commercial use of Anaconda or Anaconda Enterprise
-------------------------------------------------

It is pretty much exactly the same story, with the exception that you may not be able to update the `base` environment if it was installed by someone else (say to `/opt/anaconda/latest`). If you're not able to update the environments you are using you should be able to clone and then update:

```
conda create -n myenv --clone base
conda update -n myenv --all

```"
"I have a Pandas DataFrame with one column:

```
import pandas as pd

df = pd.DataFrame({""teams"": [[""SF"", ""NYG""] for _ in range(7)]})

       teams
0  [SF, NYG]
1  [SF, NYG]
2  [SF, NYG]
3  [SF, NYG]
4  [SF, NYG]
5  [SF, NYG]
6  [SF, NYG]

```

How can split this column of lists into two columns?

Desired result:

```
  team1 team2
0    SF   NYG
1    SF   NYG
2    SF   NYG
3    SF   NYG
4    SF   NYG
5    SF   NYG
6    SF   NYG

```","You can use the `DataFrame` constructor with `lists` created by [`to_list`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_list.html):

```
import pandas as pd

d1 = {'teams': [['SF', 'NYG'],['SF', 'NYG'],['SF', 'NYG'],
                ['SF', 'NYG'],['SF', 'NYG'],['SF', 'NYG'],['SF', 'NYG']]}
df2 = pd.DataFrame(d1)
print (df2)
       teams
0  [SF, NYG]
1  [SF, NYG]
2  [SF, NYG]
3  [SF, NYG]
4  [SF, NYG]
5  [SF, NYG]
6  [SF, NYG]

```

---

```
df2[['team1','team2']] = pd.DataFrame(df2.teams.tolist(), index= df2.index)
print (df2)
       teams team1 team2
0  [SF, NYG]    SF   NYG
1  [SF, NYG]    SF   NYG
2  [SF, NYG]    SF   NYG
3  [SF, NYG]    SF   NYG
4  [SF, NYG]    SF   NYG
5  [SF, NYG]    SF   NYG
6  [SF, NYG]    SF   NYG

```

And for a new `DataFrame`:

```
df3 = pd.DataFrame(df2['teams'].to_list(), columns=['team1','team2'])
print (df3)
  team1 team2
0    SF   NYG
1    SF   NYG
2    SF   NYG
3    SF   NYG
4    SF   NYG
5    SF   NYG
6    SF   NYG

```

A solution with `apply(pd.Series)` is very slow:

```
#7k rows
df2 = pd.concat([df2]*1000).reset_index(drop=True)

In [121]: %timeit df2['teams'].apply(pd.Series)
1.79 s ± 52.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [122]: %timeit pd.DataFrame(df2['teams'].to_list(), columns=['team1','team2'])
1.63 ms ± 54.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

```"
"What is [`numpy.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) and when should I use it?

Using it on a 1-D array `x` produces:

```
>>> x
array([0, 1, 2, 3])

>>> x[np.newaxis, :]
array([[0, 1, 2, 3]])

>>> x[:, np.newaxis]
array([[0],
       [1],
       [2],
       [3]])

```","Simply put, [`numpy.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis) is used to ***increase the dimension*** of the existing array by *one more dimension*, when used *once*. Thus,

* **1D** array will become **2D** array
* **2D** array will become **3D** array
* **3D** array will become **4D** array
* **4D** array will become **5D** array

and so on..

Here is a visual illustration which depicts *promotion* of 1D array to 2D arrays.

[![newaxis canva visualization](https://i.sstatic.net/zkMBy.png)](https://i.sstatic.net/zkMBy.png)

---

**Scenario-1**: [`np.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis) might come in handy when you want to *explicitly* convert a 1D array to either a *row vector* or a *column vector*, as depicted in the above picture.

**Example:**

```
# 1D array
In [7]: arr = np.arange(4)
In [8]: arr.shape
Out[8]: (4,)

# make it as row vector by inserting an axis along first dimension
In [9]: row_vec = arr[np.newaxis, :]     # arr[None, :]
In [10]: row_vec.shape
Out[10]: (1, 4)

# make it as column vector by inserting an axis along second dimension
In [11]: col_vec = arr[:, np.newaxis]     # arr[:, None]
In [12]: col_vec.shape
Out[12]: (4, 1)

```

---

**Scenario-2**: When we want to make use of [**numpy broadcasting**](https://numpy.org/doc/stable/user/basics.broadcasting.html) as part of some operation, for instance while doing *addition* of some arrays.

**Example:**

Let's say you want to add the following two arrays:

```
 x1 = np.array([1, 2, 3, 4, 5])
 x2 = np.array([5, 4, 3])

```

If you try to add these just like that, NumPy will raise the following `ValueError` :

```
ValueError: operands could not be broadcast together with shapes (5,) (3,)

```

In this situation, you can use [`np.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis) to increase the dimension of one of the arrays so that NumPy can [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html).

```
In [2]: x1_new = x1[:, np.newaxis]    # x1[:, None]
# now, the shape of x1_new is (5, 1)
# array([[1],
#        [2],
#        [3],
#        [4],
#        [5]])

```

Now, add:

```
In [3]: x1_new + x2
Out[3]:
array([[ 6,  5,  4],
       [ 7,  6,  5],
       [ 8,  7,  6],
       [ 9,  8,  7],
       [10,  9,  8]])

```

---

Alternatively, you can also add new axis to the array `x2`:

```
In [6]: x2_new = x2[:, np.newaxis]    # x2[:, None]
In [7]: x2_new     # shape is (3, 1)
Out[7]: 
array([[5],
       [4],
       [3]])

```

Now, add:

```
In [8]: x1 + x2_new
Out[8]: 
array([[ 6,  7,  8,  9, 10],
       [ 5,  6,  7,  8,  9],
       [ 4,  5,  6,  7,  8]])

```

**Note**: Observe that we get the same result in both cases (but one being the transpose of the other).

---

**Scenario-3**: This is similar to scenario-1. But, you can use [`np.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) more than once to *promote* the array to higher dimensions. Such an operation is sometimes needed for higher order arrays (*i.e. Tensors*).

**Example:**

```
In [124]: arr = np.arange(5*5).reshape(5,5)

In [125]: arr.shape
Out[125]: (5, 5)

# promoting 2D array to a 5D array
In [126]: arr_5D = arr[np.newaxis, ..., np.newaxis, np.newaxis]    # arr[None, ..., None, None]

In [127]: arr_5D.shape
Out[127]: (1, 5, 5, 1, 1)

```

As an alternative, you can use [`numpy.expand_dims`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html) that has an intuitive `axis` kwarg.

```
# adding new axes at 1st, 4th, and last dimension of the resulting array
In [131]: newaxes = (0, 3, -1)
In [132]: arr_5D = np.expand_dims(arr, axis=newaxes)
In [133]: arr_5D.shape
Out[133]: (1, 5, 5, 1, 1)

```

---

**More background on [np.newaxis](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) vs [np.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)**

[`newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) is also called as a pseudo-index that allows the temporary addition of an axis into a multiarray.

[`np.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) uses the slicing operator to recreate the array while [`numpy.reshape`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) reshapes the array to the desired layout (assuming that the dimensions match; And this is ***must*** for a [`reshape`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to happen).

**Example**

```
In [13]: A = np.ones((3,4,5,6))
In [14]: B = np.ones((4,6))
In [15]: (A + B[:, np.newaxis, :]).shape     # B[:, None, :]
Out[15]: (3, 4, 5, 6)

```

In the above example, we inserted a temporary axis between the first and second axes of `B` (to use broadcasting). A missing axis is filled-in here using [`np.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) to make the [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) operation work.

---

***General Tip***: You can also use `None` in place of [`np.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis); These are in fact the [same objects](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html).

```
In [13]: np.newaxis is None
Out[13]: True

```

P.S. Also see this great answer: [newaxis vs reshape to add dimensions](https://stackoverflow.com/a/28385957)"
"<https://github.com/affinelayer/pix2pix-tensorflow/tree/master/tools>

An error occurred when compiling ""process.py"" on the above site.

```
python tools/process.py --input_dir data --operation resize --output_dir data2/resize
data/0.jpg -> data2/resize/0.png

Traceback (most recent call last):
  File ""tools/process.py"", line 235, in <module>
    main()
  File ""tools/process.py"", line 167, in main
    src = load(src_path)
  File ""tools/process.py"", line 113, in load
    contents = open(path).read()
  File""/home/user/anaconda3/envs/tensorflow_2/lib/python3.5/codecs.py"", line 321, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

```

What is the cause of the error?
Python's version is 3.5.2.","Python tries to convert a byte-array (a `bytes` which it assumes to be a utf-8-encoded string) to a unicode string (`str`). This process of course is a decoding according to utf-8 rules. When it tries this, it encounters a byte sequence which is not allowed in utf-8-encoded strings (namely this 0xff at position 0).

Since you did not provide any code we could look at, we only could guess on the rest.

From the stack trace we can assume that the triggering action was the reading from a file (`contents = open(path).read()`). I propose to recode this in a fashion like this:

```
with open(path, 'rb') as f:
  contents = f.read()

```

That `b` in the mode specifier in the `open()` states that the file shall be treated as binary, so `contents` will remain a `bytes`. No decoding attempt will happen this way."
"I'm writing a python application that will make heavy use of a graph data structure. Nothing horribly complex, but I'm thinking some sort of graph/graph-algorithms library would help me out. I've googled around, but I don't find anything that particularly leaps out at me.

Anyone have any good recommendations?","There are two excellent choices:

[NetworkX](http://networkx.github.io/)

and

[igraph](http://igraph.org/)

I like NetworkX, but I read good things about igraph as well. I routinely use NetworkX with graphs with 1 million nodes with no problem (it's about double the overhead of a dict of size V + E)

If you want a feature comparison, see this from the Networkx-discuss list

[Feature comparison thread](http://groups.google.com/group/networkx-discuss/t/7597ca97abbb3f90?hl=en)"
"In Python, how do you find the number of digits in an integer?","If you want the length of an integer as in the number of digits in the integer, you can always convert it to string like `str(133)` and find its length like `len(str(123))`."
"I have the below db model:

```
from datetime import datetime    

class TermPayment(models.Model):
    # I have excluded fields that are irrelevant to the question
    date = models.DateTimeField(default=datetime.now(), blank=True)

```

I add a new instance by using the below:

```
tp = TermPayment.objects.create(**kwargs)

```

My issue: all records in database have the same value in date field, which is the date of the first payment. After the server restarts, one record has the new date and the other records have the same as the first. It looks as if some data is cached, but I can't find where.

database: mysql 5.1.25

django v1.1.1","it looks like `datetime.now()` is being evaluated when the model is defined, and not each time you add a record.

Django has a feature to accomplish what you are trying to do already:

```
date = models.DateTimeField(auto_now_add=True, blank=True)

```

or

```
date = models.DateTimeField(default=datetime.now, blank=True)

```

The difference between the second example and what you currently have is the lack of parentheses. By passing `datetime.now` without the parentheses, you are passing the actual function, which will be called each time a record is added. If you pass it `datetime.now()`, then you are just evaluating the function and passing it the return value.

More information is available at Django's [model field reference](http://docs.djangoproject.com/en/dev/ref/models/fields/#datetimefield)"
"The following code gives the error `UnboundLocalError: local variable 'Var1' referenced before assignment`:

```
Var1 = 1
Var2 = 0
def function(): 
    if Var2 == 0 and Var1 > 0:
        print(""Result 1"")
    elif Var2 == 1 and Var1 > 0:
        print(""Result 2"")
    elif Var1 < 1:
        print(""Result 3"")
    Var1 -= 1
function()

```

How can I fix this?","This is because, even though `Var1` exists, you're also using an assignment statement on the name `Var1` inside of the function (`Var1 -= 1` at the bottom line). Naturally, this creates a variable inside the function's scope called `Var1` (truthfully, a `-=` or `+=` will only update (reassign) an existing variable, but for reasons unknown (likely consistency in this context), Python treats it as an assignment). The Python interpreter sees this at module load time and decides (correctly so) that the global scope's `Var1` should not be used inside the local scope, which leads to a problem when you try to reference the variable before it is locally assigned.

Using global variables, outside of necessity, is usually frowned upon by Python developers, because it leads to confusing and problematic code. However, if you'd like to use them to accomplish what your code is implying, you can simply add, **inside the top of your function**:

```
global Var1, Var2

```

This will tell Python that you **do not** intend to define a `Var1` or `Var2` variable inside the function's local scope. The Python interpreter sees this at module load time and decides (correctly so) to look up any references to the aforementioned variables in the global scope.

### Some Resources

* the Python website has a [great explanation](https://docs.python.org/3/faq/programming.html#why-am-i-getting-an-unboundlocalerror-when-the-variable-has-a-value) for this common issue.
* Python 3 offers a related [`nonlocal`](https://docs.python.org/3/reference/simple_stmts.html#nonlocal) statement - check that out as well."
"I am dealing with dates in Python and I need to convert them to UTC timestamps to be used
inside Javascript. The following code does not work:

```
>>> d = datetime.date(2011,01,01)
>>> datetime.datetime.utcfromtimestamp(time.mktime(d.timetuple()))
datetime.datetime(2010, 12, 31, 23, 0)

```

Converting the date object first to datetime also does not help. I tried the example at this [link](http://www.reddit.com/r/Python/comments/hkw9u/create_utc_timestamp_for_given_date) from, but:

```
from pytz import utc, timezone
from datetime import datetime
from time import mktime
input_date = datetime(year=2011, month=1, day=15)

```

and now either:

```
mktime(utc.localize(input_date).utctimetuple())

```

or

```
mktime(timezone('US/Eastern').localize(input_date).utctimetuple())

```

does work.

So general question: how can I get a date converted to seconds since epoch according to UTC?","If `d = date(2011, 1, 1)` is in UTC:

```
>>> from datetime import datetime, date
>>> import calendar
>>> timestamp1 = calendar.timegm(d.timetuple())
>>> datetime.utcfromtimestamp(timestamp1)
datetime.datetime(2011, 1, 1, 0, 0)

```

If `d` is in local timezone:

```
>>> import time
>>> timestamp2 = time.mktime(d.timetuple()) # DO NOT USE IT WITH UTC DATE
>>> datetime.fromtimestamp(timestamp2)
datetime.datetime(2011, 1, 1, 0, 0)

```

`timestamp1` and `timestamp2` may differ if midnight in the local timezone is not the same time instance as midnight in UTC.

`mktime()` may return a wrong result if `d` corresponds to an [ambiguous local time (e.g., during DST transition)](https://stackoverflow.com/questions/12165691/python-datetime-with-timezone-to-epoch/12166400#comment39565270_12166400) or if `d` is a past(future) date when the utc offset might have been different *and* the C `mktime()` has no access to [the tz database](http://www.iana.org/time-zones/repository/tz-link.html) on the given platform. You could [use `pytz` module (e.g., via `tzlocal.get_localzone()`) to get access to the tz database on all platforms](https://stackoverflow.com/a/17365806/4279). Also, [`utcfromtimestamp()` may fail and `mktime()` may return non-POSIX timestamp if `""right""` timezone is used](https://stackoverflow.com/questions/28949623/python-datetime-and-utc-offset-conversion-ignoring-timezone-daylight-savings/28950133#comment46155743_28950133).

---

To convert `datetime.date` object that represents date in UTC without `calendar.timegm()`:

```
DAY = 24*60*60 # POSIX day in seconds (exact value)
timestamp = (utc_date.toordinal() - date(1970, 1, 1).toordinal()) * DAY
timestamp = (utc_date - date(1970, 1, 1)).days * DAY

```

How can I get a date converted to seconds since epoch according to UTC?
-----------------------------------------------------------------------

To convert `datetime.datetime` (not `datetime.date`) object that already represents time in UTC to the corresponding POSIX timestamp (a `float`).

### Python 3.3+

[`datetime.timestamp()`](http://docs.python.org/3.3/library/datetime#datetime.datetime.timestamp):

```
from datetime import timezone

timestamp = dt.replace(tzinfo=timezone.utc).timestamp()

```

Note: It is necessary to supply `timezone.utc` explicitly otherwise `.timestamp()` assume that your naive datetime object is in local timezone.

### Python 3 (< 3.3)

From the docs for [`datetime.utcfromtimestamp()`](http://docs.python.org/dev/library/datetime#datetime.datetime.utcfromtimestamp):

> There is no method to obtain the timestamp from a datetime instance,
> but POSIX timestamp corresponding to a datetime instance dt can be
> easily calculated as follows. For a naive dt:

```
timestamp = (dt - datetime(1970, 1, 1)) / timedelta(seconds=1)

```

> And for an aware dt:

```
timestamp = (dt - datetime(1970,1,1, tzinfo=timezone.utc)) / timedelta(seconds=1)

```

Interesting read: [Epoch time vs. time of day](http://www.ucolick.org/~sla/leapsecs/epochtime.html) on the difference between *What time is it?* and *How many seconds have elapsed?*

See also: [datetime needs an ""epoch"" method](http://bugs.python.org/issue2736)

### Python 2

To adapt the above code for Python 2:

```
timestamp = (dt - datetime(1970, 1, 1)).total_seconds()

```

where [`timedelta.total_seconds()`](http://docs.python.org/library/datetime.html#datetime.timedelta.total_seconds) is equivalent to `(td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6` computed with true division enabled.

### [Example](http://ideone.com/rvH9u)

```
from __future__ import division
from datetime import datetime, timedelta

def totimestamp(dt, epoch=datetime(1970,1,1)):
    td = dt - epoch
    # return td.total_seconds()
    return (td.microseconds + (td.seconds + td.days * 86400) * 10**6) / 10**6 

now = datetime.utcnow()
print now
print totimestamp(now)

```

Beware of [floating-point issues](http://bugs.python.org/issue8644).

### Output

```
2012-01-08 15:34:10.022403
1326036850.02

```

How to convert an aware `datetime` object to POSIX timestamp
------------------------------------------------------------

```
assert dt.tzinfo is not None and dt.utcoffset() is not None
timestamp = dt.timestamp() # Python 3.3+

```

On Python 3:

```
from datetime import datetime, timedelta, timezone

epoch = datetime(1970, 1, 1, tzinfo=timezone.utc)
timestamp = (dt - epoch) / timedelta(seconds=1)
integer_timestamp = (dt - epoch) // timedelta(seconds=1)

```

On Python 2:

```
# utc time = local time              - utc offset
utc_naive  = dt.replace(tzinfo=None) - dt.utcoffset()
timestamp = (utc_naive - datetime(1970, 1, 1)).total_seconds()

```"
"I have looked through the information that the [Python documentation for pickle](https://docs.python.org/3/library/pickle.html) gives, but I'm still a little confused. What would be some sample code that would write a new file and then use pickle to dump a dictionary into it?","Try this:

```
import pickle

a = {'hello': 'world'}

with open('filename.pickle', 'wb') as handle:
    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('filename.pickle', 'rb') as handle:
    b = pickle.load(handle)

print(a == b)

```

There's nothing about the above solution that is specific to a `dict` object. This same approach will will work for many Python objects, including instances of arbitrary classes and arbitrarily complex nestings of data structures. For example, replacing the second line with these lines:

```
import datetime
today = datetime.datetime.now()
a = [{'hello': 'world'}, 1, 2.3333, 4, True, ""x"", 
     (""y"", [[[""z""], ""y""], ""x""]), {'today', today}]

```

will produce a result of `True` as well.

Some objects can't be pickled due to their very nature. For example, it doesn't make sense to pickle a structure containing a handle to an open file."
"This piece of code is giving me an error

```
TypeError: unhashable type: dict

```

Can anyone explain to me what the solution is?

```
negids = movie_reviews.fileids('neg')
def word_feats(words):
    return dict([(word, True) for word in words])

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
stopset = set(stopwords.words('english'))

def stopword_filtered_word_feats(words):
    return dict([(word, True) for word in words if word not in stopset])

result=stopword_filtered_word_feats(negfeats)

```","You're trying to use a `dict` as a key to another `dict` or in a `set`. That does not work because the keys have to be hashable. As a general rule, only immutable objects (strings, integers, floats, frozensets, tuples of immutables) are hashable (though exceptions are possible). So this does not work:

```
>>> dict_key = {""a"": ""b""}
>>> some_dict[dict_key] = True
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'dict'

```

To use a dict as a key you need to turn it into something that may be hashed first. If the dict you wish to use as key consists of only immutable values, you can create a hashable representation of it like this:

```
>>> key = frozenset(dict_key.items())

```

Now you may use `key` as a key in a `dict` or `set`:

```
>>> some_dict[key] = True
>>> some_dict
{frozenset([('a', 'b')]): True}

```

Of course you need to repeat the exercise whenever you want to look up something using a dict:

```
>>> some_dict[dict_key]                     # Doesn't work
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'dict'
>>> some_dict[frozenset(dict_key.items())]  # Works
True

```

If the `dict` you wish to use as key has values that are themselves dicts and/or lists, you need to recursively ""freeze"" the prospective key. Here's a starting point:

```
def freeze(d):
    if isinstance(d, dict):
        return frozenset((key, freeze(value)) for key, value in d.items())
    elif isinstance(d, list):
        return tuple(freeze(value) for value in d)
    return d

```"
"I have some code like:

```
score = 100
name = 'Alice'
print('Total score for %s is %s', name, score)

```

I want it to print out `Total score for Alice is 100`, but instead I get `Total score for %s is %s Alice 100`. How can I get everything to print in the right order with the right formatting?

---

See also: [How can I print multiple things on the same line, one at a time?](https://stackoverflow.com/questions/5598181) ; [How do I put a variableâ€™s value inside a string (interpolate it into the string)?](https://stackoverflow.com/questions/2960772/)","There are many ways to do this. To fix your current code using `%`-formatting, you need to pass in a tuple:

1. Pass it as a tuple:

   ```
   print(""Total score for %s is %s"" % (name, score))

   ```

A tuple with a single element looks like `('this',)`.

Here are some other common ways of doing it:

2. Pass it as a dictionary:

   ```
   print(""Total score for %(n)s is %(s)s"" % {'n': name, 's': score})

   ```

There's also new-style string formatting, which might be a little easier to read:

3. Use new-style string formatting:

   ```
   print(""Total score for {} is {}"".format(name, score))

   ```
4. Use new-style string formatting with numbers (useful for reordering or printing the same one multiple times):

   ```
   print(""Total score for {0} is {1}"".format(name, score))

   ```
5. Use new-style string formatting with explicit names:

   ```
   print(""Total score for {n} is {s}"".format(n=name, s=score))

   ```
6. Concatenate strings:

   ```
   print(""Total score for "" + str(name) + "" is "" + str(score))

   ```

The clearest two, in my opinion:

7. Just pass the values as parameters:

   ```
   print(""Total score for"", name, ""is"", score)

   ```

   If you don't want spaces to be inserted automatically by `print` in the above example, change the `sep` parameter:

   ```
   print(""Total score for "", name, "" is "", score, sep='')

   ```

   If you're using Python 2, won't be able to use the last two because `print` isn't a function in Python 2. You can, however, import this behavior from `__future__`:

   ```
   from __future__ import print_function

   ```
8. Use the new `f`-string formatting in Python 3.6:

   ```
   print(f'Total score for {name} is {score}')

   ```"
"What's the correct way to convert a string to a corresponding instance of an `Enum` subclass? Seems like `getattr(YourEnumType, str)` does the job, but I'm not sure if it's safe enough.

As an example, suppose I have an enum like

```
class BuildType(Enum):
    debug = 200
    release = 400

```

Given the string `'debug'`, how can I get `BuildType.debug` as a result?","This functionality is already built in to [`Enum`](https://docs.python.org/3/howto/enum.html#programmatic-access-to-enumeration-members-and-their-attributes):

```
>>> from enum import Enum
>>> class Build(Enum):
...   debug = 200
...   build = 400
... 
>>> Build['debug']
<Build.debug: 200>

```

The member names are case sensitive, so if user-input is being converted you need to make sure case matches:

```
an_enum = input('Which type of build?')
build_type = Build[an_enum.lower()]

```"
"Is there a way to upgrade the version of Python used in a *virtual environment* (e.g. if a bugfix release comes out)?

I could `pip freeze --local > requirements.txt`, then remove the directory and `pip install -r requirements.txt`, but this requires a lot of reinstallation of large libraries, for instance, `numpy`, which I use a lot.

I can see this is an advantage when upgrading from, e.g., 2.6 -> 2.7, but what about 2.7.x -> 2.7.y?","If you happen to be using the venv module that comes with Python 3.3+, it supports an `--upgrade` option.
Per the [docs](https://docs.python.org/3/library/venv.html):

> Upgrade the environment directory to use this version of Python, assuming Python has been upgraded in-place

```
python3 -m venv --upgrade ENV_DIR

```"
"How to do this in pandas:

I have a function `extract_text_features` on a single text column, returning multiple output columns. Specifically, the function returns 6 values.

The function works, however there doesn't seem to be any proper return type (pandas DataFrame/ numpy array/ Python list) such that the output can get correctly assigned `df.ix[: ,10:16] = df.textcol.map(extract_text_features)`

So I think I need to drop back to iterating with `df.iterrows()`, as per [this](https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas)?

UPDATE:
Iterating with `df.iterrows()` is at least 20x slower, so I surrendered and split out the function into six distinct `.map(lambda ...)` calls.

UPDATE 2: this question was asked back around [v0.11.0](https://github.com/pandas-dev/pandas/releases?after=v0.13.0_ahl1), before the useability of `df.apply` was improved or [`df.assign()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html) was [added in v0.16](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.16.0.html?highlight=assign). Hence much of the question and answers are not too relevant since then.","I usually do this using `zip`:

```
>>> df = pd.DataFrame([[i] for i in range(10)], columns=['num'])
>>> df
    num
0    0
1    1
2    2
3    3
4    4
5    5
6    6
7    7
8    8
9    9

>>> def powers(x):
>>>     return x, x**2, x**3, x**4, x**5, x**6

>>> df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \
>>>     zip(*df['num'].map(powers))

>>> df
        num     p1      p2      p3      p4      p5      p6
0       0       0       0       0       0       0       0
1       1       1       1       1       1       1       1
2       2       2       4       8       16      32      64
3       3       3       9       27      81      243     729
4       4       4       16      64      256     1024    4096
5       5       5       25      125     625     3125    15625
6       6       6       36      216     1296    7776    46656
7       7       7       49      343     2401    16807   117649
8       8       8       64      512     4096    32768   262144
9       9       9       81      729     6561    59049   531441

```"
"I've been running a script on jupyter notebooks for about 26 hour; I haven't really been using my computer for anything else, but it needs to run this program that will take ~30 hours to complete. At about 21 hours in, it stopped saving and my terminal had this:

```
403 PUT /api/contents/[file.ipynb] (::1): '_xsrf' argument missing from POST

```

where [file.ipynb] is the location of my jupyter notebook. It also says:

```
'_xsrf' argument missing from post

```

in the top right part of the notebook again. The program is still running and I don't want to restart jupyter notebook and have to run the program again, as I have a deadline, is there anything else I can do?

I'm using google chrome, but I don't have the LastPass extension or any '%' characters in my code, as another post suggested.

Thanks for any help!","The easiest way I found is this:

<https://github.com/nteract/hydrogen/issues/922#issuecomment-405456346>

Just open another (non-running, existing) notebook on the same kernel, and the issue is magically gone; you can again save the notebooks that were previously showing the `_xsrf` error.

If you have already closed the Jupyter home page, you can find a link to it on the terminal from which Jupyter was started."
"I have the following piece of Base64 encoded data, and I want to use the Python Base64 module to extract information from it. It seems that module does not work. How can I make it work?

```
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

```","```
import base64
coded_string = '''Q5YACgA...'''
base64.b64decode(coded_string)

```

worked for me. At the risk of pasting an offensively-long result, I got:

```
>>> base64.b64decode(coded_string)
2: 'C\x96\x00\n\x00\x00\x00\x00C\x96\x00\x1b\x00\x00\x00\x00C\x96\x00-\x00\x00\x00\x00C\x96\x00?\x00\x00\x00\x00C\x96\x07M\x00\x00\x00\x00C\x96\x07_\x00\x00\x00\x00C\x96\x07p\x00\x00\x00\x00C\x96\x07\x82\x00\x00\x00\x00C\x96\x07\x94\x00\x00\x00\x00C\x96\x07\xa6Cq\xf0\x7fC\x96\x07\xb8DJ\x81\xc7C\x96\x07\xcaD\xa5\x9dtC\x96\x07\xdcD\xb6\x97\x11C\x96\x07\xeeD\x8b\x8flC\x96\x07\xffD\x03\xd4\xaaC\x96\x08\x11B\x05&\xdcC\x96\x08#\x00\x00\x00\x00C\x96\x085C\x0c\xc9\xb7C\x96\x08GCy\xc0\xebC\x96\x08YC\x81\xa4xC\x96\x08kC\x0f@\x9bC\x96\x08}\x00\x00\x00\x00C\x96\x08\x8e\x00\x00\x00\x00C\x96\x08\xa0\x00\x00\x00\x00C\x96\x08\xb2\x00\x00\x00\x00C\x96\x86\xf9\x00\x00\x00\x00C\x96\x87\x0b\x00\x00\x00\x00C\x96\x87\x1d\x00\x00\x00\x00C\x96\x87/\x00\x00\x00\x00C\x96\x87AA\x0b\xe7PC\x96\x87SCI\xf5gC\x96\x87eC\xd4J\xeaC\x96\x87wD\r\x17EC\x96\x87\x89D\x00F6C\x96\x87\x9bC\x9cg\xdeC\x96\x87\xadB\xd56\x0cC\x96\x87\xbf\x00\x00\x00\x00C\x96\x87\xd1\x00\x00\x00\x00C\x96\x87\xe3\x00\x00\x00\x00C\x96\x87\xf5\x00\x00\x00\x00C\x9cY}\x00\x00\x00\x00C\x9cY\x90\x00\x00\x00\x00C\x9cY\xa4\x00\x00\x00\x00C\x9cY\xb7\x00\x00\x00\x00C\x9cY\xcbC\x1f\xbd\xa3C\x9cY\xdeCCz{C\x9cY\xf1CD\x02\xa7C\x9cZ\x05C+\x9d\x97C\x9cZ\x18C\x03R\xe3C\x9cZ,\x00\x00\x00\x00C\x9cZ?
[stuff omitted as it exceeded SO's body length limits]
\xbb\x00\x00\x00\x00D\xc5!7\x00\x00\x00\x00D\xc5!\xb2\x00\x00\x00\x00D\xc7\x14x\x00\x00\x00\x00D\xc7\x14\xf6\x00\x00\x00\x00D\xc7\x15t\x00\x00\x00\x00D\xc7\x15\xf2\x00\x00\x00\x00D\xc7\x16pC5\x9f\xf9D\xc7\x16\xeeC[\xb5\xf5D\xc7\x17lCG\x1b;D\xc7\x17\xeaB\xe3\x0b\xa6D\xc7\x18h\x00\x00\x00\x00D\xc7\x18\xe6\x00\x00\x00\x00D\xc7\x19d\x00\x00\x00\x00D\xc7\x19\xe2\x00\x00\x00\x00D\xc7\xfe\xb4\x00\x00\x00\x00D\xc7\xff3\x00\x00\x00\x00D\xc7\xff\xb2\x00\x00\x00\x00D\xc8\x001\x00\x00\x00\x00'

```"
"I can't understand why Python doesn't have a `sign` function. It has an `abs` builtin (which I consider `sign`'s sister), but no `sign`.

In python 2.6 there is even a `copysign` function (in [math](http://docs.python.org/library/math.html#math.copysign)), but no sign. Why bother to write a `copysign(x,y)` when you could just write a `sign` and then get the `copysign` directly from `abs(x) * sign(y)`? The latter would be much more clear: x with the sign of y, whereas with copysign you have to remember if it's x with the sign of y or y with the sign of x!

Obviously `sign(x)` does not provide anything more than `cmp(x,0)`, but it would be much more readable that this too (and for a greatly readable language like python, this would have been a big plus).

If I were a python designer, I would been the other way arond: no `cmp` builtin, but a `sign`. When you need `cmp(x,y)`, you could just do a `sign(x-y)` (or, even better for non-numerical stuff, just a x>y - of course this should have required `sorted` accepting a boolean instead of an integer comparator). This would also be more clear: positive when `x>y` (whereas with `cmp` you have to remember the convention positive when the **first** is *bigger*, but it could be the other way around). Of course `cmp` makes sense in its own for other reasons (e.g. when sorting non-numerical things, or if you want the sort to be stable, which is not possible using with simply a boolean)

So, the question is: why did the Python designer(s) decide to leave the `sign` function out of the language? Why the heck bother with `copysign` and not its parent `sign`?

Am I missing something?

EDIT - after Peter Hansen comment.
Fair enough that you didn't use it, but you didn't say what you use python for. In 7 years that I use python, I needed it countless times, and the last is the straw that broke the camel's back!

Yes, you can pass cmp around, but 90% of the times that I needed to pass it was in an idiom like
`lambda x,y: cmp(score(x),score(y))` that would have worked with sign just fine.

Finally, I hope you agree that `sign` would be more useful than `copysign`, so even if I bought your view, why bother about defining that in math, instead of sign? How can copysign be so much useful than sign?","**EDIT:**

Indeed there was a [patch](http://bugs.python.org/msg58786) which included `sign()` in [math](http://docs.python.org/library/math.html#math.copysign), but it wasn't accepted, because they didn't agree on [what it should return in all the edge cases](http://bugs.python.org/msg59137) (+/-0, +/-nan, etc)

So they decided to implement only copysign, which (although more verbose) can be [used to delegate to the end user the desired behavior for edge cases](http://bugs.python.org/msg59154) - which [sometimes might require the call to `cmp(x,0)`](http://bugs.python.org/msg59152).

---

I don't know why it's not a built-in, but I have some thoughts.

```
copysign(x,y):
Return x with the sign of y.

```

Most importantly, `copysign` is a superset of `sign`! Calling `copysign` with x=1 is the same as a `sign` function. So you could just use `copysign` and *forget about it*.

```
>>> math.copysign(1, -4)
-1.0
>>> math.copysign(1, 3)
1.0

```

If you get sick of passing two whole arguments, you can implement `sign` this way, and it will still be compatible with the IEEE stuff mentioned by others:

```
>>> sign = functools.partial(math.copysign, 1) # either of these
>>> sign = lambda x: math.copysign(1, x) # two will work
>>> sign(-4)
-1.0
>>> sign(3)
1.0
>>> sign(0)
1.0
>>> sign(-0.0)
-1.0
>>> sign(float('nan'))
-1.0

```

Secondly, usually when you want the sign of something, you just end up multiplying it with another value. And of course that's basically what `copysign` does.

So, instead of:

```
s = sign(a)
b = b * s

```

You can just do:

```
b = copysign(b, a)

```

And yes, I'm surprised you've been using Python for 7 years and think `cmp` could be so easily removed and replaced by `sign`! Have you never implemented a class with a `__cmp__` method? Have you never called `cmp` and specified a custom comparator function?

In summary, I've found myself wanting a `sign` function too, but `copysign` with the first argument being 1 will work just fine. I disagree that `sign` would be more useful than `copysign`, as I've shown that it's merely a subset of the same functionality."
"I need to replace some characters as follows: `&` ➔ `\&`, `#` ➔ `\#`, ...

I coded as follows, but I guess there should be some better way. Any hints?

```
strs = strs.replace('&', '\&')
strs = strs.replace('#', '\#')
...

```","Replacing two characters
========================

I timed all the methods in the current answers along with one extra.

With an input string of `abc&def#ghi` and replacing & -> \& and # -> \#, the fastest way was to chain together the replacements like this: `text.replace('&', '\&').replace('#', '\#')`.

Timings for each function:

* a) 1000000 loops, best of 3: 1.47 μs per loop
* b) 1000000 loops, best of 3: 1.51 μs per loop
* c) 100000 loops, best of 3: 12.3 μs per loop
* d) 100000 loops, best of 3: 12 μs per loop
* e) 100000 loops, best of 3: 3.27 μs per loop
* **f) 1000000 loops, best of 3: 0.817 μs per loop**
* g) 100000 loops, best of 3: 3.64 μs per loop
* **h) 1000000 loops, best of 3: 0.927 μs per loop**
* **i) 1000000 loops, best of 3: 0.814 μs per loop**

Here are the functions:

```
def a(text):
    chars = ""&#""
    for c in chars:
        text = text.replace(c, ""\\"" + c)


def b(text):
    for ch in ['&','#']:
        if ch in text:
            text = text.replace(ch,""\\""+ch)


import re
def c(text):
    rx = re.compile('([&#])')
    text = rx.sub(r'\\\1', text)


RX = re.compile('([&#])')
def d(text):
    text = RX.sub(r'\\\1', text)


def mk_esc(esc_chars):
    return lambda s: ''.join(['\\' + c if c in esc_chars else c for c in s])
esc = mk_esc('&#')
def e(text):
    esc(text)


def f(text):
    text = text.replace('&', '\&').replace('#', '\#')


def g(text):
    replacements = {""&"": ""\&"", ""#"": ""\#""}
    text = """".join([replacements.get(c, c) for c in text])


def h(text):
    text = text.replace('&', r'\&')
    text = text.replace('#', r'\#')


def i(text):
    text = text.replace('&', r'\&').replace('#', r'\#')

```

Timed like this:

```
python -mtimeit -s""import time_functions"" ""time_functions.a('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.b('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.c('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.d('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.e('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.f('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.g('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.h('abc&def#ghi')""
python -mtimeit -s""import time_functions"" ""time_functions.i('abc&def#ghi')""

```

---

Replacing 17 characters
=======================

Here's similar code to do the same but with more characters to escape (\`\*\_{}>#+-.!$):

```
def a(text):
    chars = ""\\`*_{}[]()>#+-.!$""
    for c in chars:
        text = text.replace(c, ""\\"" + c)


def b(text):
    for ch in ['\\','`','*','_','{','}','[',']','(',')','>','#','+','-','.','!','$','\'']:
        if ch in text:
            text = text.replace(ch,""\\""+ch)


import re
def c(text):
    rx = re.compile('([&#])')
    text = rx.sub(r'\\\1', text)


RX = re.compile('([\\`*_{}[]()>#+-.!$])')
def d(text):
    text = RX.sub(r'\\\1', text)


def mk_esc(esc_chars):
    return lambda s: ''.join(['\\' + c if c in esc_chars else c for c in s])
esc = mk_esc('\\`*_{}[]()>#+-.!$')
def e(text):
    esc(text)


def f(text):
    text = text.replace('\\', '\\\\').replace('`', '\`').replace('*', '\*').replace('_', '\_').replace('{', '\{').replace('}', '\}').replace('[', '\[').replace(']', '\]').replace('(', '\(').replace(')', '\)').replace('>', '\>').replace('#', '\#').replace('+', '\+').replace('-', '\-').replace('.', '\.').replace('!', '\!').replace('$', '\$')


def g(text):
    replacements = {
        ""\\"": ""\\\\"",
        ""`"": ""\`"",
        ""*"": ""\*"",
        ""_"": ""\_"",
        ""{"": ""\{"",
        ""}"": ""\}"",
        ""["": ""\["",
        ""]"": ""\]"",
        ""("": ""\("",
        "")"": ""\)"",
        "">"": ""\>"",
        ""#"": ""\#"",
        ""+"": ""\+"",
        ""-"": ""\-"",
        ""."": ""\."",
        ""!"": ""\!"",
        ""$"": ""\$"",
    }
    text = """".join([replacements.get(c, c) for c in text])


def h(text):
    text = text.replace('\\', r'\\')
    text = text.replace('`', r'\`')
    text = text.replace('*', r'\*')
    text = text.replace('_', r'\_')
    text = text.replace('{', r'\{')
    text = text.replace('}', r'\}')
    text = text.replace('[', r'\[')
    text = text.replace(']', r'\]')
    text = text.replace('(', r'\(')
    text = text.replace(')', r'\)')
    text = text.replace('>', r'\>')
    text = text.replace('#', r'\#')
    text = text.replace('+', r'\+')
    text = text.replace('-', r'\-')
    text = text.replace('.', r'\.')
    text = text.replace('!', r'\!')
    text = text.replace('$', r'\$')


def i(text):
    text = text.replace('\\', r'\\').replace('`', r'\`').replace('*', r'\*').replace('_', r'\_').replace('{', r'\{').replace('}', r'\}').replace('[', r'\[').replace(']', r'\]').replace('(', r'\(').replace(')', r'\)').replace('>', r'\>').replace('#', r'\#').replace('+', r'\+').replace('-', r'\-').replace('.', r'\.').replace('!', r'\!').replace('$', r'\$')

```

Here's the results for the same input string `abc&def#ghi`:

* a) 100000 loops, best of 3: 6.72 μs per loop
* b) **100000 loops, best of 3: 2.64 μs per loop**
* c) 100000 loops, best of 3: 11.9 μs per loop
* d) 100000 loops, best of 3: 4.92 μs per loop
* e) **100000 loops, best of 3: 2.96 μs per loop**
* f) 100000 loops, best of 3: 4.29 μs per loop
* g) 100000 loops, best of 3: 4.68 μs per loop
* h) 100000 loops, best of 3: 4.73 μs per loop
* i) 100000 loops, best of 3: 4.24 μs per loop

And with a longer input string (`## *Something* and [another] thing in a longer sentence with {more} things to replace$`):

* a) 100000 loops, best of 3: 7.59 μs per loop
* b) 100000 loops, best of 3: 6.54 μs per loop
* c) 100000 loops, best of 3: 16.9 μs per loop
* d) 100000 loops, best of 3: 7.29 μs per loop
* e) 100000 loops, best of 3: 12.2 μs per loop
* f) **100000 loops, best of 3: 5.38 μs per loop**
* g) 10000 loops, best of 3: 21.7 μs per loop
* h) **100000 loops, best of 3: 5.7 μs per loop**
* i) **100000 loops, best of 3: 5.13 μs per loop**

Adding a couple of variants:

```
def ab(text):
    for ch in ['\\','`','*','_','{','}','[',']','(',')','>','#','+','-','.','!','$','\'']:
        text = text.replace(ch,""\\""+ch)


def ba(text):
    chars = ""\\`*_{}[]()>#+-.!$""
    for c in chars:
        if c in text:
            text = text.replace(c, ""\\"" + c)

```

With the shorter input:

* ab) 100000 loops, best of 3: 7.05 μs per loop
* ba) 100000 loops, best of 3: 2.4 μs per loop

With the longer input:

* ab) 100000 loops, best of 3: 7.71 μs per loop
* ba) 100000 loops, best of 3: 6.08 μs per loop

So I'm going to use `ba` for readability and speed.

Addendum
========

Prompted by haccks in the comments, one difference between `ab` and `ba` is the `if c in text:` check. Let's test them against two more variants:

```
def ab_with_check(text):
    for ch in ['\\','`','*','_','{','}','[',']','(',')','>','#','+','-','.','!','$','\'']:
        if ch in text:
            text = text.replace(ch,""\\""+ch)

def ba_without_check(text):
    chars = ""\\`*_{}[]()>#+-.!$""
    for c in chars:
        text = text.replace(c, ""\\"" + c)

```

Times in μs per loop on Python 2.7.14 and 3.6.3, and on a different machine from the earlier set, so cannot be compared directly.

```
╭────────────╥──────┬───────────────┬──────┬──────────────────╮
│ Py, input  ║  ab  │ ab_with_check │  ba  │ ba_without_check │
╞════════════╬══════╪═══════════════╪══════╪══════════════════╡
│ Py2, short ║ 8.81 │    4.22       │ 3.45 │    8.01          │
│ Py3, short ║ 5.54 │    1.34       │ 1.46 │    5.34          │
├────────────╫──────┼───────────────┼──────┼──────────────────┤
│ Py2, long  ║ 9.3  │    7.15       │ 6.85 │    8.55          │
│ Py3, long  ║ 7.43 │    4.38       │ 4.41 │    7.02          │
└────────────╨──────┴───────────────┴──────┴──────────────────┘

```

We can conclude that:

* Those with the check are up to 4x faster than those without the check
* `ab_with_check` is slightly in the lead on Python 3, but `ba` (with check) has a greater lead on Python 2
* However, the biggest lesson here is **Python 3 is up to 3x faster than Python 2**! There's not a huge difference between the slowest on Python 3 and fastest on Python 2!"
"I am trying to install PIL (the Python Imaging Library) using the command:

```
sudo pip install pil

```

but I get the following message:

```
Downloading/unpacking PIL
  You are installing a potentially insecure and unverifiable file. Future versions of pip will default to disallowing insecure files.
  Downloading PIL-1.1.7.tar.gz (506kB): 506kB downloaded
  Running setup.py egg_info for package PIL
    WARNING: '' not a valid package name; please use only.-separated package names in setup.py
    
Installing collected packages: PIL
  Running setup.py install for PIL
    WARNING: '' not a valid package name; please use only.-separated package names in setup.py
    --- using frameworks at /System/Library/Frameworks
    building '_imaging' extension
    clang -fno-strict-aliasing -fno-common -dynamic -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch i386 -arch x86_64 -pipe -IlibImaging -I/System/Library/Frameworks/Python.framework/Versions/2.7/include -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c _imaging.c -o build/temp.macosx-10.8-intel-2.7/_imaging.o
    unable to execute clang: No such file or directory
    error: command 'clang' failed with exit status 1
    Complete output from command /usr/bin/python -c ""import setuptools;__file__='/private/tmp/pip_build_root/PIL/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-AYrxVD-record/install-record.txt --single-version-externally-managed:
    WARNING: '' not a valid package name; please use only.-separated package names in setup.py

running install

running build

.
.
.
.

copying PIL/XVThumbImagePlugin.py -> build/lib.macosx-10.8-intel-2.7

running build_ext

--- using frameworks at /System/Library/Frameworks

building '_imaging' extension

creating build/temp.macosx-10.8-intel-2.7

creating build/temp.macosx-10.8-intel-2.7/libImaging

clang -fno-strict-aliasing -fno-common -dynamic -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch i386 -arch x86_64 -pipe -IlibImaging -I/System/Library/Frameworks/Python.framework/Versions/2.7/include -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c _imaging.c -o build/temp.macosx-10.8-intel-2.7/_imaging.o

unable to execute clang: No such file or directory

error: command 'clang' failed with exit status 1

----------------------------------------
Cleaning upâ€¦

```

Could you please help me to install `PIL`?","1. Install Xcode and Xcode Command Line Tools as mentioned.
2. Use Pillow instead, as PIL is basically dead. Pillow is a maintained fork of PIL.

<https://pypi.org/project/Pillow/>

```
pip install Pillow

```

If you have both Pythons installed and want to install this for Python3:

```
python3 -m pip install Pillow

```"
"I'm using Python's `max` and `min` functions on lists for a minimax algorithm, and I need the index of the value returned by `max()` or `min()`. In other words, I need to know which move produced the max (at a first player's turn) or min (second player) value.

```
for i in range(9):
    new_board = current_board.new_board_with_move([i / 3, i % 3], player)

    if new_board:
        temp = min_max(new_board, depth + 1, not is_min_level)  
        values.append(temp)

if is_min_level:
    return min(values)
else:
    return max(values)

```

I need to be able to return the actual index of the min or max value, not just the value.","Say that you have a list `values = [3,6,1,5]`, and need the index of the smallest element, i.e. `index_min = 2` in this case.

Avoid the solution with `itemgetter()` presented in the other answers, and use instead

```
index_min = min(range(len(values)), key=values.__getitem__)

```

because it doesn't require to `import operator` nor to use `enumerate`, and it is always faster(benchmark below) than a solution using `itemgetter()`.

If you are dealing with numpy arrays or can afford `numpy` as a dependency, consider also using

```
import numpy as np
index_min = np.argmin(values)

```

This will be faster than the first solution even if you apply it to a pure Python list if:

* it is larger than a few elements (about 2\*\*4 elements on my machine)
* you can afford the memory copy from a pure list to a `numpy` array

as this benchmark points out:
[![enter image description here](https://i.sstatic.net/UTXzO.png)](https://i.sstatic.net/UTXzO.png)

I have run the benchmark on my machine with python 2.7 for the two solutions above (blue: pure python, first solution) (red, numpy solution) and for the standard solution based on `itemgetter()` (black, reference solution).
The same benchmark with python 3.5 showed that the methods compare exactly the same of the python 2.7 case presented above"
"I have the following DataFrame from a SQL query:

```
(Pdb) pp total_rows
     ColumnID  RespondentCount
0          -1                2
1  3030096843                1
2  3030096845                1

```

and I pivot it like this:

```
total_data = total_rows.pivot_table(cols=['ColumnID'])

```

which produces

```
(Pdb) pp total_data
ColumnID         -1            3030096843   3030096845
RespondentCount            2            1            1

[1 rows x 3 columns]

```

When I convert this dataframe into a dictionary (using `total_data.to_dict('records')[0]`), I get

```
{3030096843: 1, 3030096845: 1, -1: 2}

```

but I want to make sure the 303 columns are cast as strings instead of integers so that I get this:

```
{'3030096843': 1, '3030096845': 1, -1: 2}

```","One way to convert to string is to use [astype](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html):

```
total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)

```

However, perhaps you are looking for the [`to_json`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html) function, which will convert keys to valid json (and therefore your keys to strings):

```
In [11]: df = pd.DataFrame([['A', 2], ['A', 4], ['B', 6]])

In [12]: df.to_json()
Out[12]: '{""0"":{""0"":""A"",""1"":""A"",""2"":""B""},""1"":{""0"":2,""1"":4,""2"":6}}'

In [13]: df[0].to_json()
Out[13]: '{""0"":""A"",""1"":""A"",""2"":""B""}'

```

*Note: you can pass in a buffer/file to save this to, along with some other options...*"
"I am creating a database connection.
While trying to add to the DB, I am thinking of creating tuples out of information and then add them to the DB.

I am taking information from the user and store it in variables.
Can I add these variables into a tuple? Can you please help me with the syntax?

I only need the tuple to enter info into the DB. Once the information is added to the DB, should I delete the tuple? I mean I don't need the tuple anymore.","Tuples are immutable; you can't change which variables they contain after construction. However, you can concatenate or slice them to form new tuples:

```
a = (1, 2, 3)
b = a + (4, 5, 6)  # (1, 2, 3, 4, 5, 6)
c = b[1:]  # (2, 3, 4, 5, 6)

```

And, of course, build them from existing values:

```
name = ""Joe""
age = 40
location = ""New York""
joe = (name, age, location)

```"
"How can I get a string after a specific substring?

For example, I want to get the string after `""world""` in

```
my_string=""hello python world, I'm a beginner""

```

...which in this case is: `"", I'm a beginner""`)","The easiest way is probably just to split on your target word

```
my_string=""hello python world , i'm a beginner""
print(my_string.split(""world"",1)[1])

```

split takes the word (or character) to split on and optionally a limit to the number of splits.

In this example, split on ""world"" and limit it to only one split."
"I am opening a file which has 100,000 URL's. I need to send an HTTP request to each URL and print the status code. I am using Python 2.6, and so far looked at the many confusing ways Python implements threading/concurrency. I have even looked at the python [concurrence](http://opensource.hyves.org/concurrence) library, but cannot figure out how to write this program correctly. Has anyone come across a similar problem? I guess generally I need to know how to perform thousands of tasks in Python as fast as possible - I suppose that means 'concurrently'.","Twistedless solution:

```
from urlparse import urlparse
from threading import Thread
import httplib, sys
from Queue import Queue

concurrent = 200

def doWork():
    while True:
        url = q.get()
        status, url = getStatus(url)
        doSomethingWithResult(status, url)
        q.task_done()

def getStatus(ourl):
    try:
        url = urlparse(ourl)
        conn = httplib.HTTPConnection(url.netloc)   
        conn.request(""HEAD"", url.path)
        res = conn.getresponse()
        return res.status, ourl
    except:
        return ""error"", ourl

def doSomethingWithResult(status, url):
    print status, url

q = Queue(concurrent * 2)
for i in range(concurrent):
    t = Thread(target=doWork)
    t.daemon = True
    t.start()
try:
    for url in open('urllist.txt'):
        q.put(url.strip())
    q.join()
except KeyboardInterrupt:
    sys.exit(1)

```

This one is slighty faster than the twisted solution and uses less CPU."
"I'm starting to code in various projects using Python (including Django web development and Panda3D game development).

To help me understand what's going on, I would like to basically 'look' inside the Python objects to see how they tick - like their methods and properties.

So say I have a Python object, what would I need to print out its contents? Is that even possible?","Python has a strong set of introspection features.

Take a look at the following [built-in functions](http://docs.python.org/library/functions.html):

* [`type()`](https://docs.python.org/3/library/functions.html#type)
* [`dir()`](https://docs.python.org/3/library/functions.html#dir)
* [`id()`](https://docs.python.org/3/library/functions.html#id)
* [`getattr()`](https://docs.python.org/3/library/functions.html#getattr)
* [`hasattr()`](https://docs.python.org/3/library/functions.html#hasattr)
* [`globals()`](https://docs.python.org/3/library/functions.html#globals)
* [`locals()`](https://docs.python.org/3/library/functions.html#locals)
* [`callable()`](https://docs.python.org/3/library/functions.html#callable)

`type()` and `dir()` are particularly useful for inspecting the type of an object and its set of attributes, respectively."
"How do I remove NaN values from a NumPy array?

```
[1, 2, NaN, 4, NaN, 8]   ‚ü∂   [1, 2, 4, 8]

```","To remove NaN values from a NumPy array `x`:

```
x = x[~numpy.isnan(x)]

```

##### Explanation

The inner function `numpy.isnan` returns a boolean/logical array which has the value `True` everywhere that `x` is not-a-number. Since we want the opposite, we use the logical-not operator `~` to get an array with `True`s everywhere that `x` **is** a valid number.

Lastly, we use this logical array to index into the original array `x`, in order to retrieve just the non-NaN values."
"I have a very big file 4GB and when I try to read it my computer hangs.
So I want to read it piece by piece and after processing each piece store the processed piece into another file and read next piece.

Is there any method to `yield` these pieces ?

I would love to have a **lazy method**.","To write a lazy function, just use [`yield`](http://docs.python.org/tutorial/classes.html#generators):

```
def read_in_chunks(file_object, chunk_size=1024):
    """"""Lazy function (generator) to read a file piece by piece.
    Default chunk size: 1k.""""""
    while True:
        data = file_object.read(chunk_size)
        if not data:
            break
        yield data


with open('really_big_file.dat') as f:
    for piece in read_in_chunks(f):
        process_data(piece)

```

---

Another option would be to use [`iter`](http://docs.python.org/library/functions.html#iter) and a helper function:

```
f = open('really_big_file.dat')
def read1k():
    return f.read(1024)

for piece in iter(read1k, ''):
    process_data(piece)

```

---

If the file is line-based, the file object is already a lazy generator of lines:

```
for line in open('really_big_file.dat'):
    process_data(line)

```"
"If I have a numpy dtype, how do I automatically convert it to its closest python data type? For example,

```
numpy.float32 -> ""python float""
numpy.float64 -> ""python float""
numpy.uint32  -> ""python int""
numpy.int16   -> ""python int""

```

I could try to come up with a mapping of all of these cases, but does numpy provide some automatic way of converting its dtypes into the closest possible native python types? This mapping need not be exhaustive, but it should convert the common dtypes that have a close python analog. I think this already happens somewhere in numpy.","Use [`val.item()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.item.html) to convert most NumPy values to a native Python type:

```
import numpy as np

# for example, numpy.float32 -> python float
val = np.float32(0)
pyval = val.item()
print(type(pyval))         # <class 'float'>

# and similar...
type(np.float64(0).item()) # <class 'float'>
type(np.uint32(0).item())  # <class 'int'>
type(np.int16(0).item())   # <class 'int'>
type(np.cfloat(0).item())  # <class 'complex'>
type(np.datetime64(0, 'D').item())  # <class 'datetime.date'>
type(np.datetime64('2001-01-01 00:00:00').item())  # <class 'datetime.datetime'>
type(np.timedelta64(0, 'D').item()) # <class 'datetime.timedelta'>
...

```

(A related method [`np.asscalar(val)`](https://numpy.org/doc/1.16/reference/generated/numpy.asscalar.html) was deprecated with 1.16, and removed with 1.23).

---

For the curious, to build a table of conversions of [NumPy array scalars](https://numpy.org/doc/stable/reference/arrays.scalars.html) for your system:

```
for name in dir(np):
    obj = getattr(np, name)
    if hasattr(obj, 'dtype'):
        try:
            if 'time' in name:
                npn = obj(0, 'D')
            else:
                npn = obj(0)
            nat = npn.item()
            print('{0} ({1!r}) -> {2}'.format(name, npn.dtype.char, type(nat)))
        except:
            pass

```

There are a few NumPy types that have no native Python equivalent on some systems, including: `clongdouble`, `clongfloat`, `complex192`, `complex256`, `float128`, `longcomplex`, `longdouble` and `longfloat`. These need to be converted to their nearest NumPy equivalent before using `.item()`."
"I am making a scatter plot in matplotlib and need to change the background of the actual plot to black. I know how to change the face color of the plot using:

```
fig = plt.figure()
fig.patch.set_facecolor('xkcd:mint green')

```

[![enter image description here](https://i.sstatic.net/UN6YB.png)](https://i.sstatic.net/UN6YB.png)

My issue is that this changes the color of the space around the plot. How to I change the actual background color of the plot?","Use the **`set_facecolor(color)` method of the `axes` object**, which you've created one of the following ways:

* You created a figure and axis/es together

  ```
  fig, ax = plt.subplots(nrows=1, ncols=1)

  ```
* You created a figure, then axis/es later

  ```
  fig = plt.figure()
  ax = fig.add_subplot(1, 1, 1) # nrows, ncols, index

  ```
* You used the stateful API (if you're doing anything more than a few lines, and *especially* if you have multiple plots, the object-oriented methods above make life easier because you can refer to specific figures, plot on certain axes, and customize either)

  ```
  plt.plot(...)
  ax = plt.gca()

  ```

Then you can use `set_facecolor`:

```
ax.set_facecolor('xkcd:salmon')
ax.set_facecolor((1.0, 0.47, 0.42))

```

[![example plot with pink background on the axes](https://i.sstatic.net/2wFc6.png)](https://i.sstatic.net/2wFc6.png)

As a refresher for what colors can be:

> [matplotlib.colors](https://matplotlib.org/api/colors_api.html?highlight=color#module-matplotlib.colors)
> --------------------------------------------------------------------------------------------------------
>
> Matplotlib recognizes the following formats to specify a color:
>
> * an RGB or RGBA tuple of float values in `[0, 1]` (e.g., `(0.1, 0.2, 0.5)` or `(0.1, 0.2, 0.5, 0.3)`);
> * a hex RGB or RGBA string (e.g., `'#0F0F0F'` or `'#0F0F0F0F'`);
> * a string representation of a float value in `[0, 1]` inclusive for gray level (e.g., `'0.5'`);
> * one of `{'b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'}`;
> * a X11/CSS4 color name;
> * a name from the [xkcd color survey](https://xkcd.com/color/rgb/); prefixed with `'xkcd:'` (e.g., `'xkcd:sky blue'`);
> * one of `{'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'}` which are the Tableau Colors from the ‘T10’ categorical palette (which is the default color cycle);
> * a “CN” color spec, i.e. 'C' followed by a single digit, which is an index into the default property cycle (`matplotlib.rcParams['axes.prop_cycle']`); the indexing occurs at artist creation time and defaults to black if the cycle does not include color.
>
> All string specifications of color, other than “CN”, are case-insensitive."
The `string.replace()` is deprecated on python 3.x. What is the new way of doing this?,"As in 2.x, use [`str.replace()`](https://docs.python.org/library/stdtypes.html#str.replace).

Example:

```
>>> 'Hello world'.replace('world', 'Guido')
'Hello Guido'

```"
"How do I find the arithmetic mean of a list in Python? For example:

```
[1, 2, 3, 4]  ‚ü∂  2.5

```","For Python 3.8+, use [`statistics.fmean`](https://docs.python.org/3/library/statistics.html#statistics.fmean) for numerical stability with floats. (Fast.)

For Python 3.4+, use [`statistics.mean`](https://docs.python.org/3/library/statistics.html#statistics.mean) for numerical stability with floats. (Slower.)

```
xs = [15, 18, 2, 36, 12, 78, 5, 6, 9]

import statistics
statistics.mean(xs)  # = 20.11111111111111

```

For older versions of Python 3, use

```
sum(xs) / len(xs)

```

For Python 2, convert `len` to a float to get float division:

```
sum(xs) / float(len(xs))

```"
"I'd like to get from this:

```
keys = [1,2,3]

```

to this:

```
{1: None, 2: None, 3: None}

```

Is there a pythonic way of doing it?

This is an ugly way to do it:

```
>>> keys = [1,2,3]
>>> dict([(1,2)])
{1: 2}
>>> dict(zip(keys, [None]*len(keys)))
{1: None, 2: None, 3: None}

```","[`dict.fromkeys`](https://docs.python.org/3/library/stdtypes.html#dict.fromkeys) directly solves the problem:

```
>>> dict.fromkeys([1, 2, 3, 4])
{1: None, 2: None, 3: None, 4: None}

```

This is actually a classmethod, so it works for dict-subclasses (like `collections.defaultdict`) as well.

The optional second argument, which defaults to `None`, specifies the value to use for the keys. Note that the *same object* will be used for each key, which can cause problems with mutable values:

```
>>> x = dict.fromkeys([1, 2, 3, 4], [])
>>> x[1].append('test')
>>> x
{1: ['test'], 2: ['test'], 3: ['test'], 4: ['test']}

```

If this is unacceptable, see [How can I initialize a dictionary whose values are distinct empty lists?](https://stackoverflow.com/questions/11509721) for a workaround."
How can I check which version of NumPy I'm using?,"```
import numpy
numpy.version.version

```"
"How do I count only the *files* in a directory? This counts the directory itself as a file:

```
len(glob.glob('*'))

```","`os.listdir()` will be slightly more efficient than using `glob.glob`. To test if a filename is an ordinary file (and not a directory or other entity), use `os.path.isfile()`:

```
import os, os.path

# simple version for working with CWD
print len([name for name in os.listdir('.') if os.path.isfile(name)])

# path joining version for other paths
DIR = '/tmp'
print len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])

```"
"Python will not run in git bash (Windows). When I type python in the command line, it takes me to a blank line without saying that it has entered python 2.7.10 like its does in Powershell. It doesn't give me an error message, but python just doesn't run.

I have already made sure the environmental variables in PATH included `c:\python27`. What else can I check?

---

A session wherein this issue occurs looks like the following:

```
user@hostname MINGW64 ~
$ type python
python is /c/Python27/python

user@hostname MINGW64 ~
$ python

```

...sitting there without returning to the prompt.","Temporary solution
==================

Just enter this in your git shell on windows - > `alias python='winpty python.exe'`, that is all and you are going to have alias to the python executable. This alias will be valid for the duration of the shell session.

[winpty](https://github.com/rprichard/winpty) is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.

Permanent solution
==================

Add the command to your `.bashrc` in the users home directory. You can use the CLI or a text editor:

Using CLI
---------

This can be accomplished from git bash like so:

```
echo ""alias python='winpty python.exe'"" >> ~/.bashrc

```

which will create `.bashrc` in the current users home directory if the file doesn't exist or append the alias to the end of `.bashrc` if it does.

Using a text editor
-------------------

Alternatively, you could first create a `.bashrc`. Depending on your file manager, this may be easier to accomplish in git bash like so:

```
cd ~
touch .bashrc

```

At which point you can open `.bashrc` in your prefered text editor and add it there.

To apply the change, either use the command `source .bashrc` or restart the shell.

Update
======

Newer versions of Git no longer use `.bashrc` but instead use `.bash_profile`. Conda also uses this profile when initializing, so be sure not to overwrite or delete the initialization block. See more here: [Git for Windows doesn't execute my .bashrc file](https://stackoverflow.com/questions/32186840/git-for-windows-doesnt-execute-my-bashrc-file/32189255#32189255)."
"I'm trying to extract the text included in [this](https://www.dropbox.com/s/4qad66r2361hvmu/sample.pdf?dl=1) PDF file using `Python`.

I'm using the [PyPDF2](https://pypdf2.readthedocs.io/en/latest/) package (version 1.27.2), and have the following script:

```
import PyPDF2

with open(""sample.pdf"", ""rb"") as pdf_file:
    read_pdf = PyPDF2.PdfFileReader(pdf_file)
    number_of_pages = read_pdf.getNumPages()
    page = read_pdf.pages[0]
    page_content = page.extractText()
print(page_content)

```

When I run the code, I get the following output which is different from that included in the PDF document:

```
 ! "" # $ % # $ % &% $ &' ( ) * % + , - % . / 0 1 ' * 2 3% 4
5
 ' % 1 $ # 2 6 % 3/ % 7 / ) ) / 8 % &) / 2 6 % 8 # 3"" % 3"" * % 31 3/ 9 # &)
%

```

How can I extract the text as is in the PDF document?","I was looking for a simple solution to use for python 3.x and windows. There doesn't seem to be support from [textract](http://textract.readthedocs.io/en/latest/), which is unfortunate, but if you are looking for a simple solution for windows/python 3 checkout the [tika](https://github.com/chrismattmann/tika-python) package, really straight forward for reading pdfs.

> Tika-Python is a Python binding to the Apache Tikaâ„¢ REST services allowing Tika to be called natively in the Python community.

```
from tika import parser # pip install tika

raw = parser.from_file('sample.pdf')
print(raw['content'])

```

Note that Tika is written in Java so you will need a Java runtime installed."
"I am trying to understand if it makes sense to take the content of a list and append it to another list.

I have the first list created through a loop function, that will get specific lines out of a file and will save them in a list.

Then a second list is used to save these lines, and start a new cycle over another file.

My idea was to get the list once that the for cycle is done, dump it into the second list, then start a new cycle, dump the content of the first list again into the second but appending it, so the second list will be the sum of all the smaller list files created in my loop. The list has to be appended only if certain conditions met.

It looks like something similar to this:

```
# This is done for each log in my directory, i have a loop running
for logs in mydir:

    for line in mylog:
        #...if the conditions are met
        list1.append(line)

    for item in list1:
        if ""string"" in item: #if somewhere in the list1 i have a match for a string
            list2.append(list1) # append every line in list1 to list2
            del list1 [:] # delete the content of the list1
            break
        else:
            del list1 [:] # delete the list content and start all over

```

Does this makes sense or should I go for a different route?

I need something efficient that would not take up too many cycles, since the list of logs is long and each text file is pretty big; so I thought that the lists would fit the purpose.","You probably want

```
list2.extend(list1)

```

instead of

```
list2.append(list1)

```

Here's the difference:

```
>>> a = [1, 2, 3]
>>> b = [4, 5, 6]
>>> c = [7, 8, 9]
>>> b.append(a)
>>> b
[4, 5, 6, [1, 2, 3]]
>>> c.extend(a)
>>> c
[7, 8, 9, 1, 2, 3]

```

Since `list.extend()` accepts an arbitrary iterable, you can also replace

```
for line in mylog:
    list1.append(line)

```

by

```
list1.extend(mylog)

```"
"Is there any significant difference between the two Python keywords `continue` and `pass` like in the examples

```
for element in some_list:
    if not element:
        pass

```

and

```
for element in some_list:
    if not element:
        continue

```

I should be aware of?","Yes, they do completely different things. `pass` simply does nothing, while `continue` goes on with the next loop iteration. In your example, the difference would become apparent if you added another statement after the `if`: After executing `pass`, this further statement would be executed. After `continue`, it wouldn't.

```
>>> a = [0, 1, 2]
>>> for element in a:
...     if not element:
...         pass
...     print(element)
... 
0
1
2
>>> for element in a:
...     if not element:
...         continue
...     print(element)
... 
1
2

```"
"I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using `dataframe.corr()` function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?","You can use [`pyplot.matshow()`](http://matplotlib.org/examples/pylab_examples/matshow.html) from `matplotlib`:

```
import matplotlib.pyplot as plt

plt.matshow(dataframe.corr())
plt.show()

```

---

Edit:

In the comments was a request for how to change the axis tick labels. Here's a deluxe version that is drawn on a bigger figure size, has axis labels to match the dataframe, and a colorbar legend to interpret the color scale.

I'm including how to adjust the size and rotation of the labels, and I'm using a figure ratio that makes the colorbar and the main figure come out the same height.

---

EDIT 2:
As the df.corr() method ignores non-numerical columns, `.select_dtypes(['number'])` should be used when defining the x and y labels to avoid an unwanted shift of the labels (included in the code below).

```
f = plt.figure(figsize=(19, 15))
plt.matshow(df.corr(), fignum=f.number)
plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)
plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
plt.title('Correlation Matrix', fontsize=16);

```

[![correlation plot example](https://i.sstatic.net/XfvsR.png)](https://i.sstatic.net/XfvsR.png)"
"I'm parsing some HTML with Beautiful Soup 3, but it contains HTML entities which Beautiful Soup 3 doesn't automatically decode for me:

```
>>> from BeautifulSoup import BeautifulSoup

>>> soup = BeautifulSoup(""<p>&pound;682m</p>"")
>>> text = soup.find(""p"").string

>>> print text
&pound;682m

```

How can I decode the HTML entities in `text` to get `""Â£682m""` instead of `""&pound;682m""`.","### Python 3.4+

Use [`html.unescape()`](https://docs.python.org/3/library/html.html#html.unescape):

```
import html
print(html.unescape('&pound;682m'))

```

FYI `html.parser.HTMLParser.unescape` is deprecated, and [was supposed to be removed in 3.5](https://github.com/python/cpython/blob/3.5/Lib/html/parser.py#L466-L470), although it was left in by mistake. It will be removed from the language soon.

---

### Python 2.6-3.3

You can use `HTMLParser.unescape()` from the standard library:

* For Python 2.6-2.7 it's in [`HTMLParser`](https://docs.python.org/2/library/htmlparser.html)
* For Python 3 it's in [`html.parser`](https://docs.python.org/3/library/html.parser.html)

```
>>> try:
...     # Python 2.6-2.7 
...     from HTMLParser import HTMLParser
... except ImportError:
...     # Python 3
...     from html.parser import HTMLParser
... 
>>> h = HTMLParser()
>>> print(h.unescape('&pound;682m'))
£682m

```

You can also use the [`six`](https://pythonhosted.org/six/) compatibility library to simplify the import:

```
>>> from six.moves.html_parser import HTMLParser
>>> h = HTMLParser()
>>> print(h.unescape('&pound;682m'))
£682m

```"
"I'd like to use a variable inside a regex, how can I do this in Python?

```
TEXTO = sys.argv[1]

if re.search(r""\b(?=\w)TEXTO\b(?!\w)"", subject, re.IGNORECASE):
    # Successful match
else:
    # Match attempt failed

```","You have to build the regex as a string:

```
TEXTO = sys.argv[1]
my_regex = r""\b(?=\w)"" + re.escape(TEXTO) + r""\b(?!\w)""

if re.search(my_regex, subject, re.IGNORECASE):
    etc.

```

Note the use of `re.escape` so that if your text has special characters, they won't be interpreted as such."
How do I find the duplicates in a list of integers and create another list of the duplicates?,"To remove duplicates use `set(a)`. To print duplicates, something like:

```
a = [1,2,3,2,1,5,6,5,5,5]

import collections
print([item for item, count in collections.Counter(a).items() if count > 1])

## [1, 2, 5]

```

Note that `Counter` is not particularly efficient ([timings](https://stackoverflow.com/a/25706298/989121)) and probably overkill here. `set` will perform better. This code computes a list of unique elements in the source order:

```
seen = set()
uniq = []
for x in a:
    if x not in seen:
        uniq.append(x)
        seen.add(x)

```

or, more concisely:

```
seen = set()
uniq = [x for x in a if x not in seen and not seen.add(x)]    

```

I don't recommend the latter style, because it is not obvious what `not seen.add(x)` is doing (the set `add()` method always returns `None`, hence the need for `not`).

To compute the list of duplicated elements without libraries:

```
seen = set()
dupes = set()

for x in a:
    if x in seen:
        dupes.add(x)
    else:
        seen.add(x)

```

or, more concisely:

```
seen = set()
dupes = {x for x in a if x in seen or seen.add(x)}

```

If list elements are not hashable, you cannot use sets/dicts and have to resort to a quadratic time solution (compare each with each). For example:

```
a = [[1], [2], [3], [1], [5], [3]]

no_dupes = [x for n, x in enumerate(a) if x not in a[:n]]
print no_dupes # [[1], [2], [3], [5]]

dupes = [x for n, x in enumerate(a) if x in a[:n]]
print dupes # [[1], [3]]

```"
"What's the cleanest way to test if a dictionary contains a key?

```
x = {'a' : 1, 'b' : 2}
if (x.contains_key('a')):
    ....

```","```
'a' in x

```

and a quick search reveals some nice information about it: <http://docs.python.org/3/tutorial/datastructures.html#dictionaries>"
"I've recently seen the `--no-cache-dir` being used in a Docker file. I've never seen that flag before and the help is not explaining it:

```
 --no-cache-dir              Disable the cache.

```

1. Question: **What is cached?**
2. Question: **What is the cache used for?**
3. Question: **Why would I want to disable it?**","1. **Cached is**: store away in hiding or for future use
2. **Used for**

* store the installation files(`.whl`, etc) of the modules that you install through pip
* store the source files (`.tar.gz`, etc) to avoid re-download when not expired

3. **Possible Reason** you might want to disable cache:

* you don't have space on your hard drive
* previously run `pip install` with **unexpected** settings
  + eg:
    - previously run `export PYCURL_SSL_LIBRARY=nss` and `pip install pycurl`
    - want new run `export PYCURL_SSL_LIBRARY=openssl` and `pip install pycurl --compile --no-cache-dir`
* you want to keep a Docker image as small as possible

**Links to documentation**

<https://pip.pypa.io/en/stable/topics/caching>"
"I would like to create a copy of an object. I want the new object to possess all properties of the old object (values of the fields). But I want to have independent objects. So, if I change values of the fields of the new object, the old object should not be affected by that.","To get a fully independent copy of an object you can use the [`copy.deepcopy()`](http://docs.python.org/library/copy.html#copy.deepcopy) function.

For more details about shallow and deep copying please refer to the other answers to this question and the nice explanation in [this answer to a related question](https://stackoverflow.com/questions/3975376/understanding-dict-copy-shallow-or-deep/3975388#3975388)."
"I am currently using Beautiful Soup to parse an HTML file and calling `get_text()`, but it seems like I'm being left with a lot of \xa0 Unicode representing spaces. Is there an efficient way to remove all of them in Python 2.7, and change them into spaces? I guess the more generalized question would be, is there a way to remove Unicode formatting?

I tried using: `line = line.replace(u'\xa0',' ')`, as suggested by another thread, but that changed the \xa0's to u's, so now I have ""u""s everywhere instead. ):

EDIT: The problem seems to be resolved by `str.replace(u'\xa0', ' ').encode('utf-8')`, but just doing `.encode('utf-8')` without `replace()` seems to cause it to spit out even weirder characters, \xc2 for instance. Can anyone explain this?","\xa0 is actually non-breaking space in Latin1 (ISO 8859-1), also chr(160). You should replace it with a space.

`string = string.replace(u'\xa0', u' ')`

When .encode('utf-8'), it will encode the unicode to utf-8, that means every unicode could be represented by 1 to 4 bytes. For this case, \xa0 is represented by 2 bytes \xc2\xa0.

Read up on <http://docs.python.org/howto/unicode.html>.

Please note: this answer in from 2012, Python has moved on, you should be able to use `unicodedata.normalize` now"
"Recently I started playing around with Python and I came around something peculiar in the way closures work. Consider the following code:

```
adders = [None, None, None, None]

for i in [0, 1, 2, 3]:
   adders[i] = lambda a: i+a

print adders[1](3)

```

It builds a simple array of functions that take a single input and return that input added by a number. The functions are constructed in `for` loop where the iterator `i` runs from `0` to `3`. For each of these numbers a `lambda` function is created which captures `i` and adds it to the function's input. The last line calls the second `lambda` function with `3` as a parameter. To my surprise the output was `6`.

I expected a `4`. My reasoning was: in Python everything is an object and thus every variable is essential a pointer to it. When creating the `lambda` closures for `i`, I expected it to store a pointer to the integer object currently pointed to by `i`. That means that when `i` assigned a new integer object it shouldn't effect the previously created closures. Sadly, inspecting the `adders` array within a debugger shows that it does. All `lambda` functions refer to the last value of `i`, `3`, which results in `adders[1](3)` returning `6`.

Which make me wonder about the following:

* What do the closures capture exactly?
* What is the most elegant way to convince the `lambda` functions to capture the current value of `i` in a way that will not be affected when `i` changes its value?

---

For a more accessible, practical version of the question, specific to the case where a loop (or list comprehension, generator expression etc.) is used, see [Creating functions (or lambdas) in a loop (or comprehension)](https://stackoverflow.com/questions/3431676/creating-functions-in-a-loop). This question is focused on understanding the underlying behaviour of the code in Python.

If you got here trying to fix a problem with making buttons in Tkinter, try [tkinter creating buttons in for loop passing command arguments](https://stackoverflow.com/questions/10865116/) for more specific advice.

See [What exactly is contained within a obj.\_\_closure\_\_?](https://stackoverflow.com/questions/14413946/) for technical details of **how** Python implements closures. See [What is the difference between Early and Late Binding?](https://stackoverflow.com/questions/10580) for related terminology discussion.","you may force the capture of a variable using an argument with a default value:

```
>>> for i in [0,1,2,3]:
...    adders[i]=lambda a,i=i: i+a  # note the dummy parameter with a default value
...
>>> print( adders[1](3) )
4

```

the idea is to declare a parameter (cleverly named `i`) and give it a default value of the variable you want to capture (the value of `i`)"
"I want to import `foo-bar.py`, this works:

```
foobar = __import__(""foo-bar"")

```

This does not:

```
from ""foo-bar"" import *

```

My question: Is there any way that I can use the above format i.e., `from ""foo-bar"" import *` to import a module that has a `-` in it?","Starting from Python 3.1, you can use importlib :

```
import importlib  
foobar = importlib.import_module(""foo-bar"")

```

( <https://docs.python.org/3/library/importlib.html> )"
"I'm wondering if there's any difference between the code fragment

```
from urllib import request

```

and the fragment

```
import urllib.request

```

or if they are interchangeable. If they are interchangeable, which is the ""standard""/""preferred"" syntax (if there is one)?","It depends on how you want to access the import when you refer to it.

```
from urllib import request
# access request directly.
mine = request()

import urllib.request
# used as urllib.request
mine = urllib.request()

```

You can also alias things yourself when you import for simplicity or to avoid masking built ins:

```
from os import open as open_
# lets you use os.open without destroying the 
# built in open() which returns file handles.

```"
"I've got a field in one model like:

```
class Sample(models.Model):
    date = fields.DateField(auto_now=False)

```

Now, I need to filter the objects by a date range.

How do I filter all the objects that have a date between `1-Jan-2011` and `31-Jan-2011`?","Use

```
Sample.objects.filter(date__range=[""2011-01-01"", ""2011-01-31""])

```

Or if you are just trying to filter month wise:

```
Sample.objects.filter(date__year='2011', 
                      date__month='01')

```

### Edit

As Bernhard Vallant said, if you want a queryset which excludes the `specified range ends` you should consider [his solution](https://stackoverflow.com/questions/4668619/django-database-query-how-to-filter-objects-by-date-range/4668703#4668703), which utilizes gt/lt (greater-than/less-than)."
"This question is motivated by my another question: [How to await in cdef?](https://stackoverflow.com/questions/48989065/how-to-await-in-cdef)

There are tons of articles and blog posts on the web about `asyncio`, but they are all very superficial. I couldn't find any information about how `asyncio` is actually implemented, and what makes I/O asynchronous. I was trying to read the source code, but it's thousands of lines of not the highest grade C code, a lot of which deals with auxiliary objects, but most crucially, it is hard to connect between Python syntax and what C code it would translate into.

Asycnio's own documentation is even less helpful. There's no information there about how it works, only some guidelines about how to use it, which are also sometimes misleading / very poorly written.

I'm familiar with Go's implementation of coroutines, and was kind of hoping that Python did the same thing. If that was the case, the code I came up in the post linked above would have worked. Since it didn't, I'm now trying to figure out why. My best guess so far is as follows, please correct me where I'm wrong:

1. Procedure definitions of the form `async def foo(): ...` are actually interpreted as methods of a class inheriting `coroutine`.
2. Perhaps, `async def` is actually split into multiple methods by `await` statements, where the object, on which these methods are called is able to keep track of the progress it made through the execution so far.
3. If the above is true, then, essentially, execution of a coroutine boils down to calling methods of coroutine object by some global manager (loop?).
4. The global manager is somehow (how?) aware of when I/O operations are performed by Python (only?) code and is able to choose one of the pending coroutine methods to execute after the current executing method relinquished control (hit on the `await` statement).

In other words, here's my attempt at ""desugaring"" of some `asyncio` syntax into something more understandable:

```
async def coro(name):
    print('before', name)
    await asyncio.sleep()
    print('after', name)

asyncio.gather(coro('first'), coro('second'))

# translated from async def coro(name)
class Coro(coroutine):
    def before(self, name):
        print('before', name)

    def after(self, name):
        print('after', name)

    def __init__(self, name):
        self.name = name
        self.parts = self.before, self.after
        self.pos = 0

    def __call__():
        self.parts[self.pos](self.name)
        self.pos += 1

    def done(self):
        return self.pos == len(self.parts)


# translated from asyncio.gather()
class AsyncIOManager:

    def gather(*coros):
        while not every(c.done() for c in coros):
            coro = random.choice(coros)
            coro()

```

Should my guess prove correct: then I have a problem. How does I/O actually happen in this scenario? In a separate thread? Is the whole interpreter suspended and I/O happens outside the interpreter? What exactly is meant by I/O? If my python procedure called C `open()` procedure, and it in turn sent interrupt to kernel, relinquishing control to it, how does Python interpreter know about this and is able to continue running some other code, while kernel code does the actual I/O and until it wakes up the Python procedure which sent the interrupt originally? How can Python interpreter in principle, be aware of this happening?","How does asyncio work?
======================

Before answering this question we need to understand a few base terms, skip these if you already know any of them.

[Generators](https://wiki.python.org/moin/Generators)
-----------------------------------------------------

Generators are objects that allow us to suspend the execution of a python function. User curated generators are implemented using the keyword [**`yield`**](https://docs.python.org/3/reference/expressions.html#yield-expressions). By creating a normal function containing the `yield` keyword, we turn that function into a generator:

```
>>> def test():
...     yield 1
...     yield 2
...
>>> gen = test()
>>> next(gen)
1
>>> next(gen)
2
>>> next(gen)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
StopIteration

```

As you can see, calling [`next()`](https://docs.python.org/3/library/functions.html#next) on the generator causes the interpreter to load the test's frame, and return the `yield`ed value. Calling `next()` again, causes the frame to load again into the interpreter stack, and continues on `yield`ing another value.

By the third time `next()` is called, our generator was finished, and [`StopIteration`](https://docs.python.org/3/library/exceptions.html#StopIteration) was thrown.

### Communicating with a generator

A less-known feature of generators is the fact that you can communicate with them using two methods: [`send()`](https://docs.python.org/3/reference/expressions.html#generator.send) and [`throw()`](https://docs.python.org/3/reference/expressions.html#generator.throw).

```
>>> def test():
...     val = yield 1
...     print(val)
...     yield 2
...     yield 3
...
>>> gen = test()
>>> next(gen)
1
>>> gen.send(""abc"")
abc
2
>>> gen.throw(Exception())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 4, in test
Exception

```

Upon calling `gen.send()`, the value is passed as a return value from the `yield` keyword.

`gen.throw()` on the other hand, allows throwing Exceptions inside generators, with the exception raised at the same spot `yield` was called.

### Returning values from generators

Returning a value from a generator, results in the value being put inside the `StopIteration` exception. We can later on recover the value from the exception and use it to our needs.

```
>>> def test():
...     yield 1
...     return ""abc""
...
>>> gen = test()
>>> next(gen)
1
>>> try:
...     next(gen)
... except StopIteration as exc:
...     print(exc.value)
...
abc

```

Behold, a new keyword: `yield from`
-----------------------------------

Python 3.4 came with the addition of a new keyword: [`yield from`](https://docs.python.org/3/reference/expressions.html#yield-expressions). What that keyword allows us to do, is pass on any `next()`, `send()` and `throw()` into an inner-most nested generator. If the inner generator returns a value, it is also the return value of `yield from`:

```
>>> def inner():
...     inner_result = yield 2
...     print('inner', inner_result)
...     return 3
...
>>> def outer():
...     yield 1
...     val = yield from inner()
...     print('outer', val)
...     yield 4
...
>>> gen = outer()
>>> next(gen)
1
>>> next(gen) # Goes inside inner() automatically
2
>>> gen.send(""abc"")
inner abc
outer 3
4

```

I've written [an article](https://towardsdatascience.com/cpython-internals-how-do-generators-work-ba1c4405b4bc) to further elaborate on this topic.

Putting it all together
-----------------------

Upon introducing the new keyword `yield from` in Python 3.4, we were now able to create generators inside generators that just like a tunnel, pass the data back and forth from the inner-most to the outer-most generators. This has spawned a new meaning for generators - *coroutines*.

**Coroutines** are functions that can be stopped and resumed while being run. In Python, they are defined using the **[`async def`](https://docs.python.org/3/reference/compound_stmts.html#coroutine-function-definition)** keyword. Much like generators, they too use their own form of `yield from` which is **[`await`](https://docs.python.org/3/reference/expressions.html#await)**. Before `async` and `await` were introduced in Python 3.5, we created coroutines in the exact same way generators were created (with `yield from` instead of `await`).

```
async def inner():
    return 1
    
async def outer():
    await inner()

```

Just like all iterators and generators implement the `__iter__()` method, all coroutines implement `__await__()` which allows them to continue on every time `await coro` is called.

There's a nice [sequence diagram](https://docs.python.org/3.5/_images/tulip_coro.png) inside the [Python docs](https://docs.python.org/3.5/library/asyncio-task.html#example-chain-coroutines) that you should check out.

In asyncio, apart from coroutine functions, we have 2 important objects: **tasks** and **futures**.

### [Futures](https://docs.python.org/3/library/asyncio-task.html#asyncio.Future)

Futures are objects that have the `__await__()` method implemented, and their job is to hold a certain state and result. The state can be one of the following:

1. PENDING - future does not have any result or exception set.
2. CANCELLED - future was cancelled using `fut.cancel()`
3. FINISHED - future was finished, either by a result set using [`fut.set_result()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.Future.set_result) or by an exception set using [`fut.set_exception()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.Future.set_exception)

The result, just like you have guessed, can either be a Python object, that will be returned, or an exception which may be raised.

Another **important** feature of `future` objects, is that they contain a method called **[`add_done_callback()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.Future.add_done_callback)**. This method allows functions to be called as soon as the task is done - whether it raised an exception or finished.

### [Tasks](https://docs.python.org/3/library/asyncio-task.html#task)

Task objects are special futures, which wrap around coroutines, and communicate with the inner-most and outer-most coroutines. Every time a coroutine `await`s a future, the future is passed all the way back to the task (just like in `yield from`), and the task receives it.

Next, the task binds itself to the future. It does so by calling `add_done_callback()` on the future. From now on, if the future will ever be done, by either being cancelled, passed an exception or passed a Python object as a result, the task's callback will be called, and it will rise back up to existence.

Asyncio
=======

The final burning question we must answer is - how is the IO implemented?

Deep inside asyncio, we have an event loop. An event loop of tasks. The event loop's job is to call tasks every time they are ready and coordinate all that effort into one single working machine.

The IO part of the event loop is built upon a single crucial function called **[`select`](https://docs.python.org/3/library/select.html#module-select)**. Select is a blocking function, implemented by the operating system underneath, that allows waiting on sockets for incoming or outgoing data. Upon receiving data it wakes up, and returns the sockets which received data, or the sockets which are ready for writing.

When you try to receive or send data over a socket through asyncio, what actually happens below is that the socket is first checked if it has any data that can be immediately read or sent. If its `.send()` buffer is full, or the `.recv()` buffer is empty, the socket is registered to the `select` function (by simply adding it to one of the lists, `rlist` for `recv` and `wlist` for `send`) and the appropriate function `await`s a newly created `future` object, tied to that socket.

When all available tasks are waiting for futures, the event loop calls `select` and waits. When one of the sockets has incoming data, or its `send` buffer drained up, asyncio checks for the future object tied to that socket, and sets it to done.

Now all the magic happens. The future is set to done, the task that added itself before with `add_done_callback()` rises up back to life, and calls `.send()` on the coroutine which resumes the inner-most coroutine (because of the `await` chain) and you read the newly received data from a nearby buffer it was spilled unto.

**Method chain again, in case of `recv()`:**

1. `select.select` waits.
2. A ready socket, with data is returned.
3. Data from the socket is moved into a buffer.
4. `future.set_result()` is called.
5. Task that added itself with `add_done_callback()` is now woken up.
6. Task calls `.send()` on the coroutine which goes all the way into the inner-most coroutine and wakes it up.
7. Data is being read from the buffer and returned to our humble user.

In summary, asyncio uses generator capabilities, that allow pausing and resuming functions. It uses `yield from` capabilities that allow passing data back and forth from the inner-most generator to the outer-most. It uses all of those in order to halt function execution while it's waiting for IO to complete (by using the OS `select` function).

And the best of all? While one function is paused, another may run and interleave with the delicate fabric, which is asyncio."
"How can I see what's inside a bucket in S3 with `boto3`? (i.e. do an `""ls""`)?

Doing the following:

```
import boto3
s3 = boto3.resource('s3')
my_bucket = s3.Bucket('some/path/')

```

returns:

```
s3.Bucket(name='some/path/')

```

How do I see its contents?","One way to see the contents would be:

```
for my_bucket_object in my_bucket.objects.all():
    print(my_bucket_object)

```"
"I'm not sure if this is Flask specific, but when I run an app in dev mode (`http://localhost:5000`), I cannot access it from other machines on the network (with `http://[dev-host-ip]:5000`). With Rails in dev mode, for example, it works fine. I couldn't find any docs regarding the Flask dev server configuration. Any idea what should be configured to enable this?","While this is possible, you should not use the Flask dev server in production. The Flask dev server is not designed to be particularly secure, stable, or efficient. See the docs on [deploying](https://flask.palletsprojects.com/deploying/index.html) for correct solutions.

---

The `--host` option to `flask run`, or the `host` parameter to `app.run()`, controls what address the development server listens to. By default it runs on `localhost`, change it to `flask run --host=0.0.0.0` (or `app.run(host=""0.0.0.0"")`) to run on all your machine's IP addresses.

`0.0.0.0` is a special value that you can't use in the browser directly, you'll need to navigate to the actual IP address of the machine on the network. You may also need to adjust your firewall to allow external access to the port.

The Flask [quickstart docs](https://flask.palletsprojects.com/quickstart/#a-minimal-application) explain this in the ""Externally Visible Server"" section:

> If you run the server you will notice that the server is only
> accessible from your own computer, not from any other in the network.
> This is the default because in debugging mode a user of the
> application can execute arbitrary Python code on your computer.
>
> If you have the debugger disabled or trust the users on your network,
> you can make the server publicly available simply by adding
> `--host=0.0.0.0` to the command line:
>
> ```
> $ flask run --host=0.0.0.0
>
> ```
>
> This tells your operating system to listen on all public IPs."
"I am using selenium with python and have downloaded the chromedriver for my windows computer from this site: <http://chromedriver.storage.googleapis.com/index.html?path=2.15/>

After downloading the zip file, I unpacked the zip file to my downloads folder. Then I put the path to the executable binary (C:\Users\michael\Downloads\chromedriver\_win32) into the Environment Variable ""Path"".

However, when I run the following code:

```
from selenium import webdriver

driver = webdriver.Chrome()

```

... I keep getting the following error message:

```
WebDriverException: Message: 'chromedriver' executable needs to be available in the path. Please look at     http://docs.seleniumhq.org/download/#thirdPartyDrivers and read up at http://code.google.com/p/selenium/wiki/ChromeDriver

```

But - as explained above - the executable is(!) in the path ... what is going on here?","I see the discussions still talk about the old way of setting up chromedriver by downloading the binary and configuring the path manually.

This can be done automatically using [webdriver-manager](https://pypi.org/project/webdriver-manager/)

```
pip install webdriver-manager

```

Now the above code in the question will work simply with below change,

```
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager

driver = webdriver.Chrome(ChromeDriverManager().install())

```

The same can be used to set Firefox, Edge and ie binaries."
"Say I have three dicts

```
d1={1:2,3:4}
d2={5:6,7:9}
d3={10:8,13:22}

```

How do I create a new `d4` that combines these three dictionaries? i.e.:

```
d4={1:2,3:4,5:6,7:9,10:8,13:22}

```","1. Slowest and doesn't work in Python3: concatenate the `items` and call `dict` on the resulting list:

   ```
   $ python -mtimeit -s'd1={1:2,3:4}; d2={5:6,7:9}; d3={10:8,13:22}' \
   'd4 = dict(d1.items() + d2.items() + d3.items())'

   100000 loops, best of 3: 4.93 usec per loop

   ```
2. Fastest: exploit the `dict` constructor to the hilt, then one `update`:

   ```
   $ python -mtimeit -s'd1={1:2,3:4}; d2={5:6,7:9}; d3={10:8,13:22}' \
   'd4 = dict(d1, **d2); d4.update(d3)'

   1000000 loops, best of 3: 1.88 usec per loop

   ```
3. Middling: a loop of `update` calls on an initially-empty dict:

   ```
   $ python -mtimeit -s'd1={1:2,3:4}; d2={5:6,7:9}; d3={10:8,13:22}' \
   'd4 = {}' 'for d in (d1, d2, d3): d4.update(d)'

   100000 loops, best of 3: 2.67 usec per loop

   ```
4. Or, equivalently, one copy-ctor and two updates:

   ```
   $ python -mtimeit -s'd1={1:2,3:4}; d2={5:6,7:9}; d3={10:8,13:22}' \
   'd4 = dict(d1)' 'for d in (d2, d3): d4.update(d)'

   100000 loops, best of 3: 2.65 usec per loop

   ```

I recommend approach (2), and I particularly recommend avoiding (1) (which also takes up O(N) extra auxiliary memory for the concatenated list of items temporary data structure)."
"What's the ""Bad magic number"" ImportError in python, and how do I fix it?

The only thing I can find online suggests this is caused by compiling a .py -> .pyc file and then trying to use it with the wrong version of python. In my case, however, the file seems to import fine some times but not others, and I'm not sure why.

The information python's providing in the traceback isn't particularly helpful (which is why I was asking here...), but here it is in case it helps:

```
Traceback (most recent call last):
  File ""run.py"", line 7, in <module>
    from Normalization import Normalizer

```","The magic number comes from UNIX-type systems where the first few bytes of a file held a marker indicating the file type.

Python puts a similar marker into its `pyc` files when it creates them.

Then the python interpreter makes sure this number is correct when loading it.

Anything that damages this magic number will cause your problem. This includes editing the `pyc` file or trying to run a `pyc` from a different version of python (usually later) than your interpreter.

If they are *your* `pyc` files (or you have the `py` files for them), just delete them and let the interpreter re-compile the `py` files. On UNIX type systems, that could be something as simple as:

```
rm *.pyc

```

or:

```
find . -name '*.pyc' -delete

```

If they are not yours, and the original `py` files are not provided, you'll have to either *get* the `py` files for re-compilation, or use an interpreter that can run the `pyc` files with that particular magic value.

One thing that might be causing the intermittent nature. The `pyc` that's causing the problem may only be imported under certain conditions. It's highly unlikely it would import sometimes. You should check the actual full stack trace when the import fails.

As an aside, the first word of all my `2.5.1(r251:54863)` `pyc` files is `62131`, `2.6.1(r261:67517)` is `62161`. The list of all magic numbers can be found in `Python/import.c`, reproduced here for completeness (current as at the time the answer was posted, has changed since then):

```
1.5:   20121
1.5.1: 20121
1.5.2: 20121
1.6:   50428
2.0:   50823
2.0.1: 50823
2.1:   60202
2.1.1: 60202
2.1.2: 60202
2.2:   60717
2.3a0: 62011
2.3a0: 62021
2.3a0: 62011
2.4a0: 62041
2.4a3: 62051
2.4b1: 62061
2.5a0: 62071
2.5a0: 62081
2.5a0: 62091
2.5a0: 62092
2.5b3: 62101
2.5b3: 62111
2.5c1: 62121
2.5c2: 62131
2.6a0: 62151
2.6a1: 62161
2.7a0: 62171

```"
"I've recently become interested in algorithms and have begun exploring them by writing a naive implementation and then optimizing it in various ways.

I'm already familiar with the standard Python module for profiling runtime (for most things I've found the timeit magic function in IPython to be sufficient), but I'm also interested in memory usage so I can explore those tradeoffs as well (e.g. the cost of caching a table of previously computed values versus recomputing them as needed). Is there a module that will profile the memory usage of a given function for me?","Python 3.4 includes a new module: [`tracemalloc`](https://docs.python.org/3/library/tracemalloc.html). It provides detailed statistics about which code is allocating the most memory. Here's an example that displays the top three lines allocating memory.

```
from collections import Counter
import linecache
import os
import tracemalloc

def display_top(snapshot, key_type='lineno', limit=3):
    snapshot = snapshot.filter_traces((
        tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
        tracemalloc.Filter(False, ""<unknown>""),
    ))
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(""#%s: %s:%s: %.1f KiB""
              % (index, filename, frame.lineno, stat.size / 1024))
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print('    %s' % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))


tracemalloc.start()

counts = Counter()
fname = '/usr/share/dict/american-english'
with open(fname) as words:
    words = list(words)
    for word in words:
        prefix = word[:3]
        counts[prefix] += 1
print('Top prefixes:', counts.most_common(3))

snapshot = tracemalloc.take_snapshot()
display_top(snapshot)

```

And here are the results:

```
Top prefixes: [('con', 1220), ('dis', 1002), ('pro', 809)]
Top 3 lines
#1: scratches/memory_test.py:37: 6527.1 KiB
    words = list(words)
#2: scratches/memory_test.py:39: 247.7 KiB
    prefix = word[:3]
#3: scratches/memory_test.py:40: 193.0 KiB
    counts[prefix] += 1
4 other: 4.3 KiB
Total allocated size: 6972.1 KiB

```

### When is a memory leak not a leak?

That example is great when the memory is still being held at the end of the calculation, but sometimes you have code that allocates a lot of memory and then releases it all. It's not technically a memory leak, but it's using more memory than you think it should. How can you track memory usage when it all gets released? If it's your code, you can probably add some debugging code to take snapshots while it's running. If not, you can start a background thread to monitor memory usage while the main thread runs.

Here's the previous example where the code has all been moved into the `count_prefixes()` function. When that function returns, all the memory is released. I also added some `sleep()` calls to simulate a long-running calculation.

```
from collections import Counter
import linecache
import os
import tracemalloc
from time import sleep


def count_prefixes():
    sleep(2)  # Start up time.
    counts = Counter()
    fname = '/usr/share/dict/american-english'
    with open(fname) as words:
        words = list(words)
        for word in words:
            prefix = word[:3]
            counts[prefix] += 1
            sleep(0.0001)
    most_common = counts.most_common(3)
    sleep(3)  # Shut down time.
    return most_common


def main():
    tracemalloc.start()

    most_common = count_prefixes()
    print('Top prefixes:', most_common)

    snapshot = tracemalloc.take_snapshot()
    display_top(snapshot)


def display_top(snapshot, key_type='lineno', limit=3):
    snapshot = snapshot.filter_traces((
        tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
        tracemalloc.Filter(False, ""<unknown>""),
    ))
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(""#%s: %s:%s: %.1f KiB""
              % (index, filename, frame.lineno, stat.size / 1024))
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print('    %s' % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))


main()

```

When I run that version, the memory usage has gone from 6MB down to 4KB, because the function released all its memory when it finished.

```
Top prefixes: [('con', 1220), ('dis', 1002), ('pro', 809)]
Top 3 lines
#1: collections/__init__.py:537: 0.7 KiB
    self.update(*args, **kwds)
#2: collections/__init__.py:555: 0.6 KiB
    return _heapq.nlargest(n, self.items(), key=_itemgetter(1))
#3: python3.6/heapq.py:569: 0.5 KiB
    result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]
10 other: 2.2 KiB
Total allocated size: 4.0 KiB

```

Now here's a version inspired by [another answer](https://stackoverflow.com/a/10117657/4794) that starts a second thread to monitor memory usage.

```
from collections import Counter
import linecache
import os
import tracemalloc
from datetime import datetime
from queue import Queue, Empty
from resource import getrusage, RUSAGE_SELF
from threading import Thread
from time import sleep

def memory_monitor(command_queue: Queue, poll_interval=1):
    tracemalloc.start()
    old_max = 0
    snapshot = None
    while True:
        try:
            command_queue.get(timeout=poll_interval)
            if snapshot is not None:
                print(datetime.now())
                display_top(snapshot)

            return
        except Empty:
            max_rss = getrusage(RUSAGE_SELF).ru_maxrss
            if max_rss > old_max:
                old_max = max_rss
                snapshot = tracemalloc.take_snapshot()
                print(datetime.now(), 'max RSS', max_rss)


def count_prefixes():
    sleep(2)  # Start up time.
    counts = Counter()
    fname = '/usr/share/dict/american-english'
    with open(fname) as words:
        words = list(words)
        for word in words:
            prefix = word[:3]
            counts[prefix] += 1
            sleep(0.0001)
    most_common = counts.most_common(3)
    sleep(3)  # Shut down time.
    return most_common


def main():
    queue = Queue()
    poll_interval = 0.1
    monitor_thread = Thread(target=memory_monitor, args=(queue, poll_interval))
    monitor_thread.start()
    try:
        most_common = count_prefixes()
        print('Top prefixes:', most_common)
    finally:
        queue.put('stop')
        monitor_thread.join()


def display_top(snapshot, key_type='lineno', limit=3):
    snapshot = snapshot.filter_traces((
        tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
        tracemalloc.Filter(False, ""<unknown>""),
    ))
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(""#%s: %s:%s: %.1f KiB""
              % (index, filename, frame.lineno, stat.size / 1024))
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print('    %s' % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))


main()

```

The `resource` module lets you check the current memory usage, and save the snapshot from the peak memory usage. The queue lets the main thread tell the memory monitor thread when to print its report and shut down. When it runs, it shows the memory being used by the `list()` call:

```
2018-05-29 10:34:34.441334 max RSS 10188
2018-05-29 10:34:36.475707 max RSS 23588
2018-05-29 10:34:36.616524 max RSS 38104
2018-05-29 10:34:36.772978 max RSS 45924
2018-05-29 10:34:36.929688 max RSS 46824
2018-05-29 10:34:37.087554 max RSS 46852
Top prefixes: [('con', 1220), ('dis', 1002), ('pro', 809)]
2018-05-29 10:34:56.281262
Top 3 lines
#1: scratches/scratch.py:36: 6527.0 KiB
    words = list(words)
#2: scratches/scratch.py:38: 16.4 KiB
    prefix = word[:3]
#3: scratches/scratch.py:39: 10.1 KiB
    counts[prefix] += 1
19 other: 10.8 KiB
Total allocated size: 6564.3 KiB

```

If you're on Linux, you may find [`/proc/self/statm`](https://stackoverflow.com/a/53475728/4794) more useful than the `resource` module."
"In Python, is there a portable and simple way to test if an executable program exists?

By simple I mean something like the `which` command which would be just perfect. I don't want to search PATH manually or something involving trying to execute it with `Popen` & al and see if it fails (that's what I'm doing now, but imagine it's `launchmissiles`)",Python stdlib now offers [`shutil.which`](https://docs.python.org/library/shutil.html#shutil.which).
"Let's say I have a dictionary in which the keys map to integers like:

```
d = {'key1': 1,'key2': 14,'key3': 47}

```

Is there a syntactically minimalistic way to return the sum of the values in `d`â€”i.e. `62` in this case?","As you'd expect:

```
sum(d.values())

```"
"I regularly perform pandas operations on data frames in excess of 15 million or so rows and I'd love to have access to a progress indicator for particular operations.

Does a text based progress indicator for pandas split-apply-combine operations exist?

For example, in something like:

```
df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)

```

where `feature_rollup` is a somewhat involved function that take many DF columns and creates new user columns through various methods. These operations can take a while for large data frames so I'd like to know if it is possible to have text based output in an iPython notebook that updates me on the progress.

So far, I've tried canonical loop progress indicators for Python but they don't interact with pandas in any meaningful way.

I'm hoping there's something I've overlooked in the pandas library/documentation that allows one to know the progress of a split-apply-combine. A simple implementation would maybe look at the total number of data frame subsets upon which the `apply` function is working and report progress as the completed fraction of those subsets.

Is this perhaps something that needs to be added to the library?","Due to popular demand, I've added `pandas` support in `tqdm` (`pip install ""tqdm>=4.9.0""`). Unlike the other answers, this **will not noticeably slow pandas down** -- here's an example for `DataFrameGroupBy.progress_apply`:

```
import pandas as pd
import numpy as np
from tqdm import tqdm
# from tqdm.auto import tqdm  # for notebooks

# Create new `pandas` methods which use `tqdm` progress
# (can use tqdm_gui, optional kwargs, etc.)
tqdm.pandas()

df = pd.DataFrame(np.random.randint(0, int(1e8), (10000, 1000)))
# Now you can use `progress_apply` instead of `apply`
df.groupby(0).progress_apply(lambda x: x**2)

```

In case you're interested in how this works (and how to modify it for your own callbacks), see the [examples on GitHub](https://github.com/tqdm/tqdm/tree/master/examples), the [full documentation on PyPI](https://pypi.python.org/pypi/tqdm), or import the module and run `help(tqdm)`. Other supported functions include `map`, `applymap`, `aggregate`, and `transform`.

**EDIT**

---

To directly answer the original question, replace:

```
df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)

```

with:

```
from tqdm import tqdm
tqdm.pandas()
df_users.groupby(['userID', 'requestDate']).progress_apply(feature_rollup)

```

**Note: tqdm <= v4.8**:
For versions of tqdm below 4.8, instead of `tqdm.pandas()` you had to do:

```
from tqdm import tqdm, tqdm_pandas
tqdm_pandas(tqdm())

```"
"I want to loop over the contents of a text file and do a search and replace on some lines and write the result back to the file. I could first load the whole file in memory and then write it back, but that probably is not the best way to do it.

What is the best way to do this, within the following code?

```
f = open(file)
for line in f:
    if line.contains('foo'):
        newline = line.replace('foo', 'bar')
        # how to write this newline back to the file

```","The shortest way would probably be to use the [fileinput module](https://docs.python.org/3/library/fileinput.html#fileinput.input). For example, the following adds line numbers to a file, in-place:

```
import fileinput

for line in fileinput.input(""test.txt"", inplace=True):
    print('{} {}'.format(fileinput.filelineno(), line), end='') # for Python 3
    # print ""%d: %s"" % (fileinput.filelineno(), line), # for Python 2

```

What happens here is:

1. The original file is moved to a backup file
2. The standard output is redirected to the original file within the loop
3. Thus any `print` statements write back into the original file

`fileinput` has more bells and whistles. For example, it can be used to automatically operate on all files in `sys.args[1:]`, without your having to iterate over them explicitly. Starting with Python 3.2 it also provides a convenient context manager for use in a `with` statement.

---

While `fileinput` is great for throwaway scripts, I would be wary of using it in real code because admittedly it's not very readable or familiar. In real (production) code it's worthwhile to spend just a few more lines of code to make the process explicit and thus make the code readable.

There are two options:

1. The file is not overly large, and you can just read it wholly to memory. Then close the file, reopen it in writing mode and write the modified contents back.
2. The file is too large to be stored in memory; you can move it over to a temporary file and open that, reading it line by line, writing back into the original file. Note that this requires twice the storage."
"`some_function()` raises an exception while executing, so the program jumps to the `except`:

```
try:
    some_function()
except:
    print(""exception happened!"")

```

How do I see what caused the exception to occur?","The other answers all point out that you should not catch generic exceptions, but no one seems to want to tell you why, which is essential to understanding when you can break the ""rule"". [Here](https://stackoverflow.com/a/736029/566644) is an explanation. Basically, it's so that you don't hide:

* the fact that an error occurred
* the specifics of the error that occurred ([error hiding antipattern](http://en.wikipedia.org/wiki/Error_hiding))

So as long as you take care to do none of those things, it's OK to catch the generic exception. For instance, you could provide information about the exception to the user another way, like:

* Present exceptions as dialogs in a GUI
* Transfer exceptions from a worker thread or process to the controlling thread or process in a multithreading or multiprocessing application

So how to catch the generic exception? There are several ways. If you just want the exception object, do it like this:

```
try:
    someFunction()
except Exception as ex:
    template = ""An exception of type {0} occurred. Arguments:\n{1!r}""
    message = template.format(type(ex).__name__, ex.args)
    print(message)

```

Make *sure* `message` is brought to the attention of the user in a hard-to-miss way! Printing it, as shown above, may not be enough if the message is buried in lots of other messages. Failing to get the users attention is tantamount to swallowing all exceptions, and if there's one impression you should have come away with after reading the answers on this page, it's that this is *not a good thing*. Ending the except block with a `raise` statement will remedy the problem by transparently reraising the exception that was caught.

The difference between the above and using just `except:` without any argument is twofold:

* A bare `except:` doesn't give you the exception object to inspect
* The exceptions `SystemExit`, `KeyboardInterrupt` and `GeneratorExit` aren't caught by the above code, which is generally what you want. See the [exception hierarchy](http://docs.python.org/2/library/exceptions.html#exception-hierarchy).

If you also want the same stacktrace you get if you do not catch the exception, you can get that like this (still inside the except clause):

```
import traceback
print traceback.format_exc()

```

If you use the [`logging`](http://docs.python.org/library/logging.html) module, you can print the exception to the log (along with a message) like this:

```
import logging
log = logging.getLogger()
log.exception(""Message for you, sir!"")

```

If you want to dig deeper and examine the stack, look at variables etc., use the [`post_mortem`](http://docs.python.org/library/pdb.html#pdb.post_mortem) function of the `pdb` module inside the except block:

```
import pdb
pdb.post_mortem()

```

I've found this last method to be invaluable when hunting down bugs."
"I found that in Python 3.4, there are few different libraries for multiprocessing/threading: [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) vs [threading](https://docs.python.org/3/library/threading.html) vs [asyncio](https://docs.python.org/3/library/asyncio.html).

But I don't know which one to use or is the ""recommended one"". Do they do the same thing, or are different? If so, which one is used for what? I want to write a program that uses multicores in my computer. But I don't know which library I should learn.","TL;DR
-----

### Making the Right Choice:

> We have walked through the most popular forms of concurrency. But the question remains - when should choose which one? It really depends on the use cases. From my experience (and reading), I tend to follow this pseudo code:

```
if io_bound:
    if io_very_slow:
        print(""Use Asyncio"")
    else:
        print(""Use Threads"")
else:
    print(""Multi Processing"")

```

> * CPU Bound => Multi Processing
> * I/O Bound, Fast I/O, Limited Number of Connections => Multi Threading
> * I/O Bound, Slow I/O, Many connections => Asyncio

[Reference](http://masnun.rocks/2016/10/06/async-python-the-different-forms-of-concurrency/)

---

[**NOTE**]:

* If you have a long call method (e.g. a method containing a sleep time or lazy I/O), the best choice is [asyncio](https://docs.python.org/3/library/asyncio.html), [Twisted](https://twistedmatrix.com/trac/) or [Tornado](https://www.tornadoweb.org/en/stable/) approach (coroutine methods), that works with a single thread as concurrency.
* [asyncio](https://docs.python.org/3/library/asyncio.html) works on *Python3.4* and later.
* [Tornado](https://www.tornadoweb.org/en/stable/) and [Twisted](https://twistedmatrix.com/trac/) are ready since *Python2.7*
* [uvloop](https://github.com/MagicStack/uvloop) is ultra fast `asyncio` event loop ([uvloop](https://github.com/MagicStack/uvloop) makes `asyncio` 2-4x faster).

---

[UPDATE (2019)]:

* Japranto ([GitHub](https://github.com/squeaky-pl/japronto)) is a very fast pipelining HTTP server based on [uvloop](https://github.com/MagicStack/uvloop).

---

[UPDATE (2024)]:

* `concurrent.futures`: Provides a high-level interface for asynchronously executing callables using threads or processes."
"I need help with setting the limits of y-axis on matplotlib. Here is the code that I tried, unsuccessfully.

```
import matplotlib.pyplot as plt

plt.figure(1, figsize = (8.5,11))
plt.suptitle('plot title')
ax = []
aPlot = plt.subplot(321, axisbg = 'w', title = ""Year 1"")
ax.append(aPlot)
plt.plot(paramValues,plotDataPrice[0], color = '#340B8C', 
     marker = 'o', ms = 5, mfc = '#EB1717')
plt.xticks(paramValues)
plt.ylabel('Average Price')
plt.xlabel('Mark-up')
plt.grid(True)
plt.ylim((25,250))

```

With the data I have for this plot, I get y-axis limits of 20 and 200. However, I want the limits to be 20 and 250.","Get current axis via `plt.gca()`, and then set its limits:

```
ax = plt.gca()
ax.set_xlim([xmin, xmax])
ax.set_ylim([ymin, ymax])

```"
"I'm using eSpeak on Ubuntu and have a Python 2.7 script that prints and speaks a message:

```
import subprocess
text = 'Hello World.'
print text
subprocess.call(['espeak', text])

```

eSpeak produces the desired sounds, but clutters the shell with some errors (ALSA lib..., no socket connect) so i cannot easily read what was printed earlier. Exit code is 0.

Unfortunately there is no documented option to turn off its verbosity, so I'm looking for a way to only visually silence it and keep the open shell clean for further interaction.

How can I do this?

---

See [Python os.system without the output](https://stackoverflow.com/questions/5596911) for approaches specific to `os.system` - although modern code should normally use the `subprocess` library instead.","For python >= 3.3, Redirect the output to DEVNULL:

```
import os
import subprocess

retcode = subprocess.call(['echo', 'foo'], 
    stdout=subprocess.DEVNULL,
    stderr=subprocess.STDOUT)

```

For python <3.3, including 2.7 use:

```
FNULL = open(os.devnull, 'w')
retcode = subprocess.call(['echo', 'foo'], 
    stdout=FNULL, 
    stderr=subprocess.STDOUT)

```

It is effectively the same as running this shell command:

```
retcode = os.system(""echo 'foo' &> /dev/null"")

```"
"In C#, I can say `x ?? """"`, which will give me x if x is not null, and the empty string if x is null. I've found it useful for working with databases.

Is there a way to return a default value if Python finds None in a variable?","You could use the `or` operator:

```
return x or ""default""

```

Note that this also returns `""default""` if `x` is any falsy value, including an empty list, 0, empty string, or even `datetime.time(0)` (midnight).

---

**Python 3.8+ update:** `bool(datetime.time(0))` now resolves to `True`. This was resolved with [issue13936](https://bugs.python.org/issue13936). Other ""empty"" values are still considered ""falsy"" as expected."
"With the Time module in python is it possible to measure elapsed time? If so, how do I do that?

I need to do this so that if the cursor has been in a widget for a certain duration an event happens.","```
start_time = time.time()
# your code
elapsed_time = time.time() - start_time

```

You can also write simple decorator to simplify measurement of execution time of various functions:

```
import time
from functools import wraps

PROF_DATA = {}

def profile(fn):
    @wraps(fn)
    def with_profiling(*args, **kwargs):
        start_time = time.time()

        ret = fn(*args, **kwargs)

        elapsed_time = time.time() - start_time

        if fn.__name__ not in PROF_DATA:
            PROF_DATA[fn.__name__] = [0, []]
        PROF_DATA[fn.__name__][0] += 1
        PROF_DATA[fn.__name__][1].append(elapsed_time)

        return ret

    return with_profiling

def print_prof_data():
    for fname, data in PROF_DATA.items():
        max_time = max(data[1])
        avg_time = sum(data[1]) / len(data[1])
        print ""Function %s called %d times. "" % (fname, data[0]),
        print 'Execution time max: %.3f, average: %.3f' % (max_time, avg_time)

def clear_prof_data():
    global PROF_DATA
    PROF_DATA = {}

```

Usage:

```
@profile
def your_function(...):
    ...

```

You can profile more then one function simultaneously. Then to print measurements just call the print\_prof\_data():"
"I am trying to join two pandas dataframes using two columns:

```
new_df = pd.merge(A_df, B_df,  how='left', left_on='[A_c1,c2]', right_on = '[B_c1,c2]')

```

but got the following error:

```
pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4164)()

pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4028)()

pandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13166)()

pandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13120)()

KeyError: '[B_1, c2]'

```

Any idea what should be the right way to do this?","Try this

```
new_df = pd.merge(
    left=A_df, 
    right=B_df,
    how='left',
    left_on=['A_c1', 'c2'],
    right_on=['B_c1', 'c2'],
)

```

<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html>

> left\_on : label or list, or array-like Field names to join on in left
> DataFrame. Can be a vector or list of vectors of the length of the
> DataFrame to use a particular vector as the join key instead of
> columns
>
> right\_on : label or list, or array-like Field names to join on
> in right DataFrame or vector/list of vectors per left\_on docs"
"Imagine a Python script that will take a long time to run, what will happen if I modify it while it's running? Will the result be different?","Nothing, because Python precompiles your script into a PYC file and launches that.

However, if some kind of exception occurs, you may get a slightly misleading explanation, because line **X** may have different code than before you started the script."
"I have two dictionaries, but for simplification, I will take these two:

```
>>> x = dict(a=1, b=2)
>>> y = dict(a=2, b=2)

```

Now, I want to compare whether each `key, value` pair in `x` has the same corresponding value in `y`. So I wrote this:

```
>>> for x_values, y_values in zip(x.iteritems(), y.iteritems()):
        if x_values == y_values:
            print 'Ok', x_values, y_values
        else:
            print 'Not', x_values, y_values

```

And it works since a `tuple` is returned and then compared for equality.

My questions:

Is this correct? Is there a *better* way to do this? Better not in speed, I am talking about code elegance.

UPDATE: I forgot to mention that I have to check how many `key, value` pairs are equal.","If you want to know how many values match in both the dictionaries, you should have said that :)

Maybe something like this:

```
shared_items = {k: x[k] for k in x if k in y and x[k] == y[k]}
print(len(shared_items))

```"
"```
from mechanize import Browser
br = Browser()
br.open('http://somewebpage')
html = br.response().readlines()
for line in html:
  print line

```

When printing a line in an HTML file, I'm trying to find a way to only show the contents of each HTML element and not the formatting itself. If it finds `'<a href=""whatever.example"">some text</a>'`, it will only print 'some text', `'<b>hello</b>'` prints 'hello', etc. How would one go about doing this?","I always used this function to strip HTML tags, as it requires only the Python stdlib:

For Python 3:

```
from io import StringIO
from html.parser import HTMLParser

class MLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs= True
        self.text = StringIO()
    def handle_data(self, d):
        self.text.write(d)
    def get_data(self):
        return self.text.getvalue()

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

```

For Python 2:

```
from HTMLParser import HTMLParser
from StringIO import StringIO

class MLStripper(HTMLParser):
    def __init__(self):
        self.reset()
        self.text = StringIO()
    def handle_data(self, d):
        self.text.write(d)
    def get_data(self):
        return self.text.getvalue()

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

```"
"```
Foo.objects.get(pk=""foo"")
<Foo: test>

```

In the database, I want to add another object which is a copy of the object above.

Suppose my table has one row. I want to insert the first row object into another row with a different primary key. How can I do that?","Just change the primary key of your object and run save().

```
obj = Foo.objects.get(pk=<some_existing_pk>)
obj.pk = None
obj.save()

```

If you want auto-generated key, set the new key to None.

More on UPDATE/INSERT [here](https://docs.djangoproject.com/en/dev/ref/models/instances/#how-django-knows-to-update-vs-insert).

Official docs on copying model instances: <https://docs.djangoproject.com/en/2.2/topics/db/queries/#copying-model-instances>"
"What's the **easiest**, **tersest**, and most **flexible** method or library for parsing Python command line arguments?","[`argparse`](https://docs.python.org/library/argparse.html) is the way to go. Here is a short summary of how to use it:

**1) Initialize**

```
import argparse

# Instantiate the parser
parser = argparse.ArgumentParser(description='Optional app description')

```

**2) Add Arguments**

```
# Required positional argument
parser.add_argument('pos_arg', type=int,
                    help='A required integer positional argument')

# Optional positional argument
parser.add_argument('opt_pos_arg', type=int, nargs='?',
                    help='An optional integer positional argument')

# Optional argument
parser.add_argument('--opt_arg', type=int,
                    help='An optional integer argument')

# Switch
parser.add_argument('--switch', action='store_true',
                    help='A boolean switch')

```

**3) Parse**

```
args = parser.parse_args()

```

**4) Access**

```
print(""Argument values:"")
print(args.pos_arg)
print(args.opt_pos_arg)
print(args.opt_arg)
print(args.switch)

```

**5) Check Values**

```
if args.pos_arg > 10:
    parser.error(""pos_arg cannot be larger than 10"")

```

Usage
-----

**Correct use:**

```
$ ./app 1 2 --opt_arg 3 --switch

Argument values:
1
2
3
True

```

**Incorrect arguments:**

```
$ ./app foo 2 --opt_arg 3 --switch
usage: convert [-h] [--opt_arg OPT_ARG] [--switch] pos_arg [opt_pos_arg]
app: error: argument pos_arg: invalid int value: 'foo'

$ ./app 11 2 --opt_arg 3
Argument values:
11
2
3
False
usage: app [-h] [--opt_arg OPT_ARG] [--switch] pos_arg [opt_pos_arg]
convert: error: pos_arg cannot be larger than 10

```

**Full help:**

```
$ ./app -h

usage: app [-h] [--opt_arg OPT_ARG] [--switch] pos_arg [opt_pos_arg]

Optional app description

positional arguments:
  pos_arg            A required integer positional argument
  opt_pos_arg        An optional integer positional argument

optional arguments:
  -h, --help         show this help message and exit
  --opt_arg OPT_ARG  An optional integer argument
  --switch           A boolean switch

```"
"I've created an object like this:

```
company1.name = 'banana' 
company1.value = 40

```

I would like to save this object. How can I do that?","You could use the [`pickle`](https://docs.python.org/library/pickle.html#module-pickle) module in the standard library.
Here's an elementary application of it to your example:

```
import pickle

class Company(object):
    def __init__(self, name, value):
        self.name = name
        self.value = value

with open('company_data.pkl', 'wb') as outp:
    company1 = Company('banana', 40)
    pickle.dump(company1, outp, pickle.HIGHEST_PROTOCOL)

    company2 = Company('spam', 42)
    pickle.dump(company2, outp, pickle.HIGHEST_PROTOCOL)

del company1
del company2

with open('company_data.pkl', 'rb') as inp:
    company1 = pickle.load(inp)
    print(company1.name)  # -> banana
    print(company1.value)  # -> 40

    company2 = pickle.load(inp)
    print(company2.name) # -> spam
    print(company2.value)  # -> 42

```

You could also define your own simple utility like the following which opens a file and writes a single object to it:

```
def save_object(obj, filename):
    with open(filename, 'wb') as outp:  # Overwrites any existing file.
        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)

# sample usage
save_object(company1, 'company1.pkl')

```

Update
------

Since this is such a popular answer, I'd like touch on a few slightly advanced usage topics.

### `cPickle` (or `_pickle`) vs `pickle`

It's almost always preferable to actually use the [`cPickle`](http://docs.python.org/2/library/pickle.html#module-cPickle) module rather than `pickle` because the former is written in C and is much faster. There are some subtle differences between them, but in most situations they're equivalent and the C version will provide greatly superior performance. Switching to it couldn't be easier, just change the `import` statement to this:

```
import cPickle as pickle

```

In Python 3, `cPickle` was renamed `_pickle`, but doing this is no longer necessary since the `pickle` module now does it automatically—see [What difference between pickle and \_pickle in python 3?](https://stackoverflow.com/questions/19191859/what-difference-between-pickle-and-pickle-in-python-3).

The rundown is you could use something like the following to ensure that your code will *always* use the C version when it's available in both Python 2 and 3:

```
try:
    import cPickle as pickle
except ModuleNotFoundError:
    import pickle

```

### Data stream formats (protocols)

`pickle` can read and write files in several different, Python-specific, formats, called *protocols* as described in the [documentation](https://docs.python.org/3/library/pickle.html#data-stream-format), ""Protocol version 0"" is ASCII and therefore ""human-readable"". Versions > 0 are binary and the highest one available depends on what version of Python is being used. The default also depends on Python version. In Python 2 the default was Protocol version `0`, but in Python 3.8.1, it's Protocol version `4`. In Python 3.x the module had a `pickle.DEFAULT_PROTOCOL` added to it, but that doesn't exist in Python 2.

Fortunately there's shorthand for writing `pickle.HIGHEST_PROTOCOL` in every call (assuming that's what you want, and you usually do), just use the literal number `-1` — similar to referencing the last element of a sequence via a negative index.
So, instead of writing:

```
pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)

```

You can just write:

```
pickle.dump(obj, outp, -1)

```

Either way, you'd only have specify the protocol once if you created a `Pickler` object for use in multiple pickle operations:

```
pickler = pickle.Pickler(outp, -1)
pickler.dump(obj1)
pickler.dump(obj2)
   etc...

```

*Note*: If you're in an environment running different versions of Python, then you'll probably want to explicitly use (i.e. hardcode) a specific protocol number that all of them can read (later versions can generally read files produced by earlier ones).

### Multiple Objects

While a pickle file *can* contain any number of pickled objects, as shown in the above samples, when there's an unknown number of them, it's often easier to store them all in some sort of variably-sized container, like a `list`, `tuple`, or `dict` and write them all to the file in a single call:

```
tech_companies = [
    Company('Apple', 114.18), Company('Google', 908.60), Company('Microsoft', 69.18)
]
save_object(tech_companies, 'tech_companies.pkl')

```

and restore the list and everything in it later with:

```
with open('tech_companies.pkl', 'rb') as inp:
    tech_companies = pickle.load(inp)

```

The major advantage is you don't need to know how many object instances are saved in order to load them back later (although doing so without that information *is* possible, it requires some slightly specialized code). See the answers to the related question [Saving and loading multiple objects in pickle file?](https://stackoverflow.com/questions/20716812/saving-and-loading-multiple-objects-in-pickle-file) for details on different ways to do this. Personally I liked @Lutz Prechelt's [answer](https://stackoverflow.com/a/28745948/355230) the best, so that's the approach used in the sample code below:

```
class Company:
    def __init__(self, name, value):
        self.name = name
        self.value = value

def pickle_loader(filename):
    """""" Deserialize a file of pickled objects. """"""
    with open(filename, ""rb"") as f:
        while True:
            try:
                yield pickle.load(f)
            except EOFError:
                break

print('Companies in pickle file:')
for company in pickle_loader('company_data.pkl'):
    print('  name: {}, value: {}'.format(company.name, company.value))

```"
"I'm working on a python script that starts several processes and database connections. Every now and then I want to kill the script with a `Ctrl`+`C` signal, and I'd like to do some cleanup.

In Perl I'd do this:

```
$SIG{'INT'} = 'exit_gracefully';

sub exit_gracefully {
    print ""Caught ^C \n"";
    exit (0);
}

```

How do I do the analogue of this in Python?","Register your handler with [`signal.signal`](https://docs.python.org/3/library/signal.html#signal.signal) like this:

```
#!/usr/bin/env python
import signal
import sys

def signal_handler(sig, frame):
    print('You pressed Ctrl+C!')
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
print('Press Ctrl+C')
signal.pause()

```

Code adapted from [here](http://www.linuxjournal.com/article/3946).

More documentation on `signal` can be found [here](http://docs.python.org/library/signal.html).
Â"
"I have a pandas `Series` object containing boolean values. How can I get a series containing the logical `NOT` of each value?

For example, consider a series containing:

```
True
True
True
False

```

The series I'd like to get would contain:

```
False
False
False
True

```

This seems like it should be reasonably simple, but apparently I've misplaced my mojo =(","To invert a boolean Series, [use `~s`](http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing):

```
In [7]: s = pd.Series([True, True, False, True])

In [8]: ~s
Out[8]: 
0    False
1    False
2     True
3    False
dtype: bool

```

Using Python2.7, NumPy 1.8.0, Pandas 0.13.1:

```
In [119]: s = pd.Series([True, True, False, True]*10000)

In [10]:  %timeit np.invert(s)
10000 loops, best of 3: 91.8 µs per loop

In [11]: %timeit ~s
10000 loops, best of 3: 73.5 µs per loop

In [12]: %timeit (-s)
10000 loops, best of 3: 73.5 µs per loop

```

As of Pandas 0.13.0, Series are no longer subclasses of `numpy.ndarray`; they are now subclasses of `pd.NDFrame`. This might have something to do with why `np.invert(s)` is no longer as fast as `~s` or `-s`.

Caveat: `timeit` results may vary depending on many factors including hardware, compiler, OS, Python, NumPy and Pandas versions."
"I'm gathering statistics on a list of websites and I'm using requests for it for simplicity. Here is my code:

```
data=[]
websites=['http://google.com', 'http://bbc.co.uk']
for w in websites:
    r= requests.get(w, verify=False)
    data.append( (r.url, len(r.content), r.elapsed.total_seconds(), str([(l.status_code, l.url) for l in r.history]), str(r.headers.items()), str(r.cookies.items())) )

```

Now, I want `requests.get` to timeout after 10 seconds so the loop doesn't get stuck.

This question has [been of interest before](https://stackoverflow.com/questions/13573146/how-to-perform-time-limited-response-download-with-python-requests) too but none of the answers are clean.

I hear that maybe not using requests is a good idea but then how should I get the nice things requests offer (the ones in the tuple).","*Note: The `timeout` param does NOT prevent the request from loading forever, it only stops if the remote server fails to send response data within the timeout value. It could still load indefinitely.*

Set the [timeout parameter](https://requests.readthedocs.io/en/stable/user/quickstart/#timeouts):

```
try:
    r = requests.get(""example.com"", timeout=10) # 10 seconds
except requests.exceptions.Timeout:
    print(""Timed out"")

```

The code above will cause the call to `requests.get()` to timeout if the connection or delays between reads takes more than ten seconds.

The `timeout` parameter accepts the number of seconds to wait as a float, as well as a `(connect timeout, read timeout)` tuple.

See [requests.request](https://requests.readthedocs.io/en/latest/api/#requests.request) documentation as well as the [timeout section](https://requests.readthedocs.io/en/stable/user/advanced/#timeouts) of the ""Advanced Usage"" section of the documentation."
"Here is my code,

```
for line in open('u.item'):
# Read each line

```

Whenever I run this code it gives the following error:

> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 2892: invalid continuation byte

I tried to solve this and add an extra parameter in open(). The code looks like:

```
for line in open('u.item', encoding='utf-8'):
# Read each line

```

But again it gives the same error. What should I do then?","As [suggested by Mark Ransom](https://stackoverflow.com/questions/19699367/for-line-in-results-in-unicodedecodeerror-utf-8-codec-cant-decode-byte/19699399#19699399), I found the right encoding for that problem. The encoding was [`""ISO-8859-1""`](https://en.wikipedia.org/wiki/ISO/IEC_8859-1), so replacing `open(""u.item"", encoding=""utf-8"")` with `open('u.item', encoding = ""ISO-8859-1"")` will solve the problem."
"I have created an array thusly:

```
import numpy as np
data = np.zeros( (512,512,3), dtype=np.uint8)
data[256,256] = [255,0,0]

```

What I want this to do is display a single red dot in the center of a 512x512 image. (At least to begin with... I think I can figure out the rest from there)","Use `plt.imshow` to create the figure, and `plt.show` to display it:

```
from matplotlib import pyplot as plt
plt.imshow(data, interpolation='nearest')
plt.show()

```

---

For Jupyter notebooks, add this line before importing matplotlib:

```
%matplotlib inline 

```

For interactive plots in Jupyter [[demo](https://raw.githubusercontent.com/matplotlib/ipympl/master/matplotlib.gif)], install ipyml `pip install ipympl`, then use:

```
%matplotlib widget 

```"
"What is a classy way to way truncate a python `datetime` object?

In this particular case, to the day. So basically setting hour, minute, seconds, and microseconds to 0.

I would like the output to also be a `datetime` object, not a string.","I think this is what you're looking for...

```
>>> import datetime
>>> dt = datetime.datetime.now()
>>> dt = dt.replace(hour=0, minute=0, second=0, microsecond=0) # Returns a copy
>>> dt
datetime.datetime(2011, 3, 29, 0, 0)

```

But if you really don't care about the time aspect of things, then you should really only be passing around `date` objects...

```
>>> d_truncated = datetime.date(dt.year, dt.month, dt.day)
>>> d_truncated
datetime.date(2011, 3, 29)

```"
"I am trying to compare the current date and time with dates and times specified in models using comparison operators:

```
if challenge.datetime_start <= datetime.now() <= challenge.datetime_end:

```

The script errors out with:

```
TypeError: can't compare offset-naive and offset-aware datetimes

```

The models look like this:

```
class Fundraising_Challenge(models.Model):
    name = models.CharField(max_length=100)
    datetime_start = models.DateTimeField()
    datetime_end = models.DateTimeField()

```

I also have django using locale date and times.

What I haven't been able to find is the format django uses for DateTimeField(). Is it naive or aware? And how do I get datetime.now() to recognize locale datetime?","By default, the `datetime` object is `naive` in Python, so you need to make both of them either naive or aware `datetime` objects. This can be done using:

```
import datetime
import pytz

utc=pytz.UTC

challenge.datetime_start = utc.localize(challenge.datetime_start) 
challenge.datetime_end = utc.localize(challenge.datetime_end) 
# now both the datetime objects are aware, and you can compare them

```

Note: This would raise a `ValueError` if `tzinfo` is already set. If you are not sure about that, just use

```
start_time = challenge.datetime_start.replace(tzinfo=utc)
end_time = challenge.datetime_end.replace(tzinfo=utc)

```

BTW, you could format a UNIX timestamp in datetime.datetime object with timezone info as following

```
d = datetime.datetime.utcfromtimestamp(int(unix_timestamp))
d_with_tz = datetime.datetime(
    year=d.year,
    month=d.month,
    day=d.day,
    hour=d.hour,
    minute=d.minute,
    second=d.second,
    tzinfo=pytz.UTC)

```"
"Why does [`zero_grad()`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) need to be called during training?

```
|  zero_grad(self)
|      Sets gradients of all model parameters to zero.

```","In [`PyTorch`](https://github.com/pytorch/pytorch), for every mini-batch during the *training* phase, we typically want to explicitly set the gradients to zero before starting to do backpropagation (i.e., updating the ***W**eights* and ***b**iases*) because PyTorch *accumulates the gradients* on subsequent backward passes. This accumulating behavior is convenient while training RNNs or when we want to compute the gradient of the loss summed over multiple *mini-batches*. So, the default action has been set to [accumulate (i.e. sum) the gradients](https://pytorch.org/docs/stable/_modules/torch/autograd.html) on every `loss.backward()` call.

Because of this, when you start your training loop, ideally you should [`zero out the gradients`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad) so that you do the parameter update correctly. Otherwise, the gradient would be a combination of the old gradient, which you have already used to update your model parameters and the newly-computed gradient. It would therefore point in some other direction than the intended direction towards the *minimum* (or *maximum*, in case of maximization objectives).

Here is a simple example:

```
import torch
from torch.autograd import Variable
import torch.optim as optim

def linear_model(x, W, b):
    return torch.matmul(x, W) + b

data, targets = ...

W = Variable(torch.randn(4, 3), requires_grad=True)
b = Variable(torch.randn(3), requires_grad=True)

optimizer = optim.Adam([W, b])

for sample, target in zip(data, targets):
    # clear out the gradients of all Variables 
    # in this optimizer (i.e. W, b)
    optimizer.zero_grad()
    output = linear_model(sample, W, b)
    loss = (output - target) ** 2
    loss.backward()
    optimizer.step()

```

---

Alternatively, if you're doing a *vanilla gradient descent*, then:

```
W = Variable(torch.randn(4, 3), requires_grad=True)
b = Variable(torch.randn(3), requires_grad=True)

for sample, target in zip(data, targets):
    # clear out the gradients of Variables 
    # (i.e. W, b)
    W.grad.data.zero_()
    b.grad.data.zero_()

    output = linear_model(sample, W, b)
    loss = (output - target) ** 2
    loss.backward()

    W -= learning_rate * W.grad.data
    b -= learning_rate * b.grad.data

```

---

**Note**:

* The *accumulation* (i.e., *sum*) of gradients happens when [`.backward()` is called on the `loss` tensor](https://pytorch.org/docs/stable/_modules/torch/autograd.html).
* As of v1.7.0, Pytorch offers the option to reset the gradients to `None` [`optimizer.zero_grad(set_to_none=True)`](https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer.zero_grad) instead of filling them with a tensor of zeroes. The docs claim that this setting reduces memory requirements and slightly improves performance, but might be error-prone if not handled carefully."
"Say I have a multiple inheritance scenario:

```
class A(object):
    # code for A here

class B(object):
    # code for B here

class C(A, B):
    def __init__(self):
        # What's the right code to write here to ensure 
        # A.__init__ and B.__init__ get called?

```

There's two typical approaches to writing `C`'s `__init__`:

1. (old-style) `ParentClass.__init__(self)`
2. (newer-style) `super(DerivedClass, self).__init__()`

However, in either case, if the parent classes (`A` and `B`) [don't follow the same convention, then the code will not work correctly](http://fuhm.org/super-harmful/) (some may be missed, or get called multiple times).

So what's the correct way again? It's easy to say ""just be consistent, follow one or the other"", but if `A` or `B` are from a 3rd party library, what then? Is there an approach that can ensure that all parent class constructors get called (and in the correct order, and only once)?

Edit: to see what I mean, if I do:

```
class A(object):
    def __init__(self):
        print(""Entering A"")
        super(A, self).__init__()
        print(""Leaving A"")

class B(object):
    def __init__(self):
        print(""Entering B"")
        super(B, self).__init__()
        print(""Leaving B"")

class C(A, B):
    def __init__(self):
        print(""Entering C"")
        A.__init__(self)
        B.__init__(self)
        print(""Leaving C"")

```

Then I get:

```
Entering C
Entering A
Entering B
Leaving B
Leaving A
Entering B
Leaving B
Leaving C

```

Note that `B`'s init gets called twice. If I do:

```
class A(object):
    def __init__(self):
        print(""Entering A"")
        print(""Leaving A"")

class B(object):
    def __init__(self):
        print(""Entering B"")
        super(B, self).__init__()
        print(""Leaving B"")

class C(A, B):
    def __init__(self):
        print(""Entering C"")
        super(C, self).__init__()
        print(""Leaving C"")

```

Then I get:

```
Entering C
Entering A
Leaving A
Leaving C

```

Note that `B`'s init never gets called. So it seems that unless I know/control the init's of the classes I inherit from (`A` and `B`) I cannot make a safe choice for the class I'm writing (`C`).","The answer to your question depends on one very important aspect: **Are your base classes designed for multiple inheritance?**

There are 3 different scenarios:

1. The base classes are unrelated, standalone classes.
   ---------------------------------------------------

   If your base classes are separate entities that are capable of functioning independently and they don't know each other, they're *not* designed for multiple inheritance. Example:

   ```
   class Foo:
       def __init__(self):
           self.foo = 'foo'

   class Bar:
       def __init__(self, bar):
           self.bar = bar

   ```

   **Important:** Notice that neither `Foo` nor `Bar` calls `super().__init__()`! This is why your code didn't work correctly. Because of the way diamond inheritance works in python, **classes whose base class is `object` should not call `super().__init__()`**. As you've noticed, doing so would break multiple inheritance because you end up calling another class's `__init__` rather than `object.__init__()`. *(**Disclaimer:** Avoiding `super().__init__()` in `object`-subclasses is my personal recommendation and by no means an agreed-upon consensus in the python community. Some people prefer to use `super` in every class, arguing that you can always write an [adapter](https://en.wikipedia.org/wiki/Adapter_pattern) if the class doesn't behave as you expect.)*

   This also means that you should never write a class that inherits from `object` and doesn't have an `__init__` method. Not defining a `__init__` method at all has the same effect as calling `super().__init__()`. If your class inherits directly from `object`, make sure to add an empty constructor like so:

   ```
   class Base(object):
       def __init__(self):
           pass

   ```

   Anyway, in this situation, you will have to call each parent constructor manually. There are two ways to do this:

   * **Without `super`**

     ```
     class FooBar(Foo, Bar):
         def __init__(self, bar='bar'):
             Foo.__init__(self)  # explicit calls without super
             Bar.__init__(self, bar)

     ```
   * **With `super`**

     ```
     class FooBar(Foo, Bar):
         def __init__(self, bar='bar'):
             super().__init__()  # this calls all constructors up to Foo
             super(Foo, self).__init__(bar)  # this calls all constructors after Foo up
                                             # to Bar

     ```

   Each of these two methods has its own advantages and disadvantages. If you use `super`, your class will support [dependency injection](https://stackoverflow.com/a/33469090/1222951). On the other hand, it's easier to make mistakes. For example if you change the order of `Foo` and `Bar` (like `class FooBar(Bar, Foo)`), you'd have to update the `super` calls to match. Without `super` you don't have to worry about this, and the code is much more readable.
2. One of the classes is a mixin.
   ------------------------------

   A [mixin](https://stackoverflow.com/questions/533631/what-is-a-mixin-and-why-are-they-useful) is a class that's *designed* to be used with multiple inheritance. This means we don't have to call both parent constructors manually, because the mixin will automatically call the 2nd constructor for us. Since we only have to call a single constructor this time, we can do so with `super` to avoid having to hard-code the parent class's name.

   Example:

   ```
   class FooMixin:
       def __init__(self, *args, **kwargs):
           super().__init__(*args, **kwargs)  # forwards all unused arguments
           self.foo = 'foo'

   class Bar:
       def __init__(self, bar):
           self.bar = bar

   class FooBar(FooMixin, Bar):
       def __init__(self, bar='bar'):
           super().__init__(bar)  # a single call is enough to invoke
                                  # all parent constructors

           # NOTE: `FooMixin.__init__(self, bar)` would also work, but isn't
           # recommended because we don't want to hard-code the parent class.

   ```

   The important details here are:

   * The mixin calls `super().__init__()` and passes through any arguments it receives.
   * The subclass inherits from the mixin *first*: `class FooBar(FooMixin, Bar)`. If the order of the base classes is wrong, the mixin's constructor will never be called.
3. All base classes are designed for cooperative inheritance.
   ----------------------------------------------------------

   Classes designed for cooperative inheritance are a lot like mixins: They pass through all unused arguments to the next class. Like before, we just have to call `super().__init__()` and all parent constructors will be chain-called.

   Example:

   ```
   class CoopFoo:
       def __init__(self, **kwargs):
           super().__init__(**kwargs)  # forwards all unused arguments
           self.foo = 'foo'

   class CoopBar:
       def __init__(self, bar, **kwargs):
           super().__init__(**kwargs)  # forwards all unused arguments
           self.bar = bar

   class CoopFooBar(CoopFoo, CoopBar):
       def __init__(self, bar='bar'):
           super().__init__(bar=bar)  # pass all arguments on as keyword
                                      # arguments to avoid problems with
                                      # positional arguments and the order
                                      # of the parent classes

   ```

   In this case, the order of the parent classes doesn't matter. We might as well inherit from `CoopBar` first, and the code would still work the same. But that's only true because all arguments are passed as keyword arguments. Using positional arguments would make it easy to get the order of the arguments wrong, so it's customary for cooperative classes to accept only keyword arguments.

   This is also an exception to the rule I mentioned earlier: Both `CoopFoo` and `CoopBar` inherit from `object`, but they still call `super().__init__()`. If they didn't, there would be no cooperative inheritance.

Bottom line: The correct implementation depends on the classes you're inheriting from.

The constructor is part of a class's public interface. If the class is designed as a mixin or for cooperative inheritance, that must be documented. If the docs don't mention anything of the sort, it's safe to assume that the class *isn't* designed for cooperative multiple inheritance."
"I have been searching and tried various alternatives without success and spent several days on it now; it is driving me mad.

I am running on [Red Hat Linux](https://en.wikipedia.org/wiki/Red_Hat_Linux) with Python 2.5.2.
I began using the most recent Virtualenv, but I could not activate it. I found somewhere suggesting I needed an earlier version, so I have used Virtualenv 1.6.4 as that should work with Python 2.6.

It seems to install the virtual environment ok
----------------------------------------------

```
python virtualenv-1.6.4/virtualenv.py virtual

```

Output:

```
New python executable in virtual/bin/python
Installing setuptools............done.
Installing pip...............done.

```

The environment looks ok
------------------------

```
cd virtual
dir

```

Output:

```
bin  include  lib

```

Trying to activate
------------------

```
. bin/activate

```

Output:

```
/bin/.: Permission denied.

```

I checked chmod
---------------

```
cd bin
ls -l

```

Output:

```
total 3160
 -rw-r--r--    1 necrailk biz12        2130 Jan 30 11:38 activate
 -rw-r--r--    1 necrailk biz12        1050 Jan 30 11:38 activate.csh
 -rw-r--r--    1 necrailk biz12        2869 Jan 30 11:38 activate.fish
 -rw-r--r-

```

It was a problem, so I changed it
---------------------------------

```
ls -l

```

Output:

```
total 3160
-rwxr--r--    1 necrailk biz12        2130 Jan 30 11:38 activate
-rw-r--r--    1 necrailk biz12        1050 Jan 30 11:38 activate.csh
-rw-r--r--    1 necrailk biz12        2869 Jan 30 11:38 activate.fish
-rw-r--r--    1 necrailk biz12        1005 Jan 30 11:38 activate_this.py
-rwxr-xr-x    1 necrailk biz

```

Tring `activate` again
----------------------

```
. bin/activate

```

Output:

```
/bin/.: Permission denied.

```

Still no joy...","Here is my workflow after creating a folder and `cd`'ing into it:

```
virtualenv venv --distribute

```

Output:

```
New python executable in venv/bin/python
Installing distribute.........done.
Installing pip................done.

```

And

```
source venv/bin/activate
python

```"
"I have this code:

```
BOX_LENGTH = 100
turtle.speed(0)
fill = 0
for i in range(8):
    fill += 1
    if fill % 2 == 0:
        Horizontol_drawbox(BOX_LENGTH, fillBox = False)
    else:
        Horizontol_drawbox(BOX_LENGTH, fillBox = True)
        
    for i in range(8):
        fill += 1
        if fill % 2 == 0:
            Vertical_drawbox(BOX_LENGTH,fillBox = False)
        else:
            Vertical_drawbox(BOX_LENGTH,fillBox = True)

```

But I get an error that says:

```
    Horizontol_drawbox(BOX_LENGTH, fillBox = True)
TypeError: Horizontol_drawbox() got multiple values for argument 'fillBox'
    

```

What does this mean, and how can I fix the problem?","This happens when a keyword argument is specified that overwrites a positional argument. For example, let's imagine a function that draws a colored box. The function selects the color to be used and delegates the drawing of the box to another function, relaying all extra arguments.

```
def color_box(color, *args, **kwargs):
    painter.select_color(color)
    painter.draw_box(*args, **kwargs)

```

Then the call

```
color_box(""blellow"", color=""green"", height=20, width=30)

```

will fail because two values are assigned to `color`: `""blellow""` as positional and `""green""` as keyword. (`painter.draw_box` is supposed to accept the `height` and `width` arguments).

This is easy to see in the example, but of course if one mixes up the arguments at call, it may not be easy to debug:

```
# misplaced height and width
color_box(20, 30, color=""green"")

```

Here, `color` is assigned `20`, then `args=[30]` and `color` is again assigned `""green""`."
"I have a list of lists like

```
[
    [1, 2, 3],
    [4, 5, 6],
    [7],
    [8, 9]
]

```

How can I flatten it to get `[1, 2, 3, 4, 5, 6, 7, 8, 9]`?

---

If your list of lists comes from a nested list comprehension, the problem can be solved more simply/directly by fixing the comprehension; please see [How can I get a flat result from a list comprehension instead of a nested list?](https://stackoverflow.com/questions/1077015).

The most popular solutions here generally only flatten one ""level"" of the nested list. See [Flatten an irregular (arbitrarily nested) list of lists](https://stackoverflow.com/questions/2158395) for solutions that completely flatten a deeply nested structure (recursively, in general).","A list of lists named `xss` can be flattened using a nested [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions):

```
flat_list = [
    x
    for xs in xss
    for x in xs
]

```

The above is equivalent to:

```
flat_list = []

for xs in xss:
    for x in xs:
        flat_list.append(x)

```

Here is the corresponding function:

```
def flatten(xss):
    return [x for xs in xss for x in xs]

```

This is the fastest method.
As evidence, using the [`timeit`](https://docs.python.org/3/library/timeit.html) module in the standard library, we see:

```
$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' '[x for xs in xss for x in xs]'
10000 loops, best of 3: 143 usec per loop

$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'sum(xss, [])'
1000 loops, best of 3: 969 usec per loop

$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'reduce(lambda xs, ys: xs + ys, xss)'
1000 loops, best of 3: 1.1 msec per loop

```

Explanation: the methods based on `+` (including the implied use in `sum`) are, of necessity, `O(L**2)` when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of M items each: the first M items are copied back and forth `L-1` times, the second M items `L-2` times, and so on; total number of copies is M times the sum of x for x from 1 to L excluded, i.e., `M * (L**2)/2`.

The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once."
"Suppose this string:

```
The   fox jumped   over    the log.

```

Turning into:

```
The fox jumped over the log.

```

What is the simplest (1-2 lines) to achieve this, without splitting and going into lists?","```
>>> import re
>>> re.sub(' +', ' ', 'The     quick brown    fox')
'The quick brown fox'

```"
"I am using `pyplot`. I have 4 subplots. How to set a single, main title above all the subplots? `title()` sets it above the last subplot.","Use [`pyplot.suptitle`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.suptitle.html?highlight=suptitle#matplotlib.pyplot.suptitle) or [`Figure.suptitle`](https://matplotlib.org/api/_as_gen/matplotlib.figure.Figure.html?highlight=suptitle#matplotlib.figure.Figure.suptitle):

```
import matplotlib.pyplot as plt
import numpy as np

fig=plt.figure()
data=np.arange(900).reshape((30,30))
for i in range(1,5):
    ax=fig.add_subplot(2,2,i)        
    ax.imshow(data)

fig.suptitle('Main title') # or plt.suptitle('Main title')
plt.show()

```

![enter image description here](https://i.sstatic.net/Kq15V.png)"
"Believe it or not, after profiling my current code, the repetitive operation of numpy array reversion ate a giant chunk of the running time. What I have right now is the common view-based method:

```
reversed_arr = arr[::-1]

```

Is there any other way to do it more efficiently, or is it just an illusion from my obsession with unrealistic numpy performance?","```
reversed_arr = arr[::-1]

```

gives a reversed [view](https://numpy.org/doc/stable/user/basics.copies.html) into the original array `arr`. Any changes made to the original array `arr` will also be immediately visible in `reversed_arr`. The underlying data buffers for `arr` and `reversed_arr` are *shared*, so creating this view is always instantaneous, and does not require any additional memory allocation or copying for the array contents.

See also, this discussion on NumPy views: [How do I create a view onto a NumPy array?](https://stackoverflow.com/questions/4370745/how-do-i-create-a-view-onto-a-numpy-array)

---

##### Possible solutions to performance problems regarding views

Are you re-creating the view more often than you need to? You should be able to do something like this:

```
arr = np.array(some_sequence)
reversed_arr = arr[::-1]

do_something(arr)
look_at(reversed_arr)
do_something_else(arr)
look_at(reversed_arr)

```

I'm not a numpy expert, but this seems like it would be the fastest way to do things in numpy. If this is what you are already doing, I don't think you can improve on it."
"I have this nested list:

```
l = [['40', '20', '10', '30'], ['20', '20', '20', '20', '20', '30', '20'], ['30', '20', '30', '50', '10', '30', '20', '20', '20'], ['100', '100'], ['100', '100', '100', '100', '100'], ['100', '100', '100', '100']]

```

I want to convert each element in `l` to `float`. I have this code:

```
newList = []
for x in l:
    for y in x:
        newList.append(float(y))

```

How can I solve the problem with a nested list comprehension instead?

---

See also: [How can I get a flat result from a list comprehension instead of a nested list?](https://stackoverflow.com/questions/1077015)","Here is how you would do this with a nested list comprehension:

```
[[float(y) for y in x] for x in l]

```

This would give you a list of lists, similar to what you started with except with floats instead of strings.

If you want one flat list, then you would use

```
[float(y) for x in l for y in x]

```

Note the loop order - `for x in l` comes first in this one."
"If I have that code:

```
try:
    some_method()
except Exception, e:

```

How can I get this Exception value (string
representation I mean)?","use `str`

```
try:
    some_method()
except Exception as e:
    s = str(e)

```

Also, most exception classes will have an `args` attribute. Often, `args[0]` will be an error message.

It should be noted that just using `str` will return an empty string if there's no error message whereas using `repr` as pyfunc recommends will at least display the class of the exception. My take is that if you're printing it out, it's for an end user that doesn't care what the class is and just wants an error message.

It really depends on the class of exception that you are dealing with and how it is instantiated. Did you have something in particular in mind?"
What's the easiest way to shuffle an array with python?,"```
import random
random.shuffle(array)

```"
"I would like to make several statements that give standard output without seeing newlines in between statements.

Specifically, suppose I have:

```
for item in range(1,100):
    print item

```

The result is:

```
1
2
3
4
.
.
.

```

How get this to instead look like:

```
1 2 3 4 5 ...

```

Even better, is it possible to print the single number *over* the last number, so only one number is on the screen at a time?","Change `print item` to:

* `print item,` in Python 2.7
* `print(item, end="" "")` in Python 3

If you want to print the data dynamically use following syntax:

* `print(item, sep=' ', end='', flush=True)` in Python 3"
"I have a script reading in a csv file with very huge fields:

```
# example from http://docs.python.org/3.3/library/csv.html?highlight=csv%20dictreader#examples
import csv
with open('some.csv', newline='') as f:
    reader = csv.reader(f)
    for row in reader:
        print(row)

```

However, this throws the following error on some csv files:

```
_csv.Error: field larger than field limit (131072)

```

How can I analyze csv files with huge fields? Skipping the lines with huge fields is not an option as the data needs to be analyzed in subsequent steps.","The csv file might contain very huge fields, therefore increase the `field_size_limit`:

```
import sys
import csv

csv.field_size_limit(sys.maxsize)

```

`sys.maxsize` works for Python 2.x and 3.x. `sys.maxint` would only work with Python 2.x ([SO: what-is-sys-maxint-in-python-3](https://stackoverflow.com/questions/13795758/what-is-sys-maxint-in-python-3))

### Update

As Geoff pointed out, the code above might result in the following error: `OverflowError: Python int too large to convert to C long`.
To circumvent this, you could use the following *quick and dirty* code (which should work on every system with Python 2 and Python 3):

```
import sys
import csv
maxInt = sys.maxsize

while True:
    # decrease the maxInt value by factor 10 
    # as long as the OverflowError occurs.

    try:
        csv.field_size_limit(maxInt)
        break
    except OverflowError:
        maxInt = int(maxInt/10)

```"
"I know I should avoid using global variables in the first place due to confusion like this, but if I were to use them, is the following a valid way to go about using them? (I am trying to call the global copy of a variable created in a separate function.)

```
x = ""somevalue""

def func_A ():
   global x
   # Do things to x
   return x

def func_B():
   x = func_A()
   # Do things
   return x

func_A()
func_B()

```

Does the `x` that the second function uses have the same value of the global copy of `x` that `func_a` uses and modifies? When calling the functions after definition, does order matter?","If you want to simply access a global variable you just use its name. However to **change** its value you need to use the [`global`](https://docs.python.org/reference/simple_stmts.html#the-global-statement) keyword.

E.g.

```
global someVar
someVar = 55

```

This would change the value of the global variable to 55. Otherwise it would just assign 55 to a local variable.

The order of function definition listings doesn't matter (assuming they don't refer to each other in some way), the order they are called does."
"While using the [`requests` module](http://requests.readthedocs.org/en/latest/), is there any way to print the raw HTTP request?

I don't want just the headers, I want the request line, headers, and content printout. Is it possible to see what ultimately is constructed from HTTP request?","[Since v1.2.3](https://docs.python-requests.org/en/latest/api/#requests.PreparedRequest) Requests added the PreparedRequest object. As per the documentation ""it contains the exact bytes that will be sent to the server"".

One can use this to pretty print a request, like so:

```
import requests

req = requests.Request('POST','http://stackoverflow.com',headers={'X-Custom':'Test'},data='a=1&b=2')
prepared = req.prepare()

def pretty_print_POST(req):
    """"""
    At this point it is completely built and ready
    to be fired; it is ""prepared"".

    However pay attention at the formatting used in 
    this function because it is programmed to be pretty 
    printed and may differ from the actual request.
    """"""
    print('{}\n{}\r\n{}\r\n\r\n{}'.format(
        '-----------START-----------',
        req.method + ' ' + req.url,
        '\r\n'.join('{}: {}'.format(k, v) for k, v in req.headers.items()),
        req.body,
    ))

pretty_print_POST(prepared)

```

which produces:

```
-----------START-----------
POST http://stackoverflow.com/
Content-Length: 7
X-Custom: Test

a=1&b=2

```

Then you can send the actual request with this:

```
s = requests.Session()
s.send(prepared)

```

These links are to the latest documentation available, so they might change in content:
[Advanced - Prepared requests](https://docs.python-requests.org/en/latest/user/advanced/#prepared-requests) and [API - Lower level classes](https://docs.python-requests.org/en/latest/api/#lower-level-classes)"
"I got a lot of errors with the message :

> ""DatabaseError: current transaction is aborted, commands ignored until end of transaction block""

after changed from python-psycopg to python-psycopg2 as Django project's database engine.

The code remains the same, just don't know where those errors are from.","This is what postgres does when a query produces an error and you try to run another query without first rolling back the transaction. (You might think of it as a safety feature, to keep you from corrupting your data.)

To fix this, you'll want to figure out where in the code that bad query is being executed. It might be helpful to use the [log\_statement](http://www.postgresql.org/docs/current/static/runtime-config-logging.html#GUC-LOG-STATEMENT) and [log\_min\_error\_statement](http://www.postgresql.org/docs/current/static/runtime-config-logging.html#GUC-LOG-MIN-ERROR-STATEMENT) options in your postgresql server."
"I'm trying to remove specific characters from a string using Python. This is the code I'm using right now. Unfortunately, it appears to do nothing to the string.

```
for char in line:
    if char in "" ?.!/;:"":
        line.replace(char,'')

```

How do I do this properly?

---

See [Why doesn't calling a string method (such as .replace or .strip) modify (mutate) the string?](https://stackoverflow.com/questions/9189172) for the specific debugging question about what is wrong with this approach. Answers here mainly focus on how to solve the problem.","Strings in Python are *immutable* (can't be changed). Because of this, the effect of `line.replace(...)` is just to create a new string, rather than changing the old one. You need to *rebind* (assign) it to `line` in order to have that variable take the new value, with those characters removed.

Also, the way you are doing it is going to be kind of slow, relatively. It's also likely to be a bit confusing to experienced pythonators, who will see a doubly-nested structure and think for a moment that something more complicated is going on.

Starting in Python 2.6 and newer Python 2.x versions \*, you can instead use [`str.translate`](https://docs.python.org/2/library/stdtypes.html#str.translate), (*see **Python 3 answer** below*):

```
line = line.translate(None, '!@#$')

```

or regular expression replacement with [`re.sub`](https://docs.python.org/2/library/re.html#re.sub)

```
import re
line = re.sub('[!@#$]', '', line)

```

The characters enclosed in brackets constitute a *character class*. Any characters in `line` which are in that class are replaced with the second parameter to `sub`: an empty string.

### Python 3 answer

In Python 3, strings are Unicode. You'll have to translate a little differently. kevpie mentions this in a [comment](https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python#comment-4205256) on one of the answers, and it's noted in the [documentation for `str.translate`](https://docs.python.org/2/library/stdtypes.html#str.translate).

When calling the `translate` method of a Unicode string, you cannot pass the second parameter that we used above. You also can't pass `None` as the first parameter. Instead, you pass a translation table (usually a dictionary) as the only parameter. This table maps the *ordinal values* of characters (i.e. the result of calling [`ord`](https://docs.python.org/2/library/functions.html#ord) on them) to the ordinal values of the characters which should replace them, or—usefully to us—`None` to indicate that they should be deleted.

So to do the above dance with a Unicode string you would call something like

```
translation_table = dict.fromkeys(map(ord, '!@#$'), None)
unicode_line = unicode_line.translate(translation_table)

```

Here [`dict.fromkeys`](https://docs.python.org/2/library/stdtypes.html#dict.fromkeys) and [`map`](https://docs.python.org/2/library/functions.html#map) are used to succinctly generate a dictionary containing

```
{ord('!'): None, ord('@'): None, ...}

```

Even simpler, as [another answer puts it](https://stackoverflow.com/questions/23175809/typeerror-translate-takes-one-argument-2-given-python), create the translation table in place:

```
unicode_line = unicode_line.translate({ord(c): None for c in '!@#$'})

```

Or, as brought up by [Joseph Lee](https://stackoverflow.com/a/47030484/2932052), create the same translation table with [`str.maketrans`](https://docs.python.org/3/library/stdtypes.html#str.maketrans):

```
unicode_line = unicode_line.translate(str.maketrans('', '', '!@#$'))

```

---

\* for compatibility with earlier Pythons, you can create a ""null"" translation table to pass in place of `None`:

```
import string
line = line.translate(string.maketrans('', ''), '!@#$')

```

Here [`string.maketrans`](https://docs.python.org/2/library/string.html#string.maketrans) is used to create a *translation table*, which is just a string containing the characters with ordinal values 0 to 255."
"Now that it's clear [what a metaclass is](https://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python), there is an associated concept that I use all the time without knowing what it really means.

I suppose everybody made once a mistake with parenthesis, resulting in an ""object is not callable"" exception. What's more, using `__init__` and `__new__` lead to wonder what this bloody `__call__` can be used for.

Could you give me some explanations, including examples with the magic method ?","A callable is anything that can be called.

The [built-in *callable* (PyCallable\_Check in objects.c)](http://svn.python.org/projects/python/trunk/Objects/object.c) checks if the argument is either:

* an instance of a class with a `__call__` method or
* is of a type that has a non null *tp\_call* (c struct) member which indicates callability otherwise (such as in functions, methods etc.)

The method named `__call__` is ([according to the documentation](https://docs.python.org/3/reference/datamodel.html#object.__call__))

> Called when the instance is ''called'' as a function

Example
-------

```
class Foo:
  def __call__(self):
    print 'called'

foo_instance = Foo()
foo_instance() #this is calling the __call__ method

```"
"I use the following simple code to parse some arguments; note that one of them is required. Unfortunately, when the user runs the script without providing the argument, the displayed usage/help text does not indicate that there is a non-optional argument, which I find very confusing. How can I get python to indicate that an argument is not optional?

Here is the code:

```
import argparse
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Foo')
    parser.add_argument('-i','--input', help='Input file name', required=True)
    parser.add_argument('-o','--output', help='Output file name', default=""stdout"")
    args = parser.parse_args()
    print (""Input file: %s"" % args.input )
    print (""Output file: %s"" % args.output )

```

When running above code without providing the required argument, I get the following output:

```
usage: foo.py [-h] -i INPUT [-o OUTPUT]

Foo

optional arguments:
    -h, --help            show this help message and exit
    -i INPUT, --input INPUT
                          Input file name
    -o OUTPUT, --output OUTPUT
                          Output file name

```","Parameters starting with `-` or `--` are usually considered optional. All other parameters are positional parameters and as such required by design (like positional function arguments). It is possible to require optional arguments, but this is a bit against their design. Since they are still part of the non-positional arguments, they will still be listed under the confusing header “optional arguments” even if they are required. The missing square brackets in the usage part however show that they are indeed required.

See also the [documentation](https://docs.python.org/3/library/argparse.html#required):

> In general, the argparse module assumes that flags like -f and --bar indicate optional arguments, which can always be omitted at the command line.
>
> **Note:** Required options are generally considered bad form because users expect options to be optional, and thus they should be avoided when possible.

That being said, the headers *“positional arguments”* and *“optional arguments”* in the help are generated by two argument groups in which the arguments are automatically separated into. Now, you could “hack into it” and change the name of the optional ones, but a far more elegant solution would be to create another group for “required named arguments” (or whatever you want to call them):

```
parser = argparse.ArgumentParser(description='Foo')
parser.add_argument('-o', '--output', help='Output file name', default='stdout')
requiredNamed = parser.add_argument_group('required named arguments')
requiredNamed.add_argument('-i', '--input', help='Input file name', required=True)
parser.parse_args(['-h'])

```

```
usage: [-h] [-o OUTPUT] -i INPUT

Foo

optional arguments:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Output file name

required named arguments:
  -i INPUT, --input INPUT
                        Input file name

```"
"I know it's an easy way of doing it but i didn't find it neither here nor on google.
So i was curious if there is a way to install multiple packages using pip.
Something like:

```
pip install progra1 , progra2 ,progra3 ,progra4 . 

```

or:

```
pip install (command to read some txt containing the name of the modules) 

```","For installing multiple packages on the command line, just pass them as a space-delimited list, e.g.:

```
pip install wsgiref boto

```

For installing from a text file, then, from `pip install --help`:

> -r FILENAME, --requirement=FILENAME
>
> Install all the packages listed in the given requirements file. This option can be used multiple times.

Take a look at [the pip documentation regarding requirements files](http://pip.readthedocs.org/en/latest/user_guide.html#requirements-files) for their general layout and syntax - note that you can generate one based on current environment / site-packages with `pip freeze` if you want a quick example - e.g. (based on having installed `wsgiref` and `boto` in a clean virtualenv):

```
$ pip freeze
boto==2.3.0
wsgiref==0.1.2

```"
"When piping the output of a Python program, the Python interpreter gets confused about encoding and sets it to None. This means a program like this:

```
# -*- coding: utf-8 -*-
print u""åäö""

```

will work fine when run normally, but fail with:

> UnicodeEncodeError: 'ascii' codec can't encode character u'\xa0' in position 0: ordinal not in range(128)

when used in a pipe sequence.

What is the best way to make this work when piping? Can I just tell it to use whatever encoding the shell/filesystem/whatever is using?

The suggestions I have seen thus far is to modify your site.py directly, or hardcoding the defaultencoding using this hack:

```
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
print u""åäö""

```

Is there a better way to make piping work?","Your code works when run in an script because Python encodes the output to whatever encoding your terminal application is using. If you are piping you must encode it yourself.

A rule of thumb is: Always use Unicode internally. Decode what you receive, and encode what you send.

```
# -*- coding: utf-8 -*-
print u""åäö"".encode('utf-8')

```

Another didactic example is a Python program to convert between ISO-8859-1 and UTF-8, making everything uppercase in between.

```
import sys
for line in sys.stdin:
    # Decode what you receive:
    line = line.decode('iso8859-1')

    # Work with Unicode internally:
    line = line.upper()

    # Encode what you send:
    line = line.encode('utf-8')
    sys.stdout.write(line)

```

Setting the system default encoding is a bad idea, because some modules and libraries you use can rely on the fact it is ASCII. Don't do it."
"I am getting `new_tag` from a form text field with `self.response.get(""new_tag"")` and `selected_tags` from checkbox fields with

```
self.response.get_all(""selected_tags"")

```

I combine them like this:

```
tag_string = new_tag
new_tag_list = f1.striplist(tag_string.split("","") + selected_tags)

```

(`f1.striplist` is a function that strips white spaces inside the strings in the list.)

But in the case that `tag_list` is empty (no new tags are entered) but there are some `selected_tags`, `new_tag_list` contains an empty string `"" ""`.

For example, from `logging.info`:

```
new_tag
selected_tags[u'Hello', u'Cool', u'Glam']
new_tag_list[u'', u'Hello', u'Cool', u'Glam']

```

How do I get rid of the empty string?

If there is an empty string in the list:

```
>>> s = [u'', u'Hello', u'Cool', u'Glam']
>>> i = s.index("""")
>>> del s[i]
>>> s
[u'Hello', u'Cool', u'Glam']

```

But if there is no empty string:

```
>>> s = [u'Hello', u'Cool', u'Glam']
>>> if s.index(""""):
        i = s.index("""")
        del s[i]
    else:
        print ""new_tag_list has no empty string""

```

But this gives:

```
Traceback (most recent call last):
  File ""<pyshell#30>"", line 1, in <module>
    if new_tag_list.index(""""):
        ValueError: list.index(x): x not in list

```

Why does this happen, and how do I work around it?","1) Almost-English style:
------------------------

Test for presence using the `in` operator, then apply the `remove` method.

```
if thing in some_list: some_list.remove(thing)

```

The `remove`method will remove only the first occurrence of `thing`, in order to remove all occurrences you can use `while` instead of `if`.

```
while thing in some_list: some_list.remove(thing)    

```

* Simple enough, probably my choice.for small lists (can't resist one-liners)

2) [Duck-typed](http://docs.python.org/glossary.html#term-duck-typing), [EAFP](http://docs.python.org/glossary.html#term-eafp) style:
-------------------------------------------------------------------------------------------------------------------------------------

This shoot-first-ask-questions-last attitude is common in Python. Instead of testing in advance if the object is suitable, just carry out the operation and catch relevant Exceptions:

```
try:
    some_list.remove(thing)
except ValueError:
    pass # or scream: thing not in some_list!
except AttributeError:
    call_security(""some_list not quacking like a list!"")

```

Off course the second except clause in the example above is not only of questionable humor but totally unnecessary (the point was to illustrate duck-typing for people not familiar with the concept).

If you expect multiple occurrences of thing:

```
while True:
    try:
        some_list.remove(thing)
    except ValueError:
        break

```

* a little verbose for this specific use case, but very idiomatic in Python.
* this performs better than #1
* [PEP 463](http://www.python.org/dev/peps/pep-0463/) proposed a shorter syntax for try/except simple usage that would be handy here, but it was not approved.

However, with [contextlib's suppress() contextmanager](https://docs.python.org/3/library/contextlib.html#contextlib.suppress) (introduced in python 3.4) the above code can be simplified to this:

```
with suppress(ValueError, AttributeError):
    some_list.remove(thing)

```

Again, if you expect multiple occurrences of thing:

```
with suppress(ValueError):
    while True:
        some_list.remove(thing)

```

3) Functional style:
--------------------

Around 1993, Python got `lambda`, `reduce()`, `filter()` and `map()`, courtesy of a [Lisp](https://en.wikipedia.org/wiki/Lisp_%28programming_language%29) hacker who missed them and submitted working patches\*. You can use `filter` to remove elements from the list:

```
is_not_thing = lambda x: x is not thing
cleaned_list = filter(is_not_thing, some_list)

```

There is a shortcut that may be useful for your case: if you want to filter out empty items (in fact items where `bool(item) == False`, like `None`, zero, empty strings or other empty collections), you can pass None as the first argument:

```
cleaned_list = filter(None, some_list)

```

* **[update]**: in Python 2.x, `filter(function, iterable)` used to be equivalent to `[item for item in iterable if function(item)]` (or `[item for item in iterable if item]` if the first argument is `None`); in Python 3.x, it is now equivalent to `(item for item in iterable if function(item))`. The subtle difference is that filter used to return a list, now it works like a generator expression - this is OK if you are only iterating over the cleaned list and discarding it, but if you really need a list, you have to enclose the `filter()` call with the `list()` constructor.
* \*These Lispy flavored constructs are considered a little alien in Python. Around 2005, [Guido was even talking about dropping `filter`](http://www.artima.com/weblogs/viewpost.jsp?thread=98196) - along with companions `map` and `reduce` (they are not gone yet but `reduce` was moved into the [functools](http://docs.python.org/2/library/functools.html#functools.reduce) module, which is worth a look if you like [high order functions](http://en.wikipedia.org/wiki/Higher-order_function)).

4) Mathematical style:
----------------------

[List comprehensions](http://en.wikipedia.org/wiki/List_comprehension) became the preferred style for list manipulation in Python since introduced in version 2.0 by [PEP 202](http://www.python.org/dev/peps/pep-0202/). The rationale behind it is that List comprehensions provide a more concise way to create lists in situations where `map()` and `filter()` and/or nested loops would currently be used.

```
cleaned_list = [ x for x in some_list if x is not thing ]

```

Generator expressions were introduced in version 2.4 by [PEP 289](http://www.python.org/dev/peps/pep-0289/). A generator expression is better for situations where you don't really need (or want) to have a full list created in memory - like when you just want to iterate over the elements one at a time. If you are only iterating over the list, you can think of a generator expression as a [lazy evaluated](http://en.wikipedia.org/wiki/Lazy_evaluation) list comprehension:

```
for item in (x for x in some_list if x is not thing):
    do_your_thing_with(item)

```

* See [this Python history blog](http://python-history.blogspot.com.br/2010/06/from-list-comprehensions-to-generator.html) post by [GvR](http://en.wikipedia.org/wiki/Guido_van_Rossum).
* This syntax is inspired by the [set-builder notation](http://en.wikipedia.org/wiki/Set-builder_notation) in math.
* Python 3 has also [set](http://www.python.org/dev/peps/pep-3100/#id103) and [dict comprehensions](http://www.python.org/dev/peps/pep-0274/).

Notes
-----

1. you may want to use the inequality operator `!=` instead of `is not` ([the difference is important](https://stackoverflow.com/q/2209755/))
2. for critics of methods implying a list copy: contrary to popular belief, generator expressions are not always more efficient than list comprehensions - please profile before complaining"
"I would like to know how can I set a variable with another variable in Jinja.  
I have got a submenu and I would like to show which link is active.  
I tried this:

```
{% set active_link = {{recordtype}} -%}

```

where `recordtype` is a variable available in my template.","`{{ }}` tells the template to *print* the value, this won't work in expressions like you're trying to do. Instead, use the `{% set %}` template tag and then assign the value the same way you would in normal python code.

```
{% set testing = 'it worked' %}
{% set another = testing %}
{{ another }}

```

Result:

```
it worked

```"
"I'm looking for an easy (and quick) way to determine if two **unordered** lists contain the same elements:

For example:

```
['one', 'two', 'three'] == ['one', 'two', 'three'] :  true
['one', 'two', 'three'] == ['one', 'three', 'two'] :  true
['one', 'two', 'three'] == ['one', 'two', 'three', 'three'] :  false
['one', 'two', 'three'] == ['one', 'two', 'three', 'four'] :  false
['one', 'two', 'three'] == ['one', 'two', 'four'] :  false
['one', 'two', 'three'] == ['one'] :  false

```

I'm hoping to do this without using a map.","Python has a built-in datatype for an unordered collection of (hashable) things, called a `set`. If you convert both lists to sets, the comparison will be unordered.

```
set(x) == set(y)

```

[Documentation on `set`](http://docs.python.org/library/stdtypes.html#set)

---

EDIT: @mdwhatcott points out that you want to check for duplicates. `set` ignores these, so you need a similar data structure that also keeps track of the number of items in each list. This is called a [multiset](http://en.wikipedia.org/wiki/Multiset); the best approximation in the standard library is a [`collections.Counter`](http://docs.python.org/dev/library/collections.html#collections.Counter):

```
>>> import collections
>>> compare = lambda x, y: collections.Counter(x) == collections.Counter(y)
>>> 
>>> compare([1,2,3], [1,2,3,3])
False
>>> compare([1,2,3], [1,2,3])
True
>>> compare([1,2,3,3], [1,2,2,3])
False
>>> 

```"
"I want to use input from a user as a regex pattern for a search over some text. It works, but how I can handle cases where user puts characters that have meaning in regex?

For example, the user wants to search for Word `(s)`: regex engine will take the `(s)` as a group. I want it to treat it like a string `""(s)""` . I can run `replace` on user input and replace the `(` with `\(` and the `)` with `\)` but the problem is I will need to do replace for every possible regex symbol.

Do you know some better way ?","Use the `re.escape()` function for this:

[4.2.3 `re` Module Contents](http://docs.python.org/library/re.html#re.escape)

> **escape(string)**
>
> Return string with all non-alphanumerics backslashed; this is useful if you want to match an arbitrary literal string that may have regular expression metacharacters in it.

A simplistic example, search any occurence of the provided string optionally followed by 's', and return the match object.

```
def simplistic_plural(word, text):
    word_or_plural = re.escape(word) + 's?'
    return re.match(word_or_plural, text)

```"
"I have a string in which I would like curly-brackets, but also take advantage of the f-strings feature. Is there some syntax that works for this?

Here are two ways it does not work. I would like to include the literal text `{bar}` as part of the string.

```
foo = ""test""
fstring = f""{foo} {bar}""

```

`NameError: name 'bar' is not defined`

```
fstring = f""{foo} \{bar\}""

```

`SyntaxError: f-string expression part cannot include a backslash`

Desired result:

```
'test {bar}'

```

Edit: Looks like this question has the same answer as [How can I print literal curly-brace characters in a string and also use .format on it?](https://stackoverflow.com/questions/5466451/how-can-i-print-literal-curly-brace-characters-in-python-string-and-also-use-fo), but you can only know that if you know that `str.format` uses the same rules as the f-string. So hopefully this question has value in tying f-string searchers to this answer.","Although there is a custom syntax error from the parser, the [same trick](https://stackoverflow.com/q/5466451/674039) works as for calling `.format` on regular strings.

Use double curlies:

```
>>> foo = 'test'
>>> f'{foo} {{bar}}'
'test {bar}'

```

To embed a value within braces, you can use triple-braces.

```
>>> foo = 'test'
>>> f'{{{foo}}}'
'{test}'

```

It's mentioned in the spec [here](https://www.python.org/dev/peps/pep-0498/#specification) and the docs [here](https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals)."
"I have a `Decimal('3.9')` as part of an object, and wish to encode this to a JSON string which should look like `{'x': 3.9}`. I don't care about precision on the client side, so a float is fine.

Is there a good way to serialize this? JSONDecoder doesn't accept Decimal objects, and converting to a float beforehand yields `{'x': 3.8999999999999999}` which is wrong, and will be a big waste of bandwidth.","[Simplejson 2.1](https://github.com/simplejson/simplejson) and higher has native support for Decimal type:

```
>>> import simplejson as json

>>> json.dumps(Decimal('3.9'), use_decimal=True)
'3.9'

```

Note that `use_decimal` is `True` by default:

```
def dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True,
    allow_nan=True, cls=None, indent=None, separators=None,
    encoding='utf-8', default=None, use_decimal=True,
    namedtuple_as_object=True, tuple_as_array=True,
    bigint_as_string=False, sort_keys=False, item_sort_key=None,
    for_json=False, ignore_nan=False, **kw):

```

So:

```
>>> json.dumps(Decimal('3.9'))
'3.9'

```

Hopefully, this feature will be included in standard library."
"I would like to extract all the numbers contained in a string. Which is better suited for the purpose, regular expressions or the `isdigit()` method?

Example:

```
line = ""hello 12 hi 89""

```

Result:

```
[12, 89]

```","I'd use a regexp:

```
>>> import re
>>> re.findall(r'\d+', ""hello 42 I'm a 32 string 30"")
['42', '32', '30']

```

This would also match 42 from `bla42bla`. If you only want numbers delimited by word boundaries (space, period, comma), you can use \b:

```
>>> re.findall(r'\b\d+\b', ""he33llo 42 I'm a 32 string 30"")
['42', '32', '30']

```

To end up with a list of numbers instead of a list of strings:

```
>>> [int(s) for s in re.findall(r'\b\d+\b', ""he33llo 42 I'm a 32 string 30"")]
[42, 32, 30]

```

NOTE: this does not work for negative integers"
"Could someone provide me with a good way of importing a whole directory of modules?  
I have a structure like this:

```
/Foo
    bar.py
    spam.py
    eggs.py

```

I tried just converting it to a package by adding `__init__.py` and doing `from Foo import *` but it didn't work the way I had hoped.","List all python (`.py`) files in the current folder and put them as `__all__` variable in `__init__.py`

```
from os.path import dirname, basename, isfile, join
import glob
modules = glob.glob(join(dirname(__file__), ""*.py""))
__all__ = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]

```"
"I have a problem viewing the following `DataFrame`:

```
n = 100
foo = DataFrame(index=range(n))
foo['floats'] = np.random.randn(n)
foo

```

The problem is that it does not print all rows per default in ipython notebook, but I have to slice to view the resulting rows. Even the following option does not change the output:

```
pd.set_option('display.max_rows', 500)

```

Does anyone know how to display the whole array?","Set `display.max_rows`:

```
pd.set_option('display.max_rows', 500)

```

For older versions of pandas (<=0.11.0) you need to change both `display.height` and `display.max_rows`.

```
pd.set_option('display.height', 500)
pd.set_option('display.max_rows', 500)

```

See also [`pd.describe_option('display')`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.describe_option.html).

You can set an option only *temporarily* for this one time like this:

```
from IPython.display import display
with pd.option_context('display.max_rows', 100, 'display.max_columns', 10):
    display(df) #need display to show the dataframe when using with in jupyter
    #some pandas stuff

```

You can also reset an option back to its default value like this:

`pd.reset_option('display.max_rows')`

And reset all of them back:

`pd.reset_option('all')`"
"I have a pandas DataFrame with 4 columns and I want to create a **new** DataFrame that **only** has three of the columns. This question is similar to: [Extracting specific columns from a data frame](https://stackoverflow.com/questions/10085806/extracting-specific-columns-from-a-data-frame) but for pandas not R. The following code does not work, raises an error, and is certainly not the pandas way to do it.

```
import pandas as pd
old = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})
new = pd.DataFrame(zip(old.A, old.C, old.D)) 
# raises TypeError: data argument can't be an iterator 

```

What is the pandas way to do it?","There is a way of doing this and it actually looks similar to R

```
new = old[['A', 'C', 'D']].copy()

```

Here you are just selecting the columns you want from the original data frame and creating a variable for those. If you want to modify the new dataframe at all you'll probably want to use `.copy()` to avoid a `SettingWithCopyWarning`.

An alternative method is to use `filter` which will create a copy by default:

```
new = old.filter(['A','B','D'], axis=1)

```

Finally, depending on the number of columns in your original dataframe, it might be more succinct to express this using a `drop` (this will also create a copy by default):

```
new = old.drop('B', axis=1)

```"
"In the example code below, I'd like to get the return value of the function `worker`. How can I go about doing this? Where is this value stored?

**Example Code:**

```
import multiprocessing

def worker(procnum):
    '''worker function'''
    print str(procnum) + ' represent!'
    return procnum


if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=worker, args=(i,))
        jobs.append(p)
        p.start()

    for proc in jobs:
        proc.join()
    print jobs

```

**Output:**

```
0 represent!
1 represent!
2 represent!
3 represent!
4 represent!
[<Process(Process-1, stopped)>, <Process(Process-2, stopped)>, <Process(Process-3, stopped)>, <Process(Process-4, stopped)>, <Process(Process-5, stopped)>]

```

I can't seem to find the relevant attribute in the objects stored in `jobs`.","Use a [shared variable](http://docs.python.org/library/multiprocessing.html#sharing-state-between-processes) to communicate. For example, like this,

**Example Code:**

```
import multiprocessing


def worker(procnum, return_dict):
    """"""worker function""""""
    print(str(procnum) + "" represent!"")
    return_dict[procnum] = procnum


if __name__ == ""__main__"":
    manager = multiprocessing.Manager()
    return_dict = manager.dict()
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=worker, args=(i, return_dict))
        jobs.append(p)
        p.start()

    for proc in jobs:
        proc.join()
    print(return_dict.values())

```

**Output:**

```
0 represent!
1 represent!
3 represent!
2 represent!
4 represent!
[0, 1, 3, 2, 4]

```"
"I know how to generate a random number within a range in Python.

```
random.randint(numLow, numHigh)

```

And I know I can put this in a loop to generate n amount of these numbers

```
for x in range (0, n):
    listOfNumbers.append(random.randint(numLow, numHigh))

```

However, I need to make sure each number in that list is unique. Other than a load of conditional statements, is there a straightforward way of generating n number of unique random numbers?

The important thing is that each number in the list is different to the others..

So

[12, 5, 6, 1] = good

But

[12, 5, 5, 1] = bad, because the number 5 occurs twice.","If you just need sampling without replacement:

```
>>> import random
>>> random.sample(range(1, 100), 3)
[77, 52, 45]

```

[random.sample](https://docs.python.org/2/library/random.html#random.sample) takes a population and a sample size `k` and returns `k` random members of the population.

If you have to control for the case where `k` is larger than `len(population)`, you need to be prepared to catch a `ValueError`:

```
>>> try:
...   random.sample(range(1, 2), 3)
... except ValueError:
...   print('Sample size exceeded population size.')
... 
Sample size exceeded population size

```"
"It seems so ""dirty"" emptying a list in this way:

```
while len(alist) > 0 : alist.pop()

```

Does a clear way exist to do that?","This actually removes the contents from the list, but doesn't replace the old label with a new empty list:

```
del lst[:]

```

Here's an example:

```
lst1 = [1, 2, 3]
lst2 = lst1
del lst1[:]
print(lst2)

```

For the sake of completeness, the slice assignment has the same effect:

```
lst[:] = []

```

It can also be used to shrink a part of the list while replacing a part at the same time (but that is out of the scope of the question).

Note that doing `lst = []` does not empty the list, just creates a new object and binds it to the variable `lst`, but the old list will still have the same elements, and effect will be apparent if it had other variable bindings."
"The Python requests module is simple and elegant but one thing bugs me.
It is possible to get a *requests.exception.ConnectionError* with a message like:

```
Max retries exceeded with url: ...

```

This implies that requests can attempt to access the data several times. But there is not a single mention of this possibility anywhere in the docs. Looking at the source code I didn't find any place where I could alter the default (presumably 0) value.

So is it possible to somehow set the maximum number of retries for requests?","This will not only change the *max\_retries* but also enable a backoff strategy which makes requests to all *http://* addresses sleep for a period of time before retrying (to a total of 5 times):

```
import requests

from requests.adapters import HTTPAdapter, Retry

s = requests.Session()

retries = Retry(total=5,
                backoff_factor=0.1,
                status_forcelist=[ 500, 502, 503, 504 ])

s.mount('http://', HTTPAdapter(max_retries=retries))

s.get('http://httpstat.us/500')

```

As per [documentation for `Retry`](http://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry): if the backoff\_factor is *0.1*, then sleep() will sleep for [0.05s, 0.1s, 0.2s, 0.4s, ...] between retries. It will also force a retry if the status code returned is *500*, *502*, *503* or *504*.

Various other options to `Retry` allow for more granular control:

* *total* – Total number of retries to allow.
* *connect* – How many connection-related errors to retry on.
* *read* – How many times to retry on read errors.
* *redirect* – How many redirects to perform.
* *method\_whitelist* – Set of uppercased HTTP method verbs that we should retry on.
* *status\_forcelist* – A set of HTTP status codes that we should force a retry on.
* *backoff\_factor* – A backoff factor to apply between attempts.
* *raise\_on\_redirect* – Whether, if the number of redirects is exhausted, to raise a `MaxRetryError`, or to return a response with a response code in the *3xx* range.
* raise\_on\_status – Similar meaning to *raise\_on\_redirect*: whether we should raise an exception, or return a response, if status falls in *status\_forcelist* range and retries have been exhausted.

**NB**: *raise\_on\_status* is relatively new, and has not made it into a release of urllib3 or requests yet. The *raise\_on\_status* keyword argument appears to have made it into the standard library at most in python version 3.6.

To make requests retry on specific HTTP status codes, use *status\_forcelist*. For example, *status\_forcelist=[503]* will retry on status code *503* (service unavailable).

By default, the retry only fires for these conditions:

* Could not get a connection from the pool.
* `TimeoutError`
* `HTTPException` raised (from *http.client* in Python 3 else *httplib*).
  This seems to be low-level HTTP exceptions, like URL or protocol not
  formed correctly.
* `SocketError`
* `ProtocolError`

Notice that these are all exceptions that prevent a regular HTTP response from being received. If **any** regular response is generated, no retry is done. Without using the *status\_forcelist*, even a response with status 500 will not be retried.

To make it behave in a manner which is more intuitive for working with a remote API or web server, I would use the above code snippet, which forces retries on statuses *500*, *502*, *503* and *504*, all of which are not uncommon on the web and (possibly) recoverable given a big enough backoff period."
"How does [`np.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) work?

Given arrays `A` and `B`, their matrix multiplication followed by transpose is computed using `(A @ B).T`, or equivalently, using:

```
np.einsum(""ij, jk -> ki"", A, B)

```","(Note: this answer is based on a short [blog post](http://ajcr.net/Basic-guide-to-einsum/) about `einsum` I wrote a while ago.)

What does `einsum` do?
----------------------

Imagine that we have two multi-dimensional arrays, `A` and `B`. Now let's suppose we want to...

* *multiply* `A` with `B` in a particular way to create new array of products; and then maybe
* *sum* this new array along particular axes; and then maybe
* *transpose* the axes of the new array in a particular order.

There's a good chance that `einsum` will help us do this faster and more memory-efficiently than combinations of the NumPy functions like `multiply`, `sum` and `transpose` will allow.

How does `einsum` work?
-----------------------

Here's a simple (but not completely trivial) example. Take the following two arrays:

```
A = np.array([0, 1, 2])

B = np.array([[ 0,  1,  2,  3],
              [ 4,  5,  6,  7],
              [ 8,  9, 10, 11]])

```

We will multiply `A` and `B` element-wise and then sum along the rows of the new array. In ""normal"" NumPy we'd write:

```
>>> (A[:, np.newaxis] * B).sum(axis=1)
array([ 0, 22, 76])

```

So here, the indexing operation on `A` lines up the first axes of the two arrays so that the multiplication can be broadcast. The rows of the array of products are then summed to return the answer.

Now if we wanted to use `einsum` instead, we could write:

```
>>> np.einsum('i,ij->i', A, B)
array([ 0, 22, 76])

```

The *signature* string `'i,ij->i'` is the key here and needs a little bit of explaining. You can think of it in two halves. On the left-hand side (left of the `->`) we've labelled the two input arrays. To the right of `->`, we've labelled the array we want to end up with.

Here is what happens next:

* `A` has one axis; we've labelled it `i`. And `B` has two axes; we've labelled axis 0 as `i` and axis 1 as `j`.
* By **repeating** the label `i` in both input arrays, we are telling `einsum` that these two axes should be **multiplied** together. In other words, we're multiplying array `A` with each column of array `B`, just like `A[:, np.newaxis] * B` does.
* Notice that `j` does not appear as a label in our desired output; we've just used `i` (we want to end up with a 1D array). By **omitting** the label, we're telling `einsum` to **sum** along this axis. In other words, we're summing the rows of the products, just like `.sum(axis=1)` does.

That's basically all you need to know to use `einsum`. It helps to play about a little; if we leave both labels in the output, `'i,ij->ij'`, we get back a 2D array of products (same as `A[:, np.newaxis] * B`). If we say no output labels, `'i,ij->`, we get back a single number (same as doing `(A[:, np.newaxis] * B).sum()`).

The great thing about `einsum` however, is that it does not build a temporary array of products first; it just sums the products as it goes. This can lead to big savings in memory use.

A slightly bigger example
-------------------------

To explain the dot product, here are two new arrays:

```
A = array([[1, 1, 1],
           [2, 2, 2],
           [5, 5, 5]])

B = array([[0, 1, 0],
           [1, 1, 0],
           [1, 1, 1]])

```

We will compute the dot product using `np.einsum('ij,jk->ik', A, B)`. Here's a picture showing the labelling of the `A` and `B` and the output array that we get from the function:

[![enter image description here](https://i.sstatic.net/bPCVw.png)](https://i.sstatic.net/bPCVw.png)

You can see that label `j` is repeated - this means we're multiplying the rows of `A` with the columns of `B`. Furthermore, the label `j` is not included in the output - we're summing these products. Labels `i` and `k` are kept for the output, so we get back a 2D array.

It might be even clearer to compare this result with the array where the label `j` is *not* summed. Below, on the left you can see the 3D array that results from writing `np.einsum('ij,jk->ijk', A, B)` (i.e. we've kept label `j`):

[![enter image description here](https://i.sstatic.net/kOqM5.png)](https://i.sstatic.net/kOqM5.png)

Summing axis `j` gives the expected dot product, shown on the right.

Some exercises
--------------

To get more of a feel for `einsum`, it can be useful to implement familiar NumPy array operations using the subscript notation. Anything that involves combinations of multiplying and summing axes can be written using `einsum`.

Let A and B be two 1D arrays with the same length. For example, `A = np.arange(10)` and `B = np.arange(5, 15)`.

* The sum of `A` can be written:

  ```
  np.einsum('i->', A)

  ```
* Element-wise multiplication, `A * B`, can be written:

  ```
  np.einsum('i,i->i', A, B)

  ```
* The inner product or dot product, `np.inner(A, B)` or `np.dot(A, B)`, can be written:

  ```
  np.einsum('i,i->', A, B) # or just use 'i,i'

  ```
* The outer product, `np.outer(A, B)`, can be written:

  ```
  np.einsum('i,j->ij', A, B)

  ```

For 2D arrays, `C` and `D`, provided that the axes are compatible lengths (both the same length or one of them of has length 1), here are a few examples:

* The trace of `C` (sum of main diagonal), `np.trace(C)`, can be written:

  ```
  np.einsum('ii', C)

  ```
* Element-wise multiplication of `C` and the transpose of `D`, `C * D.T`, can be written:

  ```
  np.einsum('ij,ji->ij', C, D)

  ```
* Multiplying each element of `C` by the array `D` (to make a 4D array), `C[:, :, None, None] * D`, can be written:

  ```
  np.einsum('ij,kl->ijkl', C, D)    

  ```"
"I work in an environment in which computational resources are shared, i.e., we have a few server machines equipped with a few Nvidia Titan X GPUs each.

For small to moderate size models, the 12 GB of the Titan X is usually enough for 2â€“3 people to run training concurrently on the same GPU. If the models are small enough that a single model does not take full advantage of all the computational units of the GPU, this can actually result in a speedup compared with running one training process after the other. Even in cases where the concurrent access to the GPU does slow down the individual training time, it is still nice to have the flexibility of having multiple users simultaneously train on the GPU.

The problem with TensorFlow is that, by default, it allocates the full amount of available GPU memory when it is launched. Even for a small two-layer neural network, I see that all 12 GB of the GPU memory is used up.

Is there a way to make TensorFlow only allocate, say, 4 GB of GPU memory, if one knows that this is enough for a given model?","You can set the fraction of GPU memory to be allocated when you construct a [`tf.Session`](https://www.tensorflow.org/api_docs/python/tf/Session) by passing a [`tf.GPUOptions`](https://github.com/tensorflow/tensorflow/blob/08ed32dbb9e8f67eec9efce3807b5bdb3933eb2f/tensorflow/core/protobuf/config.proto) as part of the optional `config` argument:

```
# Assume that you have 12GB of GPU memory and want to allocate ~4GB:
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)

sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))

```

The `per_process_gpu_memory_fraction` acts as a hard upper bound on the amount of GPU memory that will be used by the process on each GPU on the same machine. Currently, this fraction is applied uniformly to all of the GPUs on the same machine; there is no way to set this on a per-GPU basis."
"How can I create a `list` which contains only zeros? I want to be able to create a zeros `list` for each `int` in `range(10)`

For example, if the `int` in the range was `4` I will get:

```
[0,0,0,0]

```

and for `7`:

```
[0,0,0,0,0,0,0]

```","```
#add code here to figure out the number of 0's you need, naming the variable n.
listofzeros = [0] * n

```

if you prefer to put it in the function, just drop in that code and add `return listofzeros`

Which would look like this:

```
def zerolistmaker(n):
    listofzeros = [0] * n
    return listofzeros

```

sample output:

```
>>> zerolistmaker(4)
[0, 0, 0, 0]
>>> zerolistmaker(5)
[0, 0, 0, 0, 0]
>>> zerolistmaker(15)
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> 

```"
"I have to use [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29) and [Django](http://en.wikipedia.org/wiki/Django_%28web_framework%29) for our application. So, I have two versions of Python, 2.6 and 2.7. Now I have installed Django. I could run the sample application for testing Django successfully. But how do I check whether Django uses the 2.6 or 2.7 version and what version of modules Django uses?","Django 1.5 supports Python 2.6.5 and later.

If you're under Linux and want to check the Python version you're using, run `python -V` from the command line.

If you want to check the Django version, open a Python console and type

```
>>> import django
>>> django.VERSION
(2, 0, 0, 'final', 0)

```"
"Given that a function `a_method` has been defined like

```
def a_method(arg1, arg2):
    pass

```

Starting from `a_method` itself, how can I get the argument names - for example, as a tuple of strings, like `(""arg1"", ""arg2"")`?","Take a look at the [`inspect`](http://docs.python.org/library/inspect.html) module - this will do the inspection of the various code object properties for you.

```
>>> inspect.getfullargspec(a_method)
(['arg1', 'arg2'], None, None, None)

```

The other results are the name of the \*args and \*\*kwargs variables, and the defaults provided. ie.

```
>>> def foo(a, b, c=4, *arglist, **keywords): pass
>>> inspect.getfullargspec(foo)
(['a', 'b', 'c'], 'arglist', 'keywords', (4,))

```

Note that some callables may not be introspectable in certain implementations of Python. For Example, in CPython, some built-in functions defined in C provide no metadata about their arguments. As a result, you will get a `ValueError` if you use `inspect.getfullargspec()` on a built-in function.

Since Python 3.3, you can use [`inspect.signature()`](https://docs.python.org/library/inspect.html#introspecting-callables-with-the-signature-object) to see the call signature of a callable object:

```
>>> inspect.signature(foo)
<Signature (a, b, c=4, *arglist, **keywords)>

```"
"In Python, is it possible to have multiple `except` statements for one `try` statement? Such as:

```
try:
    #something1
    #something2
except ExceptionType1:
    #return xyz
except ExceptionType2:
    #return abc

```

---

For the case of handling multiple exceptions the same way, see [Catch multiple exceptions in one line (except block)](https://stackoverflow.com/q/6470428/6045800)","Yes, it is possible.

```
try:
   ...
except FirstException:
   handle_first_one()

except SecondException:
   handle_second_one()

except (ThirdException, FourthException, FifthException) as e:
   handle_either_of_3rd_4th_or_5th()

except Exception:
   handle_all_other_exceptions()

```

See: <http://docs.python.org/tutorial/errors.html>

The ""as"" keyword is used to assign the error to a variable so that the error can be investigated more thoroughly later on in the code. Also note that the parentheses for the triple exception case are needed in python 3. This page has more info: [Catch multiple exceptions in one line (except block)](https://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block)"
"This is how I am accustomed to `filter`, `map`, and `reduce` working in Python 2:

```
>>> def f(x):
        return x % 2 != 0 and x % 3 != 0
>>> filter(f, range(2, 25))
[5, 7, 11, 13, 17, 19, 23]

>>> def cube(x):
        return x*x*x
>>> map(cube, range(1, 11))
[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]

>>> def add(x,y):
        return x+y
>>> reduce(add, range(1, 11))
55

```

However, all of these seem to break in Python 3:

```
>>> filter(f, range(2, 25))
<filter object at 0x0000000002C14908>

>>> map(cube, range(1, 11))
<map object at 0x0000000002C82B70>

>>> reduce(add, range(1, 11))
Traceback (most recent call last):
  File ""<pyshell#8>"", line 1, in <module>
    reduce(add, range(1, 11))
NameError: name 'reduce' is not defined

```

Why are the results different? How can I get Python 3 code to work like the Python 2 code did?

---

See also: [What is the problem with reduce()?](https://stackoverflow.com/questions/181543) for specific motivation for the change to put `reduce` into a standard library module rather than leaving it as a builtin.

See [Getting a map() to return a list in Python 3.x](https://stackoverflow.com/questions/1303347) for more specific answers about `map`.","You can read about the changes in [What's New In Python 3.0](http://docs.python.org/3.0/whatsnew/3.0.html). You should read it thoroughly when you move from 2.x to 3.x since a lot has been changed.

The whole answer here are quotes from the documentation.

**[Views And Iterators Instead Of Lists](http://docs.python.org/3.0/whatsnew/3.0.html#views-and-iterators-instead-of-lists)**

> Some well-known APIs no longer return lists:
>
> * [...]
> * [`map()`](http://docs.python.org/3.0/library/functions.html#map) and [`filter()`](http://docs.python.org/3.0/library/functions.html#filter) return iterators. If you really need a list, a quick fix is e.g. `list(map(...))`, but a better fix is often to use a list comprehension (especially when the original code uses lambda), or rewriting the code so it doesnâ€™t need a list at all. Particularly tricky is `map()` invoked for the side effects of the function; the correct transformation is to use a regular `for` loop (since creating a list would just be wasteful).
> * [...]

**[Builtins](http://docs.python.org/3.0/whatsnew/3.0.html#builtins)**

> * [...]
> * Removed `reduce()`. Use [`functools.reduce()`](http://docs.python.org/3.0/library/functools.html#functools.reduce) if you really need it; however, 99 percent of the time an explicit `for` loop is more readable.
> * [...]"
"While I like to think of myself as a reasonably competent Python coder, one aspect of the language I've never been able to grok is decorators.

I know what they are (superficially), I've read tutorials, examples, questions on Stack Overflow, and I understand the syntax, can write my own, occasionally use @classmethod and @staticmethod, but it never occurs to me to use a decorator to solve a problem in my own Python code. I never encounter a problem where I think, ""Hmm...this looks like a job for a decorator!""

So, I'm wondering if you guys might offer some examples of where you've used decorators in your own programs, and hopefully I'll have an ""A-ha!"" moment and *get* them.","I use decorators mainly for timing purposes

```
def time_dec(func):

  def wrapper(*arg):
      t = time.clock()
      res = func(*arg)
      print func.func_name, time.clock()-t
      return res

  return wrapper


@time_dec
def myFunction(n):
    ...

```"
"I would like to know if a key exists in boto3. I can loop the bucket contents and check the key if it matches.

But that seems longer and an overkill. Boto3 official docs explicitly state how to do this.

May be I am missing the obvious. Can anybody point me how I can achieve this.","Boto 2's `boto.s3.key.Key` object used to have an `exists` method that checked if the key existed on S3 by doing a HEAD request and looking at the the result, but it seems that that no longer exists. You have to do it yourself:

```
import boto3
import botocore

s3 = boto3.resource('s3')

try:
    s3.Object('my-bucket', 'dootdoot.jpg').load()
except botocore.exceptions.ClientError as e:
    if e.response['Error']['Code'] == ""404"":
        # The object does not exist.
        ...
    else:
        # Something else has gone wrong.
        raise
else:
    # The object does exist.
    ...

```

`load()` does a HEAD request for a single key, which is fast, even if the object in question is large or you have many objects in your bucket.

Of course, you might be checking if the object exists because you're planning on using it. If that is the case, you can just forget about the `load()` and do a `get()` or `download_file()` directly, then handle the error case there."
"I am new to Python and I am playing with JSON data. I would like to dynamically build a JSON object by adding some key-value to an existing JSON object.

I tried the following but I get `TypeError: 'str' object does not support item assignment`:

```
import json

json_data = json.dumps({})
json_data[""key""] = ""value""

print 'JSON: ', json_data

```","You build the object **before** encoding it to a JSON string:

```
import json

data = {}
data['key'] = 'value'
json_data = json.dumps(data)

```

JSON is a *serialization* format, textual data *representing* a structure. It is not, itself, that structure."
"I've come across links that say Python is a strongly typed language.

However, I thought in strongly typed languages you couldn't do this:

```
bob = 1
bob = ""bob""

```

I thought a strongly typed language didn't accept type-changing at run-time. Maybe I've got a wrong (or too simplistic) definition of strong/weak types.

So, is Python a strongly or weakly typed language?","Python is strongly, dynamically typed.

* **Strong** typing means that the type of a value doesn't change in unexpected ways. A string containing only digits doesn't magically become a number, as may happen in Perl. Every change of type requires an explicit conversion.
* **Dynamic** typing means that runtime objects (values) have a type, as opposed to static typing where variables have a type.

As for your example

```
bob = 1
bob = ""bob""

```

This works because the variable does not have a type; it can name any object. After `bob=1`, you'll find that `type(bob)` returns `int`, but after `bob=""bob""`, it returns `str`. (Note that `type` is a regular function, so it evaluates its argument, then returns the type of the value.)

Contrast this with older dialects of C, which were weakly, statically typed, so that pointers and integers were pretty much interchangeable. (Modern ISO C requires conversions in many cases, but my compiler is still lenient about this by default.)

I must add that the strong vs. weak typing is more of a continuum than a boolean choice. C++ has stronger typing than C (more conversions required), but the type system can be subverted by using pointer casts.

The strength of the type system in a dynamic language such as Python is really determined by how its primitives and library functions respond to different types. E.g., `+` is overloaded so that it works on two numbers *or* two strings, but not a string and an number. This is a design choice made when `+` was implemented, but not really a necessity following from the language's semantics. In fact, when you overload `+` on a custom type, you can make it implicitly convert anything to a number:

```
def to_number(x):
    """"""Try to convert function argument to float-type object.""""""
    try: 
        return float(x) 
    except (TypeError, ValueError): 
        return 0 

class Foo:
    def __init__(self, number): 
        self.number = number

    def __add__(self, other):
        return self.number + to_number(other)

```

Instance of class `Foo` can be added to other objects:

```
>>> a = Foo(42)
>>> a + ""1""
43.0
>>> a + Foo
42
>>> a + 1
43.0
>>> a + None
42

```

Observe that even though strongly typed Python is completely fine with adding objects of type `int` and `float` and returns an object of type `float` (e.g., `int(42) + float(1)` returns `43.0`). On the other hand, due to the mismatch between types Haskell would complain if one tries the following `(42 :: Integer) + (1 :: Float)`. This makes Haskell a strictly typed language, where types are entirely disjoint and only a controlled form of overloading is possible via type classes."
I am having problems understanding how to email an attachment using Python. I have successfully emailed simple messages with the `smtplib`. Could someone please explain how to send an attachment in an email. I know there are other posts online but as a Python beginner I find them hard to understand.,"Here's another:

```
import smtplib
from os.path import basename
from email.mime.application import MIMEApplication
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.utils import COMMASPACE, formatdate


def send_mail(send_from, send_to, subject, text, files=None,
              server=""127.0.0.1""):
    assert isinstance(send_to, list)

    msg = MIMEMultipart()
    msg['From'] = send_from
    msg['To'] = COMMASPACE.join(send_to)
    msg['Date'] = formatdate(localtime=True)
    msg['Subject'] = subject

    msg.attach(MIMEText(text))

    for f in files or []:
        with open(f, ""rb"") as fil:
            part = MIMEApplication(
                fil.read(),
                Name=basename(f)
            )
        # After the file is closed
        part['Content-Disposition'] = 'attachment; filename=""%s""' % basename(f)
        msg.attach(part)


    smtp = smtplib.SMTP(server)
    smtp.sendmail(send_from, send_to, msg.as_string())
    smtp.close()

```

It's much the same as the first example... But it should be easier to drop in."
"I have a `pandas dataframe` in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ','). For example, `a` should become `b`:

```
In [7]: a
Out[7]: 
    var1  var2
0  a,b,c     1
1  d,e,f     2

In [8]: b
Out[8]: 
  var1  var2
0    a     1
1    b     1
2    c     1
3    d     2
4    e     2
5    f     2

```

So far, I have tried various simple functions, but the `.apply` method seems to only accept one row as return value when it is used on an axis, and I can't get `.transform` to work. Any suggestions would be much appreciated!

Example data:

```
from pandas import DataFrame
import numpy as np
a = DataFrame([{'var1': 'a,b,c', 'var2': 1},
               {'var1': 'd,e,f', 'var2': 2}])
b = DataFrame([{'var1': 'a', 'var2': 1},
               {'var1': 'b', 'var2': 1},
               {'var1': 'c', 'var2': 1},
               {'var1': 'd', 'var2': 2},
               {'var1': 'e', 'var2': 2},
               {'var1': 'f', 'var2': 2}])

```

I know this won't work because we lose DataFrame meta-data by going through numpy, but it should give you a sense of what I tried to do:

```
def fun(row):
    letters = row['var1']
    letters = letters.split(',')
    out = np.array([row] * len(letters))
    out['var1'] = letters
a['idx'] = range(a.shape[0])
z = a.groupby('idx')
z.transform(fun)

```","**UPDATE 3:** it makes more sense to use [`Series.explode()` / `DataFrame.explode()` methods](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html) (implemented in Pandas 0.25.0 and extended in Pandas 1.3.0 to support multi-column explode) as is shown in the usage example:

for a single column:

```
In [1]: df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],
   ...:                    'B': 1,
   ...:                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})

In [2]: df
Out[2]:
           A  B          C
0  [0, 1, 2]  1  [a, b, c]
1        foo  1        NaN
2         []  1         []
3     [3, 4]  1     [d, e]

In [3]: df.explode('A')
Out[3]:
     A  B          C
0    0  1  [a, b, c]
0    1  1  [a, b, c]
0    2  1  [a, b, c]
1  foo  1        NaN
2  NaN  1         []
3    3  1     [d, e]
3    4  1     [d, e]

```

for multiple columns (**for Pandas 1.3.0+**):

```
In [4]: df.explode(['A', 'C'])
Out[4]:
     A  B    C
0    0  1    a
0    1  1    b
0    2  1    c
1  foo  1  NaN
2  NaN  1  NaN
3    3  1    d
3    4  1    e

```

---

**UPDATE 2:** more generic vectorized function, which will work for multiple `normal` and multiple `list` columns

```
def explode(df, lst_cols, fill_value='', preserve_index=False):
    # make sure `lst_cols` is list-alike
    if (lst_cols is not None
        and len(lst_cols) > 0
        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):
        lst_cols = [lst_cols]
    # all columns except `lst_cols`
    idx_cols = df.columns.difference(lst_cols)
    # calculate lengths of lists
    lens = df[lst_cols[0]].str.len()
    # preserve original index values    
    idx = np.repeat(df.index.values, lens)
    # create ""exploded"" DF
    res = (pd.DataFrame({
                col:np.repeat(df[col].values, lens)
                for col in idx_cols},
                index=idx)
             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)
                            for col in lst_cols}))
    # append those rows that have empty lists
    if (lens == 0).any():
        # at least one list in cells is empty
        res = (res.append(df.loc[lens==0, idx_cols], sort=False)
                  .fillna(fill_value))
    # revert the original index order
    res = res.sort_index()
    # reset index if requested
    if not preserve_index:        
        res = res.reset_index(drop=True)
    return res

```

Demo:

Multiple `list` columns - all `list` columns must have the same # of elements in each row:

```
In [134]: df
Out[134]:
   aaa  myid        num          text
0   10     1  [1, 2, 3]  [aa, bb, cc]
1   11     2         []            []
2   12     3     [1, 2]      [cc, dd]
3   13     4         []            []

In [135]: explode(df, ['num','text'], fill_value='')
Out[135]:
   aaa  myid num text
0   10     1   1   aa
1   10     1   2   bb
2   10     1   3   cc
3   11     2
4   12     3   1   cc
5   12     3   2   dd
6   13     4

```

preserving original index values:

```
In [136]: explode(df, ['num','text'], fill_value='', preserve_index=True)
Out[136]:
   aaa  myid num text
0   10     1   1   aa
0   10     1   2   bb
0   10     1   3   cc
1   11     2
2   12     3   1   cc
2   12     3   2   dd
3   13     4

```

Setup:

```
df = pd.DataFrame({
 'aaa': {0: 10, 1: 11, 2: 12, 3: 13},
 'myid': {0: 1, 1: 2, 2: 3, 3: 4},
 'num': {0: [1, 2, 3], 1: [], 2: [1, 2], 3: []},
 'text': {0: ['aa', 'bb', 'cc'], 1: [], 2: ['cc', 'dd'], 3: []}
})

```

CSV column:

```
In [46]: df
Out[46]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

In [47]: explode(df.assign(var1=df.var1.str.split(',')), 'var1')
Out[47]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ

```

using this little trick we can convert CSV-like column to `list` column:

```
In [48]: df.assign(var1=df.var1.str.split(','))
Out[48]:
              var1  var2 var3
0        [a, b, c]     1   XX
1  [d, e, f, x, y]     2   ZZ

```

---

**UPDATE:** **generic vectorized approach (will work also for multiple columns):**

Original DF:

```
In [177]: df
Out[177]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

```

**Solution:**

first let's convert CSV strings to lists:

```
In [178]: lst_col = 'var1' 

In [179]: x = df.assign(**{lst_col:df[lst_col].str.split(',')})

In [180]: x
Out[180]:
              var1  var2 var3
0        [a, b, c]     1   XX
1  [d, e, f, x, y]     2   ZZ

```

Now we can do this:

```
In [181]: pd.DataFrame({
     ...:     col:np.repeat(x[col].values, x[lst_col].str.len())
     ...:     for col in x.columns.difference([lst_col])
     ...: }).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]
     ...:
Out[181]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ

```

---

**OLD answer:**

Inspired by [@AFinkelstein solution](https://stackoverflow.com/a/28182629/5741205), i wanted to make it bit more generalized which could be applied to DF with more than two columns and as fast, well almost, as fast as AFinkelstein's solution):

```
In [2]: df = pd.DataFrame(
   ...:    [{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},
   ...:     {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}]
   ...: )

In [3]: df
Out[3]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

In [4]: (df.set_index(df.columns.drop('var1',1).tolist())
   ...:    .var1.str.split(',', expand=True)
   ...:    .stack()
   ...:    .reset_index()
   ...:    .rename(columns={0:'var1'})
   ...:    .loc[:, df.columns]
   ...: )
Out[4]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ

```"
"**models.py:**

```
class Person(models.Model):
    name = models.CharField(max_length=200)
    CATEGORY_CHOICES = (
        ('M', 'Male'),
        ('F', 'Female'),
    )
    gender = models.CharField(max_length=200, choices=CATEGORY_CHOICES)
    to_be_listed = models.BooleanField(default=True)
    description = models.CharField(max_length=20000, blank=True)

```

**views.py:**

```
def index(request):
    latest_person_list2 = Person.objects.filter(to_be_listed=True)
    return object_list(request, template_name='polls/schol.html',
                       queryset=latest_person_list, paginate_by=5)

```

On the template, when I call `person.gender`, I get `'M'` or `'F'` instead of `'Male'` or `'Female'`.

How to display the value (`'Male'` or `'Female'`) instead of the code (`'M'`/`'F'`)?","It looks like you were on the right track - [`get_FOO_display()`](https://docs.djangoproject.com/en/stable/ref/models/instances/#django.db.models.Model.get_FOO_display) is most certainly what you want:

In **templates**, you don't include `()` in the name of a method. Do the following:

```
{{ person.get_gender_display }}

```"
"I'm writing a Python application that takes a command as an argument, for example:

```
$ python myapp.py command1

```

I want the application to be extensible, that is, to be able to add new modules that implement new commands without having to change the main application source. The tree looks something like:

```
myapp/
    __init__.py
    commands/
        __init__.py
        command1.py
        command2.py
    foo.py
    bar.py

```

So I want the application to find the available command modules at runtime and execute the appropriate one.

Python defines an `__import__()` function, which takes a string for a module name:

> `__import__(name, globals=None, locals=None, fromlist=(), level=0)`
>
> The function imports the module `name`, potentially using the given `globals` and `locals` to determine how to interpret the name in a package context. The `fromlist` gives the names of objects or submodules that should be imported from the module given by `name`.
>
> Source: <https://docs.python.org/3/library/functions.html#__import_>\_

So currently I have something like:

```
command = sys.argv[1]
try:
    command_module = __import__(""myapp.commands.%s"" % command, fromlist=[""myapp.commands""])
except ImportError:
    # Display error message

command_module.run()

```

This works just fine, I'm just wondering if there is possibly a more idiomatic way to accomplish what we are doing with this code.

Note that I specifically don't want to get in to using eggs or extension points. This is not an open-source project and I don't expect there to be ""plugins"". The point is to simplify the main application code and remove the need to modify it each time a new command module is added.

---

**See also:** [How can I import a module dynamically given the full path?](https://stackoverflow.com/questions/67631)","The [recommended](https://docs.python.org/3/library/functions.html#__import__) way for Python 2.7 and 3.1 and later is to use [`importlib`](http://docs.python.org/3/library/importlib.html#importlib.import_module) module:

> **`importlib.import_module(`*****`name, package=None)`***
>
> Import a module. The name argument specifies what module to import in absolute or relative terms (e.g. either `pkg.mod` or `..mod`). If the name is specified in relative terms, then the package argument must be set to the name of the package which is to act as the anchor for resolving the package name (e.g. `import_module('..mod', 'pkg.subpkg')` will import `pkg.mod`).

e.g.

```
my_module = importlib.import_module('os.path')

```"
"I have a pandas data frame that looks like this (its a pretty big one)

```
           date      exer exp     ifor         mat  
1092  2014-03-17  American   M  528.205  2014-04-19 
1093  2014-03-17  American   M  528.205  2014-04-19 
1094  2014-03-17  American   M  528.205  2014-04-19 
1095  2014-03-17  American   M  528.205  2014-04-19    
1096  2014-03-17  American   M  528.205  2014-05-17 

```

now I would like to iterate row by row and as I go through each row, the value of `ifor`
in each row can change depending on some conditions and I need to lookup another dataframe.

Now, how do I update this as I iterate.
Tried a few things none of them worked.

```
for i, row in df.iterrows():
    if <something>:
        row['ifor'] = x
    else:
        row['ifor'] = y

    df.ix[i]['ifor'] = x

```

None of these approaches seem to work. I don't see the values updated in the dataframe.","You can use `df.at`:

```
for i, row in df.iterrows():
    ifor_val = something
    if <condition>:
        ifor_val = something_else
    df.at[i,'ifor'] = ifor_val

```

For versions before 0.21.0, use `df.set_value`:

```
for i, row in df.iterrows():
    ifor_val = something
    if <condition>:
        ifor_val = something_else
    df.set_value(i,'ifor',ifor_val)

```

If you don't need the row values you could simply iterate over the indices of `df`, but I kept the original for-loop in case you need the row value for something not shown here."
"I want to use a new feature of httpie. This feature is in the github repo <https://github.com/jkbr/httpie> but not in the release on the python package index <https://pypi.python.org/pypi/httpie>

How can I install the httpie package from the github repo? I tried

```
pip install https://github.com/jkbr/httpie

```

But I got an error 'could not unpack'

---

In Nodejs, I can install packages from github like this

```
npm install git+https://github.com/substack/node-optimist.git

```","You need to use the proper git URL:

```
pip install git+https://github.com/jkbr/httpie.git#egg=httpie

```

Also see the [*VCS Support* section](https://pip.pypa.io/en/stable/topics/vcs-support/) of the pip documentation.

Donâ€™t forget to include the `egg=<projectname>` part to [explicitly name the project](https://pip.pypa.io/en/stable/cli/pip_install/#working-out-the-name-and-version); this way pip can track metadata for it without having to have run the setup.py script."
"For example I have simple DF:

```
import pandas as pd
from random import randint

df = pd.DataFrame({'A': [randint(1, 9) for x in range(10)],
                   'B': [randint(1, 9)*10 for x in range(10)],
                   'C': [randint(1, 9)*100 for x in range(10)]})

```

Can I select values from 'A' for which corresponding values for 'B' will be greater than 50, and for 'C' - not equal to 900, using methods and idioms of Pandas?","Sure! Setup:

```
>>> import pandas as pd
>>> from random import randint
>>> df = pd.DataFrame({'A': [randint(1, 9) for x in range(10)],
                   'B': [randint(1, 9)*10 for x in range(10)],
                   'C': [randint(1, 9)*100 for x in range(10)]})
>>> df
   A   B    C
0  9  40  300
1  9  70  700
2  5  70  900
3  8  80  900
4  7  50  200
5  9  30  900
6  2  80  700
7  2  80  400
8  5  80  300
9  7  70  800

```

We can apply column operations and get boolean Series objects:

```
>>> df[""B""] > 50
0    False
1     True
2     True
3     True
4    False
5    False
6     True
7     True
8     True
9     True
Name: B
>>> (df[""B""] > 50) & (df[""C""] != 900)

```

or

```
>>> (df[""B""] > 50) & ~(df[""C""] == 900)
0    False
1    False
2     True
3     True
4    False
5    False
6    False
7    False
8    False
9    False

```

[Update, to switch to new-style `.loc`]:

And then we can use these to index into the object. For read access, you can chain indices:

```
>>> df[""A""][(df[""B""] > 50) & (df[""C""] != 900)]
2    5
3    8
Name: A, dtype: int64

```

but you can get yourself into trouble because of the difference between a view and a copy doing this for write access. You can use `.loc` instead:

```
>>> df.loc[(df[""B""] > 50) & (df[""C""] != 900), ""A""]
2    5
3    8
Name: A, dtype: int64
>>> df.loc[(df[""B""] > 50) & (df[""C""] != 900), ""A""].values
array([5, 8], dtype=int64)
>>> df.loc[(df[""B""] > 50) & (df[""C""] != 900), ""A""] *= 1000
>>> df
      A   B    C
0     9  40  300
1     9  70  700
2  5000  70  900
3  8000  80  900
4     7  50  200
5     9  30  900
6     2  80  700
7     2  80  400
8     5  80  300
9     7  70  800

```"
"I couldn't get `virtualenv` to work despite various attempts. I installed `virtualenv` on MAC OS X using:

```
pip install virtualenv

```

and have also added the `PATH` into my `.bash_profile`. Every time I try to run the `virtualenv` command, it returns:

```
-bash: virtualenv: command not found

```

Every time I run `pip install virtualenv`, it returns:

```
Requirement already satisfied (use --upgrade to upgrade): virtualenv in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages

```

I understand that in mac, the `virtualenv` should be correctly installed in

```
/usr/local/bin

```

The `virtualenv` is indeed installed in `/usr/local/bin`, but whenever I try to run the `virtualenv` command, the command is not found. I've also tried to run the `virtualenv` command in the directory `/usr/local/bin`, and it gives me the same result:

```
-bash: virtualenv: command not found

```

These are the PATHs I added to my .bash\_profile

```
export PATH=$PATH:/usr/local/bin
export PATH=$PATH:/usr/local/bin/python
export PATH=$PATH:/Library/Framework/Python.framework/Version/2.7/lib/site-packages

```

Any workarounds for this? Why is this the case?","If you installed it with

```
pip install virtualenv

```

You need to run

```
sudo /usr/bin/easy_install virtualenv

```

which puts it in `/usr/local/bin/`.

The above directory by default should be in your `PATH`; otherwise, edit your `.zshrc` (or .`bashrc`) accordingly."
"I created a histogram plot using data from a file and no problem. Now I wanted to superpose data from another file in the same histogram, so I do something like this

```
n,bins,patchs = ax.hist(mydata1,100)
n,bins,patchs = ax.hist(mydata2,100)

```

but the problem is that for each interval, only the bar with the highest value appears, and the other is hidden. I wonder how could I plot both histograms at the same time with different colors.","Here you have a working example:

```
import random
import numpy
from matplotlib import pyplot

x = [random.gauss(3,1) for _ in range(400)]
y = [random.gauss(4,2) for _ in range(400)]

bins = numpy.linspace(-10, 10, 100)

pyplot.hist(x, bins, alpha=0.5, label='x')
pyplot.hist(y, bins, alpha=0.5, label='y')
pyplot.legend(loc='upper right')
pyplot.show()

```

![enter image description here](https://i.sstatic.net/acUlv.png)"
How to send HTML content in email using Python? I can send simple texts.,"From [Python v2.7.14 documentation - 18.1.11. email: Examples](https://docs.python.org/2/library/email-examples.html#id5):

> Hereâ€™s an example of how to create an HTML message with an alternative plain text version:

```
#! /usr/bin/python

import smtplib

from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

# me == my email address
# you == recipient's email address
me = ""my@email.com""
you = ""your@email.com""

# Create message container - the correct MIME type is multipart/alternative.
msg = MIMEMultipart('alternative')
msg['Subject'] = ""Link""
msg['From'] = me
msg['To'] = you

# Create the body of the message (a plain-text and an HTML version).
text = ""Hi!\nHow are you?\nHere is the link you wanted:\nhttp://www.python.org""
html = """"""\
<html>
  <head></head>
  <body>
    <p>Hi!<br>
       How are you?<br>
       Here is the <a href=""http://www.python.org"">link</a> you wanted.
    </p>
  </body>
</html>
""""""

# Record the MIME types of both parts - text/plain and text/html.
part1 = MIMEText(text, 'plain')
part2 = MIMEText(html, 'html')

# Attach parts into message container.
# According to RFC 2046, the last part of a multipart message, in this case
# the HTML message, is best and preferred.
msg.attach(part1)
msg.attach(part2)

# Send the message via local SMTP server.
s = smtplib.SMTP('localhost')
# sendmail function takes 3 arguments: sender's address, recipient's address
# and message to send - here it is sent as one string.
s.sendmail(me, you, msg.as_string())
s.quit()

```"
"I am using pythons mock.patch and would like to change the return value for each call.
Here is the caveat:
the function being patched has no inputs, so I can not change the return value based on the input.

Here is my code for reference.

```
def get_boolean_response():
    response = io.prompt('y/n').lower()
    while response not in ('y', 'n', 'yes', 'no'):
        io.echo('Not a valid input. Try again'])
        response = io.prompt('y/n').lower()

    return response in ('y', 'yes')

```

My Test code:

```
@mock.patch('io')
def test_get_boolean_response(self, mock_io):
    #setup
    mock_io.prompt.return_value = ['x','y']
    result = operations.get_boolean_response()

    #test
    self.assertTrue(result)
    self.assertEqual(mock_io.prompt.call_count, 2)

```

`io.prompt` is just a platform independent (python 2 and 3) version of ""input"". So ultimately I am trying to mock out the users input. I have tried using a list for the return value, but that doesn't seam to work.

You can see that if the return value is something invalid, I will just get an infinite loop here. So I need a way to eventually change the return value, so that my test actually finishes.

(another possible way to answer this question could be to explain how I could mimic user input in a unit-test)

---

Not a dup of [this question](https://stackoverflow.com/questions/7665682/python-mock-object-with-method-called-multiple-times) mainly because I do not have the ability to vary the inputs.

One of the comments of the Answer on [this question](https://stackoverflow.com/questions/21927057/mock-patch-os-path-exists-with-multiple-return-values) is along the same lines, but no answer/comment has been provided.","You can assign an [*iterable*](https://docs.python.org/3/glossary.html#term-iterable) to `side_effect`, and the mock will return the next value in the sequence each time it is called:

```
>>> from unittest.mock import Mock
>>> m = Mock()
>>> m.side_effect = ['foo', 'bar', 'baz']
>>> m()
'foo'
>>> m()
'bar'
>>> m()
'baz'

```

Quoting the [`Mock()` documentation](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock):

> If *side\_effect* is an iterable then each call to the mock will return the next value from the iterable."
How do I add a horizontal line to an existing plot?,"Use [`axhline`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axhline.html) (a horizontal axis line). For example, this plots a horizontal line at `y = 0.5`:

```
import matplotlib.pyplot as plt
plt.axhline(y=0.5, color='r', linestyle='-')
plt.show()

```

[![sample figure](https://i.sstatic.net/bGUX7.png)](https://i.sstatic.net/bGUX7.png)"
"How do I search and replace text in a file using Python 3?

Here is my code:

```
import os
import sys
import fileinput

print(""Text to search for:"")
textToSearch = input(""> "")

print(""Text to replace it with:"")
textToReplace = input(""> "")

print(""File to perform Search-Replace on:"")
fileToSearch = input(""> "")

tempFile = open(fileToSearch, 'r+')

for line in fileinput.input(fileToSearch):
    if textToSearch in line:
        print('Match Found')
    else:
        print('Match Not Found!!')
    tempFile.write(line.replace(textToSearch, textToReplace))
tempFile.close()

input('\n\n Press Enter to exit...')

```

Input file:

```
hi this is abcd hi this is abcd
This is dummy text file.
This is how search and replace works abcd

```

When I search and replace 'ram' by 'abcd' in above input file, it work like a charm. But when I do it vice versa, i.e., replacing 'abcd' by 'ram', some junk characters are left at the end.

Replacing 'abcd' by 'ram':

```
hi this is ram hi this is ram
This is dummy text file.
This is how search and replace works rambcd

```","As [pointed out by michaelb958](https://stackoverflow.com/questions/17140886/how-to-search-and-replace-text-in-a-file#comment24808323_17141040), you cannot replace in place with data of a different length because this will put the rest of the sections out of place. I disagree with the other posters suggesting you read from one file and write to another. Instead, I would read the file into memory, fix the data up, and then write it out to the same file in a separate step.

```
# Read in the file
with open('file.txt', 'r') as file:
  filedata = file.read()

# Replace the target string
filedata = filedata.replace('abcd', 'ram')

# Write the file out again
with open('file.txt', 'w') as file:
  file.write(filedata)

```

Unless you've got a massive file to work with which is too big to load into memory in one go, or you are concerned about potential data loss if the process is interrupted during the second step in which you write data to the file."
"Here's the simplest way to explain this. Here's what I'm using:

```
re.split('\W', 'foo/bar spam\neggs')
>>> ['foo', 'bar', 'spam', 'eggs']

```

Here's what I want:

```
someMethod('\W', 'foo/bar spam\neggs')
>>> ['foo', '/', 'bar', ' ', 'spam', '\n', 'eggs']

```

The reason is that I want to split a string into tokens, manipulate it, then put it back together again.","The [docs of `re.split`](https://docs.python.org/3/library/re.html#re.split) mention:

> Split *string* by the occurrences of *pattern*. **If capturing
> parentheses are used in *pattern*, then the text of all groups in the
> pattern are also returned as part of the resulting list**.

So you just need to wrap your separator with a capturing group:

```
>>> re.split('(\W)', 'foo/bar spam\neggs')
['foo', '/', 'bar', ' ', 'spam', '\n', 'eggs']

```"
"I'm trying to install TensorFlow using pip:

```
$ pip install tensorflow --user
Collecting tensorflow
Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

```

What am I doing wrong? So far I've used Python and pip with no issues.","I found this to finally work.

```
python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl

```

Edit 1: This was tested on Windows (8, 8.1, 10), Mac and Linux. Change `python3` to `python` according to your configuration. Change `py3` to `py2` in the url if you are using Python 2.x.

Edit 2: A list of different versions if someone needs: <https://storage.googleapis.com/tensorflow>

Edit 3: A list of urls for the available wheel packages is available here:
<https://www.tensorflow.org/install/pip#package-location>"
"How does one truncate a string to 75 characters in Python?

This is how it is done in JavaScript:

```
const data = ""saddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddsaddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddsadddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd"";
const info = (data.length > 75) ? data.substring(0, 75) + '..' : data;
console.log(info);
```

Run code snippetHide results

Expand snippet","```
info = (data[:75] + '..') if len(data) > 75 else data

```

This code matches the JavaScript, but you should consider using `data[:73]` so that the total result including the `..` fits in 75 characters."
"```
import csv

with open('test.csv', 'w') as outfile:
    writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)
    writer.writerow(['hi', 'dude'])
    writer.writerow(['hi2', 'dude2'])

```

The above code generates a file, `test.csv`, with an extra `\r` at each row, like so:

```
hi,dude\r\r\nhi2,dude2\r\r\n

```

instead of the expected

```
hi,dude\r\nhi2,dude2\r\n

```

Why is this happening, or is this actually the desired behavior?","Python 3:
---------

The official [`csv` documentation](https://docs.python.org/3/library/csv.html#examples) recommends `open`ing the file with `newline=''` on all platforms to [disable universal newlines translation](https://docs.python.org/3/library/functions.html#open-newline-parameter):

```
with open('output.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    ...

```

The CSV writer terminates each line with the [`lineterminator` of the dialect](https://docs.python.org/3/library/csv.html#csv.Dialect.lineterminator), which is `'\r\n'` for the default `excel` dialect on all platforms because that's what [RFC 4180](https://www.rfc-editor.org/rfc/rfc4180#section-2) recommends.

---

Python 2:
---------

On Windows, always open your files in binary mode (`""rb""` or `""wb""`), before passing them to `csv.reader` or `csv.writer`.

Although the file is a text file, CSV is regarded a **binary** format by the libraries involved, with `\r\n` separating records. If that separator is written in text mode, the Python runtime replaces the `\n` with `\r\n`, hence the `\r\r\n` observed in the file.

See [this previous answer](https://stackoverflow.com/questions/1170214/pythons-csv-writer-produces-wrong-line-terminator/1170297#1170297)."
"For Django 1.1.

I have this in my models.py:

```
class User(models.Model):
    created = models.DateTimeField(auto_now_add=True)
    modified = models.DateTimeField(auto_now=True)

```

When updating a row I get:

```
[Sun Nov 15 02:18:12 2009] [error] /home/ptarjan/projects/twitter-meme/django/db/backends/mysql/base.py:84: Warning: Column 'created' cannot be null
[Sun Nov 15 02:18:12 2009] [error]   return self.cursor.execute(query, args)

```

The relevant part of my database is:

```
  `created` datetime NOT NULL,
  `modified` datetime NOT NULL,

```

Is this cause for concern?

Side question: in my admin tool, those two fields aren't showing up. Is that expected?","Any field with the [`auto_now`](https://docs.djangoproject.com/en/2.2/ref/models/fields/#django.db.models.DateField.auto_now) attribute set will also inherit `editable=False` and therefore will not show up in the admin panel. There has been talk in the past about making the `auto_now` and [`auto_now_add`](https://docs.djangoproject.com/en/2.2/ref/models/fields/#django.db.models.DateField.auto_now_add) arguments go away, and although they still exist, I feel you're better off just using a [custom `save()` method](https://docs.djangoproject.com/en/2.2/topics/db/models/#overriding-model-methods).

So, to make this work properly, I would recommend not using `auto_now` or `auto_now_add` and instead define your own `save()` method to make sure that `created` is only updated if `id` is not set (such as when the item is first created), and have it update `modified` every time the item is saved.

I have done the exact same thing with other projects I have written using Django, and so your `save()` would look like this:

```
from django.utils import timezone

class User(models.Model):
    created     = models.DateTimeField(editable=False)
    modified    = models.DateTimeField()

    def save(self, *args, **kwargs):
        ''' On save, update timestamps '''
        if not self.id:
            self.created = timezone.now()
        self.modified = timezone.now()
        return super(User, self).save(*args, **kwargs)

```

**Edit in response to comments:**

The reason why I just stick with overloading `save()` vs. relying on these field arguments is two-fold:

1. The aforementioned ups and downs with their reliability. These arguments are heavily reliant on the way each type of database that Django knows how to interact with treats a date/time stamp field, and seems to break and/or change between every release. (Which I believe is the impetus behind the call to have them removed altogether).
2. The fact that they only work on DateField, DateTimeField, and TimeField, and by using this technique you are able to automatically populate any field type every time an item is saved.
3. Use `django.utils.timezone.now()` vs. `datetime.datetime.now()`, because it will return a TZ-aware or naive `datetime.datetime` object depending on `settings.USE_TZ`.

To address why the OP saw the error, I don't know exactly, but it looks like `created` isn't even being populated at all, despite having `auto_now_add=True`. To me it stands out as a bug, and underscores item #1 in my little list above: `auto_now` and `auto_now_add` are flaky at best."
"I'm trying to do the equivalent of `Response.redirect` as in C# - i.e.: redirect to a specific URL - how do I go about this?

Here is my code:

```
import os
from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello():
    return 'Hello World!'

if __name__ == '__main__':
    # Bind to PORT if defined, otherwise default to 5000.
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)

```","You have to return a redirect:

```
import os
from flask import Flask,redirect

app = Flask(__name__)

@app.route('/')
def hello():
    return redirect(""http://www.example.com"", code=302)

if __name__ == '__main__':
    # Bind to PORT if defined, otherwise default to 5000.
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)

```

The default value for code is 302 so `code=302` can be omitted or replaced by other redirect code (one in 301, 302, 303, 305, and 307).

See [the documentation on flask docs.](https://flask.palletsprojects.com/api/#flask.redirect)"
"I want to put output information of my program to a folder. if given folder does not exist, then the program should create a new folder with folder name as given in the program. Is this possible? If yes, please let me know how.

Suppose I have given folder path like `""C:\Program Files\alex""` and `alex` folder doesn't exist then program should create `alex` folder and should put output information in the `alex` folder.","You can create a folder with [os.makedirs()](http://docs.python.org/3/library/os.html?highlight=os.makedirs#os.makedirs)  
and use [os.path.exists()](http://docs.python.org/3/library/os.path.html?highlight=os.path.exists#os.path.exists) to see if it already exists:

```
newpath = r'C:\Program Files\arbitrary' 
if not os.path.exists(newpath):
    os.makedirs(newpath)

```

If you're trying to make an installer: [Windows Installer](http://www.advancedinstaller.com/) does a lot of work for you."
How do I convert a float NumPy array into an int NumPy array?,"Use the [`astype`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html) method.

```
>>> x = np.array([[1.0, 2.3], [1.3, 2.9]])
>>> x
array([[ 1. ,  2.3],
       [ 1.3,  2.9]])
>>> x.astype(int)
array([[1, 2],
       [1, 2]])

```"
"For the following code:

```
logger.debug('message: {}'.format('test'))

```

`pylint` produces the following warning:

> **logging-format-interpolation (W1202):**
>
> Use % formatting in logging functions and pass the % parameters as
> arguments Used when a logging statement has a call form of
> “logging.(format\_string.format(format\_args...))”. Such
> calls should use % formatting instead, but leave interpolation to the
> logging function by passing the parameters as arguments.

I know I can turn off this warning, but I'd like to understand it. I assumed using `format()` is the preferred way to print out statements in Python 3. Why is this not true for logger statements?","It is not true for logger statement because it relies on former ""%"" format like string to provide lazy interpolation of this string using extra arguments given to the logger call. For instance instead of doing:

```
logger.error('oops caused by %s' % exc)

```

you should do

```
logger.error('oops caused by %s', exc)

```

so the string will only be interpolated if the message is actually emitted.

You can't benefit of this functionality when using `.format()`.

---

Per the [Optimization](https://docs.python.org/howto/logging.html#optimization) section of the `logging` docs:

> Formatting of message arguments is deferred until it cannot be avoided. However, computing the arguments passed to the logging method can also be expensive, and you may want to avoid doing it if the logger will just throw away your event."
"I have a generator function like the following:

```
def myfunct():
  ...
  yield result

```

The usual way to call this function would be:

```
for r in myfunct():
  dostuff(r)

```

My question, is there a way to get just one element from the generator whenever I like?
For example, I'd like to do something like:

```
while True:
  ...
  if something:
      my_element = pick_just_one_element(myfunct())
      dostuff(my_element)
  ...

```","Create a generator using

```
g = myfunct()

```

Everytime you would like an item, use

```
next(g)

```

(or `g.next()` in Python 2.5 or below).

If the generator exits, it will raise `StopIteration`. You can either catch this exception if necessary, or use the `default` argument to `next()`:

```
next(g, default_value)

```"
"When selecting a sub dataframe from a parent dataframe, I noticed that some programmers make a copy of the data frame using the `.copy()` method. For example,

```
X = my_dataframe[features_list].copy()

```

...instead of just

```
X = my_dataframe[features_list]

```

Why are they making a copy of the data frame? What will happen if I don't make a copy?","This answer has been deprecated in newer versions of pandas. See [docs](https://pandas.pydata.org/docs/user_guide/indexing.html#evaluation-order-matters)

---

This expands on Paul's answer. In Pandas, indexing a DataFrame returns a reference to the initial DataFrame. Thus, changing the subset will change the initial DataFrame. Thus, you'd want to use the copy if you want to make sure the initial DataFrame shouldn't change. Consider the following code:

```
df = DataFrame({'x': [1,2]})
df_sub = df[0:1]
df_sub.x = -1
print(df)

```

You'll get:

```
   x
0 -1
1  2

```

In contrast, the following leaves df unchanged:

```
df_sub_copy = df[0:1].copy()
df_sub_copy.x = -1

```"
"I need to execute a Python script from the Django shell. I tried:

```
./manage.py shell << my_script.py

```

But it didn't work. It was just waiting for me to write something.","The `<<` part is wrong, use `<` instead:

```
$ ./manage.py shell < myscript.py

```

You could also do:

```
$ ./manage.py shell
...
>>> execfile('myscript.py')

```

For python3 you would need to use

```
>>> exec(open('myscript.py').read())

```"
"What would be your preferred way to concatenate strings from a sequence such that between every two consecutive pairs a comma is added. That is, how do you map, for instance, `['a', 'b', 'c']` to `'a,b,c'`? (The cases `['s']` and `[]` should be mapped to `'s'` and `''`, respectively.)

I usually end up using something like `''.join(map(lambda x: x+',',l))[:-1]`, but also feeling somewhat unsatisfied.","```
my_list = ['a', 'b', 'c', 'd']
my_string = ','.join(my_list)

```

```
'a,b,c,d'

```

This won't work if the list contains integers

---

And if the list contains non-string types (such as integers, floats, bools, None) then do:

```
my_string = ','.join(map(str, my_list)) 

```"
"I have a series of images that I want to create a video from. Ideally I could specify a frame duration for each frame but a fixed frame rate would be fine too. I'm doing this in wxPython, so I can render to a wxDC or I can save the images to files, like PNG. Is there a Python library that will allow me to create either a video (AVI, MPG, etc) or an animated GIF from these frames?

Edit: I've already tried PIL and it doesn't seem to work. Can someone correct me with this conclusion or suggest another toolkit? This link seems to backup my conclusion regarding PIL: <http://www.somethinkodd.com/oddthinking/2005/12/06/python-imaging-library-pil-and-animated-gifs/>","I'd recommend not using images2gif from visvis because it has problems with PIL/Pillow and is not actively maintained (I should know, because I am the author).

Instead, please use [imageio](http://imageio.github.io), which was developed to solve this problem and more, and is intended to stay.

Quick and dirty solution:

```
import imageio
images = []
for filename in filenames:
    images.append(imageio.imread(filename))
imageio.mimsave('/path/to/movie.gif', images)

```

For longer movies, use the streaming approach:

```
import imageio
with imageio.get_writer('/path/to/movie.gif', mode='I') as writer:
    for filename in filenames:
        image = imageio.imread(filename)
        writer.append_data(image)

```

To edit each frame duration, you can use kwarg `duration` (**in milliseconds**) or `fps`. Those kwargs can be used both for `mimsave()` and `get_writer()`."
"What is the Pythonic approach to achieve the following?

```
# Original lists:

list_a = [1, 2, 3, 4]
list_b = [5, 6, 7, 8]

# List of tuples from 'list_a' and 'list_b':

list_c = [(1,5), (2,6), (3,7), (4,8)]

```

Each member of `list_c` is a tuple, whose first member is from `list_a` and the second is from `list_b`.","In Python 2:

```
>>> list_a = [1, 2, 3, 4]
>>> list_b = [5, 6, 7, 8]
>>> zip(list_a, list_b)
[(1, 5), (2, 6), (3, 7), (4, 8)]

```

In Python 3:

```
>>> list_a = [1, 2, 3, 4]
>>> list_b = [5, 6, 7, 8]
>>> list(zip(list_a, list_b))
[(1, 5), (2, 6), (3, 7), (4, 8)]

```"
"I'm trying to collect some of my default settings, and one thing I realized I don't have a standard for is .gitignore files. There's a great thread showing a [good .gitignore for Visual Studio projects](https://stackoverflow.com/questions/2143956/gitignore-for-visual-studio-projects-and-solutions), but I don't see many recommendations for Python and related tools (PyGTK, Django).

So far, I have...

```
*.pyc
*.pyo

```

...for the compiled objects and...

```
build/
dist/

```

...for the setuptools output.

Are there best practices for .gitignore files, and where can I go for more about these best practices?","[Github has a great boilerplate .gitignore](https://github.com/github/gitignore/blob/master/Python.gitignore)

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]

# C extensions
*.so

# Distribution / packaging
bin/
build/
develop-eggs/
dist/
eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
.tox/
.coverage
.cache
nosetests.xml
coverage.xml

# Translations
*.mo

# Mr Developer
.mr.developer.cfg
.project
.pydevproject

# Rope
.ropeproject

# Django stuff:
*.log
*.pot

# Sphinx documentation
docs/_build/

```"
What exactly does it mean to say that an object in Python code is *hashable*?,"From the [Python glossary](http://docs.python.org/2/glossary.html):

> An object is hashable if it has a hash value which never changes during its lifetime (it needs a `__hash__()` method), and can be compared to other objects (it needs an `__eq__()` or `__cmp__()` method). Hashable objects which compare equal must have the same hash value.
>
> Hashability makes an object usable as a dictionary key and a set member, because these data structures use the hash value internally.
>
> All of Pythonâ€™s immutable built-in objects are hashable, while no mutable containers (such as lists or dictionaries) are. Objects which are instances of user-defined classes are hashable by default; they all compare unequal, and their hash value is their `id()`."
"A function to return a human-readable size from the bytes size:

```
>>> human_readable(2048)
'2 kilobytes'

```

How can I do this?","Addressing the above ""too small a task to require a library"" issue by a straightforward implementation (using f-strings, so Python 3.6+):

```
def sizeof_fmt(num, suffix=""B""):
    for unit in ("""", ""Ki"", ""Mi"", ""Gi"", ""Ti"", ""Pi"", ""Ei"", ""Zi""):
        if abs(num) < 1024.0:
            return f""{num:3.1f}{unit}{suffix}""
        num /= 1024.0
    return f""{num:.1f}Yi{suffix}""

```

Supports:

* all currently known [binary prefixes](https://en.wikipedia.org/wiki/Binary_prefix#Specific_units_of_IEC_60027-2_A.2_and_ISO.2FIEC_80000)
* negative and positive numbers
* numbers larger than 1000 Yobibytes
* arbitrary units (maybe you like to count in Gibibits!)

Example:

```
>>> sizeof_fmt(168963795964)
'157.4GiB'

```

by [Fred Cirera](https://web.archive.org/web/20111010015624/http://blogmag.net/blog/read/38/Print_human_readable_file_size)"
"Python provides different packages (`datetime`, `time`, `calendar`) as can be seen [here](https://stackoverflow.com/questions/8542723/change-datetime-to-unix-time-stamp-in-python) in order to deal with time. I made a big mistake by using the following to get current GMT time `time.mktime(datetime.datetime.utcnow().timetuple())`

What is a simple way to get current GMT time in Unix timestamp?","I would use time.time() to get a timestamp in seconds since the epoch.

```
import time

time.time()

```

Output:

```
1369550494.884832

```

For the standard CPython implementation on most platforms this will return a UTC value."
"I have a dataframe df :

```
>>> df
                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
       20060630   6.590       NaN      6.590   5.291
       20060930  10.103       NaN     10.103   7.981
       20061231  15.915       NaN     15.915  12.686
       20070331   3.196       NaN      3.196   2.710
       20070630   7.907       NaN      7.907   6.459

```

Then I want to drop rows with certain sequence numbers which indicated in a list, suppose here is `[1,2,4],` then left:

```
                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
       20061231  15.915       NaN     15.915  12.686
       20070630   7.907       NaN      7.907   6.459

```

How or what function can do that ?","Use [DataFrame.drop](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) and pass it a Series of index labels:

```
In [65]: df
Out[65]: 
       one  two
one      1    4
two      2    3
three    3    2
four     4    1
    
    
In [66]: df.drop(df.index[[1,3]])
Out[66]: 
       one  two
one      1    4
three    3    2

```"
"Is it possible to directly declare a flask URL optional parameter?

Currently I'm proceeding the following way:

```
@user.route('/<userId>')
@user.route('/<userId>/<username>')
def show(userId, username=None):
    pass

```

How can I directly say that `username` is optional?","Another way is to write

```
@user.route('/<user_id>', defaults={'username': None})
@user.route('/<user_id>/<username>')
def show(user_id, username):
    pass

```

But I guess that you want to write a single route and mark `username` as optional? If that's the case, I don't think it's possible."
"What does the `:=` operand mean, more specifically for Python?

Can someone explain how to read this snippet of code?

```
node := root, cost = 0
frontier := priority queue containing node only
explored := empty set

```","Updated answer
--------------

In the context of the question, we are dealing with pseudocode, but [starting in Python 3.8](https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions), `:=` is actually a valid operator that allows for assignment of variables within expressions:

```
# Handle a matched regex
if (match := pattern.search(data)) is not None:
    # Do something with match

# A loop that can't be trivially rewritten using 2-arg iter()
while chunk := file.read(8192):
   process(chunk)

# Reuse a value that's expensive to compute
[y := f(x), y**2, y**3]

# Share a subexpression between a comprehension filter clause and its output
filtered_data = [y for x in data if (y := f(x)) is not None]

```

See [PEP 572](https://www.python.org/dev/peps/pep-0572/) for more details.

Original Answer
---------------

What you have found is [**pseudocode**](http://en.wikipedia.org/wiki/Pseudocode)

> **Pseudocode** is an informal high-level description of the operating
> principle of a computer program or other algorithm.

`:=` is actually the assignment operator. In Python this is simply `=`.

To translate this pseudocode into Python you would need to know the data structures being referenced, and a bit more of the algorithm implementation.

Some notes about psuedocode:

* `:=` is the assignment operator or `=` in Python
* `=` is the equality operator or `==` in Python
* There are certain styles, and your mileage may vary:

### Pascal-style

```
procedure fizzbuzz
For i := 1 to 100 do
    set print_number to true;
    If i is divisible by 3 then
        print ""Fizz"";
        set print_number to false;
    If i is divisible by 5 then
        print ""Buzz"";
        set print_number to false;
    If print_number, print i;
    print a newline;
end

```

### C-style

```
void function fizzbuzz
For (i = 1; i <= 100; i++) {
    set print_number to true;
    If i is divisible by 3
        print ""Fizz"";
        set print_number to false;
    If i is divisible by 5
        print ""Buzz"";
        set print_number to false;
    If print_number, print i;
    print a newline;
}

```

Note the differences in brace usage and assignment operator."
"I am not able to get my head on how the `partial` works in `functools`.
I have the following code from [here](https://stackoverflow.com/questions/3252228/python-why-is-functools-partial-necessary):

```
>>> sum = lambda x, y : x + y
>>> sum(1, 2)
3
>>> incr = lambda y : sum(1, y)
>>> incr(2)
3
>>> def sum2(x, y):
    return x + y

>>> incr2 = functools.partial(sum2, 1)
>>> incr2(4)
5

```

Now in the line

```
incr = lambda y : sum(1, y)

```

I get that whatever argument I pass to `incr` it will be passed as `y` to `lambda` which will return `sum(1, y)` i.e `1 + y`.

I understand that. But I didn't understand this `incr2(4)`.

How does the `4` gets passed as `x` in partial function? To me, `4` should replace the `sum2`. What is the relation between `x` and `4`?","Roughly, `partial` does something like this (apart from keyword args support, etc):

```
def partial(func, *part_args):
    def wrapper(*extra_args):
        return func(*part_args, *extra_args)            
    return wrapper

```

So, by calling `partial(sum2, 4)` you create a new function (a callable, to be precise) that behaves like `sum2`, but has one positional argument less. That missing argument is always substituted by `4`, so that `partial(sum2, 4)(2) == sum2(4, 2)`

As for why it's needed, there's a variety of cases. Just for one, suppose you have to pass a function somewhere where it's expected to have 2 arguments:

```
class EventNotifier(object):
    def __init__(self):
        self._listeners = []

    def add_listener(self, callback):
        ''' callback should accept two positional arguments, event and params '''
        self._listeners.append(callback)
        # ...
    
    def notify(self, event, *params):
        for f in self._listeners:
            f(event, params)

```

But a function you already have needs access to some third `context` object to do its job:

```
def log_event(context, event, params):
    context.log_event(""Something happened %s, %s"", event, params)

```

So, there are several solutions:

A custom object:

```
class Listener(object):
   def __init__(self, context):
       self._context = context

   def __call__(self, event, params):
       self._context.log_event(""Something happened %s, %s"", event, params)


 notifier.add_listener(Listener(context))

```

Lambda:

```
log_listener = lambda event, params: log_event(context, event, params)
notifier.add_listener(log_listener)

```

With partials:

```
context = get_context()  # whatever
notifier.add_listener(partial(log_event, context))

```

Of those three, `partial` is the shortest and the fastest.
(For a more complex case you might want a custom object though)."
"How do I split a string into a list of characters? [`str.split`](https://docs.python.org/3/library/stdtypes.html#str.split) does not work.

```
""foobar""    â†’    ['f', 'o', 'o', 'b', 'a', 'r']

```","Use the [`list`](https://docs.python.org/3/library/stdtypes.html#list) constructor:

```
>>> list(""foobar"")
['f', 'o', 'o', 'b', 'a', 'r']

```

`list` builds a new list using items obtained by iterating over the input [iterable](https://docs.python.org/3/glossary.html#term-iterable). A string is an iterable -- iterating over it yields a single character at each iteration step."
"I noticed that in Python, people initialize their class attributes in two different ways.

The first way is like this:

```
class MyClass:
  __element1 = 123
  __element2 = ""this is Africa""

  def __init__(self):
    #pass or something else

```

The other style looks like:

```
class MyClass:
  def __init__(self):
    self.__element1 = 123
    self.__element2 = ""this is Africa""

```

Which is the correct way to initialize class attributes?","Neither way is necessarily correct or incorrect, they are just two different kinds of class elements:

* Elements outside the `__init__` method are static elements; they belong to the class.
* Elements inside the `__init__` method are elements of the object (`self`); they don't belong to the class.

You'll see it more clearly with some code:

```
class MyClass:
    static_elem = 123

    def __init__(self):
        self.object_elem = 456

c1 = MyClass()
c2 = MyClass()

# Initial values of both elements
>>> print c1.static_elem, c1.object_elem 
123 456
>>> print c2.static_elem, c2.object_elem
123 456

# Nothing new so far ...

# Let's try changing the static element
MyClass.static_elem = 999

>>> print c1.static_elem, c1.object_elem
999 456
>>> print c2.static_elem, c2.object_elem
999 456

# Now, let's try changing the object element
c1.object_elem = 888

>>> print c1.static_elem, c1.object_elem
999 888
>>> print c2.static_elem, c2.object_elem
999 456

```

As you can see, when we changed the class element, it changed for both objects. But, when we changed the object element, the other object remained unchanged."
"The `time` module can be initialized using seconds since epoch:

```
import time
t1 = time.gmtime(1284286794)
t1
time.struct_time(tm_year=2010, tm_mon=9, tm_mday=12, tm_hour=10, tm_min=19, 
                 tm_sec=54, tm_wday=6, tm_yday=255, tm_isdst=0)

```

Is there an elegant way to initialize a `datetime.datetime` object in the same way?","[`datetime.datetime.fromtimestamp`](http://docs.python.org/library/datetime.html#datetime.datetime.fromtimestamp) will do, if you know the time zone, you could produce the same output as with `time.gmtime`

```
>>> datetime.datetime.fromtimestamp(1284286794)
datetime.datetime(2010, 9, 12, 11, 19, 54)

```

or

```
>>> datetime.datetime.utcfromtimestamp(1284286794)
datetime.datetime(2010, 9, 12, 10, 19, 54)

```"
"I followed an [example from the documentation](https://docs.python.org/3/library/base64.html#base64.encodebytes) of how to use Base64 encoding in Python:

```
>>> import base64
>>> encoded = base64.b64encode(b'data to be encoded')
>>> encoded
b'ZGF0YSB0byBiZSBlbmNvZGVk'

```

But, if I try to encode a normal string - leaving out the leading `b`:

```
>>> encoded = base64.b64encode('data to be encoded')

```

I get a `TypeError`. In older versions of Python it looked like:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python32\lib\base64.py"", line 56, in b64encode
    raise TypeError(""expected bytes, not %s"" % s.__class__.__name__)
TypeError: expected bytes, not str

```

In more recent versions it might look like:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.12/base64.py"", line 58, in b64encode
    encoded = binascii.b2a_base64(s, newline=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: a bytes-like object is required, not 'str'

```

Why does this happen?","base64 encoding takes 8-bit binary byte data and encodes it uses only the characters `A-Z`, `a-z`, `0-9`, `+`, `/`\* so it can be transmitted over channels that do not preserve all 8-bits of data, such as email.

Hence, it wants a string of 8-bit bytes. You create those in Python 3 with the `b''` syntax.

If you remove the `b`, it becomes a string. A string is a sequence of Unicode characters. base64 has no idea what to do with Unicode data, it's not 8-bit. It's not really any bits, in fact. :-)

In your second example:

```
>>> encoded = base64.b64encode('data to be encoded')

```

All the characters fit neatly into the ASCII character set, and base64 encoding is therefore actually a bit pointless. You can convert it to ascii instead, with

```
>>> encoded = 'data to be encoded'.encode('ascii')

```

Or simpler:

```
>>> encoded = b'data to be encoded'

```

Which would be the same thing in this case.

---

\* Most base64 flavours may also include a `=` at the end as padding. In addition, some base64 variants may use characters other than `+` and `/`. See the [Variants summary table](https://en.wikipedia.org/wiki/Base64#Variants_summary_table) at Wikipedia for an overview."
"How can I find the row for which the value of a specific column is **maximal**?

`df.max()` will give me the maximal value for each column, I don't know how to get the corresponding row.","Use the pandas [`idxmax`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmax.html) function. It's straightforward:

```
>>> import pandas
>>> import numpy as np
>>> df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])
>>> df
          A         B         C
0  1.232853 -1.979459 -0.573626
1  0.140767  0.394940  1.068890
2  0.742023  1.343977 -0.579745
3  2.125299 -0.649328 -0.211692
4 -0.187253  1.908618 -1.862934
>>> df['A'].idxmax()
3
>>> df['B'].idxmax()
4
>>> df['C'].idxmax()
1

```

* Alternatively you could also use `numpy.argmax`, such as `numpy.argmax(df['A'])` -- it provides the same thing, and appears at least as fast as `idxmax` in cursory observations.
* **`idxmax()` returns indices labels, not integers.**
* Example': if you have string values as your index labels, like rows 'a' through 'e', you might want to know that the max occurs in row 4 (not row 'd').
* if you want the integer position of that label within the `Index` you have to get it manually (which can be tricky now that duplicate row labels are allowed).

---

HISTORICAL NOTES:

* `idxmax()` used to be called [`argmax()` prior to 0.11](https://pandas.pydata.org/pandas-docs/version/0.11.0/whatsnew.html)
* [`argmax` was deprecated prior to 1.0.0 and removed entirely in 1.0.0](https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html)
* back as of Pandas 0.16, `argmax` used to exist and perform the same function (though appeared to run more slowly than `idxmax`).
* `argmax` function returned the *integer position* within the index of the row location of the maximum element.
* **pandas moved to using row labels instead of integer indices.** Positional integer indices used to be very common, more common than labels, especially in applications where duplicate row labels are common.

For example, consider this toy `DataFrame` with a duplicate row label:

```
In [19]: dfrm
Out[19]: 
          A         B         C
a  0.143693  0.653810  0.586007
b  0.623582  0.312903  0.919076
c  0.165438  0.889809  0.000967
d  0.308245  0.787776  0.571195
e  0.870068  0.935626  0.606911
f  0.037602  0.855193  0.728495
g  0.605366  0.338105  0.696460
h  0.000000  0.090814  0.963927
i  0.688343  0.188468  0.352213
i  0.879000  0.105039  0.900260

In [20]: dfrm['A'].idxmax()
Out[20]: 'i'

In [21]: dfrm.iloc[dfrm['A'].idxmax()]  # .ix instead of .iloc in older versions of pandas
Out[21]: 
          A         B         C
i  0.688343  0.188468  0.352213
i  0.879000  0.105039  0.900260

```

So here a naive use of `idxmax` is not sufficient, whereas the old form of `argmax` would correctly provide the *positional* location of the max row (in this case, position 9).

This is exactly one of those nasty kinds of bug-prone behaviors in dynamically typed languages that makes this sort of thing so unfortunate, and worth beating a dead horse over. If you are writing systems code and your system suddenly gets used on some data sets that are not cleaned properly before being joined, it's very easy to end up with duplicate row labels, especially string labels like a CUSIP or SEDOL identifier for financial assets. You can't easily use the type system to help you out, and you may not be able to enforce uniqueness on the index without running into unexpectedly missing data.

So you're left with hoping that your unit tests covered everything (they didn't, or more likely no one wrote any tests) -- otherwise (most likely) you're just left waiting to see if you happen to smack into this error at runtime, in which case you probably have to go drop many hours worth of work from the database you were outputting results to, bang your head against the wall in IPython trying to manually reproduce the problem, finally figuring out that it's because `idxmax` can *only* report the *label* of the max row, and then being disappointed that no standard function automatically gets the *positions* of the max row for you, writing a buggy implementation yourself, editing the code, and praying you don't run into the problem again."
"I have a boto3 client :

```
boto3.client('kms')

```

But it happens on new machines, They open and close dynamically.

```
    if endpoint is None:
        if region_name is None:
            # Raise a more specific error message that will give
            # better guidance to the user what needs to happen.
            raise NoRegionError()

```

Why is this happening? and why only part of the time?","One way or another you must tell boto3 in which region you wish the `kms` client to be created. This could be done explicitly using the `region_name` parameter as in:

```
kms = boto3.client('kms', region_name='us-west-2')

```

or you can have a default region associated with your profile in your `~/.aws/config` file as in:

```
[default]
region=us-west-2

```

or you can use an environment variable as in:

```
export AWS_DEFAULT_REGION=us-west-2

```

but you do need to tell boto3 which region to use."
"I've seen there are actually two (maybe more) ways to concatenate lists in Python:

One way is to use the `extend()` method:

```
a = [1, 2]
b = [2, 3]
b.extend(a)

```

the other to use the plus (+) operator:

```
b += a

```

Now I wonder: which of those two options is the 'pythonic' way to do list concatenation and is there a difference between the two? (I've looked up the official Python tutorial but couldn't find anything anything about this topic).","The only difference on a bytecode level is that the [`.extend`](https://docs.python.org/2/library/array.html?#array.array.extend) way involves a function call, which is slightly more expensive in Python than the [`INPLACE_ADD`](https://docs.python.org/2/library/dis.html?highlight=inplace_add#opcode-INPLACE_ADD).

It's really nothing you should be worrying about, unless you're performing this operation billions of times. It is likely, however, that the bottleneck would lie some place else."
"I have been programming in python for about two years; mostly data stuff (pandas, mpl, numpy), but also automation scripts and small web apps. I'm trying to become a better programmer and increase my python knowledge and one of the things that bothers me is that I have never used a class (outside of copying random flask code for small web apps). I generally understand what they are, but I can't seem to wrap my head around why I would need them over a simple function.

To add specificity to my question: I write tons of automated reports which always involve pulling data from multiple data sources (mongo, sql, postgres, apis), performing a lot or a little data munging and formatting, writing the data to csv/excel/html, send it out in an email. The scripts range from ~250 lines to ~600 lines. Would there be any reason for me to use classes to do this and why?","Classes are the pillar of [Object Oriented Programming](https://en.wikipedia.org/wiki/Object-oriented_programming). OOP is highly concerned with code organization, reusability, and encapsulation.

First, a disclaimer: OOP is partially in contrast to [Functional Programming](https://en.wikipedia.org/wiki/Functional_programming), which is a different paradigm used a lot in Python. Not everyone who programs in Python (or surely most languages) uses OOP. You can do a lot in Java 8 that isn't very Object Oriented. If you don't want to use OOP, then don't. If you're just writing one-off scripts to process data that you'll never use again, then keep writing the way you are.

However, there are a lot of reasons to use OOP.

Some reasons:

* Organization:
  OOP defines well known and standard ways of describing and defining both data and procedure in code. Both data and procedure can be stored at varying levels of definition (in different classes), and there are standard ways about talking about these definitions. That is, if you use OOP in a standard way, it will help your later self and others understand, edit, and use your code. Also, instead of using a complex, arbitrary data storage mechanism (dicts of dicts or lists or dicts or lists of dicts of sets, or whatever), you can name pieces of data structures and conveniently refer to them.
* State: OOP helps you define and keep track of state. For instance, in a classic example, if you're creating a program that processes students (for instance, a grade program), you can keep all the info you need about them in one spot (name, age, gender, grade level, courses, grades, teachers, peers, diet, special needs, etc.), and this data is persisted as long as the object is alive, and is easily accessible. In contrast, in pure functional programming, state is never mutated in place.
* [Encapsulation](https://en.wikipedia.org/wiki/Encapsulation_(computer_programming)):
  With encapsulation, procedure and data are stored together. Methods (an OOP term for functions) are defined right alongside the data that they operate on and produce. In a language like Java that allows for [access control](http://docs.oracle.com/javase/tutorial/java/javaOO/accesscontrol.html), or in Python, depending upon how you describe your public API, this means that methods and data can be hidden from the user. What this means is that if you need or want to change code, you can do whatever you want to the implementation of the code, but keep the public APIs the same.
* [Inheritance](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)):
  Inheritance allows you to define data and procedure in one place (in one class), and then override or extend that functionality later. For instance, in Python, I often see people creating subclasses of the `dict` class in order to add additional functionality. A common change is overriding the method that throws an exception when a key is requested from a dictionary that doesn't exist to give a default value based on an unknown key. This allows you to extend your own code now or later, allow others to extend your code, and allows you to extend other people's code.
* Reusability: All of these reasons and others allow for greater reusability of code. Object oriented code allows you to write solid (tested) code once, and then reuse over and over. If you need to tweak something for your specific use case, you can inherit from an existing class and overwrite the existing behavior. If you need to change something, you can change it all while maintaining the existing public method signatures, and no one is the wiser (hopefully).

Again, there are several reasons not to use OOP, and you don't need to. But luckily with a language like Python, you can use just a little bit or a lot, it's up to you.

An example of the student use case (no guarantee on code quality, just an example):

Object Oriented

```
class Student(object):
    def __init__(self, name, age, gender, level, grades=None):
        self.name = name
        self.age = age
        self.gender = gender
        self.level = level
        self.grades = grades or {}

    def setGrade(self, course, grade):
        self.grades[course] = grade

    def getGrade(self, course):
        return self.grades[course]

    def getGPA(self):
        return sum(self.grades.values())/len(self.grades)

# Define some students
john = Student(""John"", 12, ""male"", 6, {""math"":3.3})
jane = Student(""Jane"", 12, ""female"", 6, {""math"":3.5})

# Now we can get to the grades easily
print(john.getGPA())
print(jane.getGPA())

```

Standard Dict

```
def calculateGPA(gradeDict):
    return sum(gradeDict.values())/len(gradeDict)

students = {}
# We can set the keys to variables so we might minimize typos
name, age, gender, level, grades = ""name"", ""age"", ""gender"", ""level"", ""grades""
john, jane = ""john"", ""jane""
math = ""math""
students[john] = {}
students[john][age] = 12
students[john][gender] = ""male""
students[john][level] = 6
students[john][grades] = {math:3.3}

students[jane] = {}
students[jane][age] = 12
students[jane][gender] = ""female""
students[jane][level] = 6
students[jane][grades] = {math:3.5}

# At this point, we need to remember who the students are and where the grades are stored. Not a huge deal, but avoided by OOP.
print(calculateGPA(students[john][grades]))
print(calculateGPA(students[jane][grades]))

```"
"I have a Python (2.7) app which is started in my dockerfile:

```
CMD [""python"",""main.py""]

```

*main.py* prints some strings when it is started and goes into a loop afterwards:

```
print ""App started""
while True:
    time.sleep(1)

```

As long as I start the container with the -it flag, everything works as expected:

```
$ docker run --name=myapp -it myappimage
> App started

```

And I can see the same output via logs later:

```
$ docker logs myapp
> App started

```

If I try to run the same container with the -d flag, the container seems to start normally, but I can't see any output:

```
$ docker run --name=myapp -d myappimage
> b82db1120fee5f92c80000f30f6bdc84e068bafa32738ab7adb47e641b19b4d1
$ docker logs myapp
$ (empty)

```

But the container still seems to run;

```
$ docker ps
Container Status ...
myapp     up 4 minutes ... 

```

Attach does not display anything either:

```
$ docker attach --sig-proxy=false myapp
(working, no output)

```

Any ideas whats going wrong? Does ""print"" behave differently when ran in background?

Docker version:

```
Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.2
Git commit (client): a8a31ef
OS/Arch (client): linux/arm
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.2
Git commit (server): a8a31ef

```","Finally I found a solution to see Python output when running daemonized in Docker, thanks to @ahmetalpbalkan over at [GitHub](https://github.com/docker/docker/issues/12447#issuecomment-94417192). Answering it here myself for further reference :

Using unbuffered output with

```
CMD [""python"",""-u"",""main.py""]

```

instead of

```
CMD [""python"",""main.py""]

```

solves the problem; you can see the output now (both, stderr and stdout) via

```
docker logs myapp

```

---

why `-u` [ref](https://github.com/docker/docker/issues/12447#issuecomment-94417192)

```
- print is indeed buffered and docker logs will eventually give you that output, just after enough of it will have piled up
- executing the same script with python -u gives instant output as said above
- import logging + logging.warning(""text"") gives the expected result even without -u

```

what it means by `python -u` ref. > python --help | grep -- -u

```
-u     : force the stdout and stderr streams to be unbuffered;

```"
"I've spent entirely too long researching how to get two subplots to share the same y-axis with a single colorbar shared between the two in Matplotlib.

What was happening was that when I called the `colorbar()` function in either `subplot1` or `subplot2`, it would autoscale the plot such that the colorbar plus the plot would fit inside the 'subplot' bounding box, causing the two side-by-side plots to be two very different sizes.

To get around this, I tried to create a third subplot which I then hacked to render no plot with just a colorbar present.
The only problem is, now the heights and widths of the two plots are uneven, and I can't figure out how to make it look okay.

Here is my code:

```
from __future__ import division
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import patches
from matplotlib.ticker import NullFormatter

# SIS Functions
TE = 1 # Einstein radius
g1 = lambda x,y: (TE/2) * (y**2-x**2)/((x**2+y**2)**(3/2)) 
g2 = lambda x,y: -1*TE*x*y / ((x**2+y**2)**(3/2))
kappa = lambda x,y: TE / (2*np.sqrt(x**2+y**2))

coords = np.linspace(-2,2,400)
X,Y = np.meshgrid(coords,coords)
g1out = g1(X,Y)
g2out = g2(X,Y)
kappaout = kappa(X,Y)
for i in range(len(coords)):
    for j in range(len(coords)):
        if np.sqrt(coords[i]**2+coords[j]**2) <= TE:
            g1out[i][j]=0
            g2out[i][j]=0

fig = plt.figure()
fig.subplots_adjust(wspace=0,hspace=0)

# subplot number 1
ax1 = fig.add_subplot(1,2,1,aspect='equal',xlim=[-2,2],ylim=[-2,2])
plt.title(r""$\gamma_{1}$"",fontsize=""18"")
plt.xlabel(r""x ($\theta_{E}$)"",fontsize=""15"")
plt.ylabel(r""y ($\theta_{E}$)"",rotation='horizontal',fontsize=""15"")
plt.xticks([-2.0,-1.5,-1.0,-0.5,0,0.5,1.0,1.5])
plt.xticks([-2.0,-1.5,-1.0,-0.5,0,0.5,1.0,1.5])
plt.imshow(g1out,extent=(-2,2,-2,2))
plt.axhline(y=0,linewidth=2,color='k',linestyle=""--"")
plt.axvline(x=0,linewidth=2,color='k',linestyle=""--"")
e1 = patches.Ellipse((0,0),2,2,color='white')
ax1.add_patch(e1)

# subplot number 2
ax2 = fig.add_subplot(1,2,2,sharey=ax1,xlim=[-2,2],ylim=[-2,2])
plt.title(r""$\gamma_{2}$"",fontsize=""18"")
plt.xlabel(r""x ($\theta_{E}$)"",fontsize=""15"")
ax2.yaxis.set_major_formatter( NullFormatter() )
plt.axhline(y=0,linewidth=2,color='k',linestyle=""--"")
plt.axvline(x=0,linewidth=2,color='k',linestyle=""--"")
plt.imshow(g2out,extent=(-2,2,-2,2))
e2 = patches.Ellipse((0,0),2,2,color='white')
ax2.add_patch(e2)

# subplot for colorbar
ax3 = fig.add_subplot(1,1,1)
ax3.axis('off')
cbar = plt.colorbar(ax=ax2)

plt.show()

```","Just place the colorbar in its own axis and use `subplots_adjust` to make room for it.

As a quick example:

```
import numpy as np
import matplotlib.pyplot as plt

fig, axes = plt.subplots(nrows=2, ncols=2)
for ax in axes.flat:
    im = ax.imshow(np.random.random((10,10)), vmin=0, vmax=1)

fig.subplots_adjust(right=0.8)
cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])
fig.colorbar(im, cax=cbar_ax)

plt.show()

```

![enter image description here](https://i.sstatic.net/vsugg.png)

Note that the color range will be set by the last image plotted (that gave rise to `im`) even if the range of values is set by `vmin` and `vmax`. If another plot has, for example, a higher max value, points with higher values than the max of `im` will show in uniform color."
"This `for` loop iterates over all elements in a list:

```
for item in my_list:
    print item

```

Is there a way to know within the loop how many times I've been looping so far? For instance, I want to take a list and after I've processed ten elements I want to do something with them.

The alternatives I thought about would be something like:

```
count=0
for item in my_list:
    print item
    count +=1
    if count % 10 == 0:
        print 'did ten'

```

Or:

```
for count in range(0,len(my_list)):
    print my_list[count]
    if count % 10 == 0:
        print 'did ten'

```

Is there a better way (just like the `for item in my_list`) to get the number of iterations so far?","The pythonic way is to use [`enumerate`](http://docs.python.org/library/functions.html#enumerate):

```
for idx, item in enumerate(my_list):

```"
"What is the python keyword ""with"" used for?

Example from: <http://docs.python.org/tutorial/inputoutput.html>

```
>>> with open('/tmp/workfile', 'r') as f:
...     read_data = f.read()
>>> f.closed
True

```","In python the `with` keyword is used when working with unmanaged resources (like file streams). It is similar to the `using` statement in VB.NET and C#. It allows you to ensure that a resource is ""cleaned up"" when the code that uses it finishes running, even if exceptions are thrown. It provides 'syntactic sugar' for `try/finally` blocks.

From [Python Docs](http://docs.python.org/whatsnew/2.6.html#pep-343-the-with-statement):

> The `with` statement clarifies code that previously would use `try...finally` blocks to ensure that clean-up code is executed.
>
> The `with` statement is a control-flow structure whose basic structure is:
>
> ```
> with expression [as variable]:
>     with-block
>
> ```
>
> The expression is evaluated, and it should result in an object that supports the context management protocol (that is, has `__enter__()` and `__exit__()` methods).

*Update* fixed VB callout per Scott Wisniewski's comment. I was indeed confusing `with` with `using`."
"As the title says, how can I find the current operating system in python?","If you want user readable data but still detailed, you can use [platform.platform()](http://docs.python.org/library/platform.html#platform.platform)

```
>>> import platform
>>> platform.platform()
'Linux-3.3.0-8.fc16.x86_64-x86_64-with-fedora-16-Verne'

```

`platform` also has some other useful methods:

```
>>> platform.system()
'Windows'
>>> platform.release()
'XP'
>>> platform.version()
'5.1.2600'

```

Here's a few different possible calls you can make to identify where you are, linux\_distribution and dist seem to have gone from recent python versions, so they have a wrapper function here.

```
import platform
import sys

def linux_distribution():
  try:
    return platform.linux_distribution()
  except:
    return ""N/A""

def dist():
  try:
    return platform.dist()
  except:
    return ""N/A""

print(""""""Python version: %s
dist: %s
linux_distribution: %s
system: %s
machine: %s
platform: %s
uname: %s
version: %s
mac_ver: %s
"""""" % (
sys.version.split('\n'),
str(dist()),
linux_distribution(),
platform.system(),
platform.machine(),
platform.platform(),
platform.uname(),
platform.version(),
platform.mac_ver(),
))

```

The outputs of this script ran on a few different systems (Linux, Windows, Solaris, MacOS) and architectures (x86, x64, Itanium, power pc, sparc) is available here: <https://github.com/hpcugent/easybuild/wiki/OS_flavor_name_version>

e.g. Solaris on sparc gave:

```
Python version: ['2.6.4 (r264:75706, Aug  4 2010, 16:53:32) [C]']
dist: ('', '', '')
linux_distribution: ('', '', '')
system: SunOS
machine: sun4u
platform: SunOS-5.9-sun4u-sparc-32bit-ELF
uname: ('SunOS', 'xxx', '5.9', 'Generic_122300-60', 'sun4u', 'sparc')
version: Generic_122300-60
mac_ver: ('', ('', '', ''), '')

```

or MacOS on M1

```
Python version: ['2.7.16 (default, Dec 21 2020, 23:00:36) ', '[GCC Apple LLVM 12.0.0 (clang-1200.0.30.4) [+internal-os, ptrauth-isa=sign+stri'] 
dist: ('', '', '') 
linux_distribution: ('', '', '') 
system: Darwin 
machine: arm64 
platform: Darwin-20.3.0-arm64-arm-64bit 
uname: ('Darwin', 'Nautilus.local', '20.3.0', 'Darwin Kernel Version 20.3.0: Thu Jan 21 00:06:51 PST 2021; root:xnu-7195.81.3~1/RELEASE_ARM64_T8101', 'arm64', 'arm') 
version: Darwin Kernel Version 20.3.0: Thu Jan 21 00:06:51 PST 2021; root:xnu-7195.81.3~1/RELEASE_ARM64_T8101 
mac_ver: ('10.16', ('', '', ''), 'arm64')

```"
"So I recently stumbled upon this great library for handling HTTP requests in Python; found here <http://docs.python-requests.org/en/latest/index.html>.

I love working with it, but I can't figure out how to add headers to my get requests. Help?","According to the [API](https://requests.readthedocs.io/en/latest/api/), the headers can all be passed in with `requests.get()`:

```
import requests
r=requests.get(""http://www.example.com/"", headers={""Content-Type"":""text""})

```"
"Ruby example:

```
name = ""Spongebob Squarepants""
puts ""Who lives in a Pineapple under the sea? \n#{name}.""

```

The successful Python string concatenation is seemingly verbose to me.","Python 3.6 will add [literal string interpolation](https://www.python.org/dev/peps/pep-0498/) similar to Ruby's string interpolation. Starting with that version of Python (which is scheduled to be released by the end of 2016), you will be able to include expressions in ""f-strings"", e.g.

```
name = ""Spongebob Squarepants""
print(f""Who lives in a Pineapple under the sea? {name}."")

```

Prior to 3.6, the closest you can get to this is

```
name = ""Spongebob Squarepants""
print(""Who lives in a Pineapple under the sea? %(name)s."" % locals())

```

The `%` operator can be used for [string interpolation](http://docs.python.org/library/stdtypes.html#string-formatting-operations) in Python. The first operand is the string to be interpolated, the second can have different types including a ""mapping"", mapping field names to the values to be interpolated. Here I used the dictionary of local variables `locals()` to map the field name `name` to its value as a local variable.

The same code using the `.format()` method of recent Python versions would look like this:

```
name = ""Spongebob Squarepants""
print(""Who lives in a Pineapple under the sea? {name!s}."".format(**locals()))

```

There is also the [`string.Template`](http://docs.python.org/library/string.html#template-strings) class:

```
tmpl = string.Template(""Who lives in a Pineapple under the sea? $name."")
print(tmpl.substitute(name=""Spongebob Squarepants""))

```"
"I was wondering how to check whether a variable is a class (not an instance!) or not.

I've tried to use the function `isinstance(object, class_or_type_or_tuple)` to do this, but I don't know what type a class would have.

For example, in the following code

```
class Foo:
    pass

isinstance(Foo, **???**) # i want to make this return True.

```

I tried to substitute ""`class`"" with **???**, but I realized that `class` is a keyword in python.","Even better: use the [`inspect.isclass`](https://docs.python.org/library/inspect.html#inspect.isclass) function.

```
>>> import inspect
>>> class X(object):
...     pass
... 
>>> inspect.isclass(X)
True

>>> x = X()
>>> isinstance(x, X)
True
>>> inspect.isclass(x)
False

```"
"I have a time difference

```
import time
import datetime

time1 = datetime.datetime.fromtimestamp(time.mktime(time.gmtime()))
...
time2 = datetime.datetime.fromtimestamp(time.mktime(time.gmtime()))
diff = time2 - time1

```

Now, how do I find the total number of seconds that passed? `diff.seconds` doesn't count days. I could do:

```
diff.seconds + diff.days * 24 * 3600

```

Is there a built-in method for this?","Use [`timedelta.total_seconds()`](http://docs.python.org/library/datetime.html#datetime.timedelta.total_seconds).

```
>>> import datetime
>>> datetime.timedelta(seconds=24*60*60).total_seconds()
86400.0

```"
"I have a class `MyClass`, which contains two member variables `foo` and `bar`:

```
class MyClass:
    def __init__(self, foo, bar):
        self.foo = foo
        self.bar = bar

```

I have two instances of this class, each of which has identical values for `foo` and `bar`:

```
x = MyClass('foo', 'bar')
y = MyClass('foo', 'bar')

```

However, when I compare them for equality, Python returns `False`:

```
>>> x == y
False

```

How can I make Python consider these two objects equal?","You should implement the method [`__eq__`](https://docs.python.org/3/reference/datamodel.html#object.__eq__):

```
class MyClass:
    def __init__(self, foo, bar):
        self.foo = foo
        self.bar = bar
        
    def __eq__(self, other): 
        if not isinstance(other, MyClass):
            # don't attempt to compare against unrelated types
            return NotImplemented

        return self.foo == other.foo and self.bar == other.bar

```

Now it outputs:

```
>>> x == y
True

```

Note that implementing `__eq__` will automatically make instances of your class unhashable, which means they can't be stored in sets and dicts. If you're not modelling an immutable type (i.e. if the attributes `foo` and `bar` may change the value within the lifetime of your object), then it's recommended to just leave your instances as unhashable.

If you are modelling an immutable type, you should also implement the data model hook [`__hash__`](https://docs.python.org/3/reference/datamodel.html#object.__hash__):

```
class MyClass:
    ...

    def __hash__(self):
        # necessary for instances to behave sanely in dicts and sets.
        return hash((self.foo, self.bar))

```

A general solution, like the idea of looping through `__dict__` and comparing values, is not advisable - it can never be truly general because the `__dict__` may have uncomparable or unhashable types contained within.

N.B.: be aware that before Python 3, you may need to use [`__cmp__`](https://portingguide.readthedocs.io/en/latest/comparisons.html#rich-comparisons) instead of `__eq__`. Python 2 users may also want to implement [`__ne__`](https://docs.python.org/2/reference/datamodel.html#object.__ne__), since a sensible default behaviour for inequality (i.e. inverting the equality result) will not be automatically created in Python 2."
"I'm looking for a way to test whether or not a given string repeats itself for the entire string or not.

Examples:

```
[
    '0045662100456621004566210045662100456621',             # '00456621'
    '0072992700729927007299270072992700729927',             # '00729927'
    '001443001443001443001443001443001443001443',           # '001443'
    '037037037037037037037037037037037037037037037',        # '037'
    '047619047619047619047619047619047619047619',           # '047619'
    '002457002457002457002457002457002457002457',           # '002457'
    '001221001221001221001221001221001221001221',           # '001221'
    '001230012300123001230012300123001230012300123',        # '00123'
    '0013947001394700139470013947001394700139470013947',    # '0013947'
    '001001001001001001001001001001001001001001001001001',  # '001'
    '001406469760900140646976090014064697609',              # '0014064697609'
]

```

are strings which repeat themselves, and

```
[
    '004608294930875576036866359447',
    '00469483568075117370892018779342723',
    '004739336492890995260663507109',
    '001508295625942684766214177978883861236802413273',
    '007518796992481203',
    '0071942446043165467625899280575539568345323741',
    '0434782608695652173913',
    '0344827586206896551724137931',
    '002481389578163771712158808933',
    '002932551319648093841642228739',
    '0035587188612099644128113879',
    '003484320557491289198606271777',
    '00115074798619102416570771',
]

```

are examples of ones that do not.

The repeating sections of the strings I'm given can be quite long, and the strings themselves can be 500 or more characters, so looping through each character trying to build a pattern then checking the pattern vs the rest of the string seems awful slow. Multiply that by potentially hundreds of strings and I can't see any intuitive solution.

I've looked into regexes a bit and they seem good for when you know what you're looking for, or at least the length of the pattern you're looking for. Unfortunately, I know neither.

How can I tell if a string is repeating itself and if it is, what the shortest repeating subsequence is?","Here's a concise solution which avoids regular expressions and slow in-Python loops:

```
def principal_period(s):
    i = (s+s).find(s, 1, -1)
    return None if i == -1 else s[:i]

```

See the [Community Wiki answer](https://stackoverflow.com/a/29482936/2447250) started by @davidism for benchmark results. In summary,

> David Zhang's solution is the clear winner, outperforming all others by at least 5x for the large example set.

(That answer's words, not mine.)

This is based on the observation that a string is periodic if and only if it is equal to a nontrivial rotation of itself. Kudos to @AleksiTorhamo for realizing that we can then recover the principal period from the index of the first occurrence of `s` in `(s+s)[1:-1]`, and for informing me of the optional `start` and `end` arguments of Python's `string.find`."
"I have a script which reads a text file, pulls decimal numbers out of it as strings and places them into a list.

So I have this list:

```
my_list = ['0.49', '0.54', '0.54', '0.55', '0.55', '0.54', '0.55', '0.55', '0.54']

```

How do I convert each of the values in the list from a string to a float?

I have tried:

```
for item in my_list:
    float(item)

```

But this doesn't seem to work for me.","```
[float(i) for i in lst]

```

to be precise, it creates a new list with float values. Unlike the `map` approach it will work in py3k."
"I have a multi-line string that I want to do an operation on each line, like so:

```
inputString = """"""Line 1
Line 2
Line 3""""""

```

I want to iterate on each line:

```
for line in inputString:
    doStuff()

```","```
inputString.splitlines()

```

Will give you a list with each item, the [`splitlines()`](https://docs.python.org/3/library/stdtypes.html#str.splitlines) method is designed to split each line into a list element."
"How can I remove some specific elements from a numpy array? Say I have

```
import numpy as np

a = np.array([1,2,3,4,5,6,7,8,9])

```

I then want to remove `3,4,7` from `a`. All I know is the index of the values (`index=[2,3,6]`).","Use [`numpy.delete()`](https://numpy.org/doc/stable/reference/generated/numpy.delete.html), which returns a *new* array with sub-arrays along an axis deleted.

```
numpy.delete(a, index)

```

For your specific question:

```
import numpy as np

a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
index = [2, 3, 6]

new_a = np.delete(a, index)

print(new_a)
# Output: [1, 2, 5, 6, 8, 9]

```

Note that `numpy.delete()` returns a new array since [array scalars](https://numpy.org/doc/stable/reference/arrays.scalars.html) are immutable, similar to strings in Python, so each time a change is made to it, a new object is created. I.e., to quote the `delete()` [docs](https://numpy.org/doc/stable/reference/generated/numpy.delete.html):

> ""A *copy* of arr with the elements specified by obj removed. *Note that
> delete does not occur in-place*...""

If the code I post has output, it is the result of running the code."
"How do I create a list of alphabet characters, without doing it manually like this?

```
['a', 'b', 'c', 'd', ..., 'z']

```","```
>>> import string
>>> string.ascii_lowercase
'abcdefghijklmnopqrstuvwxyz'
>>> list(string.ascii_lowercase)
['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

```

---

Alternatively, using `range`:

```
>>> list(map(chr, range(97, 123)))
['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

```

Or equivalently:

```
>>> list(map(chr, range(ord('a'), ord('z')+1)))
['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

```

---

Other helpful `string` module features:

```
>>> help(string)
....
DATA
    ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
    ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'
    ascii_uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    digits = '0123456789'
    hexdigits = '0123456789abcdefABCDEF'
    octdigits = '01234567'
    printable = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!""#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n\r\x0b\x0c'
    punctuation = '!""#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    whitespace = ' \t\n\r\x0b\x0c'

```"
"I can't figure out what's wrong with my code:

```
for key in tmpDict:
    print type(tmpDict[key])
    time.sleep(1)
    if(type(tmpDict[key])==list):
        print 'this is never visible'
        break

```

the output is `<type 'list'>` but the if statement never triggers. Can anyone spot my error here?","You should try using `isinstance()`

```
if isinstance(object, list):
       ## DO what you want

```

In your case

```
if isinstance(tmpDict[key], list):
      ## DO SOMETHING

```

To elaborate:

```
x = [1,2,3]
if type(x) == list():
    print ""This wont work""
if type(x) == list:                  ## one of the way to see if it's list
    print ""this will work""           
if type(x) == type(list()):
    print ""lets see if this works""
if isinstance(x, list):              ## most preferred way to check if it's list
    print ""This should work just fine""

```

The difference between `isinstance()` and `type()` though both seems to do the same job is that `isinstance()` checks for subclasses in addition, while `type()` doesnâ€™t."
What is the name of the method to override the `[]` operator (subscript notation) for a class in Python?,"You need to use the [`__getitem__` method](https://docs.python.org/reference/datamodel.html#object.__getitem__).

```
class MyClass:
    def __getitem__(self, key):
        return key * 2

myobj = MyClass()
myobj[3] #Output: 6

```

And if you're going to be setting values you'll need to implement the [`__setitem__` method](https://docs.python.org/reference/datamodel.html#object.__setitem__) too, otherwise this will happen:

```
>>> myobj[5] = 1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: MyClass instance has no attribute '__setitem__'

```"
"I have the following `soup`:

```
<a href=""some_url"">next</a>
<span class=""class"">...</span>

```

From this I want to extract the href, `""some_url""`

I can do it if I only have one tag, but here there are two tags. I can also get the text `'next'` but that's not what I want.

Also, is there a good description of the API somewhere with examples. I'm using [the standard documentation](http://www.crummy.com/software/BeautifulSoup/documentation.html), but I'm looking for something a little more organized.","You can use `find_all` in the following way to find every `a` element that has an `href` attribute, and print each one:

```
# Python2
from BeautifulSoup import BeautifulSoup
    
html = '''<a href=""some_url"">next</a>
<span class=""class""><a href=""another_url"">later</a></span>'''
    
soup = BeautifulSoup(html)
    
for a in soup.find_all('a', href=True):
    print ""Found the URL:"", a['href']

# The output would be:
# Found the URL: some_url
# Found the URL: another_url

```

```
# Python3
from bs4 import BeautifulSoup

html = '''<a href=""https://some_url.com"">next</a>
<span class=""class"">
<a href=""https://some_other_url.com"">another_url</a></span>'''

soup = BeautifulSoup(html)

for a in soup.find_all('a', href=True):
    print(""Found the URL:"", a['href'])

# The output would be:
# Found the URL: https://some_url.com
# Found the URL: https://some_other_url.com

```

Note that if you're using an older version of BeautifulSoup (before version 4) the name of this method is `findAll`. In version 4, BeautifulSoup's method names [were changed to be PEP 8 compliant](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#method-names), so you should use `find_all` instead.

---

If you want *all* tags with an `href`, you can omit the `name` parameter:

```
href_tags = soup.find_all(href=True)

```"
"When I run the following command:

```
sudo pip install python-ldap

```

I get this error:

> In file included from Modules/LDAPObject.c:9:
>
> Modules/errors.h:8: fatal error: lber.h: No such file or directory

How can I fix this?","The [python-ldap](http://www.python-ldap.org/) is based on OpenLDAP, so you need to have the development files (headers) in order to compile the Python module. If you're on Ubuntu, the package is called `libldap2-dev`.

**[Debian](https://en.wikipedia.org/wiki/Debian)/[Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_%28operating_system%29)**:

```
sudo apt-get install libsasl2-dev python-dev-is-python3 libldap2-dev libssl-dev

```

**[Red Hat](https://en.wikipedia.org/wiki/Red_Hat_Linux)/[CentOS](https://en.wikipedia.org/wiki/CentOS)**:

```
sudo yum install python-devel openldap-devel

```"
"By debugging information I mean what TensorFlow shows in my terminal about loaded libraries and found devices etc. not Python errors.

```
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Graphics Device
major: 5 minor: 2 memoryClockRate (GHz) 1.0885
pciBusID 0000:04:00.0
Total memory: 12.00GiB
Free memory: 11.83GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Graphics Device, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
...

```","You can disable all debugging logs using `os.environ` :

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf

```

Tested on tf 0.12 and 1.0

In details,

```
0 = all messages are logged (default behavior)
1 = INFO messages are not printed
2 = INFO and WARNING messages are not printed
3 = INFO, WARNING, and ERROR messages are not printed

```"
"I am trying to save plots I make using matplotlib; however, the images are saving blank.

Here is my code:

```
plt.subplot(121)
plt.imshow(dataStack, cmap=mpl.cm.bone)

plt.subplot(122)
y = copy.deepcopy(tumorStack)
y = np.ma.masked_where(y == 0, y)

plt.imshow(dataStack, cmap=mpl.cm.bone)
plt.imshow(y, cmap=mpl.cm.jet_r, interpolation='nearest')

if T0 is not None:
    plt.subplot(123)
    plt.imshow(T0, cmap=mpl.cm.bone)

    #plt.subplot(124)
    #Autozoom

#else:
    #plt.subplot(124)
    #Autozoom

plt.show()
plt.draw()
plt.savefig('tessstttyyy.png', dpi=100)

```

And tessstttyyy.png is blank (also tried with .jpg)","First, what happens when `T0 is not None`? I would test that, then I would adjust the values I pass to `plt.subplot()`; maybe try values 131, 132, and 133, or values that depend whether or not `T0` exists.

Second, after `plt.show()` is called, a new figure is created. To deal with this, you can

1. Call `plt.savefig('tessstttyyy.png', dpi=100)` before you call `plt.show()`
2. Save the figure before you `show()` by calling `plt.gcf()` for ""get current figure"", then you can call `savefig()` on this `Figure` object at any time.

For example:

```
fig1 = plt.gcf()
plt.show()
plt.draw()
fig1.savefig('tessstttyyy.png', dpi=100)

```

In your code, 'tesssttyyy.png' is blank because it is saving the new figure, to which nothing has been plotted."
How do I execute a Python file with the debugger and specify the working directory for the run?,"[@SpeedCoder5's comment](https://stackoverflow.com/questions/38623138/vscode-how-to-set-working-directory-for-debugging-a-python-program#comment93079003_38637243) deserves to be an answer.

In `launch.json`, specify a dynamic working directory (i.e. the directory where the currently-open Python file is located) using:

```
""cwd"": ""${fileDirname}""

```

This takes advantage of the [""variables/variable substitution"" feature in VS Code](https://code.visualstudio.com/docs/editor/variables-reference#_settings-command-variables-and-input-variables), and the predefined variable `fileDirname`. **Note** these variables are case sensitive. Don't use capital-F `FileDirname`, and be careful of typo's, which might be [the problem with answers like these](https://stackoverflow.com/a/76994961/1175496).

Note as comments say, you *might* also need to add the [`purpose` option](https://code.visualstudio.com/docs/python/debugging#_purpose):

```
""purpose"": [""debug-in-terminal""]

```

""Purpose"" might be required if using the play button on the top-right of the window, vs `F5` or ""Run and Debug"" in the sidebar.

If you're using the `Python: Current File (Integrated Terminal)` option when you run Python, your `launch.json` file might look like mine, below ([more info on `launch.json` files here](https://code.visualstudio.com/docs/editor/debugging#_launch-configurations)).

```
{
    ""version"": ""0.2.0"",
    ""configurations"": [
    {
            ""name"": ""Python: Current File (Integrated Terminal)"",
            ""type"": ""python"",
            ""request"": ""launch"",
            ""program"": ""${file}"",
            ""console"": ""integratedTerminal"",
            ""cwd"": ""${fileDirname}"",
            ""purpose"":[""debug-in-terminal""]
    }, 

    //... other settings, but I modified the ""Current File"" setting above ...
}

```

[The `launch.json` file controls the run/debug settings of your Visual Studio code project](https://code.visualstudio.com/docs/editor/debugging#_launch-versus-attach-configurations); my `launch.json` file was auto-generated by VS Code, in the directory of my current ""Open Project"". I just edited the file manually to add `""cwd"": ""${fileDirname}""` as shown above.

Note the `launch.json` file may be specific to your project, or specific to your directory, so confirm you're editing the *correct* `launch.json` ([see comment](https://stackoverflow.com/questions/38623138/how-to-set-the-working-directory-for-debugging-a-python-program-in-vs-code/55072246#comment114115218_55072246))

If you don't have a `launch.json` file, [try this](https://code.visualstudio.com/docs/editor/debugging#_launch-configurations):

> To create a launch.json file, open your project folder in VS Code (File > Open Folder) and then select the Configure gear icon on the Debug view top bar.

Per @kbro's comment, you might be prompted to create a `launch.json` file by clicking the Debug button itself:

> When I **clicked on the Debug button on my navigation panel** it said ""To customise Run and Debug create a launch.json file."" **Clicking on ""create...""** opened a dialog asking **what language I was debugging**. In my case I selected Python"
"The Python 2 documentation says:

> ### [Built-in Functions: `map(function, iterable, ...)`](https://docs.python.org/2.7/library/functions.html#map)
>
> Apply function to every item of iterable and return a list of the
> results. If additional iterable arguments are passed, function must
> take that many arguments and is applied to the items from all
> iterables in parallel.
>
> If one iterable is shorter than another it is assumed to be extended
> with None items.
>
> If function is `None`, the identity function is assumed; if there are
> multiple arguments, `map()` returns a list consisting of tuples
> containing the corresponding items from all iterables (a kind of
> transpose operation).
>
> The iterable arguments may be a sequence or any iterable object; the
> result is always a list.

What role does this play in making a Cartesian product?

```
content = map(tuple, array)

```

What effect does putting a tuple anywhere in there have? I also noticed that without the map function the output is `abc` and with it, it's `a, b, c`.

I want to fully understand this function. The reference definitions is also hard to understand. Too much fancy fluff.","`map` isn't particularly pythonic. I would recommend using list comprehensions instead:

```
map(f, iterable)

```

is basically equivalent to:

```
[f(x) for x in iterable]

```

`map` on its own can't do a Cartesian product, because the length of its output list is always the same as its input list. You can trivially do a Cartesian product with a list comprehension though:

```
[(a, b) for a in iterable_a for b in iterable_b]

```

The syntax is a little confusing -- that's basically equivalent to:

```
result = []
for a in iterable_a:
    for b in iterable_b:
        result.append((a, b))

```"
"Given a dictionary `{ k1: v1, k2: v2 ... }` I want to get `{ k1: f(v1), k2: f(v2) ... }` provided I pass a function `f`.

Is there any such built in function? Or do I have to do

```
dict([(k, f(v)) for (k, v) in my_dictionary.iteritems()])

```

Ideally I would just write

```
my_dictionary.map_values(f)

```

or

```
my_dictionary.mutate_values_with(f)

```

That is, it doesn't matter to me if the original dictionary is mutated or a copy is created.","There is no such function; the easiest way to do this is to use a dict comprehension:

```
my_dictionary = {k: f(v) for k, v in my_dictionary.items()}

```

Note that there is no such method on lists either; you'd have to use a list comprehension or the `map()` function.

As such, you could use the `map()` function for processing your dict as well:

```
my_dictionary = dict(map(lambda kv: (kv[0], f(kv[1])), my_dictionary.items()))

```

but that's not that readable, really.

(Note that if you're still using Python 2.7, you should use the `.iteritems()` method instead of `.items()` to save memory. Also, the dict comprehension syntax wasn't introduced until Python 2.7.)"
"Just opened a file with Sublime Text (with Sublime Linter) and noticed a PEP8 formatting error that I'd never seen before. Here's the text:

```
urlpatterns = patterns('',
    url(r'^$', listing, name='investment-listing'),
)

```

It's flagging the second argument, the line that starts `url(...)`

I was about to disable this check in ST2 but ***I'd like to know what I'm doing wrong*** before I ignore it. You never know, if it seems important I might even change my ways :)","[PEP-8 recommends](http://www.python.org/dev/peps/pep-0008/#indentation) you indent lines to the opening parentheses if you put anything on the first line, so it should either be indenting to the opening bracket:

```
urlpatterns = patterns('',
                       url(r'^$', listing, name='investment-listing'))

```

or not putting any arguments on the starting line, then indenting to a uniform level:

```
urlpatterns = patterns(
    '',
    url(r'^$', listing, name='investment-listing'),
)

urlpatterns = patterns(
    '', url(r'^$', listing, name='investment-listing'))

```

I suggest taking a read through PEP-8 - you can skim through a lot of it, and it's pretty easy to understand, unlike some of the more technical PEPs."
